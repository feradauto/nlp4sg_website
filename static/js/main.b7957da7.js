/*! For license information please see main.b7957da7.js.LICENSE.txt */
!function(){var e={463:function(e,t,a){"use strict";var i=a(791),n=a(296);function s(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,a=1;a<arguments.length;a++)t+="&args[]="+encodeURIComponent(arguments[a]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}var o=new Set,r={};function c(e,t){l(e,t),l(e+"Capture",t)}function l(e,t){for(r[e]=t,e=0;e<t.length;e++)o.add(t[e])}var d=!("undefined"===typeof window||"undefined"===typeof window.document||"undefined"===typeof window.document.createElement),h=Object.prototype.hasOwnProperty,u=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,m={},p={};function f(e,t,a,i,n,s,o){this.acceptsBooleans=2===t||3===t||4===t,this.attributeName=i,this.attributeNamespace=n,this.mustUseProperty=a,this.propertyName=e,this.type=t,this.sanitizeURL=s,this.removeEmptyString=o}var g={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach((function(e){g[e]=new f(e,0,!1,e,null,!1,!1)})),[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach((function(e){var t=e[0];g[t]=new f(t,1,!1,e[1],null,!1,!1)})),["contentEditable","draggable","spellCheck","value"].forEach((function(e){g[e]=new f(e,2,!1,e.toLowerCase(),null,!1,!1)})),["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach((function(e){g[e]=new f(e,2,!1,e,null,!1,!1)})),"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture disableRemotePlayback formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach((function(e){g[e]=new f(e,3,!1,e.toLowerCase(),null,!1,!1)})),["checked","multiple","muted","selected"].forEach((function(e){g[e]=new f(e,3,!0,e,null,!1,!1)})),["capture","download"].forEach((function(e){g[e]=new f(e,4,!1,e,null,!1,!1)})),["cols","rows","size","span"].forEach((function(e){g[e]=new f(e,6,!1,e,null,!1,!1)})),["rowSpan","start"].forEach((function(e){g[e]=new f(e,5,!1,e.toLowerCase(),null,!1,!1)}));var w=/[\-:]([a-z])/g;function b(e){return e[1].toUpperCase()}function y(e,t,a,i){var n=g.hasOwnProperty(t)?g[t]:null;(null!==n?0!==n.type:i||!(2<t.length)||"o"!==t[0]&&"O"!==t[0]||"n"!==t[1]&&"N"!==t[1])&&(function(e,t,a,i){if(null===t||"undefined"===typeof t||function(e,t,a,i){if(null!==a&&0===a.type)return!1;switch(typeof t){case"function":case"symbol":return!0;case"boolean":return!i&&(null!==a?!a.acceptsBooleans:"data-"!==(e=e.toLowerCase().slice(0,5))&&"aria-"!==e);default:return!1}}(e,t,a,i))return!0;if(i)return!1;if(null!==a)switch(a.type){case 3:return!t;case 4:return!1===t;case 5:return isNaN(t);case 6:return isNaN(t)||1>t}return!1}(t,a,n,i)&&(a=null),i||null===n?function(e){return!!h.call(p,e)||!h.call(m,e)&&(u.test(e)?p[e]=!0:(m[e]=!0,!1))}(t)&&(null===a?e.removeAttribute(t):e.setAttribute(t,""+a)):n.mustUseProperty?e[n.propertyName]=null===a?3!==n.type&&"":a:(t=n.attributeName,i=n.attributeNamespace,null===a?e.removeAttribute(t):(a=3===(n=n.type)||4===n&&!0===a?"":""+a,i?e.setAttributeNS(i,t,a):e.setAttribute(t,a))))}"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach((function(e){var t=e.replace(w,b);g[t]=new f(t,1,!1,e,null,!1,!1)})),"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach((function(e){var t=e.replace(w,b);g[t]=new f(t,1,!1,e,"http://www.w3.org/1999/xlink",!1,!1)})),["xml:base","xml:lang","xml:space"].forEach((function(e){var t=e.replace(w,b);g[t]=new f(t,1,!1,e,"http://www.w3.org/XML/1998/namespace",!1,!1)})),["tabIndex","crossOrigin"].forEach((function(e){g[e]=new f(e,1,!1,e.toLowerCase(),null,!1,!1)})),g.xlinkHref=new f("xlinkHref",1,!1,"xlink:href","http://www.w3.org/1999/xlink",!0,!1),["src","href","action","formAction"].forEach((function(e){g[e]=new f(e,1,!1,e.toLowerCase(),null,!0,!0)}));var v=i.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED,k=Symbol.for("react.element"),x=Symbol.for("react.portal"),T=Symbol.for("react.fragment"),A=Symbol.for("react.strict_mode"),M=Symbol.for("react.profiler"),C=Symbol.for("react.provider"),q=Symbol.for("react.context"),I=Symbol.for("react.forward_ref"),W=Symbol.for("react.suspense"),S=Symbol.for("react.suspense_list"),z=Symbol.for("react.memo"),E=Symbol.for("react.lazy");Symbol.for("react.scope"),Symbol.for("react.debug_trace_mode");var P=Symbol.for("react.offscreen");Symbol.for("react.legacy_hidden"),Symbol.for("react.cache"),Symbol.for("react.tracing_marker");var N=Symbol.iterator;function D(e){return null===e||"object"!==typeof e?null:"function"===typeof(e=N&&e[N]||e["@@iterator"])?e:null}var L,R=Object.assign;function O(e){if(void 0===L)try{throw Error()}catch(a){var t=a.stack.trim().match(/\n( *(at )?)/);L=t&&t[1]||""}return"\n"+L+e}var j=!1;function F(e,t){if(!e||j)return"";j=!0;var a=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{if(t)if(t=function(){throw Error()},Object.defineProperty(t.prototype,"props",{set:function(){throw Error()}}),"object"===typeof Reflect&&Reflect.construct){try{Reflect.construct(t,[])}catch(l){var i=l}Reflect.construct(e,[],t)}else{try{t.call()}catch(l){i=l}e.call(t.prototype)}else{try{throw Error()}catch(l){i=l}e()}}catch(l){if(l&&i&&"string"===typeof l.stack){for(var n=l.stack.split("\n"),s=i.stack.split("\n"),o=n.length-1,r=s.length-1;1<=o&&0<=r&&n[o]!==s[r];)r--;for(;1<=o&&0<=r;o--,r--)if(n[o]!==s[r]){if(1!==o||1!==r)do{if(o--,0>--r||n[o]!==s[r]){var c="\n"+n[o].replace(" at new "," at ");return e.displayName&&c.includes("<anonymous>")&&(c=c.replace("<anonymous>",e.displayName)),c}}while(1<=o&&0<=r);break}}}finally{j=!1,Error.prepareStackTrace=a}return(e=e?e.displayName||e.name:"")?O(e):""}function B(e){switch(e.tag){case 5:return O(e.type);case 16:return O("Lazy");case 13:return O("Suspense");case 19:return O("SuspenseList");case 0:case 2:case 15:return e=F(e.type,!1);case 11:return e=F(e.type.render,!1);case 1:return e=F(e.type,!0);default:return""}}function H(e){if(null==e)return null;if("function"===typeof e)return e.displayName||e.name||null;if("string"===typeof e)return e;switch(e){case T:return"Fragment";case x:return"Portal";case M:return"Profiler";case A:return"StrictMode";case W:return"Suspense";case S:return"SuspenseList"}if("object"===typeof e)switch(e.$$typeof){case q:return(e.displayName||"Context")+".Consumer";case C:return(e._context.displayName||"Context")+".Provider";case I:var t=e.render;return(e=e.displayName)||(e=""!==(e=t.displayName||t.name||"")?"ForwardRef("+e+")":"ForwardRef"),e;case z:return null!==(t=e.displayName||null)?t:H(e.type)||"Memo";case E:t=e._payload,e=e._init;try{return H(e(t))}catch(a){}}return null}function U(e){var t=e.type;switch(e.tag){case 24:return"Cache";case 9:return(t.displayName||"Context")+".Consumer";case 10:return(t._context.displayName||"Context")+".Provider";case 18:return"DehydratedFragment";case 11:return e=(e=t.render).displayName||e.name||"",t.displayName||(""!==e?"ForwardRef("+e+")":"ForwardRef");case 7:return"Fragment";case 5:return t;case 4:return"Portal";case 3:return"Root";case 6:return"Text";case 16:return H(t);case 8:return t===A?"StrictMode":"Mode";case 22:return"Offscreen";case 12:return"Profiler";case 21:return"Scope";case 13:return"Suspense";case 19:return"SuspenseList";case 25:return"TracingMarker";case 1:case 0:case 17:case 2:case 14:case 15:if("function"===typeof t)return t.displayName||t.name||null;if("string"===typeof t)return t}return null}function G(e){switch(typeof e){case"boolean":case"number":case"string":case"undefined":case"object":return e;default:return""}}function V(e){var t=e.type;return(e=e.nodeName)&&"input"===e.toLowerCase()&&("checkbox"===t||"radio"===t)}function _(e){e._valueTracker||(e._valueTracker=function(e){var t=V(e)?"checked":"value",a=Object.getOwnPropertyDescriptor(e.constructor.prototype,t),i=""+e[t];if(!e.hasOwnProperty(t)&&"undefined"!==typeof a&&"function"===typeof a.get&&"function"===typeof a.set){var n=a.get,s=a.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return n.call(this)},set:function(e){i=""+e,s.call(this,e)}}),Object.defineProperty(e,t,{enumerable:a.enumerable}),{getValue:function(){return i},setValue:function(e){i=""+e},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}(e))}function Q(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var a=t.getValue(),i="";return e&&(i=V(e)?e.checked?"true":"false":e.value),(e=i)!==a&&(t.setValue(e),!0)}function K(e){if("undefined"===typeof(e=e||("undefined"!==typeof document?document:void 0)))return null;try{return e.activeElement||e.body}catch(t){return e.body}}function Y(e,t){var a=t.checked;return R({},t,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=a?a:e._wrapperState.initialChecked})}function $(e,t){var a=null==t.defaultValue?"":t.defaultValue,i=null!=t.checked?t.checked:t.defaultChecked;a=G(null!=t.value?t.value:a),e._wrapperState={initialChecked:i,initialValue:a,controlled:"checkbox"===t.type||"radio"===t.type?null!=t.checked:null!=t.value}}function X(e,t){null!=(t=t.checked)&&y(e,"checked",t,!1)}function J(e,t){X(e,t);var a=G(t.value),i=t.type;if(null!=a)"number"===i?(0===a&&""===e.value||e.value!=a)&&(e.value=""+a):e.value!==""+a&&(e.value=""+a);else if("submit"===i||"reset"===i)return void e.removeAttribute("value");t.hasOwnProperty("value")?ee(e,t.type,a):t.hasOwnProperty("defaultValue")&&ee(e,t.type,G(t.defaultValue)),null==t.checked&&null!=t.defaultChecked&&(e.defaultChecked=!!t.defaultChecked)}function Z(e,t,a){if(t.hasOwnProperty("value")||t.hasOwnProperty("defaultValue")){var i=t.type;if(!("submit"!==i&&"reset"!==i||void 0!==t.value&&null!==t.value))return;t=""+e._wrapperState.initialValue,a||t===e.value||(e.value=t),e.defaultValue=t}""!==(a=e.name)&&(e.name=""),e.defaultChecked=!!e._wrapperState.initialChecked,""!==a&&(e.name=a)}function ee(e,t,a){"number"===t&&K(e.ownerDocument)===e||(null==a?e.defaultValue=""+e._wrapperState.initialValue:e.defaultValue!==""+a&&(e.defaultValue=""+a))}var te=Array.isArray;function ae(e,t,a,i){if(e=e.options,t){t={};for(var n=0;n<a.length;n++)t["$"+a[n]]=!0;for(a=0;a<e.length;a++)n=t.hasOwnProperty("$"+e[a].value),e[a].selected!==n&&(e[a].selected=n),n&&i&&(e[a].defaultSelected=!0)}else{for(a=""+G(a),t=null,n=0;n<e.length;n++){if(e[n].value===a)return e[n].selected=!0,void(i&&(e[n].defaultSelected=!0));null!==t||e[n].disabled||(t=e[n])}null!==t&&(t.selected=!0)}}function ie(e,t){if(null!=t.dangerouslySetInnerHTML)throw Error(s(91));return R({},t,{value:void 0,defaultValue:void 0,children:""+e._wrapperState.initialValue})}function ne(e,t){var a=t.value;if(null==a){if(a=t.children,t=t.defaultValue,null!=a){if(null!=t)throw Error(s(92));if(te(a)){if(1<a.length)throw Error(s(93));a=a[0]}t=a}null==t&&(t=""),a=t}e._wrapperState={initialValue:G(a)}}function se(e,t){var a=G(t.value),i=G(t.defaultValue);null!=a&&((a=""+a)!==e.value&&(e.value=a),null==t.defaultValue&&e.defaultValue!==a&&(e.defaultValue=a)),null!=i&&(e.defaultValue=""+i)}function oe(e){var t=e.textContent;t===e._wrapperState.initialValue&&""!==t&&null!==t&&(e.value=t)}function re(e){switch(e){case"svg":return"http://www.w3.org/2000/svg";case"math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function ce(e,t){return null==e||"http://www.w3.org/1999/xhtml"===e?re(t):"http://www.w3.org/2000/svg"===e&&"foreignObject"===t?"http://www.w3.org/1999/xhtml":e}var le,de,he=(de=function(e,t){if("http://www.w3.org/2000/svg"!==e.namespaceURI||"innerHTML"in e)e.innerHTML=t;else{for((le=le||document.createElement("div")).innerHTML="<svg>"+t.valueOf().toString()+"</svg>",t=le.firstChild;e.firstChild;)e.removeChild(e.firstChild);for(;t.firstChild;)e.appendChild(t.firstChild)}},"undefined"!==typeof MSApp&&MSApp.execUnsafeLocalFunction?function(e,t,a,i){MSApp.execUnsafeLocalFunction((function(){return de(e,t)}))}:de);function ue(e,t){if(t){var a=e.firstChild;if(a&&a===e.lastChild&&3===a.nodeType)return void(a.nodeValue=t)}e.textContent=t}var me={animationIterationCount:!0,aspectRatio:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},pe=["Webkit","ms","Moz","O"];function fe(e,t,a){return null==t||"boolean"===typeof t||""===t?"":a||"number"!==typeof t||0===t||me.hasOwnProperty(e)&&me[e]?(""+t).trim():t+"px"}function ge(e,t){for(var a in e=e.style,t)if(t.hasOwnProperty(a)){var i=0===a.indexOf("--"),n=fe(a,t[a],i);"float"===a&&(a="cssFloat"),i?e.setProperty(a,n):e[a]=n}}Object.keys(me).forEach((function(e){pe.forEach((function(t){t=t+e.charAt(0).toUpperCase()+e.substring(1),me[t]=me[e]}))}));var we=R({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function be(e,t){if(t){if(we[e]&&(null!=t.children||null!=t.dangerouslySetInnerHTML))throw Error(s(137,e));if(null!=t.dangerouslySetInnerHTML){if(null!=t.children)throw Error(s(60));if("object"!==typeof t.dangerouslySetInnerHTML||!("__html"in t.dangerouslySetInnerHTML))throw Error(s(61))}if(null!=t.style&&"object"!==typeof t.style)throw Error(s(62))}}function ye(e,t){if(-1===e.indexOf("-"))return"string"===typeof t.is;switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var ve=null;function ke(e){return(e=e.target||e.srcElement||window).correspondingUseElement&&(e=e.correspondingUseElement),3===e.nodeType?e.parentNode:e}var xe=null,Te=null,Ae=null;function Me(e){if(e=gn(e)){if("function"!==typeof xe)throw Error(s(280));var t=e.stateNode;t&&(t=bn(t),xe(e.stateNode,e.type,t))}}function Ce(e){Te?Ae?Ae.push(e):Ae=[e]:Te=e}function qe(){if(Te){var e=Te,t=Ae;if(Ae=Te=null,Me(e),t)for(e=0;e<t.length;e++)Me(t[e])}}function Ie(e,t){return e(t)}function We(){}var Se=!1;function ze(e,t,a){if(Se)return e(t,a);Se=!0;try{return Ie(e,t,a)}finally{Se=!1,(null!==Te||null!==Ae)&&(We(),qe())}}function Ee(e,t){var a=e.stateNode;if(null===a)return null;var i=bn(a);if(null===i)return null;a=i[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(i=!i.disabled)||(i=!("button"===(e=e.type)||"input"===e||"select"===e||"textarea"===e)),e=!i;break e;default:e=!1}if(e)return null;if(a&&"function"!==typeof a)throw Error(s(231,t,typeof a));return a}var Pe=!1;if(d)try{var Ne={};Object.defineProperty(Ne,"passive",{get:function(){Pe=!0}}),window.addEventListener("test",Ne,Ne),window.removeEventListener("test",Ne,Ne)}catch(de){Pe=!1}function De(e,t,a,i,n,s,o,r,c){var l=Array.prototype.slice.call(arguments,3);try{t.apply(a,l)}catch(d){this.onError(d)}}var Le=!1,Re=null,Oe=!1,je=null,Fe={onError:function(e){Le=!0,Re=e}};function Be(e,t,a,i,n,s,o,r,c){Le=!1,Re=null,De.apply(Fe,arguments)}function He(e){var t=e,a=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do{0!==(4098&(t=e).flags)&&(a=t.return),e=t.return}while(e)}return 3===t.tag?a:null}function Ue(e){if(13===e.tag){var t=e.memoizedState;if(null===t&&(null!==(e=e.alternate)&&(t=e.memoizedState)),null!==t)return t.dehydrated}return null}function Ge(e){if(He(e)!==e)throw Error(s(188))}function Ve(e){return null!==(e=function(e){var t=e.alternate;if(!t){if(null===(t=He(e)))throw Error(s(188));return t!==e?null:e}for(var a=e,i=t;;){var n=a.return;if(null===n)break;var o=n.alternate;if(null===o){if(null!==(i=n.return)){a=i;continue}break}if(n.child===o.child){for(o=n.child;o;){if(o===a)return Ge(n),e;if(o===i)return Ge(n),t;o=o.sibling}throw Error(s(188))}if(a.return!==i.return)a=n,i=o;else{for(var r=!1,c=n.child;c;){if(c===a){r=!0,a=n,i=o;break}if(c===i){r=!0,i=n,a=o;break}c=c.sibling}if(!r){for(c=o.child;c;){if(c===a){r=!0,a=o,i=n;break}if(c===i){r=!0,i=o,a=n;break}c=c.sibling}if(!r)throw Error(s(189))}}if(a.alternate!==i)throw Error(s(190))}if(3!==a.tag)throw Error(s(188));return a.stateNode.current===a?e:t}(e))?_e(e):null}function _e(e){if(5===e.tag||6===e.tag)return e;for(e=e.child;null!==e;){var t=_e(e);if(null!==t)return t;e=e.sibling}return null}var Qe=n.unstable_scheduleCallback,Ke=n.unstable_cancelCallback,Ye=n.unstable_shouldYield,$e=n.unstable_requestPaint,Xe=n.unstable_now,Je=n.unstable_getCurrentPriorityLevel,Ze=n.unstable_ImmediatePriority,et=n.unstable_UserBlockingPriority,tt=n.unstable_NormalPriority,at=n.unstable_LowPriority,it=n.unstable_IdlePriority,nt=null,st=null;var ot=Math.clz32?Math.clz32:function(e){return 0===(e>>>=0)?32:31-(rt(e)/ct|0)|0},rt=Math.log,ct=Math.LN2;var lt=64,dt=4194304;function ht(e){switch(e&-e){case 1:return 1;case 2:return 2;case 4:return 4;case 8:return 8;case 16:return 16;case 32:return 32;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return 4194240&e;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return 130023424&e;case 134217728:return 134217728;case 268435456:return 268435456;case 536870912:return 536870912;case 1073741824:return 1073741824;default:return e}}function ut(e,t){var a=e.pendingLanes;if(0===a)return 0;var i=0,n=e.suspendedLanes,s=e.pingedLanes,o=268435455&a;if(0!==o){var r=o&~n;0!==r?i=ht(r):0!==(s&=o)&&(i=ht(s))}else 0!==(o=a&~n)?i=ht(o):0!==s&&(i=ht(s));if(0===i)return 0;if(0!==t&&t!==i&&0===(t&n)&&((n=i&-i)>=(s=t&-t)||16===n&&0!==(4194240&s)))return t;if(0!==(4&i)&&(i|=16&a),0!==(t=e.entangledLanes))for(e=e.entanglements,t&=i;0<t;)n=1<<(a=31-ot(t)),i|=e[a],t&=~n;return i}function mt(e,t){switch(e){case 1:case 2:case 4:return t+250;case 8:case 16:case 32:case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return t+5e3;default:return-1}}function pt(e){return 0!==(e=-1073741825&e.pendingLanes)?e:1073741824&e?1073741824:0}function ft(e){for(var t=[],a=0;31>a;a++)t.push(e);return t}function gt(e,t,a){e.pendingLanes|=t,536870912!==t&&(e.suspendedLanes=0,e.pingedLanes=0),(e=e.eventTimes)[t=31-ot(t)]=a}function wt(e,t){var a=e.entangledLanes|=t;for(e=e.entanglements;a;){var i=31-ot(a),n=1<<i;n&t|e[i]&t&&(e[i]|=t),a&=~n}}var bt=0;function yt(e){return 1<(e&=-e)?4<e?0!==(268435455&e)?16:536870912:4:1}var vt,kt,xt,Tt,At,Mt=!1,Ct=[],qt=null,It=null,Wt=null,St=new Map,zt=new Map,Et=[],Pt="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset submit".split(" ");function Nt(e,t){switch(e){case"focusin":case"focusout":qt=null;break;case"dragenter":case"dragleave":It=null;break;case"mouseover":case"mouseout":Wt=null;break;case"pointerover":case"pointerout":St.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":zt.delete(t.pointerId)}}function Dt(e,t,a,i,n,s){return null===e||e.nativeEvent!==s?(e={blockedOn:t,domEventName:a,eventSystemFlags:i,nativeEvent:s,targetContainers:[n]},null!==t&&(null!==(t=gn(t))&&kt(t)),e):(e.eventSystemFlags|=i,t=e.targetContainers,null!==n&&-1===t.indexOf(n)&&t.push(n),e)}function Lt(e){var t=fn(e.target);if(null!==t){var a=He(t);if(null!==a)if(13===(t=a.tag)){if(null!==(t=Ue(a)))return e.blockedOn=t,void At(e.priority,(function(){xt(a)}))}else if(3===t&&a.stateNode.current.memoizedState.isDehydrated)return void(e.blockedOn=3===a.tag?a.stateNode.containerInfo:null)}e.blockedOn=null}function Rt(e){if(null!==e.blockedOn)return!1;for(var t=e.targetContainers;0<t.length;){var a=Qt(e.domEventName,e.eventSystemFlags,t[0],e.nativeEvent);if(null!==a)return null!==(t=gn(a))&&kt(t),e.blockedOn=a,!1;var i=new(a=e.nativeEvent).constructor(a.type,a);ve=i,a.target.dispatchEvent(i),ve=null,t.shift()}return!0}function Ot(e,t,a){Rt(e)&&a.delete(t)}function jt(){Mt=!1,null!==qt&&Rt(qt)&&(qt=null),null!==It&&Rt(It)&&(It=null),null!==Wt&&Rt(Wt)&&(Wt=null),St.forEach(Ot),zt.forEach(Ot)}function Ft(e,t){e.blockedOn===t&&(e.blockedOn=null,Mt||(Mt=!0,n.unstable_scheduleCallback(n.unstable_NormalPriority,jt)))}function Bt(e){function t(t){return Ft(t,e)}if(0<Ct.length){Ft(Ct[0],e);for(var a=1;a<Ct.length;a++){var i=Ct[a];i.blockedOn===e&&(i.blockedOn=null)}}for(null!==qt&&Ft(qt,e),null!==It&&Ft(It,e),null!==Wt&&Ft(Wt,e),St.forEach(t),zt.forEach(t),a=0;a<Et.length;a++)(i=Et[a]).blockedOn===e&&(i.blockedOn=null);for(;0<Et.length&&null===(a=Et[0]).blockedOn;)Lt(a),null===a.blockedOn&&Et.shift()}var Ht=v.ReactCurrentBatchConfig;function Ut(e,t,a,i){var n=bt,s=Ht.transition;Ht.transition=null;try{bt=1,Vt(e,t,a,i)}finally{bt=n,Ht.transition=s}}function Gt(e,t,a,i){var n=bt,s=Ht.transition;Ht.transition=null;try{bt=4,Vt(e,t,a,i)}finally{bt=n,Ht.transition=s}}function Vt(e,t,a,i){var n=Qt(e,t,a,i);if(null===n)Bi(e,t,i,_t,a),Nt(e,i);else if(function(e,t,a,i,n){switch(t){case"focusin":return qt=Dt(qt,e,t,a,i,n),!0;case"dragenter":return It=Dt(It,e,t,a,i,n),!0;case"mouseover":return Wt=Dt(Wt,e,t,a,i,n),!0;case"pointerover":var s=n.pointerId;return St.set(s,Dt(St.get(s)||null,e,t,a,i,n)),!0;case"gotpointercapture":return s=n.pointerId,zt.set(s,Dt(zt.get(s)||null,e,t,a,i,n)),!0}return!1}(n,e,t,a,i))i.stopPropagation();else if(Nt(e,i),4&t&&-1<Pt.indexOf(e)){for(;null!==n;){var s=gn(n);if(null!==s&&vt(s),null===(s=Qt(e,t,a,i))&&Bi(e,t,i,_t,a),s===n)break;n=s}null!==n&&i.stopPropagation()}else Bi(e,t,i,null,a)}var _t=null;function Qt(e,t,a,i){if(_t=null,null!==(e=fn(e=ke(i))))if(null===(t=He(e)))e=null;else if(13===(a=t.tag)){if(null!==(e=Ue(t)))return e;e=null}else if(3===a){if(t.stateNode.current.memoizedState.isDehydrated)return 3===t.tag?t.stateNode.containerInfo:null;e=null}else t!==e&&(e=null);return _t=e,null}function Kt(e){switch(e){case"cancel":case"click":case"close":case"contextmenu":case"copy":case"cut":case"auxclick":case"dblclick":case"dragend":case"dragstart":case"drop":case"focusin":case"focusout":case"input":case"invalid":case"keydown":case"keypress":case"keyup":case"mousedown":case"mouseup":case"paste":case"pause":case"play":case"pointercancel":case"pointerdown":case"pointerup":case"ratechange":case"reset":case"resize":case"seeked":case"submit":case"touchcancel":case"touchend":case"touchstart":case"volumechange":case"change":case"selectionchange":case"textInput":case"compositionstart":case"compositionend":case"compositionupdate":case"beforeblur":case"afterblur":case"beforeinput":case"blur":case"fullscreenchange":case"focus":case"hashchange":case"popstate":case"select":case"selectstart":return 1;case"drag":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"mousemove":case"mouseout":case"mouseover":case"pointermove":case"pointerout":case"pointerover":case"scroll":case"toggle":case"touchmove":case"wheel":case"mouseenter":case"mouseleave":case"pointerenter":case"pointerleave":return 4;case"message":switch(Je()){case Ze:return 1;case et:return 4;case tt:case at:return 16;case it:return 536870912;default:return 16}default:return 16}}var Yt=null,$t=null,Xt=null;function Jt(){if(Xt)return Xt;var e,t,a=$t,i=a.length,n="value"in Yt?Yt.value:Yt.textContent,s=n.length;for(e=0;e<i&&a[e]===n[e];e++);var o=i-e;for(t=1;t<=o&&a[i-t]===n[s-t];t++);return Xt=n.slice(e,1<t?1-t:void 0)}function Zt(e){var t=e.keyCode;return"charCode"in e?0===(e=e.charCode)&&13===t&&(e=13):e=t,10===e&&(e=13),32<=e||13===e?e:0}function ea(){return!0}function ta(){return!1}function aa(e){function t(t,a,i,n,s){for(var o in this._reactName=t,this._targetInst=i,this.type=a,this.nativeEvent=n,this.target=s,this.currentTarget=null,e)e.hasOwnProperty(o)&&(t=e[o],this[o]=t?t(n):n[o]);return this.isDefaultPrevented=(null!=n.defaultPrevented?n.defaultPrevented:!1===n.returnValue)?ea:ta,this.isPropagationStopped=ta,this}return R(t.prototype,{preventDefault:function(){this.defaultPrevented=!0;var e=this.nativeEvent;e&&(e.preventDefault?e.preventDefault():"unknown"!==typeof e.returnValue&&(e.returnValue=!1),this.isDefaultPrevented=ea)},stopPropagation:function(){var e=this.nativeEvent;e&&(e.stopPropagation?e.stopPropagation():"unknown"!==typeof e.cancelBubble&&(e.cancelBubble=!0),this.isPropagationStopped=ea)},persist:function(){},isPersistent:ea}),t}var ia,na,sa,oa={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},ra=aa(oa),ca=R({},oa,{view:0,detail:0}),la=aa(ca),da=R({},ca,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:xa,button:0,buttons:0,relatedTarget:function(e){return void 0===e.relatedTarget?e.fromElement===e.srcElement?e.toElement:e.fromElement:e.relatedTarget},movementX:function(e){return"movementX"in e?e.movementX:(e!==sa&&(sa&&"mousemove"===e.type?(ia=e.screenX-sa.screenX,na=e.screenY-sa.screenY):na=ia=0,sa=e),ia)},movementY:function(e){return"movementY"in e?e.movementY:na}}),ha=aa(da),ua=aa(R({},da,{dataTransfer:0})),ma=aa(R({},ca,{relatedTarget:0})),pa=aa(R({},oa,{animationName:0,elapsedTime:0,pseudoElement:0})),fa=R({},oa,{clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),ga=aa(fa),wa=aa(R({},oa,{data:0})),ba={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},ya={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},va={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function ka(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):!!(e=va[e])&&!!t[e]}function xa(){return ka}var Ta=R({},ca,{key:function(e){if(e.key){var t=ba[e.key]||e.key;if("Unidentified"!==t)return t}return"keypress"===e.type?13===(e=Zt(e))?"Enter":String.fromCharCode(e):"keydown"===e.type||"keyup"===e.type?ya[e.keyCode]||"Unidentified":""},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:xa,charCode:function(e){return"keypress"===e.type?Zt(e):0},keyCode:function(e){return"keydown"===e.type||"keyup"===e.type?e.keyCode:0},which:function(e){return"keypress"===e.type?Zt(e):"keydown"===e.type||"keyup"===e.type?e.keyCode:0}}),Aa=aa(Ta),Ma=aa(R({},da,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0})),Ca=aa(R({},ca,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:xa})),qa=aa(R({},oa,{propertyName:0,elapsedTime:0,pseudoElement:0})),Ia=R({},da,{deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:0,deltaMode:0}),Wa=aa(Ia),Sa=[9,13,27,32],za=d&&"CompositionEvent"in window,Ea=null;d&&"documentMode"in document&&(Ea=document.documentMode);var Pa=d&&"TextEvent"in window&&!Ea,Na=d&&(!za||Ea&&8<Ea&&11>=Ea),Da=String.fromCharCode(32),La=!1;function Ra(e,t){switch(e){case"keyup":return-1!==Sa.indexOf(t.keyCode);case"keydown":return 229!==t.keyCode;case"keypress":case"mousedown":case"focusout":return!0;default:return!1}}function Oa(e){return"object"===typeof(e=e.detail)&&"data"in e?e.data:null}var ja=!1;var Fa={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function Ba(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return"input"===t?!!Fa[e.type]:"textarea"===t}function Ha(e,t,a,i){Ce(i),0<(t=Ui(t,"onChange")).length&&(a=new ra("onChange","change",null,a,i),e.push({event:a,listeners:t}))}var Ua=null,Ga=null;function Va(e){Di(e,0)}function _a(e){if(Q(wn(e)))return e}function Qa(e,t){if("change"===e)return t}var Ka=!1;if(d){var Ya;if(d){var $a="oninput"in document;if(!$a){var Xa=document.createElement("div");Xa.setAttribute("oninput","return;"),$a="function"===typeof Xa.oninput}Ya=$a}else Ya=!1;Ka=Ya&&(!document.documentMode||9<document.documentMode)}function Ja(){Ua&&(Ua.detachEvent("onpropertychange",Za),Ga=Ua=null)}function Za(e){if("value"===e.propertyName&&_a(Ga)){var t=[];Ha(t,Ga,e,ke(e)),ze(Va,t)}}function ei(e,t,a){"focusin"===e?(Ja(),Ga=a,(Ua=t).attachEvent("onpropertychange",Za)):"focusout"===e&&Ja()}function ti(e){if("selectionchange"===e||"keyup"===e||"keydown"===e)return _a(Ga)}function ai(e,t){if("click"===e)return _a(t)}function ii(e,t){if("input"===e||"change"===e)return _a(t)}var ni="function"===typeof Object.is?Object.is:function(e,t){return e===t&&(0!==e||1/e===1/t)||e!==e&&t!==t};function si(e,t){if(ni(e,t))return!0;if("object"!==typeof e||null===e||"object"!==typeof t||null===t)return!1;var a=Object.keys(e),i=Object.keys(t);if(a.length!==i.length)return!1;for(i=0;i<a.length;i++){var n=a[i];if(!h.call(t,n)||!ni(e[n],t[n]))return!1}return!0}function oi(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function ri(e,t){var a,i=oi(e);for(e=0;i;){if(3===i.nodeType){if(a=e+i.textContent.length,e<=t&&a>=t)return{node:i,offset:t-e};e=a}e:{for(;i;){if(i.nextSibling){i=i.nextSibling;break e}i=i.parentNode}i=void 0}i=oi(i)}}function ci(e,t){return!(!e||!t)&&(e===t||(!e||3!==e.nodeType)&&(t&&3===t.nodeType?ci(e,t.parentNode):"contains"in e?e.contains(t):!!e.compareDocumentPosition&&!!(16&e.compareDocumentPosition(t))))}function li(){for(var e=window,t=K();t instanceof e.HTMLIFrameElement;){try{var a="string"===typeof t.contentWindow.location.href}catch(i){a=!1}if(!a)break;t=K((e=t.contentWindow).document)}return t}function di(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&("input"===t&&("text"===e.type||"search"===e.type||"tel"===e.type||"url"===e.type||"password"===e.type)||"textarea"===t||"true"===e.contentEditable)}function hi(e){var t=li(),a=e.focusedElem,i=e.selectionRange;if(t!==a&&a&&a.ownerDocument&&ci(a.ownerDocument.documentElement,a)){if(null!==i&&di(a))if(t=i.start,void 0===(e=i.end)&&(e=t),"selectionStart"in a)a.selectionStart=t,a.selectionEnd=Math.min(e,a.value.length);else if((e=(t=a.ownerDocument||document)&&t.defaultView||window).getSelection){e=e.getSelection();var n=a.textContent.length,s=Math.min(i.start,n);i=void 0===i.end?s:Math.min(i.end,n),!e.extend&&s>i&&(n=i,i=s,s=n),n=ri(a,s);var o=ri(a,i);n&&o&&(1!==e.rangeCount||e.anchorNode!==n.node||e.anchorOffset!==n.offset||e.focusNode!==o.node||e.focusOffset!==o.offset)&&((t=t.createRange()).setStart(n.node,n.offset),e.removeAllRanges(),s>i?(e.addRange(t),e.extend(o.node,o.offset)):(t.setEnd(o.node,o.offset),e.addRange(t)))}for(t=[],e=a;e=e.parentNode;)1===e.nodeType&&t.push({element:e,left:e.scrollLeft,top:e.scrollTop});for("function"===typeof a.focus&&a.focus(),a=0;a<t.length;a++)(e=t[a]).element.scrollLeft=e.left,e.element.scrollTop=e.top}}var ui=d&&"documentMode"in document&&11>=document.documentMode,mi=null,pi=null,fi=null,gi=!1;function wi(e,t,a){var i=a.window===a?a.document:9===a.nodeType?a:a.ownerDocument;gi||null==mi||mi!==K(i)||("selectionStart"in(i=mi)&&di(i)?i={start:i.selectionStart,end:i.selectionEnd}:i={anchorNode:(i=(i.ownerDocument&&i.ownerDocument.defaultView||window).getSelection()).anchorNode,anchorOffset:i.anchorOffset,focusNode:i.focusNode,focusOffset:i.focusOffset},fi&&si(fi,i)||(fi=i,0<(i=Ui(pi,"onSelect")).length&&(t=new ra("onSelect","select",null,t,a),e.push({event:t,listeners:i}),t.target=mi)))}function bi(e,t){var a={};return a[e.toLowerCase()]=t.toLowerCase(),a["Webkit"+e]="webkit"+t,a["Moz"+e]="moz"+t,a}var yi={animationend:bi("Animation","AnimationEnd"),animationiteration:bi("Animation","AnimationIteration"),animationstart:bi("Animation","AnimationStart"),transitionend:bi("Transition","TransitionEnd")},vi={},ki={};function xi(e){if(vi[e])return vi[e];if(!yi[e])return e;var t,a=yi[e];for(t in a)if(a.hasOwnProperty(t)&&t in ki)return vi[e]=a[t];return e}d&&(ki=document.createElement("div").style,"AnimationEvent"in window||(delete yi.animationend.animation,delete yi.animationiteration.animation,delete yi.animationstart.animation),"TransitionEvent"in window||delete yi.transitionend.transition);var Ti=xi("animationend"),Ai=xi("animationiteration"),Mi=xi("animationstart"),Ci=xi("transitionend"),qi=new Map,Ii="abort auxClick cancel canPlay canPlayThrough click close contextMenu copy cut drag dragEnd dragEnter dragExit dragLeave dragOver dragStart drop durationChange emptied encrypted ended error gotPointerCapture input invalid keyDown keyPress keyUp load loadedData loadedMetadata loadStart lostPointerCapture mouseDown mouseMove mouseOut mouseOver mouseUp paste pause play playing pointerCancel pointerDown pointerMove pointerOut pointerOver pointerUp progress rateChange reset resize seeked seeking stalled submit suspend timeUpdate touchCancel touchEnd touchStart volumeChange scroll toggle touchMove waiting wheel".split(" ");function Wi(e,t){qi.set(e,t),c(t,[e])}for(var Si=0;Si<Ii.length;Si++){var zi=Ii[Si];Wi(zi.toLowerCase(),"on"+(zi[0].toUpperCase()+zi.slice(1)))}Wi(Ti,"onAnimationEnd"),Wi(Ai,"onAnimationIteration"),Wi(Mi,"onAnimationStart"),Wi("dblclick","onDoubleClick"),Wi("focusin","onFocus"),Wi("focusout","onBlur"),Wi(Ci,"onTransitionEnd"),l("onMouseEnter",["mouseout","mouseover"]),l("onMouseLeave",["mouseout","mouseover"]),l("onPointerEnter",["pointerout","pointerover"]),l("onPointerLeave",["pointerout","pointerover"]),c("onChange","change click focusin focusout input keydown keyup selectionchange".split(" ")),c("onSelect","focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange".split(" ")),c("onBeforeInput",["compositionend","keypress","textInput","paste"]),c("onCompositionEnd","compositionend focusout keydown keypress keyup mousedown".split(" ")),c("onCompositionStart","compositionstart focusout keydown keypress keyup mousedown".split(" ")),c("onCompositionUpdate","compositionupdate focusout keydown keypress keyup mousedown".split(" "));var Ei="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange resize seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Pi=new Set("cancel close invalid load scroll toggle".split(" ").concat(Ei));function Ni(e,t,a){var i=e.type||"unknown-event";e.currentTarget=a,function(e,t,a,i,n,o,r,c,l){if(Be.apply(this,arguments),Le){if(!Le)throw Error(s(198));var d=Re;Le=!1,Re=null,Oe||(Oe=!0,je=d)}}(i,t,void 0,e),e.currentTarget=null}function Di(e,t){t=0!==(4&t);for(var a=0;a<e.length;a++){var i=e[a],n=i.event;i=i.listeners;e:{var s=void 0;if(t)for(var o=i.length-1;0<=o;o--){var r=i[o],c=r.instance,l=r.currentTarget;if(r=r.listener,c!==s&&n.isPropagationStopped())break e;Ni(n,r,l),s=c}else for(o=0;o<i.length;o++){if(c=(r=i[o]).instance,l=r.currentTarget,r=r.listener,c!==s&&n.isPropagationStopped())break e;Ni(n,r,l),s=c}}}if(Oe)throw e=je,Oe=!1,je=null,e}function Li(e,t){var a=t[un];void 0===a&&(a=t[un]=new Set);var i=e+"__bubble";a.has(i)||(Fi(t,e,2,!1),a.add(i))}function Ri(e,t,a){var i=0;t&&(i|=4),Fi(a,e,i,t)}var Oi="_reactListening"+Math.random().toString(36).slice(2);function ji(e){if(!e[Oi]){e[Oi]=!0,o.forEach((function(t){"selectionchange"!==t&&(Pi.has(t)||Ri(t,!1,e),Ri(t,!0,e))}));var t=9===e.nodeType?e:e.ownerDocument;null===t||t[Oi]||(t[Oi]=!0,Ri("selectionchange",!1,t))}}function Fi(e,t,a,i){switch(Kt(t)){case 1:var n=Ut;break;case 4:n=Gt;break;default:n=Vt}a=n.bind(null,t,a,e),n=void 0,!Pe||"touchstart"!==t&&"touchmove"!==t&&"wheel"!==t||(n=!0),i?void 0!==n?e.addEventListener(t,a,{capture:!0,passive:n}):e.addEventListener(t,a,!0):void 0!==n?e.addEventListener(t,a,{passive:n}):e.addEventListener(t,a,!1)}function Bi(e,t,a,i,n){var s=i;if(0===(1&t)&&0===(2&t)&&null!==i)e:for(;;){if(null===i)return;var o=i.tag;if(3===o||4===o){var r=i.stateNode.containerInfo;if(r===n||8===r.nodeType&&r.parentNode===n)break;if(4===o)for(o=i.return;null!==o;){var c=o.tag;if((3===c||4===c)&&((c=o.stateNode.containerInfo)===n||8===c.nodeType&&c.parentNode===n))return;o=o.return}for(;null!==r;){if(null===(o=fn(r)))return;if(5===(c=o.tag)||6===c){i=s=o;continue e}r=r.parentNode}}i=i.return}ze((function(){var i=s,n=ke(a),o=[];e:{var r=qi.get(e);if(void 0!==r){var c=ra,l=e;switch(e){case"keypress":if(0===Zt(a))break e;case"keydown":case"keyup":c=Aa;break;case"focusin":l="focus",c=ma;break;case"focusout":l="blur",c=ma;break;case"beforeblur":case"afterblur":c=ma;break;case"click":if(2===a.button)break e;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":c=ha;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":c=ua;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":c=Ca;break;case Ti:case Ai:case Mi:c=pa;break;case Ci:c=qa;break;case"scroll":c=la;break;case"wheel":c=Wa;break;case"copy":case"cut":case"paste":c=ga;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":c=Ma}var d=0!==(4&t),h=!d&&"scroll"===e,u=d?null!==r?r+"Capture":null:r;d=[];for(var m,p=i;null!==p;){var f=(m=p).stateNode;if(5===m.tag&&null!==f&&(m=f,null!==u&&(null!=(f=Ee(p,u))&&d.push(Hi(p,f,m)))),h)break;p=p.return}0<d.length&&(r=new c(r,l,null,a,n),o.push({event:r,listeners:d}))}}if(0===(7&t)){if(c="mouseout"===e||"pointerout"===e,(!(r="mouseover"===e||"pointerover"===e)||a===ve||!(l=a.relatedTarget||a.fromElement)||!fn(l)&&!l[hn])&&(c||r)&&(r=n.window===n?n:(r=n.ownerDocument)?r.defaultView||r.parentWindow:window,c?(c=i,null!==(l=(l=a.relatedTarget||a.toElement)?fn(l):null)&&(l!==(h=He(l))||5!==l.tag&&6!==l.tag)&&(l=null)):(c=null,l=i),c!==l)){if(d=ha,f="onMouseLeave",u="onMouseEnter",p="mouse","pointerout"!==e&&"pointerover"!==e||(d=Ma,f="onPointerLeave",u="onPointerEnter",p="pointer"),h=null==c?r:wn(c),m=null==l?r:wn(l),(r=new d(f,p+"leave",c,a,n)).target=h,r.relatedTarget=m,f=null,fn(n)===i&&((d=new d(u,p+"enter",l,a,n)).target=m,d.relatedTarget=h,f=d),h=f,c&&l)e:{for(u=l,p=0,m=d=c;m;m=Gi(m))p++;for(m=0,f=u;f;f=Gi(f))m++;for(;0<p-m;)d=Gi(d),p--;for(;0<m-p;)u=Gi(u),m--;for(;p--;){if(d===u||null!==u&&d===u.alternate)break e;d=Gi(d),u=Gi(u)}d=null}else d=null;null!==c&&Vi(o,r,c,d,!1),null!==l&&null!==h&&Vi(o,h,l,d,!0)}if("select"===(c=(r=i?wn(i):window).nodeName&&r.nodeName.toLowerCase())||"input"===c&&"file"===r.type)var g=Qa;else if(Ba(r))if(Ka)g=ii;else{g=ti;var w=ei}else(c=r.nodeName)&&"input"===c.toLowerCase()&&("checkbox"===r.type||"radio"===r.type)&&(g=ai);switch(g&&(g=g(e,i))?Ha(o,g,a,n):(w&&w(e,r,i),"focusout"===e&&(w=r._wrapperState)&&w.controlled&&"number"===r.type&&ee(r,"number",r.value)),w=i?wn(i):window,e){case"focusin":(Ba(w)||"true"===w.contentEditable)&&(mi=w,pi=i,fi=null);break;case"focusout":fi=pi=mi=null;break;case"mousedown":gi=!0;break;case"contextmenu":case"mouseup":case"dragend":gi=!1,wi(o,a,n);break;case"selectionchange":if(ui)break;case"keydown":case"keyup":wi(o,a,n)}var b;if(za)e:{switch(e){case"compositionstart":var y="onCompositionStart";break e;case"compositionend":y="onCompositionEnd";break e;case"compositionupdate":y="onCompositionUpdate";break e}y=void 0}else ja?Ra(e,a)&&(y="onCompositionEnd"):"keydown"===e&&229===a.keyCode&&(y="onCompositionStart");y&&(Na&&"ko"!==a.locale&&(ja||"onCompositionStart"!==y?"onCompositionEnd"===y&&ja&&(b=Jt()):($t="value"in(Yt=n)?Yt.value:Yt.textContent,ja=!0)),0<(w=Ui(i,y)).length&&(y=new wa(y,e,null,a,n),o.push({event:y,listeners:w}),b?y.data=b:null!==(b=Oa(a))&&(y.data=b))),(b=Pa?function(e,t){switch(e){case"compositionend":return Oa(t);case"keypress":return 32!==t.which?null:(La=!0,Da);case"textInput":return(e=t.data)===Da&&La?null:e;default:return null}}(e,a):function(e,t){if(ja)return"compositionend"===e||!za&&Ra(e,t)?(e=Jt(),Xt=$t=Yt=null,ja=!1,e):null;switch(e){case"paste":default:return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return Na&&"ko"!==t.locale?null:t.data}}(e,a))&&(0<(i=Ui(i,"onBeforeInput")).length&&(n=new wa("onBeforeInput","beforeinput",null,a,n),o.push({event:n,listeners:i}),n.data=b))}Di(o,t)}))}function Hi(e,t,a){return{instance:e,listener:t,currentTarget:a}}function Ui(e,t){for(var a=t+"Capture",i=[];null!==e;){var n=e,s=n.stateNode;5===n.tag&&null!==s&&(n=s,null!=(s=Ee(e,a))&&i.unshift(Hi(e,s,n)),null!=(s=Ee(e,t))&&i.push(Hi(e,s,n))),e=e.return}return i}function Gi(e){if(null===e)return null;do{e=e.return}while(e&&5!==e.tag);return e||null}function Vi(e,t,a,i,n){for(var s=t._reactName,o=[];null!==a&&a!==i;){var r=a,c=r.alternate,l=r.stateNode;if(null!==c&&c===i)break;5===r.tag&&null!==l&&(r=l,n?null!=(c=Ee(a,s))&&o.unshift(Hi(a,c,r)):n||null!=(c=Ee(a,s))&&o.push(Hi(a,c,r))),a=a.return}0!==o.length&&e.push({event:t,listeners:o})}var _i=/\r\n?/g,Qi=/\u0000|\uFFFD/g;function Ki(e){return("string"===typeof e?e:""+e).replace(_i,"\n").replace(Qi,"")}function Yi(e,t,a){if(t=Ki(t),Ki(e)!==t&&a)throw Error(s(425))}function $i(){}var Xi=null;function Ji(e,t){return"textarea"===e||"noscript"===e||"string"===typeof t.children||"number"===typeof t.children||"object"===typeof t.dangerouslySetInnerHTML&&null!==t.dangerouslySetInnerHTML&&null!=t.dangerouslySetInnerHTML.__html}var Zi="function"===typeof setTimeout?setTimeout:void 0,en="function"===typeof clearTimeout?clearTimeout:void 0,tn="function"===typeof Promise?Promise:void 0,an="function"===typeof queueMicrotask?queueMicrotask:"undefined"!==typeof tn?function(e){return tn.resolve(null).then(e).catch(nn)}:Zi;function nn(e){setTimeout((function(){throw e}))}function sn(e,t){var a=t,i=0;do{var n=a.nextSibling;if(e.removeChild(a),n&&8===n.nodeType)if("/$"===(a=n.data)){if(0===i)return e.removeChild(n),void Bt(t);i--}else"$"!==a&&"$?"!==a&&"$!"!==a||i++;a=n}while(a);Bt(t)}function on(e){for(;null!=e;e=e.nextSibling){var t=e.nodeType;if(1===t||3===t)break;if(8===t){if("$"===(t=e.data)||"$!"===t||"$?"===t)break;if("/$"===t)return null}}return e}function rn(e){e=e.previousSibling;for(var t=0;e;){if(8===e.nodeType){var a=e.data;if("$"===a||"$!"===a||"$?"===a){if(0===t)return e;t--}else"/$"===a&&t++}e=e.previousSibling}return null}var cn=Math.random().toString(36).slice(2),ln="__reactFiber$"+cn,dn="__reactProps$"+cn,hn="__reactContainer$"+cn,un="__reactEvents$"+cn,mn="__reactListeners$"+cn,pn="__reactHandles$"+cn;function fn(e){var t=e[ln];if(t)return t;for(var a=e.parentNode;a;){if(t=a[hn]||a[ln]){if(a=t.alternate,null!==t.child||null!==a&&null!==a.child)for(e=rn(e);null!==e;){if(a=e[ln])return a;e=rn(e)}return t}a=(e=a).parentNode}return null}function gn(e){return!(e=e[ln]||e[hn])||5!==e.tag&&6!==e.tag&&13!==e.tag&&3!==e.tag?null:e}function wn(e){if(5===e.tag||6===e.tag)return e.stateNode;throw Error(s(33))}function bn(e){return e[dn]||null}var yn=[],vn=-1;function kn(e){return{current:e}}function xn(e){0>vn||(e.current=yn[vn],yn[vn]=null,vn--)}function Tn(e,t){vn++,yn[vn]=e.current,e.current=t}var An={},Mn=kn(An),Cn=kn(!1),qn=An;function In(e,t){var a=e.type.contextTypes;if(!a)return An;var i=e.stateNode;if(i&&i.__reactInternalMemoizedUnmaskedChildContext===t)return i.__reactInternalMemoizedMaskedChildContext;var n,s={};for(n in a)s[n]=t[n];return i&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=t,e.__reactInternalMemoizedMaskedChildContext=s),s}function Wn(e){return null!==(e=e.childContextTypes)&&void 0!==e}function Sn(){xn(Cn),xn(Mn)}function zn(e,t,a){if(Mn.current!==An)throw Error(s(168));Tn(Mn,t),Tn(Cn,a)}function En(e,t,a){var i=e.stateNode;if(t=t.childContextTypes,"function"!==typeof i.getChildContext)return a;for(var n in i=i.getChildContext())if(!(n in t))throw Error(s(108,U(e)||"Unknown",n));return R({},a,i)}function Pn(e){return e=(e=e.stateNode)&&e.__reactInternalMemoizedMergedChildContext||An,qn=Mn.current,Tn(Mn,e),Tn(Cn,Cn.current),!0}function Nn(e,t,a){var i=e.stateNode;if(!i)throw Error(s(169));a?(e=En(e,t,qn),i.__reactInternalMemoizedMergedChildContext=e,xn(Cn),xn(Mn),Tn(Mn,e)):xn(Cn),Tn(Cn,a)}var Dn=null,Ln=!1,Rn=!1;function On(e){null===Dn?Dn=[e]:Dn.push(e)}function jn(){if(!Rn&&null!==Dn){Rn=!0;var e=0,t=bt;try{var a=Dn;for(bt=1;e<a.length;e++){var i=a[e];do{i=i(!0)}while(null!==i)}Dn=null,Ln=!1}catch(n){throw null!==Dn&&(Dn=Dn.slice(e+1)),Qe(Ze,jn),n}finally{bt=t,Rn=!1}}return null}var Fn=v.ReactCurrentBatchConfig;function Bn(e,t){if(e&&e.defaultProps){for(var a in t=R({},t),e=e.defaultProps)void 0===t[a]&&(t[a]=e[a]);return t}return t}var Hn=kn(null),Un=null,Gn=null,Vn=null;function _n(){Vn=Gn=Un=null}function Qn(e){var t=Hn.current;xn(Hn),e._currentValue=t}function Kn(e,t,a){for(;null!==e;){var i=e.alternate;if((e.childLanes&t)!==t?(e.childLanes|=t,null!==i&&(i.childLanes|=t)):null!==i&&(i.childLanes&t)!==t&&(i.childLanes|=t),e===a)break;e=e.return}}function Yn(e,t){Un=e,Vn=Gn=null,null!==(e=e.dependencies)&&null!==e.firstContext&&(0!==(e.lanes&t)&&(wr=!0),e.firstContext=null)}function $n(e){var t=e._currentValue;if(Vn!==e)if(e={context:e,memoizedValue:t,next:null},null===Gn){if(null===Un)throw Error(s(308));Gn=e,Un.dependencies={lanes:0,firstContext:e}}else Gn=Gn.next=e;return t}var Xn=null,Jn=!1;function Zn(e){e.updateQueue={baseState:e.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null,interleaved:null,lanes:0},effects:null}}function es(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,firstBaseUpdate:e.firstBaseUpdate,lastBaseUpdate:e.lastBaseUpdate,shared:e.shared,effects:e.effects})}function ts(e,t){return{eventTime:e,lane:t,tag:0,payload:null,callback:null,next:null}}function as(e,t){var a=e.updateQueue;null!==a&&(a=a.shared,null!==bc&&0!==(1&e.mode)&&0===(2&wc)?(null===(e=a.interleaved)?(t.next=t,null===Xn?Xn=[a]:Xn.push(a)):(t.next=e.next,e.next=t),a.interleaved=t):(null===(e=a.pending)?t.next=t:(t.next=e.next,e.next=t),a.pending=t))}function is(e,t,a){if(null!==(t=t.updateQueue)&&(t=t.shared,0!==(4194240&a))){var i=t.lanes;a|=i&=e.pendingLanes,t.lanes=a,wt(e,a)}}function ns(e,t){var a=e.updateQueue,i=e.alternate;if(null!==i&&a===(i=i.updateQueue)){var n=null,s=null;if(null!==(a=a.firstBaseUpdate)){do{var o={eventTime:a.eventTime,lane:a.lane,tag:a.tag,payload:a.payload,callback:a.callback,next:null};null===s?n=s=o:s=s.next=o,a=a.next}while(null!==a);null===s?n=s=t:s=s.next=t}else n=s=t;return a={baseState:i.baseState,firstBaseUpdate:n,lastBaseUpdate:s,shared:i.shared,effects:i.effects},void(e.updateQueue=a)}null===(e=a.lastBaseUpdate)?a.firstBaseUpdate=t:e.next=t,a.lastBaseUpdate=t}function ss(e,t,a,i){var n=e.updateQueue;Jn=!1;var s=n.firstBaseUpdate,o=n.lastBaseUpdate,r=n.shared.pending;if(null!==r){n.shared.pending=null;var c=r,l=c.next;c.next=null,null===o?s=l:o.next=l,o=c;var d=e.alternate;null!==d&&((r=(d=d.updateQueue).lastBaseUpdate)!==o&&(null===r?d.firstBaseUpdate=l:r.next=l,d.lastBaseUpdate=c))}if(null!==s){var h=n.baseState;for(o=0,d=l=c=null,r=s;;){var u=r.lane,m=r.eventTime;if((i&u)===u){null!==d&&(d=d.next={eventTime:m,lane:0,tag:r.tag,payload:r.payload,callback:r.callback,next:null});e:{var p=e,f=r;switch(u=t,m=a,f.tag){case 1:if("function"===typeof(p=f.payload)){h=p.call(m,h,u);break e}h=p;break e;case 3:p.flags=-65537&p.flags|128;case 0:if(null===(u="function"===typeof(p=f.payload)?p.call(m,h,u):p)||void 0===u)break e;h=R({},h,u);break e;case 2:Jn=!0}}null!==r.callback&&0!==r.lane&&(e.flags|=64,null===(u=n.effects)?n.effects=[r]:u.push(r))}else m={eventTime:m,lane:u,tag:r.tag,payload:r.payload,callback:r.callback,next:null},null===d?(l=d=m,c=h):d=d.next=m,o|=u;if(null===(r=r.next)){if(null===(r=n.shared.pending))break;r=(u=r).next,u.next=null,n.lastBaseUpdate=u,n.shared.pending=null}}if(null===d&&(c=h),n.baseState=c,n.firstBaseUpdate=l,n.lastBaseUpdate=d,null!==(t=n.shared.interleaved)){n=t;do{o|=n.lane,n=n.next}while(n!==t)}else null===s&&(n.shared.lanes=0);Mc|=o,e.lanes=o,e.memoizedState=h}}function os(e,t,a){if(e=t.effects,t.effects=null,null!==e)for(t=0;t<e.length;t++){var i=e[t],n=i.callback;if(null!==n){if(i.callback=null,i=a,"function"!==typeof n)throw Error(s(191,n));n.call(i)}}}var rs=(new i.Component).refs;function cs(e,t,a,i){a=null===(a=a(i,t=e.memoizedState))||void 0===a?t:R({},t,a),e.memoizedState=a,0===e.lanes&&(e.updateQueue.baseState=a)}var ls={isMounted:function(e){return!!(e=e._reactInternals)&&He(e)===e},enqueueSetState:function(e,t,a){e=e._reactInternals;var i=Hc(),n=Uc(e),s=ts(i,n);s.payload=t,void 0!==a&&null!==a&&(s.callback=a),as(e,s),null!==(t=Gc(e,n,i))&&is(t,e,n)},enqueueReplaceState:function(e,t,a){e=e._reactInternals;var i=Hc(),n=Uc(e),s=ts(i,n);s.tag=1,s.payload=t,void 0!==a&&null!==a&&(s.callback=a),as(e,s),null!==(t=Gc(e,n,i))&&is(t,e,n)},enqueueForceUpdate:function(e,t){e=e._reactInternals;var a=Hc(),i=Uc(e),n=ts(a,i);n.tag=2,void 0!==t&&null!==t&&(n.callback=t),as(e,n),null!==(t=Gc(e,i,a))&&is(t,e,i)}};function ds(e,t,a,i,n,s,o){return"function"===typeof(e=e.stateNode).shouldComponentUpdate?e.shouldComponentUpdate(i,s,o):!t.prototype||!t.prototype.isPureReactComponent||(!si(a,i)||!si(n,s))}function hs(e,t,a){var i=!1,n=An,s=t.contextType;return"object"===typeof s&&null!==s?s=$n(s):(n=Wn(t)?qn:Mn.current,s=(i=null!==(i=t.contextTypes)&&void 0!==i)?In(e,n):An),t=new t(a,s),e.memoizedState=null!==t.state&&void 0!==t.state?t.state:null,t.updater=ls,e.stateNode=t,t._reactInternals=e,i&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=n,e.__reactInternalMemoizedMaskedChildContext=s),t}function us(e,t,a,i){e=t.state,"function"===typeof t.componentWillReceiveProps&&t.componentWillReceiveProps(a,i),"function"===typeof t.UNSAFE_componentWillReceiveProps&&t.UNSAFE_componentWillReceiveProps(a,i),t.state!==e&&ls.enqueueReplaceState(t,t.state,null)}function ms(e,t,a,i){var n=e.stateNode;n.props=a,n.state=e.memoizedState,n.refs=rs,Zn(e);var s=t.contextType;"object"===typeof s&&null!==s?n.context=$n(s):(s=Wn(t)?qn:Mn.current,n.context=In(e,s)),n.state=e.memoizedState,"function"===typeof(s=t.getDerivedStateFromProps)&&(cs(e,t,s,a),n.state=e.memoizedState),"function"===typeof t.getDerivedStateFromProps||"function"===typeof n.getSnapshotBeforeUpdate||"function"!==typeof n.UNSAFE_componentWillMount&&"function"!==typeof n.componentWillMount||(t=n.state,"function"===typeof n.componentWillMount&&n.componentWillMount(),"function"===typeof n.UNSAFE_componentWillMount&&n.UNSAFE_componentWillMount(),t!==n.state&&ls.enqueueReplaceState(n,n.state,null),ss(e,a,n,i),n.state=e.memoizedState),"function"===typeof n.componentDidMount&&(e.flags|=4194308)}var ps=[],fs=0,gs=null,ws=0,bs=[],ys=0,vs=null,ks=1,xs="";function Ts(e,t){ps[fs++]=ws,ps[fs++]=gs,gs=e,ws=t}function As(e,t,a){bs[ys++]=ks,bs[ys++]=xs,bs[ys++]=vs,vs=e;var i=ks;e=xs;var n=32-ot(i)-1;i&=~(1<<n),a+=1;var s=32-ot(t)+n;if(30<s){var o=n-n%5;s=(i&(1<<o)-1).toString(32),i>>=o,n-=o,ks=1<<32-ot(t)+n|a<<n|i,xs=s+e}else ks=1<<s|a<<n|i,xs=e}function Ms(e){null!==e.return&&(Ts(e,1),As(e,1,0))}function Cs(e){for(;e===gs;)gs=ps[--fs],ps[fs]=null,ws=ps[--fs],ps[fs]=null;for(;e===vs;)vs=bs[--ys],bs[ys]=null,xs=bs[--ys],bs[ys]=null,ks=bs[--ys],bs[ys]=null}var qs=null,Is=null,Ws=!1,Ss=null;function zs(e,t){var a=vl(5,null,null,0);a.elementType="DELETED",a.stateNode=t,a.return=e,null===(t=e.deletions)?(e.deletions=[a],e.flags|=16):t.push(a)}function Es(e,t){switch(e.tag){case 5:var a=e.type;return null!==(t=1!==t.nodeType||a.toLowerCase()!==t.nodeName.toLowerCase()?null:t)&&(e.stateNode=t,qs=e,Is=on(t.firstChild),!0);case 6:return null!==(t=""===e.pendingProps||3!==t.nodeType?null:t)&&(e.stateNode=t,qs=e,Is=null,!0);case 13:return null!==(t=8!==t.nodeType?null:t)&&(a=null!==vs?{id:ks,overflow:xs}:null,e.memoizedState={dehydrated:t,treeContext:a,retryLane:1073741824},(a=vl(18,null,null,0)).stateNode=t,a.return=e,e.child=a,qs=e,Is=null,!0);default:return!1}}function Ps(e){return 0!==(1&e.mode)&&0===(128&e.flags)}function Ns(e){if(Ws){var t=Is;if(t){var a=t;if(!Es(e,t)){if(Ps(e))throw Error(s(418));t=on(a.nextSibling);var i=qs;t&&Es(e,t)?zs(i,a):(e.flags=-4097&e.flags|2,Ws=!1,qs=e)}}else{if(Ps(e))throw Error(s(418));e.flags=-4097&e.flags|2,Ws=!1,qs=e}}}function Ds(e){for(e=e.return;null!==e&&5!==e.tag&&3!==e.tag&&13!==e.tag;)e=e.return;qs=e}function Ls(e){if(e!==qs)return!1;if(!Ws)return Ds(e),Ws=!0,!1;var t;if((t=3!==e.tag)&&!(t=5!==e.tag)&&(t="head"!==(t=e.type)&&"body"!==t&&!Ji(e.type,e.memoizedProps)),t&&(t=Is)){if(Ps(e)){for(e=Is;e;)e=on(e.nextSibling);throw Error(s(418))}for(;t;)zs(e,t),t=on(t.nextSibling)}if(Ds(e),13===e.tag){if(!(e=null!==(e=e.memoizedState)?e.dehydrated:null))throw Error(s(317));e:{for(e=e.nextSibling,t=0;e;){if(8===e.nodeType){var a=e.data;if("/$"===a){if(0===t){Is=on(e.nextSibling);break e}t--}else"$"!==a&&"$!"!==a&&"$?"!==a||t++}e=e.nextSibling}Is=null}}else Is=qs?on(e.stateNode.nextSibling):null;return!0}function Rs(){Is=qs=null,Ws=!1}function Os(e){null===Ss?Ss=[e]:Ss.push(e)}function js(e,t,a){if(null!==(e=a.ref)&&"function"!==typeof e&&"object"!==typeof e){if(a._owner){if(a=a._owner){if(1!==a.tag)throw Error(s(309));var i=a.stateNode}if(!i)throw Error(s(147,e));var n=i,o=""+e;return null!==t&&null!==t.ref&&"function"===typeof t.ref&&t.ref._stringRef===o?t.ref:(t=function(e){var t=n.refs;t===rs&&(t=n.refs={}),null===e?delete t[o]:t[o]=e},t._stringRef=o,t)}if("string"!==typeof e)throw Error(s(284));if(!a._owner)throw Error(s(290,e))}return e}function Fs(e,t){throw e=Object.prototype.toString.call(t),Error(s(31,"[object Object]"===e?"object with keys {"+Object.keys(t).join(", ")+"}":e))}function Bs(e){return(0,e._init)(e._payload)}function Hs(e){function t(t,a){if(e){var i=t.deletions;null===i?(t.deletions=[a],t.flags|=16):i.push(a)}}function a(a,i){if(!e)return null;for(;null!==i;)t(a,i),i=i.sibling;return null}function i(e,t){for(e=new Map;null!==t;)null!==t.key?e.set(t.key,t):e.set(t.index,t),t=t.sibling;return e}function n(e,t){return(e=xl(e,t)).index=0,e.sibling=null,e}function o(t,a,i){return t.index=i,e?null!==(i=t.alternate)?(i=i.index)<a?(t.flags|=2,a):i:(t.flags|=2,a):(t.flags|=1048576,a)}function r(t){return e&&null===t.alternate&&(t.flags|=2),t}function c(e,t,a,i){return null===t||6!==t.tag?((t=Cl(a,e.mode,i)).return=e,t):((t=n(t,a)).return=e,t)}function l(e,t,a,i){var s=a.type;return s===T?h(e,t,a.props.children,i,a.key):null!==t&&(t.elementType===s||"object"===typeof s&&null!==s&&s.$$typeof===E&&Bs(s)===t.type)?((i=n(t,a.props)).ref=js(e,t,a),i.return=e,i):((i=Tl(a.type,a.key,a.props,null,e.mode,i)).ref=js(e,t,a),i.return=e,i)}function d(e,t,a,i){return null===t||4!==t.tag||t.stateNode.containerInfo!==a.containerInfo||t.stateNode.implementation!==a.implementation?((t=ql(a,e.mode,i)).return=e,t):((t=n(t,a.children||[])).return=e,t)}function h(e,t,a,i,s){return null===t||7!==t.tag?((t=Al(a,e.mode,i,s)).return=e,t):((t=n(t,a)).return=e,t)}function u(e,t,a){if("string"===typeof t&&""!==t||"number"===typeof t)return(t=Cl(""+t,e.mode,a)).return=e,t;if("object"===typeof t&&null!==t){switch(t.$$typeof){case k:return(a=Tl(t.type,t.key,t.props,null,e.mode,a)).ref=js(e,null,t),a.return=e,a;case x:return(t=ql(t,e.mode,a)).return=e,t;case E:return u(e,(0,t._init)(t._payload),a)}if(te(t)||D(t))return(t=Al(t,e.mode,a,null)).return=e,t;Fs(e,t)}return null}function m(e,t,a,i){var n=null!==t?t.key:null;if("string"===typeof a&&""!==a||"number"===typeof a)return null!==n?null:c(e,t,""+a,i);if("object"===typeof a&&null!==a){switch(a.$$typeof){case k:return a.key===n?l(e,t,a,i):null;case x:return a.key===n?d(e,t,a,i):null;case E:return m(e,t,(n=a._init)(a._payload),i)}if(te(a)||D(a))return null!==n?null:h(e,t,a,i,null);Fs(e,a)}return null}function p(e,t,a,i,n){if("string"===typeof i&&""!==i||"number"===typeof i)return c(t,e=e.get(a)||null,""+i,n);if("object"===typeof i&&null!==i){switch(i.$$typeof){case k:return l(t,e=e.get(null===i.key?a:i.key)||null,i,n);case x:return d(t,e=e.get(null===i.key?a:i.key)||null,i,n);case E:return p(e,t,a,(0,i._init)(i._payload),n)}if(te(i)||D(i))return h(t,e=e.get(a)||null,i,n,null);Fs(t,i)}return null}function f(n,s,r,c){for(var l=null,d=null,h=s,f=s=0,g=null;null!==h&&f<r.length;f++){h.index>f?(g=h,h=null):g=h.sibling;var w=m(n,h,r[f],c);if(null===w){null===h&&(h=g);break}e&&h&&null===w.alternate&&t(n,h),s=o(w,s,f),null===d?l=w:d.sibling=w,d=w,h=g}if(f===r.length)return a(n,h),Ws&&Ts(n,f),l;if(null===h){for(;f<r.length;f++)null!==(h=u(n,r[f],c))&&(s=o(h,s,f),null===d?l=h:d.sibling=h,d=h);return Ws&&Ts(n,f),l}for(h=i(n,h);f<r.length;f++)null!==(g=p(h,n,f,r[f],c))&&(e&&null!==g.alternate&&h.delete(null===g.key?f:g.key),s=o(g,s,f),null===d?l=g:d.sibling=g,d=g);return e&&h.forEach((function(e){return t(n,e)})),Ws&&Ts(n,f),l}function g(n,r,c,l){var d=D(c);if("function"!==typeof d)throw Error(s(150));if(null==(c=d.call(c)))throw Error(s(151));for(var h=d=null,f=r,g=r=0,w=null,b=c.next();null!==f&&!b.done;g++,b=c.next()){f.index>g?(w=f,f=null):w=f.sibling;var y=m(n,f,b.value,l);if(null===y){null===f&&(f=w);break}e&&f&&null===y.alternate&&t(n,f),r=o(y,r,g),null===h?d=y:h.sibling=y,h=y,f=w}if(b.done)return a(n,f),Ws&&Ts(n,g),d;if(null===f){for(;!b.done;g++,b=c.next())null!==(b=u(n,b.value,l))&&(r=o(b,r,g),null===h?d=b:h.sibling=b,h=b);return Ws&&Ts(n,g),d}for(f=i(n,f);!b.done;g++,b=c.next())null!==(b=p(f,n,g,b.value,l))&&(e&&null!==b.alternate&&f.delete(null===b.key?g:b.key),r=o(b,r,g),null===h?d=b:h.sibling=b,h=b);return e&&f.forEach((function(e){return t(n,e)})),Ws&&Ts(n,g),d}return function e(i,s,o,c){if("object"===typeof o&&null!==o&&o.type===T&&null===o.key&&(o=o.props.children),"object"===typeof o&&null!==o){switch(o.$$typeof){case k:e:{for(var l=o.key,d=s;null!==d;){if(d.key===l){if((l=o.type)===T){if(7===d.tag){a(i,d.sibling),(s=n(d,o.props.children)).return=i,i=s;break e}}else if(d.elementType===l||"object"===typeof l&&null!==l&&l.$$typeof===E&&Bs(l)===d.type){a(i,d.sibling),(s=n(d,o.props)).ref=js(i,d,o),s.return=i,i=s;break e}a(i,d);break}t(i,d),d=d.sibling}o.type===T?((s=Al(o.props.children,i.mode,c,o.key)).return=i,i=s):((c=Tl(o.type,o.key,o.props,null,i.mode,c)).ref=js(i,s,o),c.return=i,i=c)}return r(i);case x:e:{for(d=o.key;null!==s;){if(s.key===d){if(4===s.tag&&s.stateNode.containerInfo===o.containerInfo&&s.stateNode.implementation===o.implementation){a(i,s.sibling),(s=n(s,o.children||[])).return=i,i=s;break e}a(i,s);break}t(i,s),s=s.sibling}(s=ql(o,i.mode,c)).return=i,i=s}return r(i);case E:return e(i,s,(d=o._init)(o._payload),c)}if(te(o))return f(i,s,o,c);if(D(o))return g(i,s,o,c);Fs(i,o)}return"string"===typeof o&&""!==o||"number"===typeof o?(o=""+o,null!==s&&6===s.tag?(a(i,s.sibling),(s=n(s,o)).return=i,i=s):(a(i,s),(s=Cl(o,i.mode,c)).return=i,i=s),r(i)):a(i,s)}}var Us=Hs(!0),Gs=Hs(!1),Vs={},_s=kn(Vs),Qs=kn(Vs),Ks=kn(Vs);function Ys(e){if(e===Vs)throw Error(s(174));return e}function $s(e,t){switch(Tn(Ks,t),Tn(Qs,e),Tn(_s,Vs),e=t.nodeType){case 9:case 11:t=(t=t.documentElement)?t.namespaceURI:ce(null,"");break;default:t=ce(t=(e=8===e?t.parentNode:t).namespaceURI||null,e=e.tagName)}xn(_s),Tn(_s,t)}function Xs(){xn(_s),xn(Qs),xn(Ks)}function Js(e){Ys(Ks.current);var t=Ys(_s.current),a=ce(t,e.type);t!==a&&(Tn(Qs,e),Tn(_s,a))}function Zs(e){Qs.current===e&&(xn(_s),xn(Qs))}var eo=kn(0);function to(e){for(var t=e;null!==t;){if(13===t.tag){var a=t.memoizedState;if(null!==a&&(null===(a=a.dehydrated)||"$?"===a.data||"$!"===a.data))return t}else if(19===t.tag&&void 0!==t.memoizedProps.revealOrder){if(0!==(128&t.flags))return t}else if(null!==t.child){t.child.return=t,t=t.child;continue}if(t===e)break;for(;null===t.sibling;){if(null===t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}var ao=[];function io(){for(var e=0;e<ao.length;e++)ao[e]._workInProgressVersionPrimary=null;ao.length=0}var no=v.ReactCurrentDispatcher,so=v.ReactCurrentBatchConfig,oo=0,ro=null,co=null,lo=null,ho=!1,uo=!1,mo=0,po=0;function fo(){throw Error(s(321))}function go(e,t){if(null===t)return!1;for(var a=0;a<t.length&&a<e.length;a++)if(!ni(e[a],t[a]))return!1;return!0}function wo(e,t,a,i,n,o){if(oo=o,ro=t,t.memoizedState=null,t.updateQueue=null,t.lanes=0,no.current=null===e||null===e.memoizedState?Zo:er,e=a(i,n),uo){o=0;do{if(uo=!1,mo=0,25<=o)throw Error(s(301));o+=1,lo=co=null,t.updateQueue=null,no.current=tr,e=a(i,n)}while(uo)}if(no.current=Jo,t=null!==co&&null!==co.next,oo=0,lo=co=ro=null,ho=!1,t)throw Error(s(300));return e}function bo(){var e=0!==mo;return mo=0,e}function yo(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return null===lo?ro.memoizedState=lo=e:lo=lo.next=e,lo}function vo(){if(null===co){var e=ro.alternate;e=null!==e?e.memoizedState:null}else e=co.next;var t=null===lo?ro.memoizedState:lo.next;if(null!==t)lo=t,co=e;else{if(null===e)throw Error(s(310));e={memoizedState:(co=e).memoizedState,baseState:co.baseState,baseQueue:co.baseQueue,queue:co.queue,next:null},null===lo?ro.memoizedState=lo=e:lo=lo.next=e}return lo}function ko(e,t){return"function"===typeof t?t(e):t}function xo(e){var t=vo(),a=t.queue;if(null===a)throw Error(s(311));a.lastRenderedReducer=e;var i=co,n=i.baseQueue,o=a.pending;if(null!==o){if(null!==n){var r=n.next;n.next=o.next,o.next=r}i.baseQueue=n=o,a.pending=null}if(null!==n){o=n.next,i=i.baseState;var c=r=null,l=null,d=o;do{var h=d.lane;if((oo&h)===h)null!==l&&(l=l.next={lane:0,action:d.action,hasEagerState:d.hasEagerState,eagerState:d.eagerState,next:null}),i=d.hasEagerState?d.eagerState:e(i,d.action);else{var u={lane:h,action:d.action,hasEagerState:d.hasEagerState,eagerState:d.eagerState,next:null};null===l?(c=l=u,r=i):l=l.next=u,ro.lanes|=h,Mc|=h}d=d.next}while(null!==d&&d!==o);null===l?r=i:l.next=c,ni(i,t.memoizedState)||(wr=!0),t.memoizedState=i,t.baseState=r,t.baseQueue=l,a.lastRenderedState=i}if(null!==(e=a.interleaved)){n=e;do{o=n.lane,ro.lanes|=o,Mc|=o,n=n.next}while(n!==e)}else null===n&&(a.lanes=0);return[t.memoizedState,a.dispatch]}function To(e){var t=vo(),a=t.queue;if(null===a)throw Error(s(311));a.lastRenderedReducer=e;var i=a.dispatch,n=a.pending,o=t.memoizedState;if(null!==n){a.pending=null;var r=n=n.next;do{o=e(o,r.action),r=r.next}while(r!==n);ni(o,t.memoizedState)||(wr=!0),t.memoizedState=o,null===t.baseQueue&&(t.baseState=o),a.lastRenderedState=o}return[o,i]}function Ao(){}function Mo(e,t){var a=ro,i=vo(),n=t(),o=!ni(i.memoizedState,n);if(o&&(i.memoizedState=n,wr=!0),i=i.queue,Lo(Io.bind(null,a,i,e),[e]),i.getSnapshot!==t||o||null!==lo&&1&lo.memoizedState.tag){if(a.flags|=2048,zo(9,qo.bind(null,a,i,n,t),void 0,null),null===bc)throw Error(s(349));0!==(30&oo)||Co(a,t,n)}return n}function Co(e,t,a){e.flags|=16384,e={getSnapshot:t,value:a},null===(t=ro.updateQueue)?(t={lastEffect:null,stores:null},ro.updateQueue=t,t.stores=[e]):null===(a=t.stores)?t.stores=[e]:a.push(e)}function qo(e,t,a,i){t.value=a,t.getSnapshot=i,Wo(t)&&Gc(e,1,-1)}function Io(e,t,a){return a((function(){Wo(t)&&Gc(e,1,-1)}))}function Wo(e){var t=e.getSnapshot;e=e.value;try{var a=t();return!ni(e,a)}catch(i){return!0}}function So(e){var t=yo();return"function"===typeof e&&(e=e()),t.memoizedState=t.baseState=e,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:ko,lastRenderedState:e},t.queue=e,e=e.dispatch=Qo.bind(null,ro,e),[t.memoizedState,e]}function zo(e,t,a,i){return e={tag:e,create:t,destroy:a,deps:i,next:null},null===(t=ro.updateQueue)?(t={lastEffect:null,stores:null},ro.updateQueue=t,t.lastEffect=e.next=e):null===(a=t.lastEffect)?t.lastEffect=e.next=e:(i=a.next,a.next=e,e.next=i,t.lastEffect=e),e}function Eo(){return vo().memoizedState}function Po(e,t,a,i){var n=yo();ro.flags|=e,n.memoizedState=zo(1|t,a,void 0,void 0===i?null:i)}function No(e,t,a,i){var n=vo();i=void 0===i?null:i;var s=void 0;if(null!==co){var o=co.memoizedState;if(s=o.destroy,null!==i&&go(i,o.deps))return void(n.memoizedState=zo(t,a,s,i))}ro.flags|=e,n.memoizedState=zo(1|t,a,s,i)}function Do(e,t){return Po(8390656,8,e,t)}function Lo(e,t){return No(2048,8,e,t)}function Ro(e,t){return No(4,2,e,t)}function Oo(e,t){return No(4,4,e,t)}function jo(e,t){return"function"===typeof t?(e=e(),t(e),function(){t(null)}):null!==t&&void 0!==t?(e=e(),t.current=e,function(){t.current=null}):void 0}function Fo(e,t,a){return a=null!==a&&void 0!==a?a.concat([e]):null,No(4,4,jo.bind(null,t,e),a)}function Bo(){}function Ho(e,t){var a=vo();t=void 0===t?null:t;var i=a.memoizedState;return null!==i&&null!==t&&go(t,i[1])?i[0]:(a.memoizedState=[e,t],e)}function Uo(e,t){var a=vo();t=void 0===t?null:t;var i=a.memoizedState;return null!==i&&null!==t&&go(t,i[1])?i[0]:(e=e(),a.memoizedState=[e,t],e)}function Go(e,t){var a=bt;bt=0!==a&&4>a?a:4,e(!0);var i=so.transition;so.transition={};try{e(!1),t()}finally{bt=a,so.transition=i}}function Vo(){return vo().memoizedState}function _o(e,t,a){var i=Uc(e);a={lane:i,action:a,hasEagerState:!1,eagerState:null,next:null},Ko(e)?Yo(t,a):($o(e,t,a),null!==(e=Gc(e,i,a=Hc()))&&Xo(e,t,i))}function Qo(e,t,a){var i=Uc(e),n={lane:i,action:a,hasEagerState:!1,eagerState:null,next:null};if(Ko(e))Yo(t,n);else{$o(e,t,n);var s=e.alternate;if(0===e.lanes&&(null===s||0===s.lanes)&&null!==(s=t.lastRenderedReducer))try{var o=t.lastRenderedState,r=s(o,a);if(n.hasEagerState=!0,n.eagerState=r,ni(r,o))return}catch(c){}null!==(e=Gc(e,i,a=Hc()))&&Xo(e,t,i)}}function Ko(e){var t=e.alternate;return e===ro||null!==t&&t===ro}function Yo(e,t){uo=ho=!0;var a=e.pending;null===a?t.next=t:(t.next=a.next,a.next=t),e.pending=t}function $o(e,t,a){null!==bc&&0!==(1&e.mode)&&0===(2&wc)?(null===(e=t.interleaved)?(a.next=a,null===Xn?Xn=[t]:Xn.push(t)):(a.next=e.next,e.next=a),t.interleaved=a):(null===(e=t.pending)?a.next=a:(a.next=e.next,e.next=a),t.pending=a)}function Xo(e,t,a){if(0!==(4194240&a)){var i=t.lanes;a|=i&=e.pendingLanes,t.lanes=a,wt(e,a)}}var Jo={readContext:$n,useCallback:fo,useContext:fo,useEffect:fo,useImperativeHandle:fo,useInsertionEffect:fo,useLayoutEffect:fo,useMemo:fo,useReducer:fo,useRef:fo,useState:fo,useDebugValue:fo,useDeferredValue:fo,useTransition:fo,useMutableSource:fo,useSyncExternalStore:fo,useId:fo,unstable_isNewReconciler:!1},Zo={readContext:$n,useCallback:function(e,t){return yo().memoizedState=[e,void 0===t?null:t],e},useContext:$n,useEffect:Do,useImperativeHandle:function(e,t,a){return a=null!==a&&void 0!==a?a.concat([e]):null,Po(4194308,4,jo.bind(null,t,e),a)},useLayoutEffect:function(e,t){return Po(4194308,4,e,t)},useInsertionEffect:function(e,t){return Po(4,2,e,t)},useMemo:function(e,t){var a=yo();return t=void 0===t?null:t,e=e(),a.memoizedState=[e,t],e},useReducer:function(e,t,a){var i=yo();return t=void 0!==a?a(t):t,i.memoizedState=i.baseState=t,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:e,lastRenderedState:t},i.queue=e,e=e.dispatch=_o.bind(null,ro,e),[i.memoizedState,e]},useRef:function(e){return e={current:e},yo().memoizedState=e},useState:So,useDebugValue:Bo,useDeferredValue:function(e){var t=So(e),a=t[0],i=t[1];return Do((function(){var t=so.transition;so.transition={};try{i(e)}finally{so.transition=t}}),[e]),a},useTransition:function(){var e=So(!1),t=e[0];return e=Go.bind(null,e[1]),yo().memoizedState=e,[t,e]},useMutableSource:function(){},useSyncExternalStore:function(e,t,a){var i=ro,n=yo();if(Ws){if(void 0===a)throw Error(s(407));a=a()}else{if(a=t(),null===bc)throw Error(s(349));0!==(30&oo)||Co(i,t,a)}n.memoizedState=a;var o={value:a,getSnapshot:t};return n.queue=o,Do(Io.bind(null,i,o,e),[e]),i.flags|=2048,zo(9,qo.bind(null,i,o,a,t),void 0,null),a},useId:function(){var e=yo(),t=bc.identifierPrefix;if(Ws){var a=xs;t=":"+t+"R"+(a=(ks&~(1<<32-ot(ks)-1)).toString(32)+a),0<(a=mo++)&&(t+="H"+a.toString(32)),t+=":"}else t=":"+t+"r"+(a=po++).toString(32)+":";return e.memoizedState=t},unstable_isNewReconciler:!1},er={readContext:$n,useCallback:Ho,useContext:$n,useEffect:Lo,useImperativeHandle:Fo,useInsertionEffect:Ro,useLayoutEffect:Oo,useMemo:Uo,useReducer:xo,useRef:Eo,useState:function(){return xo(ko)},useDebugValue:Bo,useDeferredValue:function(e){var t=xo(ko),a=t[0],i=t[1];return Lo((function(){var t=so.transition;so.transition={};try{i(e)}finally{so.transition=t}}),[e]),a},useTransition:function(){return[xo(ko)[0],vo().memoizedState]},useMutableSource:Ao,useSyncExternalStore:Mo,useId:Vo,unstable_isNewReconciler:!1},tr={readContext:$n,useCallback:Ho,useContext:$n,useEffect:Lo,useImperativeHandle:Fo,useInsertionEffect:Ro,useLayoutEffect:Oo,useMemo:Uo,useReducer:To,useRef:Eo,useState:function(){return To(ko)},useDebugValue:Bo,useDeferredValue:function(e){var t=To(ko),a=t[0],i=t[1];return Lo((function(){var t=so.transition;so.transition={};try{i(e)}finally{so.transition=t}}),[e]),a},useTransition:function(){return[To(ko)[0],vo().memoizedState]},useMutableSource:Ao,useSyncExternalStore:Mo,useId:Vo,unstable_isNewReconciler:!1};function ar(e,t){try{var a="",i=t;do{a+=B(i),i=i.return}while(i);var n=a}catch(s){n="\nError generating stack: "+s.message+"\n"+s.stack}return{value:e,source:t,stack:n}}function ir(e,t){try{console.error(t.value)}catch(a){setTimeout((function(){throw a}))}}var nr,sr,or,rr="function"===typeof WeakMap?WeakMap:Map;function cr(e,t,a){(a=ts(-1,a)).tag=3,a.payload={element:null};var i=t.value;return a.callback=function(){Ec||(Ec=!0,Pc=i),ir(0,t)},a}function lr(e,t,a){(a=ts(-1,a)).tag=3;var i=e.type.getDerivedStateFromError;if("function"===typeof i){var n=t.value;a.payload=function(){return i(n)},a.callback=function(){ir(0,t)}}var s=e.stateNode;return null!==s&&"function"===typeof s.componentDidCatch&&(a.callback=function(){ir(0,t),"function"!==typeof i&&(null===Nc?Nc=new Set([this]):Nc.add(this));var e=t.stack;this.componentDidCatch(t.value,{componentStack:null!==e?e:""})}),a}function dr(e,t,a){var i=e.pingCache;if(null===i){i=e.pingCache=new rr;var n=new Set;i.set(t,n)}else void 0===(n=i.get(t))&&(n=new Set,i.set(t,n));n.has(a)||(n.add(a),e=pl.bind(null,e,t,a),t.then(e,e))}function hr(e){do{var t;if((t=13===e.tag)&&(t=null===(t=e.memoizedState)||null!==t.dehydrated),t)return e;e=e.return}while(null!==e);return null}function ur(e,t,a,i,n){return 0===(1&e.mode)?(e===t?e.flags|=65536:(e.flags|=128,a.flags|=131072,a.flags&=-52805,1===a.tag&&(null===a.alternate?a.tag=17:((t=ts(-1,1)).tag=2,as(a,t))),a.lanes|=1),e):(e.flags|=65536,e.lanes=n,e)}function mr(e,t){if(!Ws)switch(e.tailMode){case"hidden":t=e.tail;for(var a=null;null!==t;)null!==t.alternate&&(a=t),t=t.sibling;null===a?e.tail=null:a.sibling=null;break;case"collapsed":a=e.tail;for(var i=null;null!==a;)null!==a.alternate&&(i=a),a=a.sibling;null===i?t||null===e.tail?e.tail=null:e.tail.sibling=null:i.sibling=null}}function pr(e){var t=null!==e.alternate&&e.alternate.child===e.child,a=0,i=0;if(t)for(var n=e.child;null!==n;)a|=n.lanes|n.childLanes,i|=14680064&n.subtreeFlags,i|=14680064&n.flags,n.return=e,n=n.sibling;else for(n=e.child;null!==n;)a|=n.lanes|n.childLanes,i|=n.subtreeFlags,i|=n.flags,n.return=e,n=n.sibling;return e.subtreeFlags|=i,e.childLanes=a,t}function fr(e,t,a){var i=t.pendingProps;switch(Cs(t),t.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return pr(t),null;case 1:case 17:return Wn(t.type)&&Sn(),pr(t),null;case 3:return i=t.stateNode,Xs(),xn(Cn),xn(Mn),io(),i.pendingContext&&(i.context=i.pendingContext,i.pendingContext=null),null!==e&&null!==e.child||(Ls(t)?t.flags|=4:null===e||e.memoizedState.isDehydrated&&0===(256&t.flags)||(t.flags|=1024,null!==Ss&&(Yc(Ss),Ss=null))),pr(t),null;case 5:Zs(t);var n=Ys(Ks.current);if(a=t.type,null!==e&&null!=t.stateNode)sr(e,t,a,i),e.ref!==t.ref&&(t.flags|=512,t.flags|=2097152);else{if(!i){if(null===t.stateNode)throw Error(s(166));return pr(t),null}if(e=Ys(_s.current),Ls(t)){i=t.stateNode,a=t.type;var o=t.memoizedProps;switch(i[ln]=t,i[dn]=o,e=0!==(1&t.mode),a){case"dialog":Li("cancel",i),Li("close",i);break;case"iframe":case"object":case"embed":Li("load",i);break;case"video":case"audio":for(n=0;n<Ei.length;n++)Li(Ei[n],i);break;case"source":Li("error",i);break;case"img":case"image":case"link":Li("error",i),Li("load",i);break;case"details":Li("toggle",i);break;case"input":$(i,o),Li("invalid",i);break;case"select":i._wrapperState={wasMultiple:!!o.multiple},Li("invalid",i);break;case"textarea":ne(i,o),Li("invalid",i)}for(var c in be(a,o),n=null,o)if(o.hasOwnProperty(c)){var l=o[c];"children"===c?"string"===typeof l?i.textContent!==l&&(Yi(i.textContent,l,e),n=["children",l]):"number"===typeof l&&i.textContent!==""+l&&(Yi(i.textContent,l,e),n=["children",""+l]):r.hasOwnProperty(c)&&null!=l&&"onScroll"===c&&Li("scroll",i)}switch(a){case"input":_(i),Z(i,o,!0);break;case"textarea":_(i),oe(i);break;case"select":case"option":break;default:"function"===typeof o.onClick&&(i.onclick=$i)}i=n,t.updateQueue=i,null!==i&&(t.flags|=4)}else{c=9===n.nodeType?n:n.ownerDocument,"http://www.w3.org/1999/xhtml"===e&&(e=re(a)),"http://www.w3.org/1999/xhtml"===e?"script"===a?((e=c.createElement("div")).innerHTML="<script><\/script>",e=e.removeChild(e.firstChild)):"string"===typeof i.is?e=c.createElement(a,{is:i.is}):(e=c.createElement(a),"select"===a&&(c=e,i.multiple?c.multiple=!0:i.size&&(c.size=i.size))):e=c.createElementNS(e,a),e[ln]=t,e[dn]=i,nr(e,t),t.stateNode=e;e:{switch(c=ye(a,i),a){case"dialog":Li("cancel",e),Li("close",e),n=i;break;case"iframe":case"object":case"embed":Li("load",e),n=i;break;case"video":case"audio":for(n=0;n<Ei.length;n++)Li(Ei[n],e);n=i;break;case"source":Li("error",e),n=i;break;case"img":case"image":case"link":Li("error",e),Li("load",e),n=i;break;case"details":Li("toggle",e),n=i;break;case"input":$(e,i),n=Y(e,i),Li("invalid",e);break;case"option":default:n=i;break;case"select":e._wrapperState={wasMultiple:!!i.multiple},n=R({},i,{value:void 0}),Li("invalid",e);break;case"textarea":ne(e,i),n=ie(e,i),Li("invalid",e)}for(o in be(a,n),l=n)if(l.hasOwnProperty(o)){var d=l[o];"style"===o?ge(e,d):"dangerouslySetInnerHTML"===o?null!=(d=d?d.__html:void 0)&&he(e,d):"children"===o?"string"===typeof d?("textarea"!==a||""!==d)&&ue(e,d):"number"===typeof d&&ue(e,""+d):"suppressContentEditableWarning"!==o&&"suppressHydrationWarning"!==o&&"autoFocus"!==o&&(r.hasOwnProperty(o)?null!=d&&"onScroll"===o&&Li("scroll",e):null!=d&&y(e,o,d,c))}switch(a){case"input":_(e),Z(e,i,!1);break;case"textarea":_(e),oe(e);break;case"option":null!=i.value&&e.setAttribute("value",""+G(i.value));break;case"select":e.multiple=!!i.multiple,null!=(o=i.value)?ae(e,!!i.multiple,o,!1):null!=i.defaultValue&&ae(e,!!i.multiple,i.defaultValue,!0);break;default:"function"===typeof n.onClick&&(e.onclick=$i)}switch(a){case"button":case"input":case"select":case"textarea":i=!!i.autoFocus;break e;case"img":i=!0;break e;default:i=!1}}i&&(t.flags|=4)}null!==t.ref&&(t.flags|=512,t.flags|=2097152)}return pr(t),null;case 6:if(e&&null!=t.stateNode)or(0,t,e.memoizedProps,i);else{if("string"!==typeof i&&null===t.stateNode)throw Error(s(166));if(a=Ys(Ks.current),Ys(_s.current),Ls(t)){if(i=t.stateNode,a=t.memoizedProps,i[ln]=t,(o=i.nodeValue!==a)&&null!==(e=qs))switch(c=0!==(1&e.mode),e.tag){case 3:Yi(i.nodeValue,a,c);break;case 5:!0!==e.memoizedProps[void 0]&&Yi(i.nodeValue,a,c)}o&&(t.flags|=4)}else(i=(9===a.nodeType?a:a.ownerDocument).createTextNode(i))[ln]=t,t.stateNode=i}return pr(t),null;case 13:if(xn(eo),i=t.memoizedState,Ws&&null!==Is&&0!==(1&t.mode)&&0===(128&t.flags)){for(i=Is;i;)i=on(i.nextSibling);return Rs(),t.flags|=98560,t}if(null!==i&&null!==i.dehydrated){if(i=Ls(t),null===e){if(!i)throw Error(s(318));if(!(i=null!==(i=t.memoizedState)?i.dehydrated:null))throw Error(s(317));i[ln]=t}else Rs(),0===(128&t.flags)&&(t.memoizedState=null),t.flags|=4;return pr(t),null}return null!==Ss&&(Yc(Ss),Ss=null),0!==(128&t.flags)?(t.lanes=a,t):(i=null!==i,a=!1,null===e?Ls(t):a=null!==e.memoizedState,i&&!a&&(t.child.flags|=8192,0!==(1&t.mode)&&(null===e||0!==(1&eo.current)?0===Tc&&(Tc=3):nl())),null!==t.updateQueue&&(t.flags|=4),pr(t),null);case 4:return Xs(),null===e&&ji(t.stateNode.containerInfo),pr(t),null;case 10:return Qn(t.type._context),pr(t),null;case 19:if(xn(eo),null===(o=t.memoizedState))return pr(t),null;if(i=0!==(128&t.flags),null===(c=o.rendering))if(i)mr(o,!1);else{if(0!==Tc||null!==e&&0!==(128&e.flags))for(e=t.child;null!==e;){if(null!==(c=to(e))){for(t.flags|=128,mr(o,!1),null!==(i=c.updateQueue)&&(t.updateQueue=i,t.flags|=4),t.subtreeFlags=0,i=a,a=t.child;null!==a;)e=i,(o=a).flags&=14680066,null===(c=o.alternate)?(o.childLanes=0,o.lanes=e,o.child=null,o.subtreeFlags=0,o.memoizedProps=null,o.memoizedState=null,o.updateQueue=null,o.dependencies=null,o.stateNode=null):(o.childLanes=c.childLanes,o.lanes=c.lanes,o.child=c.child,o.subtreeFlags=0,o.deletions=null,o.memoizedProps=c.memoizedProps,o.memoizedState=c.memoizedState,o.updateQueue=c.updateQueue,o.type=c.type,e=c.dependencies,o.dependencies=null===e?null:{lanes:e.lanes,firstContext:e.firstContext}),a=a.sibling;return Tn(eo,1&eo.current|2),t.child}e=e.sibling}null!==o.tail&&Xe()>zc&&(t.flags|=128,i=!0,mr(o,!1),t.lanes=4194304)}else{if(!i)if(null!==(e=to(c))){if(t.flags|=128,i=!0,null!==(a=e.updateQueue)&&(t.updateQueue=a,t.flags|=4),mr(o,!0),null===o.tail&&"hidden"===o.tailMode&&!c.alternate&&!Ws)return pr(t),null}else 2*Xe()-o.renderingStartTime>zc&&1073741824!==a&&(t.flags|=128,i=!0,mr(o,!1),t.lanes=4194304);o.isBackwards?(c.sibling=t.child,t.child=c):(null!==(a=o.last)?a.sibling=c:t.child=c,o.last=c)}return null!==o.tail?(t=o.tail,o.rendering=t,o.tail=t.sibling,o.renderingStartTime=Xe(),t.sibling=null,a=eo.current,Tn(eo,i?1&a|2:1&a),t):(pr(t),null);case 22:case 23:return el(),i=null!==t.memoizedState,null!==e&&null!==e.memoizedState!==i&&(t.flags|=8192),i&&0!==(1&t.mode)?0!==(1073741824&kc)&&(pr(t),6&t.subtreeFlags&&(t.flags|=8192)):pr(t),null;case 24:case 25:return null}throw Error(s(156,t.tag))}nr=function(e,t){for(var a=t.child;null!==a;){if(5===a.tag||6===a.tag)e.appendChild(a.stateNode);else if(4!==a.tag&&null!==a.child){a.child.return=a,a=a.child;continue}if(a===t)break;for(;null===a.sibling;){if(null===a.return||a.return===t)return;a=a.return}a.sibling.return=a.return,a=a.sibling}},sr=function(e,t,a,i){var n=e.memoizedProps;if(n!==i){e=t.stateNode,Ys(_s.current);var s,o=null;switch(a){case"input":n=Y(e,n),i=Y(e,i),o=[];break;case"select":n=R({},n,{value:void 0}),i=R({},i,{value:void 0}),o=[];break;case"textarea":n=ie(e,n),i=ie(e,i),o=[];break;default:"function"!==typeof n.onClick&&"function"===typeof i.onClick&&(e.onclick=$i)}for(d in be(a,i),a=null,n)if(!i.hasOwnProperty(d)&&n.hasOwnProperty(d)&&null!=n[d])if("style"===d){var c=n[d];for(s in c)c.hasOwnProperty(s)&&(a||(a={}),a[s]="")}else"dangerouslySetInnerHTML"!==d&&"children"!==d&&"suppressContentEditableWarning"!==d&&"suppressHydrationWarning"!==d&&"autoFocus"!==d&&(r.hasOwnProperty(d)?o||(o=[]):(o=o||[]).push(d,null));for(d in i){var l=i[d];if(c=null!=n?n[d]:void 0,i.hasOwnProperty(d)&&l!==c&&(null!=l||null!=c))if("style"===d)if(c){for(s in c)!c.hasOwnProperty(s)||l&&l.hasOwnProperty(s)||(a||(a={}),a[s]="");for(s in l)l.hasOwnProperty(s)&&c[s]!==l[s]&&(a||(a={}),a[s]=l[s])}else a||(o||(o=[]),o.push(d,a)),a=l;else"dangerouslySetInnerHTML"===d?(l=l?l.__html:void 0,c=c?c.__html:void 0,null!=l&&c!==l&&(o=o||[]).push(d,l)):"children"===d?"string"!==typeof l&&"number"!==typeof l||(o=o||[]).push(d,""+l):"suppressContentEditableWarning"!==d&&"suppressHydrationWarning"!==d&&(r.hasOwnProperty(d)?(null!=l&&"onScroll"===d&&Li("scroll",e),o||c===l||(o=[])):(o=o||[]).push(d,l))}a&&(o=o||[]).push("style",a);var d=o;(t.updateQueue=d)&&(t.flags|=4)}},or=function(e,t,a,i){a!==i&&(t.flags|=4)};var gr=v.ReactCurrentOwner,wr=!1;function br(e,t,a,i){t.child=null===e?Gs(t,null,a,i):Us(t,e.child,a,i)}function yr(e,t,a,i,n){a=a.render;var s=t.ref;return Yn(t,n),i=wo(e,t,a,i,s,n),a=bo(),null===e||wr?(Ws&&a&&Ms(t),t.flags|=1,br(e,t,i,n),t.child):(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~n,jr(e,t,n))}function vr(e,t,a,i,n){if(null===e){var s=a.type;return"function"!==typeof s||kl(s)||void 0!==s.defaultProps||null!==a.compare||void 0!==a.defaultProps?((e=Tl(a.type,null,i,t,t.mode,n)).ref=t.ref,e.return=t,t.child=e):(t.tag=15,t.type=s,kr(e,t,s,i,n))}if(s=e.child,0===(e.lanes&n)){var o=s.memoizedProps;if((a=null!==(a=a.compare)?a:si)(o,i)&&e.ref===t.ref)return jr(e,t,n)}return t.flags|=1,(e=xl(s,i)).ref=t.ref,e.return=t,t.child=e}function kr(e,t,a,i,n){if(null!==e&&si(e.memoizedProps,i)&&e.ref===t.ref){if(wr=!1,0===(e.lanes&n))return t.lanes=e.lanes,jr(e,t,n);0!==(131072&e.flags)&&(wr=!0)}return Ar(e,t,a,i,n)}function xr(e,t,a){var i=t.pendingProps,n=i.children,s=null!==e?e.memoizedState:null;if("hidden"===i.mode)if(0===(1&t.mode))t.memoizedState={baseLanes:0,cachePool:null},Tn(xc,kc),kc|=a;else{if(0===(1073741824&a))return e=null!==s?s.baseLanes|a:a,t.lanes=t.childLanes=1073741824,t.memoizedState={baseLanes:e,cachePool:null},t.updateQueue=null,Tn(xc,kc),kc|=e,null;t.memoizedState={baseLanes:0,cachePool:null},i=null!==s?s.baseLanes:a,Tn(xc,kc),kc|=i}else null!==s?(i=s.baseLanes|a,t.memoizedState=null):i=a,Tn(xc,kc),kc|=i;return br(e,t,n,a),t.child}function Tr(e,t){var a=t.ref;(null===e&&null!==a||null!==e&&e.ref!==a)&&(t.flags|=512,t.flags|=2097152)}function Ar(e,t,a,i,n){var s=Wn(a)?qn:Mn.current;return s=In(t,s),Yn(t,n),a=wo(e,t,a,i,s,n),i=bo(),null===e||wr?(Ws&&i&&Ms(t),t.flags|=1,br(e,t,a,n),t.child):(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~n,jr(e,t,n))}function Mr(e,t,a,i,n){if(Wn(a)){var s=!0;Pn(t)}else s=!1;if(Yn(t,n),null===t.stateNode)null!==e&&(e.alternate=null,t.alternate=null,t.flags|=2),hs(t,a,i),ms(t,a,i,n),i=!0;else if(null===e){var o=t.stateNode,r=t.memoizedProps;o.props=r;var c=o.context,l=a.contextType;"object"===typeof l&&null!==l?l=$n(l):l=In(t,l=Wn(a)?qn:Mn.current);var d=a.getDerivedStateFromProps,h="function"===typeof d||"function"===typeof o.getSnapshotBeforeUpdate;h||"function"!==typeof o.UNSAFE_componentWillReceiveProps&&"function"!==typeof o.componentWillReceiveProps||(r!==i||c!==l)&&us(t,o,i,l),Jn=!1;var u=t.memoizedState;o.state=u,ss(t,i,o,n),c=t.memoizedState,r!==i||u!==c||Cn.current||Jn?("function"===typeof d&&(cs(t,a,d,i),c=t.memoizedState),(r=Jn||ds(t,a,r,i,u,c,l))?(h||"function"!==typeof o.UNSAFE_componentWillMount&&"function"!==typeof o.componentWillMount||("function"===typeof o.componentWillMount&&o.componentWillMount(),"function"===typeof o.UNSAFE_componentWillMount&&o.UNSAFE_componentWillMount()),"function"===typeof o.componentDidMount&&(t.flags|=4194308)):("function"===typeof o.componentDidMount&&(t.flags|=4194308),t.memoizedProps=i,t.memoizedState=c),o.props=i,o.state=c,o.context=l,i=r):("function"===typeof o.componentDidMount&&(t.flags|=4194308),i=!1)}else{o=t.stateNode,es(e,t),r=t.memoizedProps,l=t.type===t.elementType?r:Bn(t.type,r),o.props=l,h=t.pendingProps,u=o.context,"object"===typeof(c=a.contextType)&&null!==c?c=$n(c):c=In(t,c=Wn(a)?qn:Mn.current);var m=a.getDerivedStateFromProps;(d="function"===typeof m||"function"===typeof o.getSnapshotBeforeUpdate)||"function"!==typeof o.UNSAFE_componentWillReceiveProps&&"function"!==typeof o.componentWillReceiveProps||(r!==h||u!==c)&&us(t,o,i,c),Jn=!1,u=t.memoizedState,o.state=u,ss(t,i,o,n);var p=t.memoizedState;r!==h||u!==p||Cn.current||Jn?("function"===typeof m&&(cs(t,a,m,i),p=t.memoizedState),(l=Jn||ds(t,a,l,i,u,p,c)||!1)?(d||"function"!==typeof o.UNSAFE_componentWillUpdate&&"function"!==typeof o.componentWillUpdate||("function"===typeof o.componentWillUpdate&&o.componentWillUpdate(i,p,c),"function"===typeof o.UNSAFE_componentWillUpdate&&o.UNSAFE_componentWillUpdate(i,p,c)),"function"===typeof o.componentDidUpdate&&(t.flags|=4),"function"===typeof o.getSnapshotBeforeUpdate&&(t.flags|=1024)):("function"!==typeof o.componentDidUpdate||r===e.memoizedProps&&u===e.memoizedState||(t.flags|=4),"function"!==typeof o.getSnapshotBeforeUpdate||r===e.memoizedProps&&u===e.memoizedState||(t.flags|=1024),t.memoizedProps=i,t.memoizedState=p),o.props=i,o.state=p,o.context=c,i=l):("function"!==typeof o.componentDidUpdate||r===e.memoizedProps&&u===e.memoizedState||(t.flags|=4),"function"!==typeof o.getSnapshotBeforeUpdate||r===e.memoizedProps&&u===e.memoizedState||(t.flags|=1024),i=!1)}return Cr(e,t,a,i,s,n)}function Cr(e,t,a,i,n,s){Tr(e,t);var o=0!==(128&t.flags);if(!i&&!o)return n&&Nn(t,a,!1),jr(e,t,s);i=t.stateNode,gr.current=t;var r=o&&"function"!==typeof a.getDerivedStateFromError?null:i.render();return t.flags|=1,null!==e&&o?(t.child=Us(t,e.child,null,s),t.child=Us(t,null,r,s)):br(e,t,r,s),t.memoizedState=i.state,n&&Nn(t,a,!0),t.child}function qr(e){var t=e.stateNode;t.pendingContext?zn(0,t.pendingContext,t.pendingContext!==t.context):t.context&&zn(0,t.context,!1),$s(e,t.containerInfo)}function Ir(e,t,a,i,n){return Rs(),Os(n),t.flags|=256,br(e,t,a,i),t.child}var Wr={dehydrated:null,treeContext:null,retryLane:0};function Sr(e){return{baseLanes:e,cachePool:null}}function zr(e,t,a){var i,n=t.pendingProps,o=eo.current,r=!1,c=0!==(128&t.flags);if((i=c)||(i=(null===e||null!==e.memoizedState)&&0!==(2&o)),i?(r=!0,t.flags&=-129):null!==e&&null===e.memoizedState||(o|=1),Tn(eo,1&o),null===e)return Ns(t),null!==(e=t.memoizedState)&&null!==(e=e.dehydrated)?(0===(1&t.mode)?t.lanes=1:"$!"===e.data?t.lanes=8:t.lanes=1073741824,null):(o=n.children,e=n.fallback,r?(n=t.mode,r=t.child,o={mode:"hidden",children:o},0===(1&n)&&null!==r?(r.childLanes=0,r.pendingProps=o):r=Ml(o,n,0,null),e=Al(e,n,a,null),r.return=t,e.return=t,r.sibling=e,t.child=r,t.child.memoizedState=Sr(a),t.memoizedState=Wr,e):Er(t,o));if(null!==(o=e.memoizedState)){if(null!==(i=o.dehydrated)){if(c)return 256&t.flags?(t.flags&=-257,Dr(e,t,a,Error(s(422)))):null!==t.memoizedState?(t.child=e.child,t.flags|=128,null):(r=n.fallback,o=t.mode,n=Ml({mode:"visible",children:n.children},o,0,null),(r=Al(r,o,a,null)).flags|=2,n.return=t,r.return=t,n.sibling=r,t.child=n,0!==(1&t.mode)&&Us(t,e.child,null,a),t.child.memoizedState=Sr(a),t.memoizedState=Wr,r);if(0===(1&t.mode))t=Dr(e,t,a,null);else if("$!"===i.data)t=Dr(e,t,a,Error(s(419)));else if(n=0!==(a&e.childLanes),wr||n){if(null!==(n=bc)){switch(a&-a){case 4:r=2;break;case 16:r=8;break;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:r=32;break;case 536870912:r=268435456;break;default:r=0}0!==(n=0!==(r&(n.suspendedLanes|a))?0:r)&&n!==o.retryLane&&(o.retryLane=n,Gc(e,n,-1))}nl(),t=Dr(e,t,a,Error(s(421)))}else"$?"===i.data?(t.flags|=128,t.child=e.child,t=gl.bind(null,e),i._reactRetry=t,t=null):(a=o.treeContext,Is=on(i.nextSibling),qs=t,Ws=!0,Ss=null,null!==a&&(bs[ys++]=ks,bs[ys++]=xs,bs[ys++]=vs,ks=a.id,xs=a.overflow,vs=t),(t=Er(t,t.pendingProps.children)).flags|=4096);return t}return r?(n=Nr(e,t,n.children,n.fallback,a),r=t.child,o=e.child.memoizedState,r.memoizedState=null===o?Sr(a):{baseLanes:o.baseLanes|a,cachePool:null},r.childLanes=e.childLanes&~a,t.memoizedState=Wr,n):(a=Pr(e,t,n.children,a),t.memoizedState=null,a)}return r?(n=Nr(e,t,n.children,n.fallback,a),r=t.child,o=e.child.memoizedState,r.memoizedState=null===o?Sr(a):{baseLanes:o.baseLanes|a,cachePool:null},r.childLanes=e.childLanes&~a,t.memoizedState=Wr,n):(a=Pr(e,t,n.children,a),t.memoizedState=null,a)}function Er(e,t){return(t=Ml({mode:"visible",children:t},e.mode,0,null)).return=e,e.child=t}function Pr(e,t,a,i){var n=e.child;return e=n.sibling,a=xl(n,{mode:"visible",children:a}),0===(1&t.mode)&&(a.lanes=i),a.return=t,a.sibling=null,null!==e&&(null===(i=t.deletions)?(t.deletions=[e],t.flags|=16):i.push(e)),t.child=a}function Nr(e,t,a,i,n){var s=t.mode,o=(e=e.child).sibling,r={mode:"hidden",children:a};return 0===(1&s)&&t.child!==e?((a=t.child).childLanes=0,a.pendingProps=r,t.deletions=null):(a=xl(e,r)).subtreeFlags=14680064&e.subtreeFlags,null!==o?i=xl(o,i):(i=Al(i,s,n,null)).flags|=2,i.return=t,a.return=t,a.sibling=i,t.child=a,i}function Dr(e,t,a,i){return null!==i&&Os(i),Us(t,e.child,null,a),(e=Er(t,t.pendingProps.children)).flags|=2,t.memoizedState=null,e}function Lr(e,t,a){e.lanes|=t;var i=e.alternate;null!==i&&(i.lanes|=t),Kn(e.return,t,a)}function Rr(e,t,a,i,n){var s=e.memoizedState;null===s?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:i,tail:a,tailMode:n}:(s.isBackwards=t,s.rendering=null,s.renderingStartTime=0,s.last=i,s.tail=a,s.tailMode=n)}function Or(e,t,a){var i=t.pendingProps,n=i.revealOrder,s=i.tail;if(br(e,t,i.children,a),0!==(2&(i=eo.current)))i=1&i|2,t.flags|=128;else{if(null!==e&&0!==(128&e.flags))e:for(e=t.child;null!==e;){if(13===e.tag)null!==e.memoizedState&&Lr(e,a,t);else if(19===e.tag)Lr(e,a,t);else if(null!==e.child){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;null===e.sibling;){if(null===e.return||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}i&=1}if(Tn(eo,i),0===(1&t.mode))t.memoizedState=null;else switch(n){case"forwards":for(a=t.child,n=null;null!==a;)null!==(e=a.alternate)&&null===to(e)&&(n=a),a=a.sibling;null===(a=n)?(n=t.child,t.child=null):(n=a.sibling,a.sibling=null),Rr(t,!1,n,a,s);break;case"backwards":for(a=null,n=t.child,t.child=null;null!==n;){if(null!==(e=n.alternate)&&null===to(e)){t.child=n;break}e=n.sibling,n.sibling=a,a=n,n=e}Rr(t,!0,a,null,s);break;case"together":Rr(t,!1,null,null,void 0);break;default:t.memoizedState=null}return t.child}function jr(e,t,a){if(null!==e&&(t.dependencies=e.dependencies),Mc|=t.lanes,0===(a&t.childLanes))return null;if(null!==e&&t.child!==e.child)throw Error(s(153));if(null!==t.child){for(a=xl(e=t.child,e.pendingProps),t.child=a,a.return=t;null!==e.sibling;)e=e.sibling,(a=a.sibling=xl(e,e.pendingProps)).return=t;a.sibling=null}return t.child}function Fr(e,t){switch(Cs(t),t.tag){case 1:return Wn(t.type)&&Sn(),65536&(e=t.flags)?(t.flags=-65537&e|128,t):null;case 3:return Xs(),xn(Cn),xn(Mn),io(),0!==(65536&(e=t.flags))&&0===(128&e)?(t.flags=-65537&e|128,t):null;case 5:return Zs(t),null;case 13:if(xn(eo),null!==(e=t.memoizedState)&&null!==e.dehydrated){if(null===t.alternate)throw Error(s(340));Rs()}return 65536&(e=t.flags)?(t.flags=-65537&e|128,t):null;case 19:return xn(eo),null;case 4:return Xs(),null;case 10:return Qn(t.type._context),null;case 22:case 23:return el(),null;default:return null}}var Br=!1,Hr=!1,Ur="function"===typeof WeakSet?WeakSet:Set,Gr=null;function Vr(e,t){var a=e.ref;if(null!==a)if("function"===typeof a)try{a(null)}catch(i){ml(e,t,i)}else a.current=null}function _r(e,t,a){try{a()}catch(i){ml(e,t,i)}}var Qr=!1;function Kr(e,t,a){var i=t.updateQueue;if(null!==(i=null!==i?i.lastEffect:null)){var n=i=i.next;do{if((n.tag&e)===e){var s=n.destroy;n.destroy=void 0,void 0!==s&&_r(t,a,s)}n=n.next}while(n!==i)}}function Yr(e,t){if(null!==(t=null!==(t=t.updateQueue)?t.lastEffect:null)){var a=t=t.next;do{if((a.tag&e)===e){var i=a.create;a.destroy=i()}a=a.next}while(a!==t)}}function $r(e){var t=e.ref;if(null!==t){var a=e.stateNode;e.tag,e=a,"function"===typeof t?t(e):t.current=e}}function Xr(e,t,a){if(st&&"function"===typeof st.onCommitFiberUnmount)try{st.onCommitFiberUnmount(nt,t)}catch(o){}switch(t.tag){case 0:case 11:case 14:case 15:if(null!==(e=t.updateQueue)&&null!==(e=e.lastEffect)){var i=e=e.next;do{var n=i,s=n.destroy;n=n.tag,void 0!==s&&(0!==(2&n)||0!==(4&n))&&_r(t,a,s),i=i.next}while(i!==e)}break;case 1:if(Vr(t,a),"function"===typeof(e=t.stateNode).componentWillUnmount)try{e.props=t.memoizedProps,e.state=t.memoizedState,e.componentWillUnmount()}catch(o){ml(t,a,o)}break;case 5:Vr(t,a);break;case 4:nc(e,t,a)}}function Jr(e){var t=e.alternate;null!==t&&(e.alternate=null,Jr(t)),e.child=null,e.deletions=null,e.sibling=null,5===e.tag&&(null!==(t=e.stateNode)&&(delete t[ln],delete t[dn],delete t[un],delete t[mn],delete t[pn])),e.stateNode=null,e.return=null,e.dependencies=null,e.memoizedProps=null,e.memoizedState=null,e.pendingProps=null,e.stateNode=null,e.updateQueue=null}function Zr(e){return 5===e.tag||3===e.tag||4===e.tag}function ec(e){e:for(;;){for(;null===e.sibling;){if(null===e.return||Zr(e.return))return null;e=e.return}for(e.sibling.return=e.return,e=e.sibling;5!==e.tag&&6!==e.tag&&18!==e.tag;){if(2&e.flags)continue e;if(null===e.child||4===e.tag)continue e;e.child.return=e,e=e.child}if(!(2&e.flags))return e.stateNode}}function tc(e){e:{for(var t=e.return;null!==t;){if(Zr(t))break e;t=t.return}throw Error(s(160))}var a=t;switch(a.tag){case 5:t=a.stateNode,32&a.flags&&(ue(t,""),a.flags&=-33),ic(e,a=ec(e),t);break;case 3:case 4:t=a.stateNode.containerInfo,ac(e,a=ec(e),t);break;default:throw Error(s(161))}}function ac(e,t,a){var i=e.tag;if(5===i||6===i)e=e.stateNode,t?8===a.nodeType?a.parentNode.insertBefore(e,t):a.insertBefore(e,t):(8===a.nodeType?(t=a.parentNode).insertBefore(e,a):(t=a).appendChild(e),null!==(a=a._reactRootContainer)&&void 0!==a||null!==t.onclick||(t.onclick=$i));else if(4!==i&&null!==(e=e.child))for(ac(e,t,a),e=e.sibling;null!==e;)ac(e,t,a),e=e.sibling}function ic(e,t,a){var i=e.tag;if(5===i||6===i)e=e.stateNode,t?a.insertBefore(e,t):a.appendChild(e);else if(4!==i&&null!==(e=e.child))for(ic(e,t,a),e=e.sibling;null!==e;)ic(e,t,a),e=e.sibling}function nc(e,t,a){for(var i,n,o=t,r=!1;;){if(!r){r=o.return;e:for(;;){if(null===r)throw Error(s(160));switch(i=r.stateNode,r.tag){case 5:n=!1;break e;case 3:case 4:i=i.containerInfo,n=!0;break e}r=r.return}r=!0}if(5===o.tag||6===o.tag){e:for(var c=e,l=o,d=a,h=l;;)if(Xr(c,h,d),null!==h.child&&4!==h.tag)h.child.return=h,h=h.child;else{if(h===l)break e;for(;null===h.sibling;){if(null===h.return||h.return===l)break e;h=h.return}h.sibling.return=h.return,h=h.sibling}n?(c=i,l=o.stateNode,8===c.nodeType?c.parentNode.removeChild(l):c.removeChild(l)):i.removeChild(o.stateNode)}else if(18===o.tag)n?(c=i,l=o.stateNode,8===c.nodeType?sn(c.parentNode,l):1===c.nodeType&&sn(c,l),Bt(c)):sn(i,o.stateNode);else if(4===o.tag){if(null!==o.child){i=o.stateNode.containerInfo,n=!0,o.child.return=o,o=o.child;continue}}else if(Xr(e,o,a),null!==o.child){o.child.return=o,o=o.child;continue}if(o===t)break;for(;null===o.sibling;){if(null===o.return||o.return===t)return;4===(o=o.return).tag&&(r=!1)}o.sibling.return=o.return,o=o.sibling}}function sc(e,t){switch(t.tag){case 0:case 11:case 14:case 15:return Kr(3,t,t.return),Yr(3,t),void Kr(5,t,t.return);case 1:case 12:case 17:return;case 5:var a=t.stateNode;if(null!=a){var i=t.memoizedProps,n=null!==e?e.memoizedProps:i;e=t.type;var o=t.updateQueue;if(t.updateQueue=null,null!==o){for("input"===e&&"radio"===i.type&&null!=i.name&&X(a,i),ye(e,n),t=ye(e,i),n=0;n<o.length;n+=2){var r=o[n],c=o[n+1];"style"===r?ge(a,c):"dangerouslySetInnerHTML"===r?he(a,c):"children"===r?ue(a,c):y(a,r,c,t)}switch(e){case"input":J(a,i);break;case"textarea":se(a,i);break;case"select":e=a._wrapperState.wasMultiple,a._wrapperState.wasMultiple=!!i.multiple,null!=(o=i.value)?ae(a,!!i.multiple,o,!1):e!==!!i.multiple&&(null!=i.defaultValue?ae(a,!!i.multiple,i.defaultValue,!0):ae(a,!!i.multiple,i.multiple?[]:"",!1))}a[dn]=i}}return;case 6:if(null===t.stateNode)throw Error(s(162));return void(t.stateNode.nodeValue=t.memoizedProps);case 3:return void(null!==e&&e.memoizedState.isDehydrated&&Bt(t.stateNode.containerInfo));case 13:case 19:return void oc(t)}throw Error(s(163))}function oc(e){var t=e.updateQueue;if(null!==t){e.updateQueue=null;var a=e.stateNode;null===a&&(a=e.stateNode=new Ur),t.forEach((function(t){var i=wl.bind(null,e,t);a.has(t)||(a.add(t),t.then(i,i))}))}}function rc(e,t,a){Gr=e,cc(e,t,a)}function cc(e,t,a){for(var i=0!==(1&e.mode);null!==Gr;){var n=Gr,s=n.child;if(22===n.tag&&i){var o=null!==n.memoizedState||Br;if(!o){var r=n.alternate,c=null!==r&&null!==r.memoizedState||Hr;r=Br;var l=Hr;if(Br=o,(Hr=c)&&!l)for(Gr=n;null!==Gr;)c=(o=Gr).child,22===o.tag&&null!==o.memoizedState?hc(n):null!==c?(c.return=o,Gr=c):hc(n);for(;null!==s;)Gr=s,cc(s,t,a),s=s.sibling;Gr=n,Br=r,Hr=l}lc(e)}else 0!==(8772&n.subtreeFlags)&&null!==s?(s.return=n,Gr=s):lc(e)}}function lc(e){for(;null!==Gr;){var t=Gr;if(0!==(8772&t.flags)){var a=t.alternate;try{if(0!==(8772&t.flags))switch(t.tag){case 0:case 11:case 15:Hr||Yr(5,t);break;case 1:var i=t.stateNode;if(4&t.flags&&!Hr)if(null===a)i.componentDidMount();else{var n=t.elementType===t.type?a.memoizedProps:Bn(t.type,a.memoizedProps);i.componentDidUpdate(n,a.memoizedState,i.__reactInternalSnapshotBeforeUpdate)}var o=t.updateQueue;null!==o&&os(t,o,i);break;case 3:var r=t.updateQueue;if(null!==r){if(a=null,null!==t.child)switch(t.child.tag){case 5:case 1:a=t.child.stateNode}os(t,r,a)}break;case 5:var c=t.stateNode;if(null===a&&4&t.flags){a=c;var l=t.memoizedProps;switch(t.type){case"button":case"input":case"select":case"textarea":l.autoFocus&&a.focus();break;case"img":l.src&&(a.src=l.src)}}break;case 6:case 4:case 12:case 19:case 17:case 21:case 22:case 23:break;case 13:if(null===t.memoizedState){var d=t.alternate;if(null!==d){var h=d.memoizedState;if(null!==h){var u=h.dehydrated;null!==u&&Bt(u)}}}break;default:throw Error(s(163))}Hr||512&t.flags&&$r(t)}catch(m){ml(t,t.return,m)}}if(t===e){Gr=null;break}if(null!==(a=t.sibling)){a.return=t.return,Gr=a;break}Gr=t.return}}function dc(e){for(;null!==Gr;){var t=Gr;if(t===e){Gr=null;break}var a=t.sibling;if(null!==a){a.return=t.return,Gr=a;break}Gr=t.return}}function hc(e){for(;null!==Gr;){var t=Gr;try{switch(t.tag){case 0:case 11:case 15:var a=t.return;try{Yr(4,t)}catch(c){ml(t,a,c)}break;case 1:var i=t.stateNode;if("function"===typeof i.componentDidMount){var n=t.return;try{i.componentDidMount()}catch(c){ml(t,n,c)}}var s=t.return;try{$r(t)}catch(c){ml(t,s,c)}break;case 5:var o=t.return;try{$r(t)}catch(c){ml(t,o,c)}}}catch(c){ml(t,t.return,c)}if(t===e){Gr=null;break}var r=t.sibling;if(null!==r){r.return=t.return,Gr=r;break}Gr=t.return}}var uc,mc=Math.ceil,pc=v.ReactCurrentDispatcher,fc=v.ReactCurrentOwner,gc=v.ReactCurrentBatchConfig,wc=0,bc=null,yc=null,vc=0,kc=0,xc=kn(0),Tc=0,Ac=null,Mc=0,Cc=0,qc=0,Ic=null,Wc=null,Sc=0,zc=1/0,Ec=!1,Pc=null,Nc=null,Dc=!1,Lc=null,Rc=0,Oc=0,jc=null,Fc=-1,Bc=0;function Hc(){return 0!==(6&wc)?Xe():-1!==Fc?Fc:Fc=Xe()}function Uc(e){return 0===(1&e.mode)?1:0!==(2&wc)&&0!==vc?vc&-vc:null!==Fn.transition?(0===Bc&&(e=lt,0===(4194240&(lt<<=1))&&(lt=64),Bc=e),Bc):0!==(e=bt)?e:e=void 0===(e=window.event)?16:Kt(e.type)}function Gc(e,t,a){if(50<Oc)throw Oc=0,jc=null,Error(s(185));var i=Vc(e,t);return null===i?null:(gt(i,t,a),0!==(2&wc)&&i===bc||(i===bc&&(0===(2&wc)&&(Cc|=t),4===Tc&&$c(i,vc)),_c(i,a),1===t&&0===wc&&0===(1&e.mode)&&(zc=Xe()+500,Ln&&jn())),i)}function Vc(e,t){e.lanes|=t;var a=e.alternate;for(null!==a&&(a.lanes|=t),a=e,e=e.return;null!==e;)e.childLanes|=t,null!==(a=e.alternate)&&(a.childLanes|=t),a=e,e=e.return;return 3===a.tag?a.stateNode:null}function _c(e,t){var a=e.callbackNode;!function(e,t){for(var a=e.suspendedLanes,i=e.pingedLanes,n=e.expirationTimes,s=e.pendingLanes;0<s;){var o=31-ot(s),r=1<<o,c=n[o];-1===c?0!==(r&a)&&0===(r&i)||(n[o]=mt(r,t)):c<=t&&(e.expiredLanes|=r),s&=~r}}(e,t);var i=ut(e,e===bc?vc:0);if(0===i)null!==a&&Ke(a),e.callbackNode=null,e.callbackPriority=0;else if(t=i&-i,e.callbackPriority!==t){if(null!=a&&Ke(a),1===t)0===e.tag?function(e){Ln=!0,On(e)}(Xc.bind(null,e)):On(Xc.bind(null,e)),an((function(){0===wc&&jn()})),a=null;else{switch(yt(i)){case 1:a=Ze;break;case 4:a=et;break;case 16:default:a=tt;break;case 536870912:a=it}a=bl(a,Qc.bind(null,e))}e.callbackPriority=t,e.callbackNode=a}}function Qc(e,t){if(Fc=-1,Bc=0,0!==(6&wc))throw Error(s(327));var a=e.callbackNode;if(hl()&&e.callbackNode!==a)return null;var i=ut(e,e===bc?vc:0);if(0===i)return null;if(0!==(30&i)||0!==(i&e.expiredLanes)||t)t=sl(e,i);else{t=i;var n=wc;wc|=2;var o=il();for(bc===e&&vc===t||(zc=Xe()+500,tl(e,t));;)try{rl();break}catch(c){al(e,c)}_n(),pc.current=o,wc=n,null!==yc?t=0:(bc=null,vc=0,t=Tc)}if(0!==t){if(2===t&&(0!==(n=pt(e))&&(i=n,t=Kc(e,n))),1===t)throw a=Ac,tl(e,0),$c(e,i),_c(e,Xe()),a;if(6===t)$c(e,i);else{if(n=e.current.alternate,0===(30&i)&&!function(e){for(var t=e;;){if(16384&t.flags){var a=t.updateQueue;if(null!==a&&null!==(a=a.stores))for(var i=0;i<a.length;i++){var n=a[i],s=n.getSnapshot;n=n.value;try{if(!ni(s(),n))return!1}catch(r){return!1}}}if(a=t.child,16384&t.subtreeFlags&&null!==a)a.return=t,t=a;else{if(t===e)break;for(;null===t.sibling;){if(null===t.return||t.return===e)return!0;t=t.return}t.sibling.return=t.return,t=t.sibling}}return!0}(n)&&(2===(t=sl(e,i))&&(0!==(o=pt(e))&&(i=o,t=Kc(e,o))),1===t))throw a=Ac,tl(e,0),$c(e,i),_c(e,Xe()),a;switch(e.finishedWork=n,e.finishedLanes=i,t){case 0:case 1:throw Error(s(345));case 2:case 5:dl(e,Wc);break;case 3:if($c(e,i),(130023424&i)===i&&10<(t=Sc+500-Xe())){if(0!==ut(e,0))break;if(((n=e.suspendedLanes)&i)!==i){Hc(),e.pingedLanes|=e.suspendedLanes&n;break}e.timeoutHandle=Zi(dl.bind(null,e,Wc),t);break}dl(e,Wc);break;case 4:if($c(e,i),(4194240&i)===i)break;for(t=e.eventTimes,n=-1;0<i;){var r=31-ot(i);o=1<<r,(r=t[r])>n&&(n=r),i&=~o}if(i=n,10<(i=(120>(i=Xe()-i)?120:480>i?480:1080>i?1080:1920>i?1920:3e3>i?3e3:4320>i?4320:1960*mc(i/1960))-i)){e.timeoutHandle=Zi(dl.bind(null,e,Wc),i);break}dl(e,Wc);break;default:throw Error(s(329))}}}return _c(e,Xe()),e.callbackNode===a?Qc.bind(null,e):null}function Kc(e,t){var a=Ic;return e.current.memoizedState.isDehydrated&&(tl(e,t).flags|=256),2!==(e=sl(e,t))&&(t=Wc,Wc=a,null!==t&&Yc(t)),e}function Yc(e){null===Wc?Wc=e:Wc.push.apply(Wc,e)}function $c(e,t){for(t&=~qc,t&=~Cc,e.suspendedLanes|=t,e.pingedLanes&=~t,e=e.expirationTimes;0<t;){var a=31-ot(t),i=1<<a;e[a]=-1,t&=~i}}function Xc(e){if(0!==(6&wc))throw Error(s(327));hl();var t=ut(e,0);if(0===(1&t))return _c(e,Xe()),null;var a=sl(e,t);if(0!==e.tag&&2===a){var i=pt(e);0!==i&&(t=i,a=Kc(e,i))}if(1===a)throw a=Ac,tl(e,0),$c(e,t),_c(e,Xe()),a;if(6===a)throw Error(s(345));return e.finishedWork=e.current.alternate,e.finishedLanes=t,dl(e,Wc),_c(e,Xe()),null}function Jc(e,t){var a=wc;wc|=1;try{return e(t)}finally{0===(wc=a)&&(zc=Xe()+500,Ln&&jn())}}function Zc(e){null!==Lc&&0===Lc.tag&&0===(6&wc)&&hl();var t=wc;wc|=1;var a=gc.transition,i=bt;try{if(gc.transition=null,bt=1,e)return e()}finally{bt=i,gc.transition=a,0===(6&(wc=t))&&jn()}}function el(){kc=xc.current,xn(xc)}function tl(e,t){e.finishedWork=null,e.finishedLanes=0;var a=e.timeoutHandle;if(-1!==a&&(e.timeoutHandle=-1,en(a)),null!==yc)for(a=yc.return;null!==a;){var i=a;switch(Cs(i),i.tag){case 1:null!==(i=i.type.childContextTypes)&&void 0!==i&&Sn();break;case 3:Xs(),xn(Cn),xn(Mn),io();break;case 5:Zs(i);break;case 4:Xs();break;case 13:case 19:xn(eo);break;case 10:Qn(i.type._context);break;case 22:case 23:el()}a=a.return}if(bc=e,yc=e=xl(e.current,null),vc=kc=t,Tc=0,Ac=null,qc=Cc=Mc=0,Wc=Ic=null,null!==Xn){for(t=0;t<Xn.length;t++)if(null!==(i=(a=Xn[t]).interleaved)){a.interleaved=null;var n=i.next,s=a.pending;if(null!==s){var o=s.next;s.next=n,i.next=o}a.pending=i}Xn=null}return e}function al(e,t){for(;;){var a=yc;try{if(_n(),no.current=Jo,ho){for(var i=ro.memoizedState;null!==i;){var n=i.queue;null!==n&&(n.pending=null),i=i.next}ho=!1}if(oo=0,lo=co=ro=null,uo=!1,mo=0,fc.current=null,null===a||null===a.return){Tc=1,Ac=t,yc=null;break}e:{var o=e,r=a.return,c=a,l=t;if(t=vc,c.flags|=32768,null!==l&&"object"===typeof l&&"function"===typeof l.then){var d=l,h=c,u=h.tag;if(0===(1&h.mode)&&(0===u||11===u||15===u)){var m=h.alternate;m?(h.updateQueue=m.updateQueue,h.memoizedState=m.memoizedState,h.lanes=m.lanes):(h.updateQueue=null,h.memoizedState=null)}var p=hr(r);if(null!==p){p.flags&=-257,ur(p,r,c,0,t),1&p.mode&&dr(o,d,t),l=d;var f=(t=p).updateQueue;if(null===f){var g=new Set;g.add(l),t.updateQueue=g}else f.add(l);break e}if(0===(1&t)){dr(o,d,t),nl();break e}l=Error(s(426))}else if(Ws&&1&c.mode){var w=hr(r);if(null!==w){0===(65536&w.flags)&&(w.flags|=256),ur(w,r,c,0,t),Os(l);break e}}o=l,4!==Tc&&(Tc=2),null===Ic?Ic=[o]:Ic.push(o),l=ar(l,c),c=r;do{switch(c.tag){case 3:c.flags|=65536,t&=-t,c.lanes|=t,ns(c,cr(0,l,t));break e;case 1:o=l;var b=c.type,y=c.stateNode;if(0===(128&c.flags)&&("function"===typeof b.getDerivedStateFromError||null!==y&&"function"===typeof y.componentDidCatch&&(null===Nc||!Nc.has(y)))){c.flags|=65536,t&=-t,c.lanes|=t,ns(c,lr(c,o,t));break e}}c=c.return}while(null!==c)}ll(a)}catch(v){t=v,yc===a&&null!==a&&(yc=a=a.return);continue}break}}function il(){var e=pc.current;return pc.current=Jo,null===e?Jo:e}function nl(){0!==Tc&&3!==Tc&&2!==Tc||(Tc=4),null===bc||0===(268435455&Mc)&&0===(268435455&Cc)||$c(bc,vc)}function sl(e,t){var a=wc;wc|=2;var i=il();for(bc===e&&vc===t||tl(e,t);;)try{ol();break}catch(n){al(e,n)}if(_n(),wc=a,pc.current=i,null!==yc)throw Error(s(261));return bc=null,vc=0,Tc}function ol(){for(;null!==yc;)cl(yc)}function rl(){for(;null!==yc&&!Ye();)cl(yc)}function cl(e){var t=uc(e.alternate,e,kc);e.memoizedProps=e.pendingProps,null===t?ll(e):yc=t,fc.current=null}function ll(e){var t=e;do{var a=t.alternate;if(e=t.return,0===(32768&t.flags)){if(null!==(a=fr(a,t,kc)))return void(yc=a)}else{if(null!==(a=Fr(a,t)))return a.flags&=32767,void(yc=a);if(null===e)return Tc=6,void(yc=null);e.flags|=32768,e.subtreeFlags=0,e.deletions=null}if(null!==(t=t.sibling))return void(yc=t);yc=t=e}while(null!==t);0===Tc&&(Tc=5)}function dl(e,t){var a=bt,i=gc.transition;try{gc.transition=null,bt=1,function(e,t,a){do{hl()}while(null!==Lc);if(0!==(6&wc))throw Error(s(327));var i=e.finishedWork,n=e.finishedLanes;if(null===i)return null;if(e.finishedWork=null,e.finishedLanes=0,i===e.current)throw Error(s(177));e.callbackNode=null,e.callbackPriority=0;var o=i.lanes|i.childLanes;if(function(e,t){var a=e.pendingLanes&~t;e.pendingLanes=t,e.suspendedLanes=0,e.pingedLanes=0,e.expiredLanes&=t,e.mutableReadLanes&=t,e.entangledLanes&=t,t=e.entanglements;var i=e.eventTimes;for(e=e.expirationTimes;0<a;){var n=31-ot(a),s=1<<n;t[n]=0,i[n]=-1,e[n]=-1,a&=~s}}(e,o),e===bc&&(yc=bc=null,vc=0),0===(2064&i.subtreeFlags)&&0===(2064&i.flags)||Dc||(Dc=!0,bl(tt,(function(){return hl(),null}))),o=0!==(15990&i.flags),0!==(15990&i.subtreeFlags)||o){o=gc.transition,gc.transition=null;var r=bt;bt=1;var c=wc;wc|=4,fc.current=null,function(e,t){if(di(e=li())){if("selectionStart"in e)var a={start:e.selectionStart,end:e.selectionEnd};else e:{var i=(a=(a=e.ownerDocument)&&a.defaultView||window).getSelection&&a.getSelection();if(i&&0!==i.rangeCount){a=i.anchorNode;var n=i.anchorOffset,o=i.focusNode;i=i.focusOffset;try{a.nodeType,o.nodeType}catch(x){a=null;break e}var r=0,c=-1,l=-1,d=0,h=0,u=e,m=null;t:for(;;){for(var p;u!==a||0!==n&&3!==u.nodeType||(c=r+n),u!==o||0!==i&&3!==u.nodeType||(l=r+i),3===u.nodeType&&(r+=u.nodeValue.length),null!==(p=u.firstChild);)m=u,u=p;for(;;){if(u===e)break t;if(m===a&&++d===n&&(c=r),m===o&&++h===i&&(l=r),null!==(p=u.nextSibling))break;m=(u=m).parentNode}u=p}a=-1===c||-1===l?null:{start:c,end:l}}else a=null}a=a||{start:0,end:0}}else a=null;for(Xi={focusedElem:e,selectionRange:a},Gr=t;null!==Gr;)if(e=(t=Gr).child,0!==(1028&t.subtreeFlags)&&null!==e)e.return=t,Gr=e;else for(;null!==Gr;){t=Gr;try{var f=t.alternate;if(0!==(1024&t.flags))switch(t.tag){case 0:case 11:case 15:case 5:case 6:case 4:case 17:break;case 1:if(null!==f){var g=f.memoizedProps,w=f.memoizedState,b=t.stateNode,y=b.getSnapshotBeforeUpdate(t.elementType===t.type?g:Bn(t.type,g),w);b.__reactInternalSnapshotBeforeUpdate=y}break;case 3:var v=t.stateNode.containerInfo;if(1===v.nodeType)v.textContent="";else if(9===v.nodeType){var k=v.body;null!=k&&(k.textContent="")}break;default:throw Error(s(163))}}catch(x){ml(t,t.return,x)}if(null!==(e=t.sibling)){e.return=t.return,Gr=e;break}Gr=t.return}f=Qr,Qr=!1}(e,i),function(e,t){for(Gr=t;null!==Gr;){var a=(t=Gr).deletions;if(null!==a)for(var i=0;i<a.length;i++){var n=a[i];try{nc(e,n,t);var s=n.alternate;null!==s&&(s.return=null),n.return=null}catch(A){ml(n,t,A)}}if(a=t.child,0!==(12854&t.subtreeFlags)&&null!==a)a.return=t,Gr=a;else for(;null!==Gr;){t=Gr;try{var o=t.flags;if(32&o&&ue(t.stateNode,""),512&o){var r=t.alternate;if(null!==r){var c=r.ref;null!==c&&("function"===typeof c?c(null):c.current=null)}}if(8192&o)switch(t.tag){case 13:if(null!==t.memoizedState){var l=t.alternate;null!==l&&null!==l.memoizedState||(Sc=Xe())}break;case 22:var d=null!==t.memoizedState,h=t.alternate,u=null!==h&&null!==h.memoizedState;e:{n=d;for(var m=null,p=i=a=t;;){if(5===p.tag){if(null===m){m=p;var f=p.stateNode;if(n){var g=f.style;"function"===typeof g.setProperty?g.setProperty("display","none","important"):g.display="none"}else{var w=p.stateNode,b=p.memoizedProps.style,y=void 0!==b&&null!==b&&b.hasOwnProperty("display")?b.display:null;w.style.display=fe("display",y)}}}else if(6===p.tag)null===m&&(p.stateNode.nodeValue=n?"":p.memoizedProps);else if((22!==p.tag&&23!==p.tag||null===p.memoizedState||p===i)&&null!==p.child){p.child.return=p,p=p.child;continue}if(p===i)break;for(;null===p.sibling;){if(null===p.return||p.return===i)break e;m===p&&(m=null),p=p.return}m===p&&(m=null),p.sibling.return=p.return,p=p.sibling}}if(d&&!u&&0!==(1&a.mode)){Gr=a;for(var v=a.child;null!==v;){for(a=Gr=v;null!==Gr;){var k=(i=Gr).child;switch(i.tag){case 0:case 11:case 14:case 15:Kr(4,i,i.return);break;case 1:Vr(i,i.return);var x=i.stateNode;if("function"===typeof x.componentWillUnmount){var T=i.return;try{x.props=i.memoizedProps,x.state=i.memoizedState,x.componentWillUnmount()}catch(A){ml(i,T,A)}}break;case 5:Vr(i,i.return);break;case 22:if(null!==i.memoizedState){dc(a);continue}}null!==k?(k.return=i,Gr=k):dc(a)}v=v.sibling}}}switch(4102&o){case 2:tc(t),t.flags&=-3;break;case 6:tc(t),t.flags&=-3,sc(t.alternate,t);break;case 4096:t.flags&=-4097;break;case 4100:t.flags&=-4097,sc(t.alternate,t);break;case 4:sc(t.alternate,t)}}catch(A){ml(t,t.return,A)}if(null!==(a=t.sibling)){a.return=t.return,Gr=a;break}Gr=t.return}}}(e,i),hi(Xi),Xi=null,e.current=i,rc(i,e,n),$e(),wc=c,bt=r,gc.transition=o}else e.current=i;if(Dc&&(Dc=!1,Lc=e,Rc=n),0===(o=e.pendingLanes)&&(Nc=null),function(e){if(st&&"function"===typeof st.onCommitFiberRoot)try{st.onCommitFiberRoot(nt,e,void 0,128===(128&e.current.flags))}catch(t){}}(i.stateNode),_c(e,Xe()),null!==t)for(a=e.onRecoverableError,i=0;i<t.length;i++)a(t[i]);if(Ec)throw Ec=!1,e=Pc,Pc=null,e;0!==(1&Rc)&&0!==e.tag&&hl(),0!==(1&(o=e.pendingLanes))?e===jc?Oc++:(Oc=0,jc=e):Oc=0,jn()}(e,t,a)}finally{gc.transition=i,bt=a}return null}function hl(){if(null!==Lc){var e=yt(Rc),t=gc.transition,a=bt;try{if(gc.transition=null,bt=16>e?16:e,null===Lc)var i=!1;else{if(e=Lc,Lc=null,Rc=0,0!==(6&wc))throw Error(s(331));var n=wc;for(wc|=4,Gr=e.current;null!==Gr;){var o=Gr,r=o.child;if(0!==(16&Gr.flags)){var c=o.deletions;if(null!==c){for(var l=0;l<c.length;l++){var d=c[l];for(Gr=d;null!==Gr;){var h=Gr;switch(h.tag){case 0:case 11:case 15:Kr(8,h,o)}var u=h.child;if(null!==u)u.return=h,Gr=u;else for(;null!==Gr;){var m=(h=Gr).sibling,p=h.return;if(Jr(h),h===d){Gr=null;break}if(null!==m){m.return=p,Gr=m;break}Gr=p}}}var f=o.alternate;if(null!==f){var g=f.child;if(null!==g){f.child=null;do{var w=g.sibling;g.sibling=null,g=w}while(null!==g)}}Gr=o}}if(0!==(2064&o.subtreeFlags)&&null!==r)r.return=o,Gr=r;else e:for(;null!==Gr;){if(0!==(2048&(o=Gr).flags))switch(o.tag){case 0:case 11:case 15:Kr(9,o,o.return)}var b=o.sibling;if(null!==b){b.return=o.return,Gr=b;break e}Gr=o.return}}var y=e.current;for(Gr=y;null!==Gr;){var v=(r=Gr).child;if(0!==(2064&r.subtreeFlags)&&null!==v)v.return=r,Gr=v;else e:for(r=y;null!==Gr;){if(0!==(2048&(c=Gr).flags))try{switch(c.tag){case 0:case 11:case 15:Yr(9,c)}}catch(x){ml(c,c.return,x)}if(c===r){Gr=null;break e}var k=c.sibling;if(null!==k){k.return=c.return,Gr=k;break e}Gr=c.return}}if(wc=n,jn(),st&&"function"===typeof st.onPostCommitFiberRoot)try{st.onPostCommitFiberRoot(nt,e)}catch(x){}i=!0}return i}finally{bt=a,gc.transition=t}}return!1}function ul(e,t,a){as(e,t=cr(0,t=ar(a,t),1)),t=Hc(),null!==(e=Vc(e,1))&&(gt(e,1,t),_c(e,t))}function ml(e,t,a){if(3===e.tag)ul(e,e,a);else for(;null!==t;){if(3===t.tag){ul(t,e,a);break}if(1===t.tag){var i=t.stateNode;if("function"===typeof t.type.getDerivedStateFromError||"function"===typeof i.componentDidCatch&&(null===Nc||!Nc.has(i))){as(t,e=lr(t,e=ar(a,e),1)),e=Hc(),null!==(t=Vc(t,1))&&(gt(t,1,e),_c(t,e));break}}t=t.return}}function pl(e,t,a){var i=e.pingCache;null!==i&&i.delete(t),t=Hc(),e.pingedLanes|=e.suspendedLanes&a,bc===e&&(vc&a)===a&&(4===Tc||3===Tc&&(130023424&vc)===vc&&500>Xe()-Sc?tl(e,0):qc|=a),_c(e,t)}function fl(e,t){0===t&&(0===(1&e.mode)?t=1:(t=dt,0===(130023424&(dt<<=1))&&(dt=4194304)));var a=Hc();null!==(e=Vc(e,t))&&(gt(e,t,a),_c(e,a))}function gl(e){var t=e.memoizedState,a=0;null!==t&&(a=t.retryLane),fl(e,a)}function wl(e,t){var a=0;switch(e.tag){case 13:var i=e.stateNode,n=e.memoizedState;null!==n&&(a=n.retryLane);break;case 19:i=e.stateNode;break;default:throw Error(s(314))}null!==i&&i.delete(t),fl(e,a)}function bl(e,t){return Qe(e,t)}function yl(e,t,a,i){this.tag=e,this.key=a,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=i,this.subtreeFlags=this.flags=0,this.deletions=null,this.childLanes=this.lanes=0,this.alternate=null}function vl(e,t,a,i){return new yl(e,t,a,i)}function kl(e){return!(!(e=e.prototype)||!e.isReactComponent)}function xl(e,t){var a=e.alternate;return null===a?((a=vl(e.tag,t,e.key,e.mode)).elementType=e.elementType,a.type=e.type,a.stateNode=e.stateNode,a.alternate=e,e.alternate=a):(a.pendingProps=t,a.type=e.type,a.flags=0,a.subtreeFlags=0,a.deletions=null),a.flags=14680064&e.flags,a.childLanes=e.childLanes,a.lanes=e.lanes,a.child=e.child,a.memoizedProps=e.memoizedProps,a.memoizedState=e.memoizedState,a.updateQueue=e.updateQueue,t=e.dependencies,a.dependencies=null===t?null:{lanes:t.lanes,firstContext:t.firstContext},a.sibling=e.sibling,a.index=e.index,a.ref=e.ref,a}function Tl(e,t,a,i,n,o){var r=2;if(i=e,"function"===typeof e)kl(e)&&(r=1);else if("string"===typeof e)r=5;else e:switch(e){case T:return Al(a.children,n,o,t);case A:r=8,n|=8;break;case M:return(e=vl(12,a,t,2|n)).elementType=M,e.lanes=o,e;case W:return(e=vl(13,a,t,n)).elementType=W,e.lanes=o,e;case S:return(e=vl(19,a,t,n)).elementType=S,e.lanes=o,e;case P:return Ml(a,n,o,t);default:if("object"===typeof e&&null!==e)switch(e.$$typeof){case C:r=10;break e;case q:r=9;break e;case I:r=11;break e;case z:r=14;break e;case E:r=16,i=null;break e}throw Error(s(130,null==e?e:typeof e,""))}return(t=vl(r,a,t,n)).elementType=e,t.type=i,t.lanes=o,t}function Al(e,t,a,i){return(e=vl(7,e,i,t)).lanes=a,e}function Ml(e,t,a,i){return(e=vl(22,e,i,t)).elementType=P,e.lanes=a,e.stateNode={},e}function Cl(e,t,a){return(e=vl(6,e,null,t)).lanes=a,e}function ql(e,t,a){return(t=vl(4,null!==e.children?e.children:[],e.key,t)).lanes=a,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}function Il(e,t,a,i,n){this.tag=t,this.containerInfo=e,this.finishedWork=this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.callbackNode=this.pendingContext=this.context=null,this.callbackPriority=0,this.eventTimes=ft(0),this.expirationTimes=ft(-1),this.entangledLanes=this.finishedLanes=this.mutableReadLanes=this.expiredLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=ft(0),this.identifierPrefix=i,this.onRecoverableError=n,this.mutableSourceEagerHydrationData=null}function Wl(e,t,a,i,n,s,o,r,c){return e=new Il(e,t,a,r,c),1===t?(t=1,!0===s&&(t|=8)):t=0,s=vl(3,null,null,t),e.current=s,s.stateNode=e,s.memoizedState={element:i,isDehydrated:a,cache:null,transitions:null},Zn(s),e}function Sl(e,t,a){var i=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:x,key:null==i?null:""+i,children:e,containerInfo:t,implementation:a}}function zl(e){if(!e)return An;e:{if(He(e=e._reactInternals)!==e||1!==e.tag)throw Error(s(170));var t=e;do{switch(t.tag){case 3:t=t.stateNode.context;break e;case 1:if(Wn(t.type)){t=t.stateNode.__reactInternalMemoizedMergedChildContext;break e}}t=t.return}while(null!==t);throw Error(s(171))}if(1===e.tag){var a=e.type;if(Wn(a))return En(e,a,t)}return t}function El(e,t,a,i,n,s,o,r,c){return(e=Wl(a,i,!0,e,0,s,0,r,c)).context=zl(null),a=e.current,(s=ts(i=Hc(),n=Uc(a))).callback=void 0!==t&&null!==t?t:null,as(a,s),e.current.lanes=n,gt(e,n,i),_c(e,i),e}function Pl(e,t,a,i){var n=t.current,s=Hc(),o=Uc(n);return a=zl(a),null===t.context?t.context=a:t.pendingContext=a,(t=ts(s,o)).payload={element:e},null!==(i=void 0===i?null:i)&&(t.callback=i),as(n,t),null!==(e=Gc(n,o,s))&&is(e,n,o),o}function Nl(e){return(e=e.current).child?(e.child.tag,e.child.stateNode):null}function Dl(e,t){if(null!==(e=e.memoizedState)&&null!==e.dehydrated){var a=e.retryLane;e.retryLane=0!==a&&a<t?a:t}}function Ll(e,t){Dl(e,t),(e=e.alternate)&&Dl(e,t)}uc=function(e,t,a){if(null!==e)if(e.memoizedProps!==t.pendingProps||Cn.current)wr=!0;else{if(0===(e.lanes&a)&&0===(128&t.flags))return wr=!1,function(e,t,a){switch(t.tag){case 3:qr(t),Rs();break;case 5:Js(t);break;case 1:Wn(t.type)&&Pn(t);break;case 4:$s(t,t.stateNode.containerInfo);break;case 10:var i=t.type._context,n=t.memoizedProps.value;Tn(Hn,i._currentValue),i._currentValue=n;break;case 13:if(null!==(i=t.memoizedState))return null!==i.dehydrated?(Tn(eo,1&eo.current),t.flags|=128,null):0!==(a&t.child.childLanes)?zr(e,t,a):(Tn(eo,1&eo.current),null!==(e=jr(e,t,a))?e.sibling:null);Tn(eo,1&eo.current);break;case 19:if(i=0!==(a&t.childLanes),0!==(128&e.flags)){if(i)return Or(e,t,a);t.flags|=128}if(null!==(n=t.memoizedState)&&(n.rendering=null,n.tail=null,n.lastEffect=null),Tn(eo,eo.current),i)break;return null;case 22:case 23:return t.lanes=0,xr(e,t,a)}return jr(e,t,a)}(e,t,a);wr=0!==(131072&e.flags)}else wr=!1,Ws&&0!==(1048576&t.flags)&&As(t,ws,t.index);switch(t.lanes=0,t.tag){case 2:var i=t.type;null!==e&&(e.alternate=null,t.alternate=null,t.flags|=2),e=t.pendingProps;var n=In(t,Mn.current);Yn(t,a),n=wo(null,t,i,e,n,a);var o=bo();return t.flags|=1,"object"===typeof n&&null!==n&&"function"===typeof n.render&&void 0===n.$$typeof?(t.tag=1,t.memoizedState=null,t.updateQueue=null,Wn(i)?(o=!0,Pn(t)):o=!1,t.memoizedState=null!==n.state&&void 0!==n.state?n.state:null,Zn(t),n.updater=ls,t.stateNode=n,n._reactInternals=t,ms(t,i,e,a),t=Cr(null,t,i,!0,o,a)):(t.tag=0,Ws&&o&&Ms(t),br(null,t,n,a),t=t.child),t;case 16:i=t.elementType;e:{switch(null!==e&&(e.alternate=null,t.alternate=null,t.flags|=2),e=t.pendingProps,i=(n=i._init)(i._payload),t.type=i,n=t.tag=function(e){if("function"===typeof e)return kl(e)?1:0;if(void 0!==e&&null!==e){if((e=e.$$typeof)===I)return 11;if(e===z)return 14}return 2}(i),e=Bn(i,e),n){case 0:t=Ar(null,t,i,e,a);break e;case 1:t=Mr(null,t,i,e,a);break e;case 11:t=yr(null,t,i,e,a);break e;case 14:t=vr(null,t,i,Bn(i.type,e),a);break e}throw Error(s(306,i,""))}return t;case 0:return i=t.type,n=t.pendingProps,Ar(e,t,i,n=t.elementType===i?n:Bn(i,n),a);case 1:return i=t.type,n=t.pendingProps,Mr(e,t,i,n=t.elementType===i?n:Bn(i,n),a);case 3:e:{if(qr(t),null===e)throw Error(s(387));i=t.pendingProps,n=(o=t.memoizedState).element,es(e,t),ss(t,i,null,a);var r=t.memoizedState;if(i=r.element,o.isDehydrated){if(o={element:i,isDehydrated:!1,cache:r.cache,transitions:r.transitions},t.updateQueue.baseState=o,t.memoizedState=o,256&t.flags){t=Ir(e,t,i,a,n=Error(s(423)));break e}if(i!==n){t=Ir(e,t,i,a,n=Error(s(424)));break e}for(Is=on(t.stateNode.containerInfo.firstChild),qs=t,Ws=!0,Ss=null,a=Gs(t,null,i,a),t.child=a;a;)a.flags=-3&a.flags|4096,a=a.sibling}else{if(Rs(),i===n){t=jr(e,t,a);break e}br(e,t,i,a)}t=t.child}return t;case 5:return Js(t),null===e&&Ns(t),i=t.type,n=t.pendingProps,o=null!==e?e.memoizedProps:null,r=n.children,Ji(i,n)?r=null:null!==o&&Ji(i,o)&&(t.flags|=32),Tr(e,t),br(e,t,r,a),t.child;case 6:return null===e&&Ns(t),null;case 13:return zr(e,t,a);case 4:return $s(t,t.stateNode.containerInfo),i=t.pendingProps,null===e?t.child=Us(t,null,i,a):br(e,t,i,a),t.child;case 11:return i=t.type,n=t.pendingProps,yr(e,t,i,n=t.elementType===i?n:Bn(i,n),a);case 7:return br(e,t,t.pendingProps,a),t.child;case 8:case 12:return br(e,t,t.pendingProps.children,a),t.child;case 10:e:{if(i=t.type._context,n=t.pendingProps,o=t.memoizedProps,r=n.value,Tn(Hn,i._currentValue),i._currentValue=r,null!==o)if(ni(o.value,r)){if(o.children===n.children&&!Cn.current){t=jr(e,t,a);break e}}else for(null!==(o=t.child)&&(o.return=t);null!==o;){var c=o.dependencies;if(null!==c){r=o.child;for(var l=c.firstContext;null!==l;){if(l.context===i){if(1===o.tag){(l=ts(-1,a&-a)).tag=2;var d=o.updateQueue;if(null!==d){var h=(d=d.shared).pending;null===h?l.next=l:(l.next=h.next,h.next=l),d.pending=l}}o.lanes|=a,null!==(l=o.alternate)&&(l.lanes|=a),Kn(o.return,a,t),c.lanes|=a;break}l=l.next}}else if(10===o.tag)r=o.type===t.type?null:o.child;else if(18===o.tag){if(null===(r=o.return))throw Error(s(341));r.lanes|=a,null!==(c=r.alternate)&&(c.lanes|=a),Kn(r,a,t),r=o.sibling}else r=o.child;if(null!==r)r.return=o;else for(r=o;null!==r;){if(r===t){r=null;break}if(null!==(o=r.sibling)){o.return=r.return,r=o;break}r=r.return}o=r}br(e,t,n.children,a),t=t.child}return t;case 9:return n=t.type,i=t.pendingProps.children,Yn(t,a),i=i(n=$n(n)),t.flags|=1,br(e,t,i,a),t.child;case 14:return n=Bn(i=t.type,t.pendingProps),vr(e,t,i,n=Bn(i.type,n),a);case 15:return kr(e,t,t.type,t.pendingProps,a);case 17:return i=t.type,n=t.pendingProps,n=t.elementType===i?n:Bn(i,n),null!==e&&(e.alternate=null,t.alternate=null,t.flags|=2),t.tag=1,Wn(i)?(e=!0,Pn(t)):e=!1,Yn(t,a),hs(t,i,n),ms(t,i,n,a),Cr(null,t,i,!0,e,a);case 19:return Or(e,t,a);case 22:return xr(e,t,a)}throw Error(s(156,t.tag))};var Rl="function"===typeof reportError?reportError:function(e){console.error(e)};function Ol(e){this._internalRoot=e}function jl(e){this._internalRoot=e}function Fl(e){return!(!e||1!==e.nodeType&&9!==e.nodeType&&11!==e.nodeType)}function Bl(e){return!(!e||1!==e.nodeType&&9!==e.nodeType&&11!==e.nodeType&&(8!==e.nodeType||" react-mount-point-unstable "!==e.nodeValue))}function Hl(){}function Ul(e,t,a,i,n){var s=a._reactRootContainer;if(s){var o=s;if("function"===typeof n){var r=n;n=function(){var e=Nl(o);r.call(e)}}Pl(t,o,e,n)}else o=function(e,t,a,i,n){if(n){if("function"===typeof i){var s=i;i=function(){var e=Nl(o);s.call(e)}}var o=El(t,i,e,0,null,!1,0,"",Hl);return e._reactRootContainer=o,e[hn]=o.current,ji(8===e.nodeType?e.parentNode:e),Zc(),o}for(;n=e.lastChild;)e.removeChild(n);if("function"===typeof i){var r=i;i=function(){var e=Nl(c);r.call(e)}}var c=Wl(e,0,!1,null,0,!1,0,"",Hl);return e._reactRootContainer=c,e[hn]=c.current,ji(8===e.nodeType?e.parentNode:e),Zc((function(){Pl(t,c,a,i)})),c}(a,t,e,n,i);return Nl(o)}jl.prototype.render=Ol.prototype.render=function(e){var t=this._internalRoot;if(null===t)throw Error(s(409));Pl(e,t,null,null)},jl.prototype.unmount=Ol.prototype.unmount=function(){var e=this._internalRoot;if(null!==e){this._internalRoot=null;var t=e.containerInfo;Zc((function(){Pl(null,e,null,null)})),t[hn]=null}},jl.prototype.unstable_scheduleHydration=function(e){if(e){var t=Tt();e={blockedOn:null,target:e,priority:t};for(var a=0;a<Et.length&&0!==t&&t<Et[a].priority;a++);Et.splice(a,0,e),0===a&&Lt(e)}},vt=function(e){switch(e.tag){case 3:var t=e.stateNode;if(t.current.memoizedState.isDehydrated){var a=ht(t.pendingLanes);0!==a&&(wt(t,1|a),_c(t,Xe()),0===(6&wc)&&(zc=Xe()+500,jn()))}break;case 13:var i=Hc();Zc((function(){return Gc(e,1,i)})),Ll(e,1)}},kt=function(e){13===e.tag&&(Gc(e,134217728,Hc()),Ll(e,134217728))},xt=function(e){if(13===e.tag){var t=Hc(),a=Uc(e);Gc(e,a,t),Ll(e,a)}},Tt=function(){return bt},At=function(e,t){var a=bt;try{return bt=e,t()}finally{bt=a}},xe=function(e,t,a){switch(t){case"input":if(J(e,a),t=a.name,"radio"===a.type&&null!=t){for(a=e;a.parentNode;)a=a.parentNode;for(a=a.querySelectorAll("input[name="+JSON.stringify(""+t)+'][type="radio"]'),t=0;t<a.length;t++){var i=a[t];if(i!==e&&i.form===e.form){var n=bn(i);if(!n)throw Error(s(90));Q(i),J(i,n)}}}break;case"textarea":se(e,a);break;case"select":null!=(t=a.value)&&ae(e,!!a.multiple,t,!1)}},Ie=Jc,We=Zc;var Gl={usingClientEntryPoint:!1,Events:[gn,wn,bn,Ce,qe,Jc]},Vl={findFiberByHostInstance:fn,bundleType:0,version:"18.0.0-fc46dba67-20220329",rendererPackageName:"react-dom"},_l={bundleType:Vl.bundleType,version:Vl.version,rendererPackageName:Vl.rendererPackageName,rendererConfig:Vl.rendererConfig,overrideHookState:null,overrideHookStateDeletePath:null,overrideHookStateRenamePath:null,overrideProps:null,overridePropsDeletePath:null,overridePropsRenamePath:null,setErrorHandler:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:v.ReactCurrentDispatcher,findHostInstanceByFiber:function(e){return null===(e=Ve(e))?null:e.stateNode},findFiberByHostInstance:Vl.findFiberByHostInstance||function(){return null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null,reconcilerVersion:"18.0.0-fc46dba67-20220329"};if("undefined"!==typeof __REACT_DEVTOOLS_GLOBAL_HOOK__){var Ql=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!Ql.isDisabled&&Ql.supportsFiber)try{nt=Ql.inject(_l),st=Ql}catch(de){}}t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=Gl,t.createPortal=function(e,t){var a=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;if(!Fl(t))throw Error(s(200));return Sl(e,t,null,a)},t.createRoot=function(e,t){if(!Fl(e))throw Error(s(299));var a=!1,i="",n=Rl;return null!==t&&void 0!==t&&(!0===t.unstable_strictMode&&(a=!0),void 0!==t.identifierPrefix&&(i=t.identifierPrefix),void 0!==t.onRecoverableError&&(n=t.onRecoverableError)),t=Wl(e,1,!1,null,0,a,0,i,n),e[hn]=t.current,ji(8===e.nodeType?e.parentNode:e),new Ol(t)},t.findDOMNode=function(e){if(null==e)return null;if(1===e.nodeType)return e;var t=e._reactInternals;if(void 0===t){if("function"===typeof e.render)throw Error(s(188));throw e=Object.keys(e).join(","),Error(s(268,e))}return e=null===(e=Ve(t))?null:e.stateNode},t.flushSync=function(e){return Zc(e)},t.hydrate=function(e,t,a){if(!Bl(t))throw Error(s(200));return Ul(null,e,t,!0,a)},t.hydrateRoot=function(e,t,a){if(!Fl(e))throw Error(s(405));var i=null!=a&&a.hydratedSources||null,n=!1,o="",r=Rl;if(null!==a&&void 0!==a&&(!0===a.unstable_strictMode&&(n=!0),void 0!==a.identifierPrefix&&(o=a.identifierPrefix),void 0!==a.onRecoverableError&&(r=a.onRecoverableError)),t=El(t,null,e,1,null!=a?a:null,n,0,o,r),e[hn]=t.current,ji(e),i)for(e=0;e<i.length;e++)n=(n=(a=i[e])._getVersion)(a._source),null==t.mutableSourceEagerHydrationData?t.mutableSourceEagerHydrationData=[a,n]:t.mutableSourceEagerHydrationData.push(a,n);return new jl(t)},t.render=function(e,t,a){if(!Bl(t))throw Error(s(200));return Ul(null,e,t,!1,a)},t.unmountComponentAtNode=function(e){if(!Bl(e))throw Error(s(40));return!!e._reactRootContainer&&(Zc((function(){Ul(null,null,e,!1,(function(){e._reactRootContainer=null,e[hn]=null}))})),!0)},t.unstable_batchedUpdates=Jc,t.unstable_renderSubtreeIntoContainer=function(e,t,a,i){if(!Bl(a))throw Error(s(200));if(null==e||void 0===e._reactInternals)throw Error(s(38));return Ul(e,t,a,!1,i)},t.version="18.0.0-fc46dba67-20220329"},250:function(e,t,a){"use strict";var i=a(164);t.createRoot=i.createRoot,t.hydrateRoot=i.hydrateRoot},164:function(e,t,a){"use strict";!function e(){if("undefined"!==typeof __REACT_DEVTOOLS_GLOBAL_HOOK__&&"function"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE)try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(e)}catch(t){console.error(t)}}(),e.exports=a(463)},263:function(e,t,a){e.exports=function(e){var t={};function a(i){if(t[i])return t[i].exports;var n=t[i]={exports:{},id:i,loaded:!1};return e[i].call(n.exports,n,n.exports,a),n.loaded=!0,n.exports}return a.m=e,a.c=t,a.p="",a(0)}([function(e,t,a){e.exports=a(1)},function(e,t,a){"use strict";Object.defineProperty(t,"__esModule",{value:!0});var i,n=a(2),s=(i=n)&&i.__esModule?i:{default:i};t.default=s.default,e.exports=t.default},function(e,t,a){"use strict";Object.defineProperty(t,"__esModule",{value:!0});var i=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var a=arguments[t];for(var i in a)Object.prototype.hasOwnProperty.call(a,i)&&(e[i]=a[i])}return e};function n(e){return e&&e.__esModule?e:{default:e}}t.default=l;var s=a(3),o=n(a(4)),r=a(14),c=n(a(15));function l(e){var t=e.activeClassName,a=void 0===t?"":t,n=e.activeIndex,o=void 0===n?-1:n,l=e.activeStyle,d=e.autoEscape,h=e.caseSensitive,u=void 0!==h&&h,m=e.className,p=e.findChunks,f=e.highlightClassName,g=void 0===f?"":f,w=e.highlightStyle,b=void 0===w?{}:w,y=e.highlightTag,v=void 0===y?"mark":y,k=e.sanitize,x=e.searchWords,T=e.textToHighlight,A=e.unhighlightClassName,M=void 0===A?"":A,C=e.unhighlightStyle,q=function(e,t){var a={};for(var i in e)t.indexOf(i)>=0||Object.prototype.hasOwnProperty.call(e,i)&&(a[i]=e[i]);return a}(e,["activeClassName","activeIndex","activeStyle","autoEscape","caseSensitive","className","findChunks","highlightClassName","highlightStyle","highlightTag","sanitize","searchWords","textToHighlight","unhighlightClassName","unhighlightStyle"]),I=(0,s.findAll)({autoEscape:d,caseSensitive:u,findChunks:p,sanitize:k,searchWords:x,textToHighlight:T}),W=v,S=-1,z="",E=void 0,P=(0,c.default)((function(e){var t={};for(var a in e)t[a.toLowerCase()]=e[a];return t}));return(0,r.createElement)("span",i({className:m},q,{children:I.map((function(e,t){var i=T.substr(e.start,e.end-e.start);if(e.highlight){S++;var n=void 0;n="object"===typeof g?u?g[i]:(g=P(g))[i.toLowerCase()]:g;var s=S===+o;z=n+" "+(s?a:""),E=!0===s&&null!=l?Object.assign({},b,l):b;var c={children:i,className:z,key:t,style:E};return"string"!==typeof W&&(c.highlightIndex=S),(0,r.createElement)(W,c)}return(0,r.createElement)("span",{children:i,className:M,key:t,style:C})}))}))}l.propTypes={activeClassName:o.default.string,activeIndex:o.default.number,activeStyle:o.default.object,autoEscape:o.default.bool,className:o.default.string,findChunks:o.default.func,highlightClassName:o.default.oneOfType([o.default.object,o.default.string]),highlightStyle:o.default.object,highlightTag:o.default.oneOfType([o.default.node,o.default.func,o.default.string]),sanitize:o.default.func,searchWords:o.default.arrayOf(o.default.oneOfType([o.default.string,o.default.instanceOf(RegExp)])).isRequired,textToHighlight:o.default.string.isRequired,unhighlightClassName:o.default.string,unhighlightStyle:o.default.object},e.exports=t.default},function(e,t){e.exports=function(e){var t={};function a(i){if(t[i])return t[i].exports;var n=t[i]={exports:{},id:i,loaded:!1};return e[i].call(n.exports,n,n.exports,a),n.loaded=!0,n.exports}return a.m=e,a.c=t,a.p="",a(0)}([function(e,t,a){e.exports=a(1)},function(e,t,a){"use strict";Object.defineProperty(t,"__esModule",{value:!0});var i=a(2);Object.defineProperty(t,"combineChunks",{enumerable:!0,get:function(){return i.combineChunks}}),Object.defineProperty(t,"fillInChunks",{enumerable:!0,get:function(){return i.fillInChunks}}),Object.defineProperty(t,"findAll",{enumerable:!0,get:function(){return i.findAll}}),Object.defineProperty(t,"findChunks",{enumerable:!0,get:function(){return i.findChunks}})},function(e,t){"use strict";Object.defineProperty(t,"__esModule",{value:!0});t.findAll=function(e){var t=e.autoEscape,s=e.caseSensitive,o=void 0!==s&&s,r=e.findChunks,c=void 0===r?i:r,l=e.sanitize,d=e.searchWords,h=e.textToHighlight;return n({chunksToHighlight:a({chunks:c({autoEscape:t,caseSensitive:o,sanitize:l,searchWords:d,textToHighlight:h})}),totalLength:h?h.length:0})};var a=t.combineChunks=function(e){var t=e.chunks;return t=t.sort((function(e,t){return e.start-t.start})).reduce((function(e,t){if(0===e.length)return[t];var a=e.pop();if(t.start<=a.end){var i=Math.max(a.end,t.end);e.push({start:a.start,end:i})}else e.push(a,t);return e}),[])},i=function(e){var t=e.autoEscape,a=e.caseSensitive,i=e.sanitize,n=void 0===i?s:i,o=e.searchWords,r=e.textToHighlight;return r=n(r),o.filter((function(e){return e})).reduce((function(e,i){i=n(i),t&&(i=i.replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g,"\\$&"));for(var s=new RegExp(i,a?"g":"gi"),o=void 0;o=s.exec(r);){var c=o.index,l=s.lastIndex;l>c&&e.push({start:c,end:l}),o.index==s.lastIndex&&s.lastIndex++}return e}),[])};t.findChunks=i;var n=t.fillInChunks=function(e){var t=e.chunksToHighlight,a=e.totalLength,i=[],n=function(e,t,a){t-e>0&&i.push({start:e,end:t,highlight:a})};if(0===t.length)n(0,a,!1);else{var s=0;t.forEach((function(e){n(s,e.start,!1),n(e.start,e.end,!0),s=e.end})),n(s,a,!1)}return i};function s(e){return e}}])},function(e,t,a){(function(t){if("production"!==t.env.NODE_ENV){var i="function"===typeof Symbol&&Symbol.for&&Symbol.for("react.element")||60103;e.exports=a(6)((function(e){return"object"===typeof e&&null!==e&&e.$$typeof===i}),!0)}else e.exports=a(13)()}).call(t,a(5))},function(e,t){var a,i,n=e.exports={};function s(){throw new Error("setTimeout has not been defined")}function o(){throw new Error("clearTimeout has not been defined")}function r(e){if(a===setTimeout)return setTimeout(e,0);if((a===s||!a)&&setTimeout)return a=setTimeout,setTimeout(e,0);try{return a(e,0)}catch(t){try{return a.call(null,e,0)}catch(t){return a.call(this,e,0)}}}!function(){try{a="function"===typeof setTimeout?setTimeout:s}catch(e){a=s}try{i="function"===typeof clearTimeout?clearTimeout:o}catch(e){i=o}}();var c,l=[],d=!1,h=-1;function u(){d&&c&&(d=!1,c.length?l=c.concat(l):h=-1,l.length&&m())}function m(){if(!d){var e=r(u);d=!0;for(var t=l.length;t;){for(c=l,l=[];++h<t;)c&&c[h].run();h=-1,t=l.length}c=null,d=!1,function(e){if(i===clearTimeout)return clearTimeout(e);if((i===o||!i)&&clearTimeout)return i=clearTimeout,clearTimeout(e);try{i(e)}catch(t){try{return i.call(null,e)}catch(t){return i.call(this,e)}}}(e)}}function p(e,t){this.fun=e,this.array=t}function f(){}n.nextTick=function(e){var t=new Array(arguments.length-1);if(arguments.length>1)for(var a=1;a<arguments.length;a++)t[a-1]=arguments[a];l.push(new p(e,t)),1!==l.length||d||r(m)},p.prototype.run=function(){this.fun.apply(null,this.array)},n.title="browser",n.browser=!0,n.env={},n.argv=[],n.version="",n.versions={},n.on=f,n.addListener=f,n.once=f,n.off=f,n.removeListener=f,n.removeAllListeners=f,n.emit=f,n.prependListener=f,n.prependOnceListener=f,n.listeners=function(e){return[]},n.binding=function(e){throw new Error("process.binding is not supported")},n.cwd=function(){return"/"},n.chdir=function(e){throw new Error("process.chdir is not supported")},n.umask=function(){return 0}},function(e,t,a){(function(t){"use strict";var i=a(7),n=a(8),s=a(9),o=a(10),r=a(11),c=a(12);e.exports=function(e,a){var l="function"===typeof Symbol&&Symbol.iterator;var d="<<anonymous>>",h={array:f("array"),bool:f("boolean"),func:f("function"),number:f("number"),object:f("object"),string:f("string"),symbol:f("symbol"),any:p(i.thatReturnsNull),arrayOf:function(e){return p((function(t,a,i,n,s){if("function"!==typeof e)return new m("Property `"+s+"` of component `"+i+"` has invalid PropType notation inside arrayOf.");var o=t[a];if(!Array.isArray(o))return new m("Invalid "+n+" `"+s+"` of type `"+w(o)+"` supplied to `"+i+"`, expected an array.");for(var c=0;c<o.length;c++){var l=e(o,c,i,n,s+"["+c+"]",r);if(l instanceof Error)return l}return null}))},element:p((function(t,a,i,n,s){var o=t[a];return e(o)?null:new m("Invalid "+n+" `"+s+"` of type `"+w(o)+"` supplied to `"+i+"`, expected a single ReactElement.")})),instanceOf:function(e){return p((function(t,a,i,n,s){if(!(t[a]instanceof e)){var o=e.name||d;return new m("Invalid "+n+" `"+s+"` of type `"+(((r=t[a]).constructor&&r.constructor.name?r.constructor.name:d)+"` supplied to `")+i+"`, expected instance of `"+o+"`.")}var r;return null}))},node:p((function(e,t,a,i,n){return g(e[t])?null:new m("Invalid "+i+" `"+n+"` supplied to `"+a+"`, expected a ReactNode.")})),objectOf:function(e){return p((function(t,a,i,n,s){if("function"!==typeof e)return new m("Property `"+s+"` of component `"+i+"` has invalid PropType notation inside objectOf.");var o=t[a],c=w(o);if("object"!==c)return new m("Invalid "+n+" `"+s+"` of type `"+c+"` supplied to `"+i+"`, expected an object.");for(var l in o)if(o.hasOwnProperty(l)){var d=e(o,l,i,n,s+"."+l,r);if(d instanceof Error)return d}return null}))},oneOf:function(e){if(!Array.isArray(e))return"production"!==t.env.NODE_ENV&&s(!1,"Invalid argument supplied to oneOf, expected an instance of array."),i.thatReturnsNull;return p((function(t,a,i,n,s){for(var o=t[a],r=0;r<e.length;r++)if(u(o,e[r]))return null;return new m("Invalid "+n+" `"+s+"` of value `"+o+"` supplied to `"+i+"`, expected one of "+JSON.stringify(e)+".")}))},oneOfType:function(e){if(!Array.isArray(e))return"production"!==t.env.NODE_ENV&&s(!1,"Invalid argument supplied to oneOfType, expected an instance of array."),i.thatReturnsNull;for(var a=0;a<e.length;a++){var n=e[a];if("function"!==typeof n)return s(!1,"Invalid argument supplied to oneOfType. Expected an array of check functions, but received %s at index %s.",y(n),a),i.thatReturnsNull}return p((function(t,a,i,n,s){for(var o=0;o<e.length;o++)if(null==(0,e[o])(t,a,i,n,s,r))return null;return new m("Invalid "+n+" `"+s+"` supplied to `"+i+"`.")}))},shape:function(e){return p((function(t,a,i,n,s){var o=t[a],c=w(o);if("object"!==c)return new m("Invalid "+n+" `"+s+"` of type `"+c+"` supplied to `"+i+"`, expected `object`.");for(var l in e){var d=e[l];if(d){var h=d(o,l,i,n,s+"."+l,r);if(h)return h}}return null}))},exact:function(e){return p((function(t,a,i,n,s){var c=t[a],l=w(c);if("object"!==l)return new m("Invalid "+n+" `"+s+"` of type `"+l+"` supplied to `"+i+"`, expected `object`.");var d=o({},t[a],e);for(var h in d){var u=e[h];if(!u)return new m("Invalid "+n+" `"+s+"` key `"+h+"` supplied to `"+i+"`.\nBad object: "+JSON.stringify(t[a],null,"  ")+"\nValid keys: "+JSON.stringify(Object.keys(e),null,"  "));var p=u(c,h,i,n,s+"."+h,r);if(p)return p}return null}))}};function u(e,t){return e===t?0!==e||1/e===1/t:e!==e&&t!==t}function m(e){this.message=e,this.stack=""}function p(e){if("production"!==t.env.NODE_ENV)var i={},o=0;function c(c,l,h,u,p,f,g){if(u=u||d,f=f||h,g!==r)if(a)n(!1,"Calling PropTypes validators directly is not supported by the `prop-types` package. Use `PropTypes.checkPropTypes()` to call them. Read more at http://fb.me/use-check-prop-types");else if("production"!==t.env.NODE_ENV&&"undefined"!==typeof console){var w=u+":"+h;!i[w]&&o<3&&(s(!1,"You are manually calling a React.PropTypes validation function for the `%s` prop on `%s`. This is deprecated and will throw in the standalone `prop-types` package. You may be seeing this warning due to a third-party PropTypes library. See https://fb.me/react-warning-dont-call-proptypes for details.",f,u),i[w]=!0,o++)}return null==l[h]?c?null===l[h]?new m("The "+p+" `"+f+"` is marked as required in `"+u+"`, but its value is `null`."):new m("The "+p+" `"+f+"` is marked as required in `"+u+"`, but its value is `undefined`."):null:e(l,h,u,p,f)}var l=c.bind(null,!1);return l.isRequired=c.bind(null,!0),l}function f(e){return p((function(t,a,i,n,s,o){var r=t[a];return w(r)!==e?new m("Invalid "+n+" `"+s+"` of type `"+b(r)+"` supplied to `"+i+"`, expected `"+e+"`."):null}))}function g(t){switch(typeof t){case"number":case"string":case"undefined":return!0;case"boolean":return!t;case"object":if(Array.isArray(t))return t.every(g);if(null===t||e(t))return!0;var a=function(e){var t=e&&(l&&e[l]||e["@@iterator"]);if("function"===typeof t)return t}(t);if(!a)return!1;var i,n=a.call(t);if(a!==t.entries){for(;!(i=n.next()).done;)if(!g(i.value))return!1}else for(;!(i=n.next()).done;){var s=i.value;if(s&&!g(s[1]))return!1}return!0;default:return!1}}function w(e){var t=typeof e;return Array.isArray(e)?"array":e instanceof RegExp?"object":function(e,t){return"symbol"===e||"Symbol"===t["@@toStringTag"]||"function"===typeof Symbol&&t instanceof Symbol}(t,e)?"symbol":t}function b(e){if("undefined"===typeof e||null===e)return""+e;var t=w(e);if("object"===t){if(e instanceof Date)return"date";if(e instanceof RegExp)return"regexp"}return t}function y(e){var t=b(e);switch(t){case"array":case"object":return"an "+t;case"boolean":case"date":case"regexp":return"a "+t;default:return t}}return m.prototype=Error.prototype,h.checkPropTypes=c,h.PropTypes=h,h}}).call(t,a(5))},function(e,t){"use strict";function a(e){return function(){return e}}var i=function(){};i.thatReturns=a,i.thatReturnsFalse=a(!1),i.thatReturnsTrue=a(!0),i.thatReturnsNull=a(null),i.thatReturnsThis=function(){return this},i.thatReturnsArgument=function(e){return e},e.exports=i},function(e,t,a){(function(t){"use strict";var a=function(e){};"production"!==t.env.NODE_ENV&&(a=function(e){if(void 0===e)throw new Error("invariant requires an error message argument")}),e.exports=function(e,t,i,n,s,o,r,c){if(a(t),!e){var l;if(void 0===t)l=new Error("Minified exception occurred; use the non-minified dev environment for the full error message and additional helpful warnings.");else{var d=[i,n,s,o,r,c],h=0;(l=new Error(t.replace(/%s/g,(function(){return d[h++]})))).name="Invariant Violation"}throw l.framesToPop=1,l}}}).call(t,a(5))},function(e,t,a){(function(t){"use strict";var i=a(7);if("production"!==t.env.NODE_ENV){var n=function(e){for(var t=arguments.length,a=Array(t>1?t-1:0),i=1;i<t;i++)a[i-1]=arguments[i];var n=0,s="Warning: "+e.replace(/%s/g,(function(){return a[n++]}));"undefined"!==typeof console&&console.error(s);try{throw new Error(s)}catch(o){}};i=function(e,t){if(void 0===t)throw new Error("`warning(condition, format, ...args)` requires a warning message argument");if(0!==t.indexOf("Failed Composite propType: ")&&!e){for(var a=arguments.length,i=Array(a>2?a-2:0),s=2;s<a;s++)i[s-2]=arguments[s];n.apply(void 0,[t].concat(i))}}}e.exports=i}).call(t,a(5))},function(e,t){"use strict";var a=Object.getOwnPropertySymbols,i=Object.prototype.hasOwnProperty,n=Object.prototype.propertyIsEnumerable;function s(e){if(null===e||void 0===e)throw new TypeError("Object.assign cannot be called with null or undefined");return Object(e)}e.exports=function(){try{if(!Object.assign)return!1;var e=new String("abc");if(e[5]="de","5"===Object.getOwnPropertyNames(e)[0])return!1;for(var t={},a=0;a<10;a++)t["_"+String.fromCharCode(a)]=a;if("0123456789"!==Object.getOwnPropertyNames(t).map((function(e){return t[e]})).join(""))return!1;var i={};return"abcdefghijklmnopqrst".split("").forEach((function(e){i[e]=e})),"abcdefghijklmnopqrst"===Object.keys(Object.assign({},i)).join("")}catch(n){return!1}}()?Object.assign:function(e,t){for(var o,r,c=s(e),l=1;l<arguments.length;l++){for(var d in o=Object(arguments[l]))i.call(o,d)&&(c[d]=o[d]);if(a){r=a(o);for(var h=0;h<r.length;h++)n.call(o,r[h])&&(c[r[h]]=o[r[h]])}}return c}},function(e,t){"use strict";e.exports="SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED"},function(e,t,a){(function(t){"use strict";if("production"!==t.env.NODE_ENV)var i=a(8),n=a(9),s=a(11),o={};e.exports=function(e,a,r,c,l){if("production"!==t.env.NODE_ENV)for(var d in e)if(e.hasOwnProperty(d)){var h;try{i("function"===typeof e[d],"%s: %s type `%s` is invalid; it must be a function, usually from the `prop-types` package, but received `%s`.",c||"React class",r,d,typeof e[d]),h=e[d](a,d,c,r,null,s)}catch(m){h=m}if(n(!h||h instanceof Error,"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).",c||"React class",r,d,typeof h),h instanceof Error&&!(h.message in o)){o[h.message]=!0;var u=l?l():"";n(!1,"Failed %s type: %s%s",r,h.message,null!=u?u:"")}}}}).call(t,a(5))},function(e,t,a){"use strict";var i=a(7),n=a(8),s=a(11);e.exports=function(){function e(e,t,a,i,o,r){r!==s&&n(!1,"Calling PropTypes validators directly is not supported by the `prop-types` package. Use PropTypes.checkPropTypes() to call them. Read more at http://fb.me/use-check-prop-types")}function t(){return e}e.isRequired=e;var a={array:e,bool:e,func:e,number:e,object:e,string:e,symbol:e,any:e,arrayOf:t,element:e,instanceOf:t,node:e,objectOf:t,oneOf:t,oneOfType:t,shape:t,exact:t};return a.checkPropTypes=i,a.PropTypes=a,a}},function(e,t){e.exports=a(791)},function(e,t){"use strict";var a=function(e,t){return e===t};e.exports=function(e){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:a,i=void 0,n=[],s=void 0,o=!1,r=function(e,a){return t(e,n[a])},c=function(){for(var t=arguments.length,a=Array(t),c=0;c<t;c++)a[c]=arguments[c];return o&&i===this&&a.length===n.length&&a.every(r)?s:(o=!0,i=this,n=a,s=e.apply(this,a))};return c}}])},374:function(e,t,a){"use strict";var i=a(791),n=Symbol.for("react.element"),s=Symbol.for("react.fragment"),o=Object.prototype.hasOwnProperty,r=i.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.ReactCurrentOwner,c={key:!0,ref:!0,__self:!0,__source:!0};function l(e,t,a){var i,s={},l=null,d=null;for(i in void 0!==a&&(l=""+a),void 0!==t.key&&(l=""+t.key),void 0!==t.ref&&(d=t.ref),t)o.call(t,i)&&!c.hasOwnProperty(i)&&(s[i]=t[i]);if(e&&e.defaultProps)for(i in t=e.defaultProps)void 0===s[i]&&(s[i]=t[i]);return{$$typeof:n,type:e,key:l,ref:d,props:s,_owner:r.current}}t.jsx=l,t.jsxs=l},117:function(e,t){"use strict";var a=Symbol.for("react.element"),i=Symbol.for("react.portal"),n=Symbol.for("react.fragment"),s=Symbol.for("react.strict_mode"),o=Symbol.for("react.profiler"),r=Symbol.for("react.provider"),c=Symbol.for("react.context"),l=Symbol.for("react.forward_ref"),d=Symbol.for("react.suspense"),h=Symbol.for("react.memo"),u=Symbol.for("react.lazy"),m=Symbol.iterator;var p={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},f=Object.assign,g={};function w(e,t,a){this.props=e,this.context=t,this.refs=g,this.updater=a||p}function b(){}function y(e,t,a){this.props=e,this.context=t,this.refs=g,this.updater=a||p}w.prototype.isReactComponent={},w.prototype.setState=function(e,t){if("object"!==typeof e&&"function"!==typeof e&&null!=e)throw Error("setState(...): takes an object of state variables to update or a function which returns an object of state variables.");this.updater.enqueueSetState(this,e,t,"setState")},w.prototype.forceUpdate=function(e){this.updater.enqueueForceUpdate(this,e,"forceUpdate")},b.prototype=w.prototype;var v=y.prototype=new b;v.constructor=y,f(v,w.prototype),v.isPureReactComponent=!0;var k=Array.isArray,x=Object.prototype.hasOwnProperty,T={current:null},A={key:!0,ref:!0,__self:!0,__source:!0};function M(e,t,i){var n,s={},o=null,r=null;if(null!=t)for(n in void 0!==t.ref&&(r=t.ref),void 0!==t.key&&(o=""+t.key),t)x.call(t,n)&&!A.hasOwnProperty(n)&&(s[n]=t[n]);var c=arguments.length-2;if(1===c)s.children=i;else if(1<c){for(var l=Array(c),d=0;d<c;d++)l[d]=arguments[d+2];s.children=l}if(e&&e.defaultProps)for(n in c=e.defaultProps)void 0===s[n]&&(s[n]=c[n]);return{$$typeof:a,type:e,key:o,ref:r,props:s,_owner:T.current}}function C(e){return"object"===typeof e&&null!==e&&e.$$typeof===a}var q=/\/+/g;function I(e,t){return"object"===typeof e&&null!==e&&null!=e.key?function(e){var t={"=":"=0",":":"=2"};return"$"+e.replace(/[=:]/g,(function(e){return t[e]}))}(""+e.key):t.toString(36)}function W(e,t,n,s,o){var r=typeof e;"undefined"!==r&&"boolean"!==r||(e=null);var c=!1;if(null===e)c=!0;else switch(r){case"string":case"number":c=!0;break;case"object":switch(e.$$typeof){case a:case i:c=!0}}if(c)return o=o(c=e),e=""===s?"."+I(c,0):s,k(o)?(n="",null!=e&&(n=e.replace(q,"$&/")+"/"),W(o,t,n,"",(function(e){return e}))):null!=o&&(C(o)&&(o=function(e,t){return{$$typeof:a,type:e.type,key:t,ref:e.ref,props:e.props,_owner:e._owner}}(o,n+(!o.key||c&&c.key===o.key?"":(""+o.key).replace(q,"$&/")+"/")+e)),t.push(o)),1;if(c=0,s=""===s?".":s+":",k(e))for(var l=0;l<e.length;l++){var d=s+I(r=e[l],l);c+=W(r,t,n,d,o)}else if(d=function(e){return null===e||"object"!==typeof e?null:"function"===typeof(e=m&&e[m]||e["@@iterator"])?e:null}(e),"function"===typeof d)for(e=d.call(e),l=0;!(r=e.next()).done;)c+=W(r=r.value,t,n,d=s+I(r,l++),o);else if("object"===r)throw t=String(e),Error("Objects are not valid as a React child (found: "+("[object Object]"===t?"object with keys {"+Object.keys(e).join(", ")+"}":t)+"). If you meant to render a collection of children, use an array instead.");return c}function S(e,t,a){if(null==e)return e;var i=[],n=0;return W(e,i,"","",(function(e){return t.call(a,e,n++)})),i}function z(e){if(-1===e._status){var t=e._result;(t=t()).then((function(t){0!==e._status&&-1!==e._status||(e._status=1,e._result=t)}),(function(t){0!==e._status&&-1!==e._status||(e._status=2,e._result=t)})),-1===e._status&&(e._status=0,e._result=t)}if(1===e._status)return e._result.default;throw e._result}var E={current:null},P={transition:null},N={ReactCurrentDispatcher:E,ReactCurrentBatchConfig:P,ReactCurrentOwner:T};t.Children={map:S,forEach:function(e,t,a){S(e,(function(){t.apply(this,arguments)}),a)},count:function(e){var t=0;return S(e,(function(){t++})),t},toArray:function(e){return S(e,(function(e){return e}))||[]},only:function(e){if(!C(e))throw Error("React.Children.only expected to receive a single React element child.");return e}},t.Component=w,t.Fragment=n,t.Profiler=o,t.PureComponent=y,t.StrictMode=s,t.Suspense=d,t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=N,t.cloneElement=function(e,t,i){if(null===e||void 0===e)throw Error("React.cloneElement(...): The argument must be a React element, but you passed "+e+".");var n=f({},e.props),s=e.key,o=e.ref,r=e._owner;if(null!=t){if(void 0!==t.ref&&(o=t.ref,r=T.current),void 0!==t.key&&(s=""+t.key),e.type&&e.type.defaultProps)var c=e.type.defaultProps;for(l in t)x.call(t,l)&&!A.hasOwnProperty(l)&&(n[l]=void 0===t[l]&&void 0!==c?c[l]:t[l])}var l=arguments.length-2;if(1===l)n.children=i;else if(1<l){c=Array(l);for(var d=0;d<l;d++)c[d]=arguments[d+2];n.children=c}return{$$typeof:a,type:e.type,key:s,ref:o,props:n,_owner:r}},t.createContext=function(e){return(e={$$typeof:c,_currentValue:e,_currentValue2:e,_threadCount:0,Provider:null,Consumer:null,_defaultValue:null,_globalName:null}).Provider={$$typeof:r,_context:e},e.Consumer=e},t.createElement=M,t.createFactory=function(e){var t=M.bind(null,e);return t.type=e,t},t.createRef=function(){return{current:null}},t.forwardRef=function(e){return{$$typeof:l,render:e}},t.isValidElement=C,t.lazy=function(e){return{$$typeof:u,_payload:{_status:-1,_result:e},_init:z}},t.memo=function(e,t){return{$$typeof:h,type:e,compare:void 0===t?null:t}},t.startTransition=function(e){var t=P.transition;P.transition={};try{e()}finally{P.transition=t}},t.unstable_act=function(){throw Error("act(...) is not supported in production builds of React.")},t.useCallback=function(e,t){return E.current.useCallback(e,t)},t.useContext=function(e){return E.current.useContext(e)},t.useDebugValue=function(){},t.useDeferredValue=function(e){return E.current.useDeferredValue(e)},t.useEffect=function(e,t){return E.current.useEffect(e,t)},t.useId=function(){return E.current.useId()},t.useImperativeHandle=function(e,t,a){return E.current.useImperativeHandle(e,t,a)},t.useInsertionEffect=function(e,t){return E.current.useInsertionEffect(e,t)},t.useLayoutEffect=function(e,t){return E.current.useLayoutEffect(e,t)},t.useMemo=function(e,t){return E.current.useMemo(e,t)},t.useReducer=function(e,t,a){return E.current.useReducer(e,t,a)},t.useRef=function(e){return E.current.useRef(e)},t.useState=function(e){return E.current.useState(e)},t.useSyncExternalStore=function(e,t,a){return E.current.useSyncExternalStore(e,t,a)},t.useTransition=function(){return E.current.useTransition()},t.version="18.0.0-fc46dba67-20220329"},791:function(e,t,a){"use strict";e.exports=a(117)},184:function(e,t,a){"use strict";e.exports=a(374)},813:function(e,t){"use strict";function a(e,t){var a=e.length;e.push(t);e:for(;0<a;){var i=a-1>>>1,n=e[i];if(!(0<s(n,t)))break e;e[i]=t,e[a]=n,a=i}}function i(e){return 0===e.length?null:e[0]}function n(e){if(0===e.length)return null;var t=e[0],a=e.pop();if(a!==t){e[0]=a;e:for(var i=0,n=e.length,o=n>>>1;i<o;){var r=2*(i+1)-1,c=e[r],l=r+1,d=e[l];if(0>s(c,a))l<n&&0>s(d,c)?(e[i]=d,e[l]=a,i=l):(e[i]=c,e[r]=a,i=r);else{if(!(l<n&&0>s(d,a)))break e;e[i]=d,e[l]=a,i=l}}}return t}function s(e,t){var a=e.sortIndex-t.sortIndex;return 0!==a?a:e.id-t.id}if("object"===typeof performance&&"function"===typeof performance.now){var o=performance;t.unstable_now=function(){return o.now()}}else{var r=Date,c=r.now();t.unstable_now=function(){return r.now()-c}}var l=[],d=[],h=1,u=null,m=3,p=!1,f=!1,g=!1,w="function"===typeof setTimeout?setTimeout:null,b="function"===typeof clearTimeout?clearTimeout:null,y="undefined"!==typeof setImmediate?setImmediate:null;function v(e){for(var t=i(d);null!==t;){if(null===t.callback)n(d);else{if(!(t.startTime<=e))break;n(d),t.sortIndex=t.expirationTime,a(l,t)}t=i(d)}}function k(e){if(g=!1,v(e),!f)if(null!==i(l))f=!0,P(x);else{var t=i(d);null!==t&&N(k,t.startTime-e)}}function x(e,a){f=!1,g&&(g=!1,b(C),C=-1),p=!0;var s=m;try{for(v(a),u=i(l);null!==u&&(!(u.expirationTime>a)||e&&!W());){var o=u.callback;if("function"===typeof o){u.callback=null,m=u.priorityLevel;var r=o(u.expirationTime<=a);a=t.unstable_now(),"function"===typeof r?u.callback=r:u===i(l)&&n(l),v(a)}else n(l);u=i(l)}if(null!==u)var c=!0;else{var h=i(d);null!==h&&N(k,h.startTime-a),c=!1}return c}finally{u=null,m=s,p=!1}}"undefined"!==typeof navigator&&void 0!==navigator.scheduling&&void 0!==navigator.scheduling.isInputPending&&navigator.scheduling.isInputPending.bind(navigator.scheduling);var T,A=!1,M=null,C=-1,q=5,I=-1;function W(){return!(t.unstable_now()-I<q)}function S(){if(null!==M){var e=t.unstable_now();I=e;var a=!0;try{a=M(!0,e)}finally{a?T():(A=!1,M=null)}}else A=!1}if("function"===typeof y)T=function(){y(S)};else if("undefined"!==typeof MessageChannel){var z=new MessageChannel,E=z.port2;z.port1.onmessage=S,T=function(){E.postMessage(null)}}else T=function(){w(S,0)};function P(e){M=e,A||(A=!0,T())}function N(e,a){C=w((function(){e(t.unstable_now())}),a)}t.unstable_IdlePriority=5,t.unstable_ImmediatePriority=1,t.unstable_LowPriority=4,t.unstable_NormalPriority=3,t.unstable_Profiling=null,t.unstable_UserBlockingPriority=2,t.unstable_cancelCallback=function(e){e.callback=null},t.unstable_continueExecution=function(){f||p||(f=!0,P(x))},t.unstable_forceFrameRate=function(e){0>e||125<e?console.error("forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported"):q=0<e?Math.floor(1e3/e):5},t.unstable_getCurrentPriorityLevel=function(){return m},t.unstable_getFirstCallbackNode=function(){return i(l)},t.unstable_next=function(e){switch(m){case 1:case 2:case 3:var t=3;break;default:t=m}var a=m;m=t;try{return e()}finally{m=a}},t.unstable_pauseExecution=function(){},t.unstable_requestPaint=function(){},t.unstable_runWithPriority=function(e,t){switch(e){case 1:case 2:case 3:case 4:case 5:break;default:e=3}var a=m;m=e;try{return t()}finally{m=a}},t.unstable_scheduleCallback=function(e,n,s){var o=t.unstable_now();switch("object"===typeof s&&null!==s?s="number"===typeof(s=s.delay)&&0<s?o+s:o:s=o,e){case 1:var r=-1;break;case 2:r=250;break;case 5:r=1073741823;break;case 4:r=1e4;break;default:r=5e3}return e={id:h++,callback:n,priorityLevel:e,startTime:s,expirationTime:r=s+r,sortIndex:-1},s>o?(e.sortIndex=s,a(d,e),null===i(l)&&e===i(d)&&(g?(b(C),C=-1):g=!0,N(k,s-o))):(e.sortIndex=r,a(l,e),f||p||(f=!0,P(x))),e},t.unstable_shouldYield=W,t.unstable_wrapCallback=function(e){var t=m;return function(){var a=m;m=t;try{return e.apply(this,arguments)}finally{m=a}}}},296:function(e,t,a){"use strict";e.exports=a(813)}},t={};function a(i){var n=t[i];if(void 0!==n)return n.exports;var s=t[i]={exports:{}};return e[i](s,s.exports,a),s.exports}a.m=e,a.n=function(e){var t=e&&e.__esModule?function(){return e.default}:function(){return e};return a.d(t,{a:t}),t},a.d=function(e,t){for(var i in t)a.o(t,i)&&!a.o(e,i)&&Object.defineProperty(e,i,{enumerable:!0,get:t[i]})},a.f={},a.e=function(e){return Promise.all(Object.keys(a.f).reduce((function(t,i){return a.f[i](e,t),t}),[]))},a.u=function(e){return"static/js/"+e+".62a2c9c4.chunk.js"},a.miniCssF=function(e){},a.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},function(){var e={},t="nlp4sg:";a.l=function(i,n,s,o){if(e[i])e[i].push(n);else{var r,c;if(void 0!==s)for(var l=document.getElementsByTagName("script"),d=0;d<l.length;d++){var h=l[d];if(h.getAttribute("src")==i||h.getAttribute("data-webpack")==t+s){r=h;break}}r||(c=!0,(r=document.createElement("script")).charset="utf-8",r.timeout=120,a.nc&&r.setAttribute("nonce",a.nc),r.setAttribute("data-webpack",t+s),r.src=i),e[i]=[n];var u=function(t,a){r.onerror=r.onload=null,clearTimeout(m);var n=e[i];if(delete e[i],r.parentNode&&r.parentNode.removeChild(r),n&&n.forEach((function(e){return e(a)})),t)return t(a)},m=setTimeout(u.bind(null,void 0,{type:"timeout",target:r}),12e4);r.onerror=u.bind(null,r.onerror),r.onload=u.bind(null,r.onload),c&&document.head.appendChild(r)}}}(),a.r=function(e){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},a.p="/nlp4sg_website/",function(){var e={179:0};a.f.j=function(t,i){var n=a.o(e,t)?e[t]:void 0;if(0!==n)if(n)i.push(n[2]);else{var s=new Promise((function(a,i){n=e[t]=[a,i]}));i.push(n[2]=s);var o=a.p+a.u(t),r=new Error;a.l(o,(function(i){if(a.o(e,t)&&(0!==(n=e[t])&&(e[t]=void 0),n)){var s=i&&("load"===i.type?"missing":i.type),o=i&&i.target&&i.target.src;r.message="Loading chunk "+t+" failed.\n("+s+": "+o+")",r.name="ChunkLoadError",r.type=s,r.request=o,n[1](r)}}),"chunk-"+t,t)}};var t=function(t,i){var n,s,o=i[0],r=i[1],c=i[2],l=0;if(o.some((function(t){return 0!==e[t]}))){for(n in r)a.o(r,n)&&(a.m[n]=r[n]);if(c)c(a)}for(t&&t(i);l<o.length;l++)s=o[l],a.o(e,s)&&e[s]&&e[s][0](),e[s]=0},i=self.webpackChunknlp4sg=self.webpackChunknlp4sg||[];i.forEach(t.bind(null,0)),i.push=t.bind(null,i.push.bind(i))}(),function(){"use strict";var e=a(791),t=a(250);function i(e,t){(null==t||t>e.length)&&(t=e.length);for(var a=0,i=new Array(t);a<t;a++)i[a]=e[a];return i}function n(e,t){return function(e){if(Array.isArray(e))return e}(e)||function(e,t){var a=null==e?null:"undefined"!==typeof Symbol&&e[Symbol.iterator]||e["@@iterator"];if(null!=a){var i,n,s=[],o=!0,r=!1;try{for(a=a.call(e);!(o=(i=a.next()).done)&&(s.push(i.value),!t||s.length!==t);o=!0);}catch(c){r=!0,n=c}finally{try{o||null==a.return||a.return()}finally{if(r)throw n}}return s}}(e,t)||function(e,t){if(e){if("string"===typeof e)return i(e,t);var a=Object.prototype.toString.call(e).slice(8,-1);return"Object"===a&&e.constructor&&(a=e.constructor.name),"Map"===a||"Set"===a?Array.from(e):"Arguments"===a||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(a)?i(e,t):void 0}}(e,t)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}var s=JSON.parse('[{"url":"https://www.semanticscholar.org/paper/e21ab639b0308a71aeac643a12f59a32603d091d","title":"UPSTAGE : Unsupervised Context Augmentation for Utterance Classification in Patient - Provider Communication","abstract":"Conversations between patients and providers in clinical settings provide a source of natural language data that may reflect and correlate with the patients\u2019 experience and response to the treatment they are receiving . When analyzing utterances in such conversations , it is not sufficient to consider each sentence in isolation , since its context may play a role in determining its semantic meaning . Recently , contextual information in natural language documents has been modeled using various techniques , such as recurrent neural networks with latent variables , or neural networks with attention mechanisms . In this paper , we present UnsuPerviSed conText AuGmEntation (Upstage) , a classification framework that relies on both local and global contextual information from different sources . Upstage uses transformer models with pretrained language models and joint sentence representation to solve the task of classifying health topics in patient - provider conversations . In addition , Upstage leverages unlabeled corpora for pretraining and data augmentation to provide additional context , which leads to improved classification performance . ","Method":["attention mechanisms","classification framework","neural networks","unsupervised context augmentation","recurrent neural networks","transformer models","joint sentence representation","pretrained language models","upstage"],"Task":["pretraining","patient - provider communication","unsupervised context augmentation","classifying health topics in patient - provider conversations","data augmentation","classification","utterance classification"]},{"url":"https://www.semanticscholar.org/paper/d77719ba4be4196c9a918ce8dff0edf5aa3c04e7","title":"A Review of Challenges and Opportunities in Machine Learning for Health . ","abstract":"Modern electronic health records (EHRs) provide data to answer clinically meaningful questions . The growing data in EHRs makes healthcare ripe for the use of machine learning . However , learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies . For example , diseases in EHRs are poorly labeled , conditions can encompass multiple underlying endotypes , and healthy individuals are underrepresented . This article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare . ","Method":["machine learning","machine learning methodologies"],"Task":["machine learning community","machine learning","health","learning"]},{"url":"https://www.semanticscholar.org/paper/e8d330f11df9c69f38b78a7cc4b1333ebecf7c55","title":"Ethical Machine Learning in Health Care","abstract":"The use of machine learning (ML) in healthcare raises numerous ethical concerns , especially as models can amplify existing health inequities . Here , we outline ethical considerations for equitable ML in the advancement of healthcare . Specifically , we frame ethics of ML in healthcare through the lens of social justice . We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health , ranging from problem selection to postdeployment considerations . We close by summarizing recommendations to address these challenges . ","Method":["machine learning","ml"],"Task":["health","healthcare","ethical ml","health care","equitable ml","problem selection","postdeployment considerations","ethical machine learning"]},{"url":"https://www.semanticscholar.org/paper/4fd75d18b231f73e9c878cb6029900e4b0e327bb","title":"Intimate Partner Violence and Injury Prediction From Radiology Reports","abstract":"Intimate partner violence (IPV) is an urgent , prevalent , and under - detected public health issue . We present machine learning models to assess patients for IPV and injury . We train the predictive algorithms on radiology reports with 1) IPV labels based on entry to a violence prevention program and 2) injury labels provided by emergency radiology fellowship - trained physicians . Our dataset includes 34 , 642 radiology reports and 1479 patients of IPV victims and control patients . Our best model predicts IPV a median of 3 . 08 years before violence prevention program entry with a sensitivity of 64% and a specificity of 95% . We conduct error analysis to determine for which patients our model has especially high or low performance and discuss next steps for a deployed clinical risk model . ","Method":["violence prevention program","clinical risk model","predictive algorithms","machine learning models","error analysis"],"Task":["intimate partner violence and injury prediction","intimate partner violence","public health issue","injury"]},{"url":"https://www.semanticscholar.org/paper/e7bf950be4cf8309d0df68d18fa09e77f5b2511a","title":"De - identification of patient notes with recurrent neural networks","abstract":"Objective\\nPatient notes in electronic health records (EHRs) may contain critical information for medical investigations . However , the vast majority of medical investigators can only access de - identified notes , in order to protect the confidentiality of patients . In the United States , the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information that needs to be removed to de - identify patient notes . Manual de - identification is impractical given the size of electronic health record databases , the limited number of researchers with access to non - de - identified notes , and the frequent mistakes of human annotators . A reliable automated de - identification system would consequently be of high value . \\n\\n\\nMaterials and Methods\\nWe introduce the first de - identification system based on artificial neural networks (ANNs) , which requires no handcrafted features or rules , unlike existing systems . We compare the performance of the system with state - of - the - art systems on two datasets : the i2b2 2014 de - identification challenge dataset , which is the largest publicly available de - identification dataset , and the MIMIC de - identification dataset , which we assembled and is twice as large as the i2b2 2014 dataset . \\n\\n\\nResults\\nOur ANN model outperforms the state - of - the - art systems . It yields an F1 - score of 97 . 85 on the i2b2 2014 dataset , with a recall of 97 . 38 and a precision of 98 . 32 , and an F1 - score of 99 . 23 on the MIMIC de - identification dataset , with a recall of 99 . 25 and a precision of 99 . 21 . \\n\\n\\nConclusion\\nOur findings support the use of ANNs for de - identification of patient notes , as they show better performance than previously published systems while requiring no manual feature engineering . ","Method":["artificial neural networks","de - identification system","recurrent neural networks","anns","automated de - identification system","ann model"],"Task":["medical investigations","manual de - identification","mimic de - identification dataset","de - identification of patient notes"]},{"url":"https://www.semanticscholar.org/paper/defeaddec92b53fc2e9787cc3652e3d8be812193","title":"Segment convolutional neural networks (Seg - CNNs) for classifying relations in clinical notes","abstract":"We propose Segment Convolutional Neural Networks (Seg - CNNs) for classifying relations from clinical notes . Seg - CNNs use only word - embedding features without manual feature engineering . Unlike typical CNN models , relations between 2 concepts are identified by simultaneously learning separate representations for text segments in a sentence : preceding , concept1 , middle , concept2 , and succeeding . We evaluate Seg - CNN on the i2b2/VA relation classification challenge dataset . We show that Seg - CNN achieves a state - of - the - art micro - average F - measure of 0 . 742 for overall evaluation , 0 . 686 for classifying medical problem - treatment relations , 0 . 820 for medical problem - test relations , and 0 . 702 for medical problem - medical problem relations . We demonstrate the benefits of learning segment - level representations . We show that medical domain word embeddings help improve relation classification . Seg - CNNs can be trained quickly for the i2b2/VA dataset on a graphics processing unit (GPU) platform . These results support the use of CNNs computed over segments of text for classifying medical relations , as they show state - of - the - art performance while requiring no manual feature engineering . ","Method":["seg - cnn","cnns)","segment convolutional neural networks","segment - level representations","cnns","- cnns)","graphics processing unit","cnn models","seg - cnns","manual feature engineering"],"Task":["classifying relations","classifying medical problem - treatment relations","relation classification","medical problem - medical problem relations","classifying medical relations","medical problem - test relations"]},{"url":"https://www.semanticscholar.org/paper/4554bc45be85cca73e3f94220c8cb056cf6ebf74","title":"Fast , Structured Clinical Documentation via Contextual Autocomplete","abstract":"We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi - structured clinical documentation . We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data . By constraining our architecture to shallow neural networks , we are able to make these suggestions in real time . Furthermore , as our algorithm is used to write a note , we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies , making notes more structured and readable for physicians , patients , and future algorithms . To our knowledge , this system is the only machine learning - based documentation utility for clinical notes deployed in a live hospital setting , and it reduces keystroke burden of clinical concepts by 67% in real environments . ","Method":["machine learning - based documentation utility","shallow neural networks","contextual autocomplete","learned autocompletion mechanism"],"Task":["fast , structured clinical documentation","rapid creation of semi - structured clinical documentation"]},{"url":"https://www.semanticscholar.org/paper/4a10dffca6dcce9c570cb75aa4d76522c34a2fd4","title":"CORD - 19 : The COVID - 19 Open Research Dataset","abstract":"The COVID - 19 Open Research Dataset (CORD - 19) is a growing resource of scientific papers on COVID - 19 and related historical coronavirus research . CORD - 19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers . Since its release , CORD - 19 has been downloaded over 200K times and has served as the basis of many COVID - 19 text mining and discovery systems . In this article , we describe the mechanics of dataset construction , highlighting challenges and key design decisions , provide an overview of how CORD - 19 has been used , and describe several shared tasks built around the dataset . We hope this resource will continue to bring together the computing community , biomedical experts , and policy makers in the search for effective treatments and management policies for COVID - 19 . ","Method":["cord - 19"],"Task":["covid - 19","text mining and information retrieval systems","dataset construction","management policies"]},{"url":"https://www.semanticscholar.org/paper/3f8d4444cd124e21d4fa47b514c8267a3d5d9649","title":"Can AI Help Reduce Disparities in General Medical and Mental Health Care?","abstract":"Background\\nAs machine learning becomes increasingly common in health care applications , concerns have been raised about bias in these systems\' data , algorithms , and recommendations . Simply put , as health care improves for some , it might not improve for all . \\n\\n\\nMethods\\nTwo case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (ICU) mortality and 30 - day psychiatric readmission with respect to race , gender , and insurance payer type as a proxy for socioeconomic status . \\n\\n\\nResults\\nClinical note topics and psychiatric note topics were heterogenous with respect to race , gender , and insurance payer type , which reflects known clinical findings . Differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for ICU mortality and with respect to insurance policy for psychiatric 30 - day readmission . \\n\\n\\nConclusions\\nThis analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care . ","Method":["machine learning","machine learning algorithm","ai"],"Task":["icu mortality","psychiatric 30 - day readmission","artificial intelligence","general medical and mental health care?","health care applications"]},{"url":"https://www.semanticscholar.org/paper/2d40f76055a7c28dd30c8b8f60c31724f8991cd8","title":"The Ivory Tower Lost : How College Students Respond Differently than the General Public to the COVID - 19 Pandemic","abstract":"In the United States , the country with the highest confirmed COVID - 19 infection cases , a nationwide social distancing protocol has been implemented by the President . Following the closure of the University of Washington on March 7th , more than 1000 colleges and universities in the United States have cancelled in - person classes and campus activities , impacting millions of students . This paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people\'s opinions on social media . We discover several topics embedded in a large number of COVID - 19 tweets that represent the most central issues related to the pandemic , which are of great concerns for both college students and the general public . Moreover , we find significant differences between these two groups of Twitter users with respect to the sentiments they expressed towards the COVID - 19 issues . To our best knowledge , this is the first social media - based study which focuses on the college student community\'s demographics and responses to prevalent social issues during a major crisis . ","Method":["social distancing protocol"],"Task":["covid - 19 pandemic"]},{"url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","title":"HumAID : Human - Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination , especially during time - critical events such as natural disasters . Despite its significantly large volume , social media content is often too noisy for direct use in any application . Therefore , it is important to filter , categorize , and concisely summarize the available content to facilitate effective consumption and decision - making . To address such issues automatic classification systems have been developed using supervised modeling approaches , thanks to the earlier efforts on creating labeled datasets . However , existing datasets are limited in different aspects (e . g . , size , contains duplicates) and less suitable to support more advanced and data - hungry deep learning models . In this paper , we present a new large - scale dataset with \u223c77K human - labeled tweets , sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019 . Moreover , we propose a data collection and sampling pipeline , which is important for social media data sampling for human annotation . We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies . The dataset and associated resources are publicly available at https : //crisisnlp . qcri . org/humaid_dataset . html . ","Method":["deep learning","transformer) based models","data - hungry deep learning models","humaid","data collection","sampling pipeline","supervised modeling approaches"],"Task":["consumption and decision - making","social media data sampling","information consumption and dissemination","classification","time - critical events","human annotation","multiclass classification"]},{"url":"https://www.semanticscholar.org/paper/2a29f7cc10fc18ce6e031e0cf6fc307a6b85d8e9","title":"CrisisMMD : Multimodal Twitter Datasets from Natural Disasters","abstract":"During natural and man - made disasters , people use social media platforms such as Twitter to post textual and multime - dia content to report updates about injured or dead people , infrastructure damage , and missing or found people among other information types . Studies have revealed that this on - line information , if processed timely and effectively , is ex - tremely useful for humanitarian organizations to gain situational awareness and plan relief operations . In addition to the analysis of textual content , recent studies have shown that imagery content on social media can boost disaster response significantly . Despite extensive research that mainly focuses on textual content to extract useful information , limited work has focused on the use of imagery content or the combination of both content types . One of the reasons is the lack of labeled imagery data in this domain . Therefore , in this paper , we aim to tackle this limitation by releasing a large multi - modal dataset collected from Twitter during different natural disasters . We provide three types of annotations , which are useful to address a number of crisis response and management tasks for different humanitarian organizations . ","Method":["crisismmd"],"Task":["situational awareness","analysis of textual content","humanitarian organizations","crisis response and management tasks","disaster response","relief operations"]},{"url":"https://www.semanticscholar.org/paper/561ede166947a8bedb8be9acff182913156e06c6","title":"Domain Adaptation with Adversarial Training and Graph Embeddings","abstract":"The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data . However , obtaining labeled data is a big challenge in many real - world problems . In such scenarios , a DNN model can leverage labeled and unlabeled data from a related domain , but it has to deal with the shift in data distributions between the source and the target domains . In this paper , we study the problem of classifying social media posts during a crisis event (e . g . , Earthquake) . For that , we use labeled and unlabeled data from past similar events (e . g . , Flood) and unlabeled data for the current event . We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi - supervised learning to leverage unlabeled data within a single unified deep learning framework . Our experiments with two real - world crisis datasets collected from Twitter demonstrate significant improvements over several baselines . ","Method":["adversarial training","dnn model","graph based semi - supervised learning","domain adaptation","adversarial learning","deep neural networks","unified deep learning framework","graph embeddings"],"Task":["classifying social media posts","real - world problems"]},{"url":"https://www.semanticscholar.org/paper/579914dac924d6b4a17889ea7b0dd744a62bbd74","title":"IBC - C : A Dataset for Armed Conflict Event Analysis","abstract":"We describe the Iraq Body Count Corpus (IBC - C) dataset , the first substantial armed conflict - related dataset which can be used for conflict analysis . IBC - C provides a ground - truth dataset for conflict specific named entity recognition , slot filling , and event de - duplication . IBC - C is constructed using data collected by the Iraq Body Count project which has been recording casualties resulting from the ongoing war in Iraq since 2003 . We describe the dataset\u2019s creation , how it can be used for the above three tasks and provide initial baseline results for the first task (named entity recognition) using Hidden Markov Models , Conditional Random Fields , and Recursive Neural Networks . ","Method":["hidden markov models","recursive neural networks","ibc - c","conditional random fields"],"Task":["conflict specific named entity recognition","event de - duplication","armed conflict event analysis","slot filling","conflict analysis","entity recognition)"]},{"url":"https://www.semanticscholar.org/paper/3004f78d84752f3ffe8e54fa265b2ee85dac9728","title":"Text as Data for Conflict Research : A Literature Survey","abstract":"Computer - aided text analysis (CATA) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology , political science , communication studies , and computer science . The chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres . This includes both conflict as it is verbalized in the news media , in political speeches , and other public documents and conflict as it occurs in online spaces (social media platforms , forums) and that is largely confined to such spaces (e . g . , flaming and trolling) . Particular emphasis is placed on research that aims to find commonalities between online and offline conflict , and that systematically investigates the dynamics of group behavior . Both work using inductive computational procedures , such as topic modeling , and supervised machine learning approaches are assessed , as are more traditional forms of content analysis , such as dictionaries . Finally , cross - validation is highlighted as a crucial step in CATA , in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research . ","Method":["content analysis","inductive computational procedures","dictionaries","topic modeling","supervised machine learning approaches","cata"],"Task":["sociology","computer - aided text analysis","political science","text mining","conflict research","computer science","communication studies"]},{"url":"https://www.semanticscholar.org/paper/42c63d952f0cce7f89738e818fc9ab4e723bf7b6","title":"One - to - X Analogical Reasoning on Word Embeddings : a Case for Diachronic Armed Conflict Prediction from News Texts","abstract":"We extend the well - known word analogy task to a one - to - X formulation , including one - to - none cases , when no correct answer exists . The task is cast as a relation discovery problem and applied to historical armed conflicts datasets , attempting to predict new relations of type \u2018location : armed - group\u2019 based on data about past events . As the source of semantic information , we use diachronic word embedding models trained on English news texts . A simple technique to improve diachronic performance in such task is demonstrated , using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora . Finally , we publish a ready - to - use test set for one - to - X analogy evaluation on historical armed conflicts data . ","Method":["diachronic word embedding models","one - to - x analogical reasoning"],"Task":["word embeddings","one - to - x analogy evaluation","relation discovery problem","one - to - x formulation","diachronic armed conflict prediction","diachronic","word analogy task"]},{"url":"https://www.semanticscholar.org/paper/636d4c0b0fe6919abe6eb546907d28ed39bf56e6","title":"Using Natural Language Processing for Automatic Detection of Plagiarism","abstract":"Current plagiarism detection tools are mostly limited to comparisons of suspicious plagiarised texts and potential original texts at string level . In this study the aim is to improve the accuracy of plagiarism detection by incorporating Natural Language Processing (NLP) techniques into existing approaches . We propose a framework for external plagiarism detection in which a number of NLP techniques are applied to process a set of suspicious and original documents , not only to analyse strings but also the structure of the text , using resources to account for text relations . Initial results obtained with a corpus of plagiarised short paragraphs have showed that NLP techniques improve the accuracy of existing approaches . ","Method":["nlp techniques","natural language processing","plagiarism detection tools"],"Task":["plagiarism detection","external plagiarism detection","automatic detection of plagiarism"]},{"url":"https://www.semanticscholar.org/paper/aea14f23a951975f605a981d003386e46bf8acfe","title":"A Neural Approach to Automated Essay Scoring","abstract":"Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays . The performance of such systems is tightly bound to the quality of the underlying features . However , it is laborious to manually design the most informative features for such a system . In this paper , we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score , without any feature engineering . We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models . The results show that our best system , which is based on long short - term memory networks , outperforms a strong baseline by 5 . 6% in terms of quadratic weighted Kappa , without requiring any feature engineering . ","Method":["neural approach","feature engineering","automated essay scoring systems","neural network models","recurrent neural networks","long short - term memory networks"],"Task":["automated essay scoring"]},{"url":"https://www.semanticscholar.org/paper/0eb5872733e643f43a0c1a7ff78953dfea74dfea","title":"Automated Scoring : Beyond Natural Language Processing","abstract":"In this position paper , we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy . Automated scoring systems warrant significant cross - discipline collaboration of which natural language processing and machine learning are just two of many important components . Such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other . Our position is that it is essential for us as NLP researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate , fair , unbiased , and useful . ","Method":["machine learning","machine learning techniques","natural language processing","automated scoring systems"],"Task":["operational automated scoring systems","natural language processing","nlp","competitive shared tasks","automated scoring"]},{"url":"https://www.semanticscholar.org/paper/0cdeb238355617a640471987af36d8e09cd905c0","title":"Event Data on Armed Conflict and Security : New Perspectives , Old Challenges , and Some Solutions","abstract":"This article presents the Event Data on Conflict and Security (EDACS) dataset , discusses the inherent problems of georeferenced conflict data , and shows how these challenges are met within EDACS . Based on an event data approach , EDACS contributes to the growing number of novel georeferenced datasets that allow researchers to identify causal pathways of violence and the dynamics of (transboundary) violence through spatiotemporal disaggregation . However , the unreflected use of any of these datasets will give researchers unjustified confidence in their findings , as the pitfalls are many and propagating errors can result in misleading conclusions . To identify and handle the different challenges to overall event data quality , we argue in favor of transparency in the data collection and coding process , to empower analysts to challenge the data and avoid cascading errors . In particular , we investigate how the choice of news sources , the handling of geographic precision , and the use of auxiliary data can bias event data . We demonstrate how the EDACS dataset design enables the analyst to deal with these issues by providing a set of variables indicating the news sources , possible sources of bias , and detailed information on geographic precision . This allows for a flexible use of the data based on individual analytical requirements . ","Method":["edacs","event data approach","spatiotemporal disaggregation"],"Task":["event data on conflict and security","armed conflict and security","coding process","data collection"]},{"url":"https://www.semanticscholar.org/paper/2ca0403eabc3893fed255fc119a927d83a1af739","title":"Tracing armed conflicts with diachronic word embedding models","abstract":"Recent studies have shown that word embedding models can be used to trace time - related (diachronic) semantic shifts in particular words . In this paper , we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year - to - year basis , using a dataset from the conflict research field as the gold standard and the Gigaword news corpus as the training data . The results show that much work still remains in extracting \u2018cultural\u2019 semantic shifts from diachronic word embedding models . At the same time , we present a new task complete with an evaluation set and introduce the \u2018anchor words\u2019 method which outperforms previous approaches on this set . ","Method":["\u2018anchor words\u2019 method","diachronic word embedding models","word embedding models"],"Task":["tracing armed conflicts","conflict research field","dynamics of global armed conflicts"]},{"url":"https://www.semanticscholar.org/paper/4ed75f35ae7343cd906f311390f572a58e36805c","title":"Enriching textbooks through data mining","abstract":"Textbooks play an important role in any educational system . Unfortunately , many textbooks produced in developing countries are not written well and they often lack adequate coverage of important concepts . We propose a technological solution to address this problem based on enriching textbooks with authoritative web content . We augment textbooks at the section level for key concepts discussed in the section . We use ideas from data mining for identifying the concepts that need augmentation as well as to determine the links to the authoritative content that should be used for augmentation . Our evaluation , employing textbooks from India , shows that we are able to enrich textbooks on different subjects and across different grades with high quality augmentations using automated techniques . ","Method":["automated techniques","technological solution","data mining"],"Task":["educational system","enriching textbooks","augmentation"]},{"url":"https://www.semanticscholar.org/paper/bc6a4304afdf59783638a6a0d9fc2c2acb6e5b67","title":"Educational Question Answering Motivated by Question - Specific Concept Maps","abstract":"Question answering (QA) is the automated process of answering general questions submitted by humans in natural language . QA has previously been explored within the educational context to facilitate learning , however the majority of works have focused on text - based answering . As an alternative , this paper proposes an approach to return answers as a concept map , which further encourages meaningful learning and knowledge organisation . Additionally , this paper investigates whether adapting the returned concept map to the specific question context provides further learning benefit . A randomised experiment was conducted with a sample of 59 Computer Science undergraduates , obtaining statistically significant results on learning gain when students are provided with the question - specific concept maps . Further , time spent on studying the concept maps were positively correlated with the learning gain . ","Method":["concept map","question - specific concept maps"],"Task":["qa","question answering","meaningful learning","educational question answering","text - based answering","learning","answering general questions","knowledge organisation","educational context","automated process"]},{"url":"https://www.semanticscholar.org/paper/911e61212eb26325d3fe58f454f36f4e70c54c7a","title":"Characterizing Stage - aware Writing Assistance for Collaborative Document Authoring","abstract":"Writing is a complex non - linear process that begins with a mental model of intent , and progresses through an outline of ideas , to words on paper (and their subsequent refinement) . Despite past research in understanding writing , Web - scale consumer and enterprise collaborative digital writing environments are yet to greatly benefit from intelligent systems that understand the stages of document evolution , providing opportune assistance based on authors\' situated actions and context . In this paper , we present three studies that explore temporal stages of document authoring . We first survey information workers at a large technology company about their writing habits and preferences , concluding that writers do in fact conceptually progress through several distinct phases while authoring documents . We also explore , qualitatively , how writing stages are linked to document lifespan . We supplement these qualitative findings with an analysis of the longitudinal user interaction logs of a popular digital writing platform over several million documents . Finally , as a first step towards facilitating an intelligent digital writing assistant , we conduct a preliminary investigation into the utility of user interaction log data for predicting the temporal stage of a document . Our results support the benefit of tools tailored to writing stages , identify primary tasks associated with these stages , and show that it is possible to predict stages from anonymous interaction logs . Together , these results argue for the benefit and feasibility of more tailored digital writing assistance . ","Method":["intelligent systems","mental model of intent","non - linear process","digital writing platform"],"Task":["document evolution","digital writing assistance","web - scale consumer and enterprise collaborative digital writing environments","collaborative document authoring writing","intelligent digital writing assistant","stage - aware writing assistance","writing","document authoring"]},{"url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre - structured database or a collection of natural language documents . It presents only the requested information instead of searching full documents like search engine . As information in day to day life is increasing , so to retrieve the exact fragment of information even for a simple query requires large and expensive resources . This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques . ","Method":["nlp techniques","question answering system","closed domain qa system","search engine"],"Task":["question answering system","question answering","education acts","information retrieval"]},{"url":"https://www.semanticscholar.org/paper/f8868be52c8c0d64920e5f67870c01ba48601608","title":"Natural Language Processing and Language Learning","abstract":"As a relatively young field of research and development started by work on cryptanalysis and machine translation around 50 years ago , Natural Language Processing (NLP) is concerned with the automated processing of human language . It addresses the analysis and generation of written and spoken language , though speech processing is often regarded as a separate subfield . NLP emphasizes processing and applications and as such can be seen as the applied side of Computational Linguistics , the interdisciplinary field of research concerned with formal analysis and modeling of language and its applications at the intersection of Linguistics , Computer Science , and Psychology . In terms of the language aspects dealt with in NLP , traditionally lexical , morphological and syntactic aspects of language were at the center of attention , but aspects of meaning , discourse , and the relation to the extra - linguistic context have become increasingly prominent in the last decade . A good introduction and overview of the field is provided in Jurafsky & Martin (2009) . ","Method":["NOT DETECTED"],"Task":["speech processing","natural language processing","linguistics","psychology","nlp","machine translation","formal analysis and modeling of language","cryptanalysis","processing","computational linguistics","computer science","analysis and generation of written and spoken language","automated processing of human language","language learning"]},{"url":"https://www.semanticscholar.org/paper/311381feeb6346bfcb2ba622bd8f713261a4075d","title":"Modeling the Relationship between User Comments and Edits in Document Revision","abstract":"Management of collaborative documents can be difficult , given the profusion of edits and comments that multiple authors make during a document\u2019s evolution . Reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux . A number of authoring tasks , such as categorizing and summarizing edits , detecting completed to - dos , and visually rearranging comments could benefit from such a contribution . Thus , in this paper we explore the relationship between comments and edits by defining two novel , related tasks : Comment Ranking and Edit Anchoring . We begin by collecting a dataset with more than half a million comment - edit pairs based on Wikipedia revision histories . We then propose a hierarchical multi - layer deep neural - network to model the relationship between edits and comments . Our architecture tackles both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions such as additions and deletions , while also accounting for document context . In a number of evaluation settings , our experimental results show that our approach outperforms several strong baselines significantly . We are able to achieve a precision@1 of 71 . 0% and a precision@3 of 94 . 4% for Comment Ranking , while we achieve 74 . 4% accuracy on Edit Anchoring . ","Method":["hierarchical multi - layer deep neural - network"],"Task":["authoring tasks","detecting completed to - dos","edit anchoring","comment ranking","edit anchoring tasks","visually rearranging comments","categorizing and summarizing edits","document revision management of collaborative documents"]},{"url":"https://www.semanticscholar.org/paper/d114af5d3dcb3792bd9faec1476feba5aaf3617f","title":"A Multimodal Human - Computer Interaction System and Its Application in Smart Learning Environments","abstract":"A multimodal human - computer interaction system is composed of the comprehensive usage of various input and output channels . For the information input , apart from the traditional keyboard typing , mouse clicking , screen touching , the latest speech and face recognition technology can be used . For the output , the traditional screen display , the latest speech and facial expression synthesis and gesture generation can be used . After literature review of related works , this paper at first presents such a system , MMISE (Multimodal Interaction System for Education) , about its architecture and working mechanism , POOOIIM (Pedagogical Objective Oriented Output , Input and Implementation Mechanism) illustrated with practical examples . Then this paper introduces this system\u2019s pilot applications in the epidemic time of novel coronavirus in 2020 . ","Method":["speech and face recognition technology","multimodal human - computer interaction system","poooiim","mmise (multimodal interaction system","implementation mechanism)","screen display","speech and facial expression synthesis"],"Task":["gesture generation","smart learning environments","coronavirus","education)","information input"]},{"url":"https://www.semanticscholar.org/paper/9b0c9d241269b98c80f65a14d5d65263d0688d70","title":"What Makes a Good Counselor? Learning to Distinguish between High - quality and Low - quality Counseling Conversations","abstract":"The quality of a counseling intervention relies highly on the active collaboration between clients and counselors . In this paper , we explore several linguistic aspects of the collaboration process occurring during counseling conversations . Specifically , we address the differences between high - quality and low - quality counseling . Our approach examines participants\u2019 turn - by - turn interaction , their linguistic alignment , the sentiment expressed by speakers during the conversation , as well as the different topics being discussed . Our results suggest important language differences in low - and high - quality counseling , which we further use to derive linguistic features able to capture the differences between the two groups . These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88% . ","Method":["automatic classifiers"],"Task":["counseling conversations","collaboration process","counseling intervention","counselor? learning"]},{"url":"https://www.semanticscholar.org/paper/aa2bbf5a1485cc90401e4a35c04463fcb4e632f0","title":"Quantifying the Effects of COVID - 19 on Mental Health Support Forums","abstract":"The COVID - 19 pandemic , like many of the disease outbreaks that have preceded it , is likely to have a profound effect on mental health . Understanding its impact can inform strategies for mitigating negative consequences . In this work , we seek to better understand the effects of COVID - 19 on mental health by examining discussions within mental health support communities on Reddit . First , we quantify the rate at which COVID - 19 is discussed in each community , or subreddit , in order to understand levels of preoccupation with the pandemic . Next , we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen . Finally , we analyze how COVID - 19 has influenced language use and topics of discussion within each subreddit . ","Method":["covid - 19"],"Task":["covid - 19","mental health","mitigating negative consequences","mental health support forums"]},{"url":"https://www.semanticscholar.org/paper/81d13af7eb78725492854bb9860b57af2b2a1778","title":"Data Mining and Student e - Learning Profiles","abstract":"Data mining techniques have been applied to educational research in various ways . In this paper , I presented the application of sequential data mining algorithms to analyze computer logs to profile learners in terms of their learning tactic use and motivation in a web - based learning environment (gStudy) . The data mining algorithms are employed to discover patterns which characterize learners either across session or groups based on their study tactic choice and goal orientation . The use of this method is illustrated through a sequential pattern analysis of gStudy log files generated by university students . ","Method":["sequential data mining algorithms","sequential pattern analysis","data mining algorithms","data mining techniques"],"Task":["educational research","student e - learning profiles","data mining"]},{"url":"https://www.semanticscholar.org/paper/065332576fd9b264a5a338549239bb39e6733819","title":"Inferring Social Media Users\u2019 Mental Health Status from Multimodal Information","abstract":"Worldwide , an increasing number of people are suffering from mental health disorders such as depression and anxiety . In the United States alone , one in every four adults suffers from a mental health condition , which makes mental health a pressing concern . In this paper , we explore the use of multimodal cues present in social media posts to predict users\u2019 mental health status . Specifically , we focus on identifying social media activity that either indicates a mental health condition or its onset . We collect posts from Flickr and apply a multimodal approach that consists of jointly analyzing language , visual , and metadata cues and their relation to mental health . We conduct several classification experiments aiming to discriminate between (1) healthy users and users affected by a mental health illness; and (2) healthy users and users prone to mental illness . Our experimental results indicate that using multiple modalities can improve the performance of this classification task as compared to the use of one modality at a time , and can provide important cues into a user\u2019s mental status . ","Method":["multimodal approach"],"Task":["classification task","inferring social media users\u2019 mental health status","mental health","classification"]},{"url":"https://www.semanticscholar.org/paper/c278eb333c78313a3762b5b2f07ace7443582b94","title":"Expressive Interviewing : A Conversational System for Coping with COVID - 19","abstract":"The ongoing COVID - 19 pandemic has raised concerns for many regarding personal and public health implications , financial security and economic stability . Alongside many other unprecedented challenges , there are increasing concerns over social isolation and mental health . We introduce \\\\textit{Expressive Interviewing} - - an interview - style conversational system that draws on ideas from motivational interviewing and expressive writing . Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID - 19 has impacted their lives . We present relevant aspects of the system\'s design and implementation as well as quantitative and qualitative analyses of user interactions with the system . In addition , we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID - 19 issues . ","Method":["motivational interviewing","conversational system","interview - style conversational system","interviewing}","general purpose dialogue system","expressive interviewing"],"Task":["expressive writing","financial security","mental health","personal and public health implications","covid - 19","expressive interviewing"]},{"url":"https://www.semanticscholar.org/paper/35112c6bfa715b2bc86fc5b6c331f8a1a0b07add","title":"Understanding and Predicting Empathic Behavior in Counseling Therapy","abstract":"Counselor empathy is associated with better outcomes in psychology and behavioral counseling . In this paper , we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters . Particularly , we analyze aspects such as participants\u2019 engagement , participants\u2019 verbal and nonverbal accommodation , as well as topics being discussed during the conversation , with the final goal of identifying linguistic and acoustic markers of counselor empathy . We also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80% . ","Method":["counselor empathy classifiers"],"Task":["counseling interaction dynamics","psychology and behavioral counseling","counselor empathy","predicting empathic behavior","motivational interviewing encounters","linguistic and acoustic markers of counselor empathy","counseling therapy"]},{"url":"https://www.semanticscholar.org/paper/c151f144c2c0e8d3b176edaf2ce5369c7707bd31","title":"Large - scale Analysis of Counseling Conversations : An Application of Natural Language Processing to Mental Health","abstract":"Mental illness is one of the most pressing public health issues of our time . While counseling and psychotherapy can be effective treatments , our knowledge about how to conduct successful counseling conversations has been limited due to lack of large - scale data with labeled outcomes of the conversations . In this paper , we present a large - scale , quantitative study on the discourse of text - message - based counseling conversations . We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes . Applying techniques such as sequence - based conversation models , language model comparisons , message clustering , and psycholinguistics - inspired word frequency analyses , we discover actionable conversation strategies that are associated with better conversation outcomes . ","Method":["psycholinguistics - inspired word frequency analyses","natural language processing","counseling","message clustering","language model comparisons","sequence - based conversation models","computational discourse analysis methods","psychotherapy"],"Task":["mental health","mental illness","large - scale analysis of counseling conversations","counseling conversations","discourse of text - message - based counseling conversations"]},{"url":"https://www.semanticscholar.org/paper/9de154d3c886177380062be7c8d50304a335752f","title":"Fermi at SemEval - 2019 Task 6 : Identifying and Categorizing Offensive Language in Social Media using Sentence Embeddings","abstract":"This paper describes our system (Fermi) for Task 6 : OffensEval : Identifying and Categorizing Offensive Language in Social Media of SemEval - 2019 . We participated in all the three sub - tasks within Task 6 . We evaluate multiple sentence embeddings in conjunction with various supervised machine learning algorithms and evaluate the performance of simple yet effective embedding - ML combination algorithms . Our team Fermi\u2019s model achieved an F1 - score of 64 . 40% , 62 . 00% and 62 . 60% for sub - task A , B and C respectively on the official leaderboard . Our model for sub - task C which uses pre - trained ELMo embeddings for transforming the input and uses SVM (RBF kernel) for training , scored third position on the official leaderboard . Through the paper we provide a detailed description of the approach , as well as the results obtained for the task . ","Method":["sentence embeddings","embedding - ml combination algorithms","fermi\u2019s","svm (rbf kernel)","supervised machine learning algorithms","elmo embeddings"],"Task":["offenseval","sub - task a","identifying and categorizing offensive language"]},{"url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters . Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI) . We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition , transcription , expert data annotations , and reliability assessments . The dataset contains a total of 22 , 719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes . The reliability analysis showed that annotators achieved excellent agreement at session level , with Intraclass Correlation Coefficient (ICC) scores in the range of 0 . 75 to 1 , and fair to good agreement at utterance level , with Cohen\u2019s Kappa scores ranging from 0 . 31 to 0 . 64 . Behavioral interventions are a promising approach to address public health issues such as smoking cessation , increasing physical activity , and reducing substance abuse , among others (Resnicow et al . , 2002) . In particular , Motivational Interviewing (MI) , a client centered psychotherapy style , has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al . , 2009; Apodaca et al . , 2014; Barnett et al . , 2014; Catley et al . , 2012) . Despite its potential benefits in combating addiction and in providing broader disease prevention and management , implementing MI counseling at larger scale or in other domains is limited by the need for human - based evaluations . Currently , this requires a human either watching or listening to video - tapes and then providing evaluative feedback . Recently , computational approaches have been proposed to aid the MI evaluation process (Atkins et al . , 2014; Xiao et al . , 2014; Klonek et al . , 2015) . However , learning resources for this task are not readily available . Having such resources will enable the application of data - driven strategies for the automatic coding of counseling behaviors , thus providing researchers with automatic means for the evaluation of MI . Moreover , this can also be useful to explore how MI works by relating MI behaviors to health outcomes , and to provide counselors with evaluative feedback that helps them improve their MI skills . In this paper , we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4 . 0 (MITI) , which is the current gold standard for MI - based psychology interventions . The dataset is derived from 277 MI sessions containing a total of 22 , 719 coded utterances . 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative , goal - oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick , 2013) . MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol , tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Method":["behavioral coding","computational approaches","data - driven strategies","treatment method","motivational interviewing","motivational interviewing integrity treatment","client centered psychotherapy style","motivational interviewing treatment integrity 4 . 0","mi","collaborative , goal - oriented style of psychotherapy","behavioral interventions"],"Task":["mi evaluation process","transcription","human - based evaluations","evaluation of mi","addiction","substance abuse","public health issues","reliability analysis","mi - based psychology interventions","smoking cessation","annotations","physical activity","broader disease prevention and management","automatic coding of counseling behaviors","expert data annotations","mi","data acquisition","psychotherapy research"]},{"url":"https://www.semanticscholar.org/paper/55925bc2522a1d4cb007ac78273f7c2efafa3916","title":"Predicting Counselor Behaviors in Motivational Interviewing Encounters","abstract":"As the number of people receiving psycho - therapeutic treatment increases , the automatic evaluation of counseling practice arises as an important challenge in the clinical domain . In this paper , we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients . In particular , we present a model towards the automation of Motivational Interviewing (MI) coding , which is the current gold standard to evaluate MI counseling . First , we build a dataset of hand labeled MI encounters; second , we use text - based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third , we develop an automatic system to predict these behaviors . We introduce a new set of features based on semantic information and syntactic patterns , and show that they lead to accuracy figures of up to 90% , which represent a significant improvement with respect to features used in the past . ","Method":["automatic system","text - based methods"],"Task":["motivational interviewing encounters","predicting counselor behaviors","automatic evaluation of counseling practice","automation of motivational interviewing (mi) coding","mi counseling","automatic evaluation of counseling performance"]},{"url":"https://www.semanticscholar.org/paper/c2fd13dab7a581f2f875826b204d063f42b63a8b","title":"Happiness Entailment : Automating Suggestions for Well - Being","abstract":"Understanding what makes people happy is a central topic in psychology . Prior work has mostly focused on developing self - reporting assessment tools for individuals and relies on experts to analyze the periodic reported assessments . One of the goals of the analysis is to understand what actions are necessary to encourage modifications in the behaviors of the individuals to improve their overall well - being . In this paper , we outline a complementary approach; on the assumption that the user journals her happy moments as short texts , a system can analyze these texts and propose sustainable suggestions for the user that may lead to an overall improvement in her well - being . We prototype one necessary component of such a system , the Happiness Entailment Recognition (HER)module , which takes as input a short text describing an event , a candidate suggestion , and outputs a determination about whether the suggestion is more likely to be good for this user based on the event described . This component is implemented as a neural network model with two encoders , one for the user input and one for the candidate actionable suggestion , with additional layers to capture psychologically significant features in the happy moment and suggestion . Our model achieves an AU - ROC of 0 . 831 and outperforms our baseline as well as the current state - of - the - art Textual Entailment model from AllenNLP by more than 48% of improvements , confirming the uniqueness and complexity of the HER task . ","Method":["allennlp","textual entailment model","neural network model","happiness entailment recognition","encoders"],"Task":["automating suggestions","psychology","happiness entailment","self - reporting assessment tools","well - being"]},{"url":"https://www.semanticscholar.org/paper/64c68fa52491c4a815f21917e068e4c19dd404b0","title":"FERMI at SemEval - 2019 Task 5 : Using Sentence embeddings to Identify Hate Speech Against Immigrants and Women in Twitter","abstract":"This paper describes our system (Fermi) for Task 5 of SemEval - 2019 : HatEval : Multilingual Detection of Hate Speech Against Immigrants and Women on Twitter . We participated in the subtask A for English and ranked first in the evaluation on the test set . We evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding - ML combination algorithms . Our team - Fermi\u2019s model achieved an accuracy of 65 . 00% for English language in task A . Our models , which use pretrained Universal Encoder sentence embeddings for transforming the input and SVM (with RBF kernel) for classification , scored first position (among 68) in the leaderboard on the test set for Subtask A in English language . In this paper we provide a detailed description of the approach , as well as the results obtained in the task . ","Method":["sentence embeddings","embedding - ml combination algorithms","training models","svm (with rbf kernel)","pretrained universal encoder sentence embeddings","team - fermi\u2019s model"],"Task":["multilingual detection of hate speech","classification","hateval"]},{"url":"https://www.semanticscholar.org/paper/7cb5bc9f8bc023f1f2c94e2e83012f1d35e4e01d","title":"Ingredients for Happiness : Modeling constructs via semi - supervised content driven inductive transfer","abstract":"Modeling affect via understanding the social constructs behind them is an important task in devising robust and accurate systems for socially relevant scenarios . In the CL - Aff Shared Task (part of Affective Content Analysis workshop @ AAAI 2019) , the organizers released a dataset of \u2018happy\u2019 moments , called the HappyDB corpus . The task is to detect two social constructs : the agency (i . e . , whether the author is in control of the happy moment) and the social characteristics (i . e . , whether anyone else other than the author was also involved in the happy moment) . We employ an inductive transfer learning technique where we utilize a pre - trained language model and fine - tune it on the target task for both the binary classification tasks . At first , we use a language model pre - trained on the huge WikiText - 103 corpus . This step utilizes an AWDLSTM with three hidden layers for training the language model . In the second step , we fine - tune the pre - trained language model on both the labeled and unlabeled instances from the HappyDB dataset . Finally , we train a classifier on top of the language model for each of the identification tasks . Our experiments using 10 - fold cross validation on the corpus show that we achieve a high accuracy of \u223c93% for detection of the social characteristic and \u223c87% for agency of the author , showing significant gains over other baselines . We also show that using the unlabeled dataset for fine - tuning the language model in the second step improves our accuracy by 1 - 2% across detection of both the constructs . ","Method":["awdlstm","classifier","10 - fold cross validation","language model","hidden layers","semi - supervised content driven inductive transfer","inductive transfer learning technique"],"Task":["detection","socially relevant scenarios","modeling constructs","identification tasks","binary classification tasks","modeling affect","affective content analysis","happiness","cl - aff shared task"]},{"url":"https://www.semanticscholar.org/paper/c239aaa2a87ea63fae666b2051f1923afaa45fc2","title":"HappyDB : A Corpus of 100 , 000 Crowdsourced Happy Moments","abstract":"The science of happiness is an area of positive psychology concerned with understanding what behaviors make people happy in a sustainable fashion . Recently , there has been interest in developing technologies that help incorporate the findings of the science of happiness into users\' daily lives by steering them towards behaviors that increase happiness . With the goal of building technology that can understand how people express their happy moments in text , we crowd - sourced HappyDB , a corpus of 100 , 000 happy moments that we make publicly available . This paper describes HappyDB and its properties , and outlines several important NLP problems that can be studied with the help of the corpus . We also apply several state - of - the - art analysis techniques to analyze HappyDB . Our results demonstrate the need for deeper NLP techniques to be developed which makes HappyDB an exciting resource for follow - on research . ","Method":["nlp techniques","analysis techniques"],"Task":["happiness","nlp problems","positive psychology"]},{"url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","title":"HopeEDI : A Multilingual Hope Speech Detection Dataset for Equality , Diversity , and Inclusion","abstract":"Over the past few years , systems have been developed to control online content and eliminate abusive , offensive or hate speech content . However , people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech . Therefore , it is imperative that research should take a positive reinforcement approach towards online content that is encouraging , positive and supportive contents . Until now , most studies have focused on solving this problem of negativity in the English language , though the problem is much more than just harmful content . Furthermore , it is multilingual as well . Thus , we have constructed a Hope Speech dataset for Equality , Diversity and Inclusion (HopeEDI) containing user - generated comments from the social media platform YouTube with 28 , 451 , 20 , 198 and 10 , 705 comments in English , Tamil and Malayalam , respectively , manually labelled as containing hope speech or not . To our knowledge , this is the first research of its kind to annotate hope speech for equality , diversity and inclusion in a multilingual setting . We determined that the inter - annotator agreement of our dataset using Krippendorff\u2019s alpha . Further , we created several baselines to benchmark the resulting dataset and the results have been expressed using precision , recall and F1 - score . The dataset is publicly available for the research community . We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness . ","Method":["positive reinforcement approach","hopeedi"],"Task":["inclusion","multilingual setting","equality"]},{"url":"https://www.semanticscholar.org/paper/32bf7ad3fdda71036b48f7dc85cad407674277b7","title":"Women worry about family , men about the economy : Gender differences in emotional responses to COVID - 19","abstract":"Among the critical challenges around the COVID - 19 pandemic is dealing with the potentially detrimental effects on people\'s mental health . Designing appropriate interventions and identifying the concerns of those most at risk requires methods that can extract worries , concerns and emotional responses from text data . We examine gender differences and the effect of document length on worries about the ongoing COVID - 19 situation . Our findings suggest that i) short texts do not offer as adequate insights into psychological processes as longer texts . We further find ii) marked gender differences in topics concerning emotional responses . Women worried more about their loved ones and severe health concerns while men were more occupied with effects on the economy and society . This paper adds to the understanding of general gender differences in language found elsewhere , and shows that the current unique circumstances likely amplified these effects . We close this paper with a call for more high - quality datasets due to the limitations of Tweet - sized data . ","Method":["NOT DETECTED"],"Task":["covid - 19 pandemic"]},{"url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent , its causes , and the necessary responses\u2014is intense and of global importance . Yet , in the natural language processing (NLP) community , this domain has so far received little attention . In contrast , it is of enormous prominence in various social science disciplines , and some of that work follows the \u201dtext - as - data\u201d paradigm , seeking to employ quantitative methods for analyzing large amounts of CC - related text . Other research is qualitative in nature and studies details , nuances , actors , and motivations within CC discourses . Coming from both NLP and Political Science , and reviewing key works in both disciplines , we discuss how social science approaches to CC debates can inform advances in text - mining/NLP , and how , in return , NLP can support policy - makers and activists in making sense of large - scale and complex CC discourses across multiple genres , channels , topics , and communities . This is paramount for their ability to make rapid and meaningful impact on the discourse , and for shaping the necessary policy change . ","Method":["quantitative methods","social science approaches","nlp"],"Task":["natural language processing","cc debates","social science disciplines","policy change","text - mining/nlp","debate around climate change","nlp and political science","climate change debate","policy - makers"]},{"url":"https://www.semanticscholar.org/paper/a9d09338a3a0d9102c1e623e6ad434a446c0bbfe","title":"Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism","abstract":"Neutralisation techniques , e . g . denial of responsibility and denial of victim , are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view . We first draw on social science to introduce the problem to the community of nlp , present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change , and experiment with supervised and semi - supervised BERT - based models . ","Method":["automatic classification of neutralization techniques","neutralised techniques","supervised and semi - supervised bert - based models","neutralisation techniques","social science","coding schema"],"Task":["climate change","narrative of climate change scepticism","nlp","narrative of climate change"]},{"url":"https://www.semanticscholar.org/paper/086e6733e5fda70bbce0c7545bd06d5634918a60","title":"CLIMATE - FEVER : A Dataset for Verification of Real - World Climate Claims","abstract":"We introduce CLIMATE - FEVER , a new publicly available dataset for verification of climate change - related claims . By providing a dataset for the research community , we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate - specific claims , addressing the underlying language understanding challenges , and ultimately help alleviate the impact of misinformation on climate change . We adapt the methodology of FEVER [1] , the largest dataset of artificially designed claims , to real - life claims collected from the Internet . While during this process , we could rely on the expertise of renowned climate scientists , it turned out to be no easy task . We discuss the surprising , subtle complexity of modeling real - world climate - related claims within the \\\\textsc{fever} framework , which we believe provides a valuable challenge for general natural language understanding . We hope that our work will mark the beginning of a new exciting long - term joint effort by the climate science and AI community . ","Method":["\\\\textsc{fever} framework"],"Task":["verification of real - world climate claims","natural language understanding","language understanding challenges","climate - specific claims","climate change","retrieving evidential support","climate science and ai community","verification of climate change - related claims","real - world climate - related claims"]},{"url":"https://www.semanticscholar.org/paper/52ef2bea83eba1575fcfd02c82b4228f7aab0bd6","title":"Cheap Talk and Cherry - Picking : What ClimateBert has to say on Corporate Climate Risk Disclosures","abstract":"Disclosure of climate - related financial risks greatly helps investors assess companies\' preparedness for climate change . Voluntary disclosures such as those based on the recommendations of the Task Force for Climate - related Financial Disclosures (TCFD) are being hailed as an effective measure for better climate risk management . We ask whether this expectation is justified . We do so with the help of a deep neural language model , which we christen ClimateBert . We train ClimateBert on thousands of sentences related to climate - risk disclosures aligned with the TCFD recommendations . In analyzing the disclosures of TCFD - supporting firms , ClimateBert comes to the sobering conclusion that the firms\' TCFD support is mostly cheap talk and that firms cherry - pick to report primarily non - material climate risk information . From our analysis , we conclude that the only way out of this dilemma is to turn voluntary reporting into regulatory disclosures . ","Method":["climatebert","deep neural language model","cherry - picking"],"Task":["voluntary reporting","climate - related financial disclosures","regulatory disclosures","climate risk management","corporate climate risk disclosures","cheap talk","disclosure of climate - related financial risks","tcfd - supporting firms"]},{"url":"https://www.semanticscholar.org/paper/998039a4876edc440e0cabb0bc42239b0eb29644","title":"Tackling Climate Change with Machine Learning","abstract":"Climate change is one of the greatest challenges facing humanity , and we , as machine learning experts , may wonder how we can help . Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate . From smart grids to disaster management , we identify high impact problems where existing gaps can be filled by machine learning , in collaboration with other fields . Our recommendations encompass exciting research questions as well as promising business opportunities . We call on the machine learning community to join the global effort against climate change . ","Method":["machine learning"],"Task":["tackling climate change","machine learning experts","smart grids","machine learning community","climate change","reducing greenhouse gas emissions","disaster management"]},{"url":"https://www.semanticscholar.org/paper/f071e7c19fc18fc4d4ab69ce13f0c4e92cfb08c4","title":"Learning Twitter User Sentiments on Climate Change with Limited Labeled Data","abstract":"While it is well - documented that climate change accepters and deniers have become increasingly polarized in the United States over time , there has been no large - scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences . On the sub - population of Twitter users , we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the U . S . in 2018 . We begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic - level results in line with prior research . We then apply RNNs to conduct a cohort - level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change . However , this effect does not hold for the 2018 blizzard and wildfires studied , implying that Twitter users\' opinions on climate change are fairly ingrained on this subset of natural disasters . ","Method":["rnns","machine learning models"],"Task":["climate change","learning twitter user sentiments","cohort - level analysis"]},{"url":"https://www.semanticscholar.org/paper/bbc6486ad37365b77b9d0de8894f595d70af49ac","title":"Ask BERT : How Regulatory Disclosure of Transition and Physical Climate Risks affects the CDS Term Structure","abstract":"We use BERT , an AI - based algorithm for language understanding , to decipher regulatory climate - risk disclosures and measure their impact on the credit default swap (CDS) market . Risk disclosures can either increase or decrease credit spreads , depending on whether disclosure reveals new risks or sharpens the signal and decreases the uncertainty . Training BERT to differentiate between transition and physical climate risks , we find that disclosing transition risks increases CDS spreads , especially after the Paris Climate Agreement of 2015 , while disclosing physical climate risks leads to a decrease in CDS spreads . These impacts are statistically and economically highly significant . ","Method":["ai - based algorithm","bert"],"Task":["language understanding","credit default swap","regulatory climate - risk disclosures","regulatory disclosure of transition and physical climate risks","risk disclosures"]},{"url":"https://www.semanticscholar.org/paper/9b1653e3b57016958d10ff8531475eb0483d156c","title":"Social Privacy in Networked Publics : Teens\u2019 Attitudes , Practices , and Strategies","abstract":"This paper examines how teens understand privacy in highly public networked environments like Facebook and Twitter . We describe both teens\u2019 practices , their privacy strategies , and the structural conditions in which they are embedded , highlighting the ways in which privacy , as it plays out in everyday life , is related more to agency and the ability to control a social situation than particular properties of information . Finally , we discuss the implications of teens\u2019 practices and strategies , revealing the importance of social norms as a regulatory force . (This paper was presented at Oxford Internet Institute\u2019s \u201cA Decade in Internet Time : Symposium on the Dynamics of the Internet and Society\u201d on September 22 , 2011 . )","Method":["privacy strategies"],"Task":["social privacy"]},{"url":"https://www.semanticscholar.org/paper/1d7a0e8be2bbe7d148bb48a1f9de218beaea4ed5","title":"DeSMOG : Detecting Stance in Media On Global Warming","abstract":"Citing opinions is a powerful yet understudied strategy in argumentation . For example , an environmental activist might say , \u201cLeading scientists agree that global warming is a serious concern , \u201d framing a clause which affirms their own stance (\u201cthat global warming is serious\u201d) as an opinion endorsed (\\"[scientists] agree\u201d) by a reputable source (\u201cleading\u201d) . In contrast , a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt : \u201cMistaken scientists claim [ . . . ] . \\" Our work studies opinion - framing in the global warming (GW) debate , an increasingly partisan issue that has received little attention in NLP . We introduce DeSMOG , a dataset of stance - labeled GW sentences , and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other\u2019s opinions . From 56K news articles , we find that similar linguistic devices for self - affirming and opponent - doubting discourse are used across GW - accepting and skeptic media , though GW - skeptical media shows more opponent - doubt . We also find that authors often characterize sources as hypocritical , by ascribing opinions expressing the author\u2019s own view to source entities known to publicly endorse the opposing view . We release our stance dataset , model , and lexicons of framing devices for future work on opinion - framing and the automatic detection of GW stance . ","Method":["linguistic devices","lexicons of framing devices","bert classifier","desmog"],"Task":["global warming","global warming (gw) debate","argumentation","nlp","automatic detection of gw stance","opinion - framing","detecting stance in media","self - affirming and opponent - doubting discourse"]},{"url":"https://www.semanticscholar.org/paper/418b9f37b07a05b38798e97f57e17a6cc1048b92","title":"You are right . I am ALARMED - But by Climate Change Counter Movement","abstract":"The world is facing the challenge of climate crisis . Despite the consensus in scientific community about anthropogenic global warming , the web is flooded with articles spreading climate misinformation . These articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change . We revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of NLP . Despite considerable work in detection of fake news , there is no misinformation dataset available that is specific to the domain . of climate change . We try to bridge this gap by scraping and releasing articles with known climate change misinformation . ","Method":["NOT DETECTED"],"Task":["climate misinformation","nlp","detection of fake news","climate change","climate crisis","climate change counter movement","social sciences"]},{"url":"https://www.semanticscholar.org/paper/3475876c3f5e624587181744bc5acf4223aecd00","title":"Analyzing Polarization in Social Media : Method and Application to Tweets on 21 Mass Shootings","abstract":"We provide an NLP framework to uncover four linguistic dimensions of political polarization in social media : topic choice , framing , affect and illocutionary force . We quantify these aspects with existing lexical methods , and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional LDA - based models . We apply our methods to study 4 . 4M tweets on 21 mass shootings . We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice . We identify framing devices , such as grounding and the contrasting use of the terms \u201cterrorist\u201d and \u201ccrazy\u201d , that contribute to polarization . Results pertaining to topic choice , affect and illocutionary force suggest that Republicans focus more on the shooter and event - specific facts (news) while Democrats focus more on the victims and call for policy changes . Our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them . ","Method":["computational methods","lda - based models","clustering of tweet embeddings","lexical methods","nlp framework"],"Task":["framing","linguistic dimensions of political polarization","analyzing polarization in social media"]},{"url":"https://www.semanticscholar.org/paper/134c8486ae58b421681656c85bbc48dc862f6f98","title":"ClimaText : A Dataset for Climate Change Topic Detection","abstract":"Climate change communication in the mass media and other textual sources may affect and shape public perception . Extracting climate change information from these sources is an important task , e . g . , for filtering content and e - discovery , sentiment analysis , automatic summarization , question - answering , and fact - checking . However , automating this process is a challenge , as climate change is a complex , fast - moving , and often ambiguous topic with scarce resources for popular text - based AI tasks . In this paper , we introduce \\\\textsc{ClimaText} , a dataset for sentence - based climate change topic detection , which we make publicly available . We explore different approaches to identify the climate change topic in various text sources . We find that popular keyword - based models are not adequate for such a complex and evolving task . Context - based algorithms like BERT \\\\cite{devlin2018bert} can detect , in addition to many trivial cases , a variety of complex and implicit topic patterns . Nevertheless , our analysis reveals a great potential for improvement in several directions , such as , e . g . , capturing the discussion on indirect effects of climate change . Hence , we hope this work can serve as a good starting point for further research on this topic . ","Method":["\\\\textsc{climatext}","context - based algorithms","keyword - based models"],"Task":["sentiment analysis","sentence - based climate change topic detection","indirect effects of climate change","text - based ai tasks","climate change communication","fact - checking","filtering content and e - discovery","extracting climate change information","climate change topic detection","question - answering","climate change","climate change topic","automatic summarization"]},{"url":"https://www.semanticscholar.org/paper/171847842d161743f2b52dc4f85c121087ceb426","title":"Comparing Attitudes to Climate Change in the Media using sentiment analysis based on Latent Dirichlet Allocation","abstract":"News media typically present biased accounts of news stories , and different publications present different angles on the same event . In this research , we investigate how different publications differ in their approach to stories about climate change , by examining the sentiment and topics presented . To understand these attitudes , we find sentiment targets by combining Latent Dirichlet Allocation (LDA) with SentiWordNet , a general sentiment lexicon . Using LDA , we generate topics containing keywords which represent the sentiment targets , and then annotate the data using SentiWordNet before regrouping the articles based on topic similarity . Preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources . Ongoing work is investigating how systematic these attitudes are between different publications , and how these may change over time . ","Method":["lda","latent dirichlet allocation","general sentiment lexicon","sentiwordnet"],"Task":["sentiment analysis"]},{"url":"https://www.semanticscholar.org/paper/9c2cab0e72f2588e857671c1fbb13b5fba8f2586","title":"Cross - Platform Disinformation Campaigns : Lessons Learned and Next Steps","abstract":"We conducted a mixed - method , interpretative analysis of an online , cross - platform disinformation campaign targeting the White Helmets , a rescue group operating in rebel - held areas of Syria that has become the subject of a persistent effort of delegitimization . This research helps to conceptualize what a disinformation campaign is and how it works . Based on what we learned from this case study , we conclude that a comprehensive understanding of disinformation requires accounting for the spread of content across platforms and that social media platforms should increase collaboration to detect and characterize disinformation campaigns . ","Method":["mixed - method"],"Task":["disinformation","interpretative analysis","online , cross - platform disinformation campaign","disinformation campaign","disinformation campaigns","cross - platform disinformation campaigns"]},{"url":"https://www.semanticscholar.org/paper/7de1c0db3b3848d1a51dbfefce8b73e89a682177","title":"Classification of Moral Foundations in Microblog Political Discourse","abstract":"Previous works in computer science , as well as political and social science , have shown correlation in text between political ideologies and the moral foundations expressed within that text . Additional work has shown that policy frames , which are used by politicians to bias the public towards their stance on an issue , are also correlated with political ideology . Based on these associations , this work takes a first step towards modeling both the language and how politicians frame issues on Twitter , in order to predict the moral foundations that are used by politicians to express their stances on issues . The contributions of this work includes a dataset annotated for the moral foundations , annotation guidelines , and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans , as opposed to the unigrams of previous works , with policy frames for the prediction of the morality underlying political tweets . ","Method":["policy frames","probabilistic graphical models"],"Task":["microblog political discourse","classification of moral foundations","computer science","prediction of the morality underlying political tweets","political and social science"]},{"url":"https://www.semanticscholar.org/paper/29584ed6d68a06fdf91440a018f6bc83a44fd177","title":"Paragraph - level Rationale Extraction through Regularization : A case study on European Court of Human Rights Cases","abstract":"Interpretability or explainability is an emerging research field in NLP . From a user - centric point of view , the goal is to build models that provide proper justification for their decisions , similar to those of humans , by requiring the models to satisfy additional constraints . To this end , we introduce a new application on legal text where , contrary to mainstream literature targeting word - level rationales , we conceive rationales as selected paragraphs in multi - paragraph structured court cases . We also release a new dataset comprising European Court of Human Rights cases , including annotations for paragraph - level rationales . We use this dataset to study the effect of already proposed rationale constraints , i . e . , sparsity , continuity , and comprehensiveness , formulated as regularizers . Our findings indicate that some of these constraints are not beneficial in paragraph - level rationale extraction , while others need re - formulation to better handle the multi - label nature of the task we consider . We also introduce a new constraint , singularity , which further improves the quality of rationales , even compared with noisy rationale supervision . Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research . ","Method":["regularizers","regularization"],"Task":["interpretability","paragraph - level rationale extraction","nlp"]},{"url":"https://www.semanticscholar.org/paper/08e3ce4454012d928470efa96a54aa08dd8a96b0","title":"Automatically Characterizing Targeted Information Operations Through Biases Present in Discourse on Twitter","abstract":"This paper considers the problem of automatically characterizing biases that may be associated with emerging information operations via artificial intelligence . Accurate analysis of these emerging topics usually requires laborious , manual analysis by experts to annotate millions of tweets to identify biases in new topics . We introduce adaptations of the Word Embedding Association Test [1] to a new domain : information operations . We validate our method using known information operation - related tweets from Twitter\'s Transparency Reports , and we perform a case study on the COVID - 19 pandemic to evaluate our method\'s performance on non - labeled Twitter data , demonstrating its usability in emerging domains . ","Method":["word embedding association test [1]","artificial intelligence"],"Task":["information operations","automatically characterizing biases","accurate analysis","automatically characterizing targeted information operations"]},{"url":"https://www.semanticscholar.org/paper/2547836827e0423f198320977d393f574e0fb3d6","title":"Framing and Agenda - setting in Russian News : a Computational Analysis of Intricate Political Strategies","abstract":"Amidst growing concern over media manipulation , NLP attention has focused on overt strategies like censorship and \u201cfake news\u201d . Here , we draw on two concepts from political science literature to explore subtler strategies for government media manipulation : agenda - setting (selecting what topics to cover) and framing (deciding how topics are covered) . We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction : articles mention the U . S . more frequently in the month directly following an economic downturn in Russia . We introduce embedding - based methods for cross - lingually projecting English frames to Russian , and discover that these articles emphasize U . S . moral failings and threats to the U . S . Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda - setting and framing . ","Method":["computational analysis of intricate political strategies","embedding - based methods","subtler strategies","media manipulation strategies"],"Task":["distraction","agenda - setting","government media manipulation","framing","nlp attention","cross - lingually projecting english frames","media manipulation","political science literature"]},{"url":"https://www.semanticscholar.org/paper/546c87abc26137359c43898559884ba9d6c5ae64","title":"Predicting the Role of Political Trolls in Social Media","abstract":"We investigate the political roles of \u201cInternet trolls\u201d in social media . Political trolls , such as the ones linked to the Russian Internet Research Agency (IRA) , have recently gained enormous attention for their ability to sway public opinion and even influence elections . Analysis of the online traces of trolls has shown different behavioral patterns , which target different slices of the population . However , this analysis is manual and labor - intensive , thus making it impractical as a first - response tool for newly - discovered troll farms . In this paper , we show how to automate this analysis by using machine learning in a realistic setting . In particular , we show how to classify trolls according to their political role \u2014left , news feed , right\u2014 by using features extracted from social media , i . e . , Twitter , in two scenarios : (i) in a traditional supervised learning scenario , where labels for trolls are available , and (ii) in a distant supervision scenario , where labels for trolls are not available , and we rely on more - commonly - available labels for news outlets mentioned by the trolls . Technically , we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph , from which we extract several types of learned representations , i . e . , embeddings , for the trolls . Experiments on the \u201cIRA Russian Troll\u201d dataset show that our methodology improves over the state - of - the - art in the first scenario , while providing a compelling case for the second scenario , which has not been explored in the literature thus far . ","Method":["machine learning","learned representations","first - response tool"],"Task":["political trolls","supervised learning scenario","distant supervision scenario"]},{"url":"https://www.semanticscholar.org/paper/c0a2ee56fc80d971e920fba6179f81b515533592","title":"Historical Change in the Moral Foundations of Political Persuasion","abstract":"How have attempts at political persuasion changed over time? Using nine corpora dating back through 1789 , containing over 7 million words of speech (1 , 666 documents in total) , covering three different countries , plus the entire Google nGram corpus , we find that language relating to togetherness permanently crowded out language relating to duties and obligations in the persuasive speeches of politicians during the early 20th century . This shift is temporally predicted by a rise in Western nationalism and the mass movement of people from more rural to more urban areas and is unexplained by changes in language , private political speech , or nonmoral persuasion . We theorize that the emergence of the modern state in the 1920s had psychopolitical consequences for the ways that people understood and communicated their relationships with their government , which was then reflected in the levers of persuasion chosen by political elites . ","Method":["NOT DETECTED"],"Task":["political persuasion","nonmoral persuasion"]},{"url":"https://www.semanticscholar.org/paper/d89e89eac7882790b48ad73376742e7a93fea4fd","title":"Red Bots Do It Better : Comparative Analysis of Social Bot Partisan Behavior","abstract":"Recent research brought awareness of the issue of bots on social media and the significant risks of mass manipulation of public opinion in the context of political discussion . In this work , we leverage Twitter to study the discourse during the 2018 US midterm elections and analyze social bot activity and interactions with humans . We collected 2 . 6 million tweets for 42 days around the election day from nearly 1 million users . We use the collected tweets to answer three research questions : (i) Do social bots lean and behave according to a political ideology? (ii) Can we observe different strategies among liberal and conservative bots? (iii) How effective are bot strategies in engaging humans? We show that social bots can be accurately classified according to their political leaning and behave accordingly . Conservative bots share most of the topics of discussion with their human counterparts , while liberal bots show less overlap and a more inflammatory attitude . We studied bot interactions with humans and observed different strategies . Finally , we measured bots embeddedness in the social network and the extent of human engagement with each group of bots . Results show that conservative bots are more deeply embedded in the social network and more effective than liberal bots at exerting influence on humans . ","Method":["bot strategies","conservative bots","red bots","social bots","liberal bots"],"Task":["mass manipulation of public opinion","comparative analysis of social bot partisan behavior","political discussion"]},{"url":"https://www.semanticscholar.org/paper/a12d22ff91ce159a0d3558ed5aaed115115beabd","title":"Fine - Grained Analysis of Propaganda in News Article","abstract":"Propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda . Previous work has addressed propaganda detection at document level , typically labelling all articles from a propagandistic news outlet as propaganda . Such noisy gold labels inevitably affect the quality of any learning system trained on them . A further issue with most existing systems is the lack of explainability . To overcome these limitations , we propose a novel task : performing fine - grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type . In particular , we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure . We further design a novel multi - granularity neural network , and we show that it outperforms several strong BERT - based baselines . ","Method":["propaganda techniques","bert - based baselines","learning system","multi - granularity neural network"],"Task":["fine - grained analysis of texts","fine - grained analysis of propaganda","propaganda detection"]},{"url":"https://www.semanticscholar.org/paper/9e82aba279807a6c8541c8811f049a40ebca36c1","title":"Issue Framing in Online Discussion Fora","abstract":"In online discussion fora , speakers often make arguments for or against something , say birth control , by highlighting certain aspects of the topic . In social science , this is referred to as issue framing . In this paper , we introduce a new issue frame annotated corpus of online discussions . We explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora , using a combination of multi - task and adversarial training , assuming only unlabeled training data in the target domain . ","Method":["adversarial training","multi - task"],"Task":["social science","issue framing","online discussion"]},{"url":"https://www.semanticscholar.org/paper/c0472b63d7d45d948dea42c4b5efffb6859541f9","title":"Technology , Autonomy , and Manipulation","abstract":"Since 2016 , when the Facebook/Cambridge Analytica scandal began to emerge , public concern has grown around the threat of \u201conline manipulation\u201d . While these worries are familiar to privacy researchers , this paper aims to make them more salient to policymakers \u2014 first , by defining \u201conline manipulation\u201d , thus enabling identification of manipulative practices; and second , by drawing attention to the specific harms online manipulation threatens . We argue that online manipulation is the use of information technology to covertly influence another person\u2019s decision - making , by targeting and exploiting their decision - making vulnerabilities . Engaging in such practices can harm individuals by diminishing their economic interests , but its deeper , more insidious harm is its challenge to individual autonomy . We explore this autonomy harm , emphasising its implications for both individuals and society , and we briefly outline some strategies for combating online manipulation and strengthening autonomy in an increasingly digital world . ","Method":["information technology"],"Task":["online manipulation","manipulation","manipulation\u201d","autonomy","privacy researchers","identification of manipulative practices;"]},{"url":"https://www.semanticscholar.org/paper/20a37289d0b7c90065850eb7bf96d5ac3c8a21e4","title":"Modeling Frames in Argumentation","abstract":"In argumentation , framing is used to emphasize a specific aspect of a controversial topic while concealing others . When talking about legalizing drugs , for instance , its economical aspect may be emphasized . In general , we call a set of arguments that focus on the same aspect a frame . An argumentative text has to serve the \u201cright\u201d frame(s) to convince the audience to adopt the author\u2019s stance (e . g . , being pro or con legalizing drugs) . More specifically , an author has to choose frames that fit the audience\u2019s cultural background and interests . This paper introduces frame identification , which is the task of splitting a set of arguments into non - overlapping frames . We present a fully unsupervised approach to this task , which first removes topical information and then identifies frames using clustering . For evaluation purposes , we provide a corpus with 12 , 326 debate - portal arguments , organized along the frames of the debates\u2019 topics . On this corpus , our approach outperforms different strong baselines , achieving an F1 - score of 0 . 28 . ","Method":["clustering","unsupervised approach"],"Task":["modeling frames","framing","frame identification","argumentation"]},{"url":"https://www.semanticscholar.org/paper/16981cc4ddefd3ea7655754fd83a2a8ff2203a8b","title":"Automatically Neutralizing Subjective Bias in Text","abstract":"Texts like news , encyclopedias , and some social media strive for objectivity . Yet bias in the form of inappropriate subjectivity - introducing attitudes via framing , presupposing truth , and casting doubt - remains ubiquitous . This kind of bias erodes our collective trust and fuels social conflict . To address this issue , we introduce a novel testbed for natural language generation : automatically bringing inappropriately subjective text into a neutral point of view (\\"neutralizing\\" biased text) . We also offer the first parallel corpus of biased language . The corpus contains 180 , 000 sentence pairs and originates from Wikipedia edits that removed various framings , presuppositions , and attitudes from biased sentences . Last , we propose two strong encoder - decoder baselines for the task . A straightforward yet opaque CONCURRENT system uses a BERT encoder to identify subjective words as part of the generation process . An interpretable and controllable MODULAR algorithm separates these steps , using (1) a BERT - based classifier to identify problematic words and (2) a novel join embedding through which the classifier can edit the hidden states of the encoder . Large - scale human evaluation across four domains (encyclopedias , news headlines , books , and political speeches) suggests that these algorithms are a first step towards the automatic identification and reduction of bias . ","Method":["bert encoder","modular algorithm","classifier","encoder","concurrent system","join embedding","bert - based classifier","encoder - decoder baselines"],"Task":["generation process","automatically neutralizing subjective bias","automatic identification and reduction of bias","natural language generation"]},{"url":"https://www.semanticscholar.org/paper/a0a5669b310ceb3840a599bf4f27a077a643d613","title":"A Systematic Media Frame Analysis of 1 . 5 Million New York Times Articles from 2000 to 2017","abstract":"Framing is an indispensable narrative device for news media because even the same facts may lead to conflicting understandings if deliberate framing is employed . Therefore , identifying media framing is a crucial step to understanding how news media influence the public . Framing is , however , difficult to operationalize and detect , and thus traditional media framing studies had to rely on manual annotation , which is challenging to scale up to massive news datasets . Here , by developing a media frame classifier that achieves state - of - the - art performance , we systematically analyze the media frames of 1 . 5 million New York Times articles published from 2000 to 2017 . By examining the ebb and flow of media frames over almost two decades , we show that short - term frame abundance fluctuation closely corresponds to major events , while there also exist several long - term trends , such as the gradually increasing prevalence of the \u201cCultural identity\u201d frame . By examining specific topics and sentiments , we identify characteristics and dynamics of each frame . Finally , as a case study , we delve into the framing of mass shootings , revealing three major framing patterns . Our scalable , computational approach to massive news datasets opens up new pathways for systematic media framing studies . ","Method":["computational approach","narrative device","media frame analysis","deliberate framing","media frame classifier"],"Task":["framing of mass shootings","framing","systematic media framing studies","media framing","media framing studies","manual annotation"]},{"url":"https://www.semanticscholar.org/paper/245bcbdfff19721c226059a5d5b1ae71b67e0572","title":"FrameAxis : Characterizing Framing Bias and Intensity with Word Embedding","abstract":"We propose FrameAxis , a method of characterizing the framing of a given text by identifying the most relevant semantic axes (\\"microframes\\") defined by antonym word pairs . In contrast to the traditional framing analysis , which has been constrained by a small number of manually annotated general frames , our unsupervised approach provides much more detailed insights , by considering a host of semantic axes . Our method is capable of quantitatively teasing out framing bias - - how biased a text is in each microframe - - and framing intensity - - how much each microframe is used - - from the text , offering a nuanced characterization of framing . We evaluate our approach using SemEval datasets as well as three other datasets and human evaluations , demonstrating that FrameAxis can reliably characterize documents with relevant microframes . Our method may allow scalable and nuanced computational analyses of framing across disciplines . ","Method":["framing analysis","frameaxis","word embedding","unsupervised approach"],"Task":["computational analyses of framing","framing"]},{"url":"https://www.semanticscholar.org/paper/2cd74561109e2675634e0f4f0129fcfbc238a1d8","title":"Connotation Frames of Power and Agency in Modern Films","abstract":"The framing of an action influences how we perceive its actor . We introduce connotation frames of power and agency , a pragmatic formalism organized using frame semantic representations , to model how different levels of power and agency are implicitly projected on actors through their actions . We use the new power and agency frames to measure the subtle , but prevalent , gender bias in the portrayal of modern film characters and provide insights that deviate from the well - known Bechdel test . Our contributions include an extended lexicon of connotation frames along with a web interface that provides a comprehensive analysis through the lens of connotation frames . ","Method":["frame semantic representations","pragmatic formalism","connotation frames"],"Task":["NOT DETECTED"]},{"url":"https://www.semanticscholar.org/paper/15130cdd46ff7e3a69384e6c62ab1775c4aba96f","title":"Analyzing Framing through the Casts of Characters in the News","abstract":"We present an unsupervised model for the discovery and clustering of latent \u201cpersonas\u201d (characterizations of entities) . Our model simultaneously clusters documents featuring similar collections of personas . We evaluate this model on a collection of news articles about immigration , showing that personas help predict the coarse - grained framing annotations in the Media Frames Corpus . We also introduce automated model selection as a fair and robust form of feature evaluation . ","Method":["automated model selection","unsupervised model"],"Task":["analyzing framing","feature evaluation","discovery and clustering of latent \u201cpersonas\u201d (characterizations of entities)"]},{"url":"https://www.semanticscholar.org/paper/92408cc19033cc4af29accef3793014ab79355c2","title":"The Media Frames Corpus : Annotations of Frames Across Issues","abstract":"We describe the first version of the Media Frames Corpus : several thousand news articles on three policy issues , annotated in terms of media framing . We motivate framing as a phenomenon of study for computational linguistics and describe our annotation process . ","Method":["annotation process"],"Task":["computational linguistics","framing","annotations of frames","media framing"]},{"url":"https://www.semanticscholar.org/paper/c899c6048a1bd99590972c5fb2f9fd00db8d4e18","title":"Who Falls for Online Political Manipulation?","abstract":"Social media , once hailed as a vehicle for democratization and the promotion of positive social change across the globe , are under attack for becoming a tool of political manipulation and spread of disinformation . A case in point is the alleged use of trolls by Russia to spread malicious content in Western elections . This paper examines the Russian interference campaign in the 2016 US presidential election on Twitter . Our aim is twofold : first , we test whether predicting users who spread trolls\u2019 content is feasible in order to gain insight on how to contain their influence in the future; second , we identify features that are most predictive of users who either intentionally or unintentionally play a vital role in spreading this malicious content . We collected a dataset with over 43 million election - related posts shared on Twitter between September 16 and November 9 , 2016 , by about 5 . 7 million users . This dataset includes accounts associated with the Russian trolls identified by the US Congress . Proposed models are able to very accurately identify users who spread the trolls\u2019 content (average AUC score of 96% , using 10 - fold validation) . We show that political ideology , bot likelihood scores , and some activity - related account meta data are the most predictive features of whether a user spreads trolls\u2019 content or not . ","Method":["NOT DETECTED"],"Task":["russian interference campaign","political manipulation","online political manipulation?","positive social change","democratization","spread of disinformation"]},{"url":"https://www.semanticscholar.org/paper/2a501b074261e81b9126e80a0a308cfa5e76f8c1","title":"Linguistic Models for Analyzing and Detecting Biased Language","abstract":"Unbiased language is a requirement for reference sources like encyclopedias and scientific texts . Bias is , nonetheless , ubiquitous , making it crucial to understand its nature and linguistic realization and hence detect bias automatically . To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles . The analysis uncovers two classes of bias : framing bias , such as praising or perspective - specific words , which we link to the literature on subjectivity; and epistemological bias , related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true . We identify common linguistic cues for these classes , including factive verbs , implicatives , hedges , and subjective intensifiers . These insights help us develop features for a model to solve a new prediction task of practical importance : given a biased sentence , identify the bias - inducing word . Our linguistically - informed model performs almost as well as humans tested on the same task . ","Method":["linguistically - informed model","linguistic models"],"Task":["unbiased language","analyzing and detecting biased language","prediction task"]},{"url":"https://www.semanticscholar.org/paper/1778d1da6264ed4274fb94115ebbc3b4f34c9f7a","title":"Misinfo Belief Frames : A Case Study on Covid & Climate News","abstract":"Prior beliefs of readers impact the way in which they project meaning onto news headlines . These beliefs can influence their perception of news reliability , as well as their reaction to news , and their likelihood of spreading the misinformation through social networks . However , most prior work focuses on fact - checking veracity of news or stylometry rather than measuring impact of misinformation . We propose Misinfo Belief Frames , a formalism for understanding how readers perceive the reliability of news and the impact of misinformation . We also introduce the Misinfo Belief Frames (MBF) corpus , a dataset of 66k inferences over 23 . 5k headlines . Misinformation frames use commonsense reasoning to uncover implications of real and fake news headlines focused on global crises : the Covid - 19 pandemic and climate change . Our results using large - scale language modeling to predict misinformation frames show that machine - generated inferences can influence readers\u2019 trust in news headlines (readers\u2019 trust in news headlines was affected in 29 . 3% of cases) . This demonstrates the potential effectiveness of using generated frames to counter misinformation . ","Method":["misinfo belief frames","commonsense reasoning","generated frames","large - scale language modeling","machine - generated inferences"],"Task":["fact - checking veracity of news"]},{"url":"https://www.semanticscholar.org/paper/460986c12b9fdedc3bf9a710c429c5a84608055a","title":"Fighting the COVID - 19 Infodemic in Social Media : A Holistic Perspective and a Call to Arms","abstract":"With the outbreak of the COVID - 19 pandemic , people turned to social media to read and to share timely information including statistics , warnings , advice , and inspirational stories . Unfortunately , alongside all this useful information , there was also a new blending of medical and political misinformation and disinformation , which gave rise to the first global infodemic . While fighting this infodemic is typically thought of in terms of factuality , the problem is much broader as malicious content includes not only fake news , rumors , and conspiracy theories , but also promotion of fake cures , panic , racism , xenophobia , and mistrust in the authorities , among others . This is a complex problem that needs a holistic approach combining the perspectives of journalists , fact - checkers , policymakers , government entities , social media platforms , and society as a whole . Taking them into account we define an annotation schema and detailed annotation instructions , which reflect these perspectives . We performed initial annotations using this schema , and our initial experiments demonstrated sizable improvements over the baselines . Now , we issue a call to arms to the research community and beyond to join the fight by supporting our crowdsourcing annotation efforts . ","Method":["annotation schema","crowdsourcing annotation efforts"],"Task":["NOT DETECTED"]},{"url":"https://www.semanticscholar.org/paper/9129399d6b1599508709e3a72f80defd52ef89c9","title":"The online competition between pro - and anti - vaccination views","abstract":"Distrust in scientific expertise 1 \u2013 14 is dangerous . Opposition to vaccination with a future vaccine against SARS - CoV - 2 , the causal agent of COVID - 19 , for example , could amplify outbreaks 2 \u2013 4 , as happened for measles in 2019 5 , 6 . Homemade remedies 7 , 8 and falsehoods are being shared widely on the Internet , as well as dismissals of expert advice 9 \u2013 11 . There is a lack of understanding about how this distrust evolves at the system level 13 , 14 . Here we provide a map of the contention surrounding vaccines that has emerged from the global pool of around three billion Facebook users . Its core reveals a multi - sided landscape of unprecedented intricacy that involves nearly 100\xa0million individuals partitioned into highly dynamic , interconnected clusters across cities , countries , continents and languages . Although smaller in overall size , anti - vaccination clusters manage to become highly entangled with undecided clusters in the main online network , whereas pro - vaccination clusters are more peripheral . Our theoretical framework reproduces the recent explosive growth in anti - vaccination views , and predicts that these views will dominate in a decade . Insights provided by this framework can inform new policies and approaches to interrupt this shift to negative views . Our results challenge the conventional thinking about undecided individuals in issues of contention surrounding health , shed light on other issues of contention such as climate change 11 , and highlight the key role of network cluster dynamics in multi - species ecologies 15 . Insights into the interactions between pro - and anti - vaccination clusters on Facebook can enable policies and approaches that attempt to interrupt the shift to anti - vaccination views and persuade undecided individuals to adopt a pro - vaccination stance . ","Method":["NOT DETECTED"],"Task":["contention","contention surrounding health","vaccination","climate change","network cluster dynamics","online competition","multi - species ecologies"]},{"url":"https://www.semanticscholar.org/paper/ed1c17451a23471afde91c109ecadc6aab8b2ba6","title":"A Survey on Multimodal Disinformation Detection","abstract":"Recent years have witnessed the proliferation of fake news , propaganda , misinformation , and disinformation online . While initially this was mostly about textual content , over time images and videos gained popularity , as they are much easier to consume , attract much more attention , and spread further than simple text . As a result , researchers started targeting different modalities and combinations thereof . As different modalities are studied in different research communities , with insufficient interaction , here we offer a survey that explores the state - of - the - art on multimodal disinformation detection covering various combinations of modalities : text , images , audio , video , network structure , and temporal information . Moreover , while some studies focused on factuality , others investigated how harmful the content is . While these two components in the definition of disinformation \u2013 (i) factuality and (ii) harmfulness , are equally important , they are typically studied in isolation . Thus , we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness , in the same framework . Finally , we discuss current challenges and future research directions . ","Method":["NOT DETECTED"],"Task":["disinformation detection","disinformation","multimodal disinformation detection"]},{"url":"https://www.semanticscholar.org/paper/38d243b9f6e2c786699dbc83513fb190372cde07","title":"Automated Fact - Checking for Assisting Human Fact - Checkers","abstract":"The reporting and analysis of current events around the globe has expanded from professional , editorlead journalism all the way to citizen journalism . Politicians and other key players enjoy direct access to their audiences through social media , bypassing the filters of official cables or traditional media . However , the multiple advantages of free speech and direct communication are dimmed by the misuse of the media to spread inaccurate or misleading claims . These phenomena have led to the modern incarnation of the fact - checker \u2014 a professional whose main aim is to examine claims using available evidence to assess their veracity . As in other text forensics tasks , the amount of information available makes the work of the fact - checker more difficult . With this in mind , starting from the perspective of the professional fact - checker , we survey the available intelligent technologies that can support the human expert in the different steps of her fact - checking endeavor . These include identifying claims worth fact - checking; detecting relevant previously fact - checked claims; retrieving relevant evidence to fact - check a claim; and actually verifying a claim . In each case , we pay attention to the challenges in future work and the potential impact on real - world fact - checking . ","Method":["intelligent technologies","fact - checker"],"Task":["reporting and analysis of current events","fact - checking","fact - checking endeavor","citizen journalism","human fact - checkers","fact - checking;","text forensics tasks","automated fact - checking"]},{"url":"https://www.semanticscholar.org/paper/76de89ca66f898e8211acba7392ef2d4a7e14125","title":"Fakeddit : A New Multimodal Benchmark Dataset for Fine - grained Fake News Detection","abstract":"Fake news has altered society in negative ways in politics and culture . It has adversely affected both online social network systems as well as offline communities and conversations . Using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news . However , a lack of effective , comprehensive datasets has been a problem for fake news research and detection model development . Prior fake news datasets do not provide multimodal text and image data , metadata , comment data , and fine - grained fake news categorization at the scale and breadth of our dataset . We present Fakeddit , a novel multimodal dataset consisting of over 1 million samples from multiple categories of fake news . After being processed through several stages of review , the samples are labeled according to 2 - way , 3 - way , and 6 - way classification categories through distant supervision . We construct hybrid text+image models and perform extensive experiments for multiple variations of classification , demonstrating the importance of the novel aspect of multimodality and fine - grained classification unique to Fakeddit . ","Method":["hybrid text+image models","fakeddit","automatic machine learning classification models"],"Task":["fine - grained fake news detection","dissemination of fake news","detection model development","fake news research","review","online social network systems","classification","fine - grained classification","fine - grained fake news categorization"]},{"url":"https://www.semanticscholar.org/paper/c6a9ca56c93323c0199dd22631d1cf731bdd7ec1","title":"Automatic Detection of Fake News","abstract":"The proliferation of misleading information in everyday access media outlets such as social media feeds , news blogs , and online newspapers have made it challenging to identify trustworthy news sources , thus increasing the need for computational tools able to provide insights into the reliability of online content . In this paper , we focus on the automatic identification of fake content in online news . Our contribution is twofold . First , we introduce two novel datasets for the task of fake news detection , covering seven different news domains . We describe the collection , annotation , and validation process in detail and present several exploratory analyses on the identification of linguistic differences in fake and legitimate news content . Second , we conduct a set of learning experiments to build accurate fake news detectors , and show that we can achieve accuracies of up to 76% . In addition , we provide comparative analyses of the automatic and manual identification of fake news . ","Method":["fake news detectors","computational tools"],"Task":["automatic identification of fake content","fake news detection","automatic detection of fake news","automatic and manual identification of fake news","identification of linguistic differences","learning","validation process","annotation"]},{"url":"https://www.semanticscholar.org/paper/03c294ad75bd1bac92217419ac25358227f6a901","title":"\\"Liar , Liar Pants on Fire\\" : A New Benchmark Dataset for Fake News Detection","abstract":"Automatic fake news detection is a challenging problem in deception detection , and it has tremendous real - world political and social impacts . However , statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets . In this paper , we present liar : a new , publicly available dataset for fake news detection . We collected a decade - long , 12 . 8K manually labeled short statements in various contexts from PolitiFact . com , which provides detailed analysis report and links to source documents for each case . This dataset can be used for fact - checking research as well . Notably , this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type . Empirically , we investigate automatic fake news detection based on surface - level linguistic patterns . We have designed a novel , hybrid convolutional neural network to integrate meta - data with text . We show that this hybrid approach can improve a text - only deep learning model . ","Method":["text - only deep learning model","statistical approaches","liar","hybrid convolutional neural network"],"Task":["fake news detection","deception detection","fake news","fact - checking research","automatic fake news detection"]},{"url":"https://www.semanticscholar.org/paper/8415274c8fb370cbab84ad82ab2f469786ddee72","title":"Weaponized Health Communication : Twitter Bots and Russian Trolls Amplify the Vaccine Debate","abstract":"Objectives To understand how Twitter bots and trolls (\u201cbots\u201d) promote online health content . Methods We compared bots\u2019 to average users\u2019 rates of vaccine - relevant messages , which we collected online from July 2014 through September 2017 . We estimated the likelihood that users were bots , comparing proportions of polarized and antivaccine tweets across user types . We conducted a content analysis of a Twitter hashtag associated with Russian troll activity . Results Compared with average users , Russian trolls (\u03c72(1)\u2009=\u2009102 . 0; P\u2009<\u2009 . 001) , sophisticated bots (\u03c72(1)\u2009=\u200928 . 6; P\u2009<\u2009 . 001) , and \u201ccontent polluters\u201d (\u03c72(1)\u2009=\u20097 . 0; P\u2009<\u2009 . 001) tweeted about vaccination at higher rates . Whereas content polluters posted more antivaccine content (\u03c72(1)\u2009=\u200911 . 18; P\u2009<\u2009 . 001) , Russian trolls amplified both sides . Unidentifiable accounts were more polarized (\u03c72(1)\u2009=\u200912 . 1; P\u2009<\u2009 . 001) and antivaccine (\u03c72(1)\u2009=\u200935 . 9; P\u2009<\u2009 . 001) . Analysis of the Russian troll hashtag showed that its messages were more political and divisive . Conclusions Whereas bots that spread malware and unsolicited content disseminated antivaccine messages , Russian trolls promoted discord . Accounts masquerading as legitimate users create false equivalency , eroding public consensus on vaccination . Public Health Implications . Directly confronting vaccine skeptics enables bots to legitimize the vaccine debate . More research is needed to determine how best to combat bot - driven content . ","Method":["content analysis"],"Task":["bot - driven content","weaponized health communication","vaccine debate"]},{"url":"https://www.semanticscholar.org/paper/1dad69f1fd4403aed4d3d709ab794113291d625c","title":"Fake News : Spread of Misinformation about Urological Conditions on Social Media . ","abstract":"Although there is a large amount of user - generated content about urological health issues on social media , much of this content has not been vetted for information accuracy . In this article , we review the literature on the quality and balance of information on urological health conditions on social networks . Across a wide range of benign and malignant urological conditions , studies show a substantial amount of commercial , biased and/or inaccurate information present on popular social networking sites . The healthcare community should take proactive steps to improve the quality of medical information on social networks . PATIENT SUMMARY : In this review , we examined the spread of misinformation about urological health conditions on social media . We found that a significant amount of the circulating information is commercial , biased or misinformative . ","Method":["NOT DETECTED"],"Task":["urological health conditions","medical information"]},{"url":"https://www.semanticscholar.org/paper/7d3c2ff37d04914836f9cbd9ce54b6c97aa74a22","title":"Truth of Varying Shades : Analyzing Language in Fake News and Political Fact - Checking","abstract":"We present an analytic study on the language of news media in the context of political fact - checking and fake news detection . We compare the language of real news with that of satire , hoaxes , and propaganda to find linguistic characteristics of untrustworthy text . To probe the feasibility of automatic political fact - checking , we also present a case study based on PolitiFact . com using their factuality judgments on a 6 - point scale . Experiments show that while media fact - checking remains to be an open research question , stylistic cues can help determine the truthfulness of text . ","Method":["NOT DETECTED"],"Task":["analyzing language in fake news","fake news detection","automatic political fact - checking","political fact - checking","media fact - checking"]},{"url":"https://www.semanticscholar.org/paper/29370adbf0674ad72d5e0bcc59582c22035bd0aa","title":"Coronavirus on Social Media : Analyzing Misinformation in Twitter Conversations","abstract":"The ongoing Coronavirus Disease (COVID - 19) pandemic highlights the interconnected - ness of our present - day globalized world . With social distancing policies in place , virtual communication has become an important source of (mis)information . As increasing number of people rely on social media platforms for news , identifying misinformation has emerged as a critical task in these unprecedented times . In addition to being malicious , the spread of such information poses a serious public health risk . To this end , we design a dashboard to track misinformation on popular social media news sharing platform - Twitter . Our dashboard allows visibility into the social media discussions around Coronavirus and the quality of information shared on the platform as the situation evolves . We collect streaming data using the Twitter API from March 1 , 2020 to date and provide analysis of topic clusters and social sentiments related to important emerging policies such as \\"#socialdistancing\\" and \\"#workfromhome\\" . We track emerging hashtags over time , and provide location and time sensitive analysis of sentiments . In addition , we study the challenging problem of misinformation on social media , and provide a detection method to identify false , misleading and clickbait contents from Twitter information cascades . The dashboard maintains an evolving list of detected misinformation cascades with the corresponding detection scores , accessible online athttps : //ksharmar . this http URL . ","Method":["twitter api","dashboard","detection method"],"Task":["coronavirus on social media","misinformation","analyzing misinformation","virtual communication","location and time sensitive analysis of sentiments","twitter conversations","analysis of topic clusters","identifying misinformation","track misinformation","coronavirus disease"]},{"url":"https://www.semanticscholar.org/paper/1213d98f3d0a42d575bbd2c2af0309c7f76024a1","title":"Combating Fake News : A Survey on Identification and Mitigation Techniques","abstract":"The proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news , and mitigation of its widespread impact on public opinion . While much of the earlier research was focused on identification of fake news based on its contents or by exploiting users\' engagements with the news on social media , there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society . In this survey , we describe the modern - day problem of fake news and , in particular , highlight the technical challenges associated with it . We discuss existing methods and techniques applicable to both identification and mitigation , with a focus on the significant advances in each method and their advantages and limitations . In addition , research has often been limited by the quality of existing datasets and their specific application contexts . To alleviate this problem , we comprehensively compile and summarize characteristic features of available datasets . Furthermore , we outline new directions of research to facilitate future development of effective and interdisciplinary solutions . ","Method":["identification and mitigation techniques","intervention strategies","interdisciplinary solutions"],"Task":["identification of fake news","identification","combating fake news","fake news","mitigation","timely identification and containment of fake news"]},{"url":"https://www.semanticscholar.org/paper/bc8c416f821b93795370524247c8a455c373ee6e","title":"Fake News : A Survey of Research , Detection Methods , and Opportunities","abstract":"The explosive growth in fake news and its erosion to democracy , justice , and public trust has increased the demand for fake news analysis , detection and intervention . This survey comprehensively and systematically reviews fake news research . The survey identifies and specifies fundamental theories across various disciplines , e . g . , psychology and social science , to facilitate and enhance the interdisciplinary research of fake news . Current fake news research is reviewed , summarized and evaluated . These studies focus on fake news from four perspective : (1) the false knowledge it carries , (2) its writing style , (3) its propagation patterns , and (4) the credibility of its creators and spreaders . We characterize each perspective with various analyzable and utilizable information provided by news and its spreaders , various strategies and frameworks that are adaptable , and techniques that are applicable . By reviewing the characteristics of fake news and open issues in fake news studies , we highlight some potential research tasks at the end of this survey . ","Method":["detection methods"],"Task":["fake news analysis","detection and intervention","interdisciplinary research of fake news","social science"]},{"url":"https://www.semanticscholar.org/paper/9e8ac8df8b46c36cad3f307f85975012479b5a32","title":"Fact or Fiction : Verifying Scientific Claims","abstract":"We introduce scientific claim verification , a new task to select abstracts from the research literature containing evidence that supports or refutes a given scientific claim , and to identify rationales justifying each decision . To study this task , we construct SciFact , a dataset of 1 . 4K expert - written scientific claims paired with evidence - containing abstracts annotated with labels and rationales . We develop baseline models for SciFact , and demonstrate that these models benefit from combined training on a large dataset of claims about Wikipedia articles , together with the new SciFact data . We show that our claim verification system is able to identify plausible evidence for 23 / 36 claims relevant to COVID - 19 on the CORD - 19 corpus . Our results and experiments strongly suggest that our new task and data will support significant future research efforts . ","Method":["claim verification system"],"Task":["scifact","verifying scientific claims","scientific claim verification"]},{"url":"https://www.semanticscholar.org/paper/1e3d1055fc65966bc2c64505785207d7e8d71022","title":"Rumor Cascades","abstract":"Online social networks provide a rich substrate for rumor propagation . Information received via friends tends to be trusted , and online social networks allow individuals to transmit information to many friends at once . By referencing known rumors from Snopes . com , a popular website documenting memes and urban legends , we track the propagation of thousands of rumors appearing on Facebook . From this sample we infer the rates at which rumors from different categories and of varying truth value are uploaded and reshared . We find that rumor cascades run deeper in the social network than reshare cascades in general . We then examine the effect of individual reshares receiving a comment containing a link to a Snopes article on the evolution of the cascade . We find that receiving such a comment increases the likelihood that a reshare of a rumor will be deleted . Furthermore , large cascades are able to accumulate hundreds of Snopes comments while continuing to propagate . Finally , using a dataset of rumors copied and pasted from one status update to another , we show that rumors change over time and that different variants tend to dominate different bursts in popularity . ","Method":["NOT DETECTED"],"Task":["rumor propagation"]},{"url":"https://www.semanticscholar.org/paper/6f373f4711e1285bdec23069c9503d3bf77bfaef","title":"A Benchmark Dataset of Check - worthy Factual Claims","abstract":"In this paper we present the ClaimBuster dataset of 23 , 533 statements extracted from all U . S . general election presidential debates and annotated by human coders . The ClaimBuster dataset can be leveraged in building computational methods to identify claims that are worth fact - checking from the myriad of sources of digital or traditional media . The ClaimBuster dataset is publicly available to the research community , and it can be found at this http URL . ","Method":["computational methods"],"Task":["NOT DETECTED"]},{"url":"https://www.semanticscholar.org/paper/44ecb9783e43d726fc87b7ffdd580b01e13a85cb","title":"Rumors , False Flags , and Digital Vigilantes : Misinformation on Twitter after the 2013 Boston Marathon Bombing","abstract":"The Boston Marathon bombing story unfolded on every possible carrier of information available in the spring of 2013 , including Twitter . As information spread , it was filled with rumors (unsubstantiated information) , and many of these rumors contained misinformation . Earlier studies have suggested that crowdsourced information flows can correct misinformation , and our research investigates this proposition . This exploratory research examines three rumors , later demonstrated to be false , that circulated on Twitter in the aftermath of the bombings . Our findings suggest that corrections to the misinformation emerge but are muted compared with the propagation of the misinformation . The similarities and differences we observe in the patterns of the misinformation and corrections contained within the stream over the days that followed the attacks suggest directions for possible research strategies to automatically detect misinformation . ","Method":["NOT DETECTED"],"Task":["digital vigilantes"]},{"url":"https://www.semanticscholar.org/paper/69116800a8a8195531d29c8e14cefb1c92cbb8a7","title":"Extractive and Abstractive Explanations for Fact - Checking and Evaluation of News","abstract":"In this paper , we explore the construction of natural language explanations for news claims , with the goal of assisting fact - checking and news evaluation applications . We experiment with two methods : (1) an extractive method based on Biased TextRank \u2013 a resource - effective unsupervised graph - based algorithm for content extraction; and (2) an abstractive method based on the GPT - 2 language model . We perform comparative evaluations on two misinformation datasets in the political and health news domains , and find that the extractive method shows the most promise . ","Method":["abstractive method","extractive and abstractive explanations","unsupervised graph - based algorithm","biased textrank","extractive method","gpt - 2 language model"],"Task":["fact - checking and evaluation of news","news claims","natural language explanations","fact - checking","content extraction;","news evaluation applications"]},{"url":"https://www.semanticscholar.org/paper/20b2f18aaf10a9221c5edf3720d4cce7da672104","title":"That is a Known Lie : Detecting Previously Fact - Checked Claims","abstract":"The recent proliferation of \u201dfake news\u201d has triggered a number of responses , most notably the emergence of several manual fact - checking initiatives . As a result and over time , a large number of fact - checked claims have been accumulated , which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact - checked by some trusted fact - checking organization , as viral claims often come back after a while in social media , and politicians like to repeat their favorite statements , true or false , over and over again . As manual fact - checking is very time - consuming (and fully automatic fact - checking has credibility issues) , it is important to try to save this effort and to avoid wasting time on claims that have already been fact - checked . Interestingly , despite the importance of the task , it has been largely ignored by the research community so far . Here , we aim to bridge this gap . In particular , we formulate the task and we discuss how it relates to , but also differs from , previous work . We further create a specialized dataset , which we release to the research community . Finally , we present learning - to - rank experiments that demonstrate sizable improvements over state - of - the - art retrieval and textual similarity approaches . ","Method":["retrieval and textual similarity approaches","fact - checking organization"],"Task":["automatic fact - checking","fact - checking initiatives","manual fact - checking","detecting previously fact - checked claims"]},{"url":"https://www.semanticscholar.org/paper/ad7129af0644dbcafa9aa2f111cb76526ea444a1","title":"Defending Against Neural Fake News","abstract":"Recent progress in natural language generation has raised dual - use concerns . While applications like summarization and translation are positive , the underlying technology also might enable adversaries to generate neural fake news : targeted propaganda that closely mimics the style of real news . \\nModern computer security relies on careful threat modeling : identifying potential threats and vulnerabilities from an adversary\'s point of view , and exploring potential mitigations to these threats . Likewise , developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models . We thus present a model for controllable text generation called Grover . Given a headline like `Link Found Between Vaccines and Autism , \' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human - written disinformation . \\nDeveloping robust verification techniques against generators like Grover is critical . We find that best current discriminators can classify neural fake news from real , human - written , news with 73% accuracy , assuming access to a moderate level of training data . Counterintuitively , the best defense against Grover turns out to be Grover itself , with 92% accuracy , demonstrating the importance of public release of strong generators . We investigate these results further , showing that exposure bias - - and sampling strategies that alleviate its effects - - both leave artifacts that similar discriminators can pick up on . We conclude by discussing ethical issues regarding the technology , and plan to release Grover publicly , helping pave the way for better detection of neural fake news . ","Method":["grover","robust defenses","threat modeling","verification techniques","discriminators","sampling strategies","generators"],"Task":["translation","summarization","detection of neural fake news","neural fake news","dual - use concerns","computer security","controllable text generation","natural language generation"]},{"url":"https://www.semanticscholar.org/paper/d8cb11d4be955f9869387a18967dee366eb851d9","title":"MultiFC : A Real - World Multi - Domain Dataset for Evidence - Based Fact Checking of Claims","abstract":"We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification . It is collected from 26 fact checking websites in English , paired with textual sources and rich metadata , and labelled for veracity by human expert journalists . We present an in - depth analysis of the dataset , highlighting characteristics and challenges . Further , we present results for automatic veracity prediction , both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines . Significant performance increases are achieved by encoding evidence , and by modelling metadata . Our best - performing model achieves a Macro F1 of 49 . 2% , showing that this is a challenging testbed for claim veracity prediction . ","Method":["NOT DETECTED"],"Task":["evidence - based fact checking of claims","predicting veracity","automatic veracity prediction","claim veracity prediction","automatic claim verification","joint ranking of evidence pages"]},{"url":"https://www.semanticscholar.org/paper/9aefd614e52336151966d8dca2ed0ea62a8f30af","title":"The Limitations of Stylometry for Detecting Machine - Generated Fake News","abstract":"Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation . In light of these concerns , several studies have proposed to detect machine - generated fake news by capturing their stylistic differences from human - written text . These approaches , broadly termed stylometry , have found success in source attribution and misinformation detection in human - written texts . However , in this work , we show that stylometry is limited against machine - generated misinformation . Whereas humans speak differently when trying to deceive , LMs generate stylistically consistent text , regardless of underlying motive . Thus , though stylometry can successfully prevent impersonation by identifying text provenance , it fails to distinguish legitimate LM applications from those that introduce false information . We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs , utilized in auto - completion and editing - assistance settings . 1 Our findings highlight the need for non - stylometry approaches in detecting machine - generated misinformation , and open up the discussion on the desired evaluation benchmarks . ","Method":["lm","non - stylometry approaches","stylometry","lms","neural language models"],"Task":["detecting machine - generated misinformation","source attribution","machine - generated fake news","automatically spreading misinformation","auto - completion","detecting machine - generated fake news","editing - assistance settings","misinformation detection"]},{"url":"https://www.semanticscholar.org/paper/b1d24e8e08435b7c52335485a0d635abf9bc604c","title":"FEVER : a Large - scale Dataset for Fact Extraction and VERification","abstract":"In this paper we introduce a new publicly available dataset for verification against textual sources , FEVER : Fact Extraction and VERification . It consists of 185 , 445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from . The claims are classified as Supported , Refuted or NotEnoughInfo by annotators achieving 0 . 6841 in Fleiss kappa . For the first two classes , the annotators also recorded the sentence(s) forming the necessary evidence for their judgment . To characterize the challenge of the dataset presented , we develop a pipeline approach and compare it to suitably designed oracles . The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31 . 87% , while if we ignore the evidence we achieve 50 . 91% . Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources . ","Method":["pipeline approach","fever"],"Task":["textual sources","verification","claim verification","fact extraction","fever"]},{"url":"https://www.semanticscholar.org/paper/0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World","abstract":"Language technologies contribute to promoting multilingualism and linguistic diversity around the world . However , only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications . In this paper we look at the relation between the types of languages , resources , and their representation in NLP conferences to understand the trajectory that different languages have followed over time . Our quantitative investigation underlines the disparity between languages , especially in terms of their resources , and calls into question the \u201clanguage agnostic\u201d status of current models and systems . Through this paper , we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here , so that no language is left behind . ","Method":["language technologies","nlp"],"Task":["acl community","nlp"]},{"url":"https://www.semanticscholar.org/paper/c3bcdea205ec9fb1b84d75d4767f346844082b38","title":"Stereotypes in High - Stakes Decisions : Evidence from U . S . Circuit Courts","abstract":"Attitudes towards social groups such as women and racial minorities have been shown to be important determinants of individual\u2019s decisions but are hard to measure for those in policy - making roles . We propose a way to address the challenge in the case of U . S . appellate court judges , for whom we have large corpora of written text (their published opinions) . Using the universe of published opinions in U . S . Circuit Courts 1890 - 2013 , we construct a judge - specific measure of gender - stereotyped language (gender slant) by looking at the relative co - occurrence of words identifying gender (male versus female) and words identifying gender stereotypes (career versus family) . We find that female and younger judges tend to use less stereotyped language in their opinions . In addition , the attitudes measured by gender slant matter for judicial decisions : judges with higher slant vote more conservatively on women rights\u2019 issues . These more slanted judges also influence workplace outcomes for female colleagues : they are less likely to assign opinions to female judges , they cite fewer female - authored opinions , and they are more likely to reverse lower - court decisions if the lower - court judge is a woman . Our results expose a possible use of text to detect decision - makers\u2019 stereotypes that predict behavior and disparate outcomes . \u2217Arianna Ornaghi , University of Warwick , a . ornaghi@warwick . ac . uk (corresponding author); Elliott Ash , ETH Zurich , ashe@ethz . ch; Daniel Chen , Toulouse School of Economics , daniel . chen@iast . fr . We thank Jacopo Bregolin , David Cai , Christoph Goessmann , and Ornelie Manzambi for helpful research assistance . ","Method":["NOT DETECTED"],"Task":["high - stakes decisions","judicial decisions","policy - making roles"]},{"url":"https://www.semanticscholar.org/paper/5e0daaeceb75ffbbe23be13d34ffae830cb4e8c4","title":"Generating Fact Checking Explanations","abstract":"Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata , social network spread , language used in claims , and , more recently , evidence supporting or denying claims . A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process \u2013 generating justifications for verdicts on claims . This paper provides the first study of how these explanations can be generated automatically based on available claim context , and how this task can be modelled jointly with veracity prediction . Our results indicate that optimising both objectives at the same time , rather than training them separately , improves the performance of a fact checking system . The results of a manual evaluation further suggest that the informativeness , coverage and overall quality of the generated explanations are also improved in the multi - task model . ","Method":["fact checking system","multi - task model"],"Task":["fact checking explanations","veracity prediction","justifications","veracity of claims","automated fact checking","verdicts"]},{"url":"https://www.semanticscholar.org/paper/a4947468ceac25c11a665c4f5f95a49d6dbea3cc","title":"Towards Debiasing Fact Verification Models","abstract":"Fact verification requires validating a claim in the context of evidence . We show , however , that in the popular FEVER dataset this might not necessarily be the case . Claim - only classifiers perform competitively with top evidence - aware models . In this paper , we investigate the cause of this phenomenon , identifying strong cues for predicting labels solely based on the claim , without considering any evidence . We create an evaluation set that avoids those idiosyncrasies . The performance of FEVER - trained models significantly drops when evaluated on this test set . Therefore , we introduce a regularization method which alleviates the effect of bias in the training data , obtaining improvements on the newly created test set . This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models . ","Method":["fever - trained models","fact verification models","evidence - aware models","claim - only classifiers","regularization method"],"Task":["predicting labels","reasoning capabilities","fact verification","fact verification models"]},{"url":"https://www.semanticscholar.org/paper/3a16c38294ac8899825c488490199b854e05473a","title":"Evaluating adversarial attacks against multiple fact verification systems","abstract":"Automated fact verification has been progressing owing to advancements in modeling and availability of large datasets . Due to the nature of the task , it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly . We introduce two novel scoring metrics , attack potency and system resilience which take into account the correctness of the adversarial instances , an aspect often ignored in adversarial evaluations . We consider six fact verification systems from the recent Fact Extraction and VERification (FEVER) challenge : the four best - scoring ones and two baselines . We evaluate adversarial instances generated by a recently proposed state - of - the - art method , a paraphrasing method , and rule - based attacks devised for fact verification . We find that our rule - based attacks have higher potency , and that while the rankings among the top systems changed , they exhibited higher resilience than the baselines . ","Method":["rule - based attacks","adversarial attacks","paraphrasing method","fact verification systems"],"Task":["verification","fact extraction","adversarial evaluations","fact verification"]},{"url":"https://www.semanticscholar.org/paper/d4eeb40b9bd06ed53a26282cd527609f71e6496f","title":"Unsupervised Discovery of Gendered Language through Latent - Variable Modeling","abstract":"Studying the ways in which language is gendered has long been an area of interest in sociolinguistics . Studies have explored , for example , the speech of male and female characters in film and the language used to describe male and female politicians . In this paper , we aim not to merely study this phenomenon qualitatively , but instead to quantify the degree to which the language used to describe men and women is different and , moreover , different in a positive or negative way . To that end , we introduce a generative latent - variable model that jointly represents adjective (or verb) choice , with its sentiment , given the natural gender of a head (or dependent) noun . We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes : Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men . ","Method":["latent - variable modeling","generative latent - variable model"],"Task":["unsupervised discovery of gendered language"]},{"url":"https://www.semanticscholar.org/paper/bb53946c7da617a05bbeef47fff74012db27ee78","title":"ChrEnTranslate : Cherokee - English Machine Translation Demo with Quality Estimation and Corrective Feedback","abstract":"We introduce ChrEnTranslate , an online ma\xad chine translation demonstration system for translation between English and an endangered language Cherokee . It supports both statistical and neural translation models as well as pro\xad vides quality estimation to inform users of re\xad liability , two user feedback interfaces for ex\xad perts and common users respectively , exam\xad ple inputs to collect human translations for monolingual data , word alignment visualiza\xad tion , and relevant terms from the Cherokee\xad English dictionary . The quantitative evalu\xad ation demonstrates that our backbone trans\xad lation models achieve state\xadof\xadthe\xadart transla\xad tion performance and our quality estimation well correlates with both BLEU and human judgment . By analyzing 216 pieces of expert feedback , we find that NMT is preferable be\xad cause it copies less than SMT , and , in gen\xad eral , current models can translate fragments of the source sentence but make major mistakes . When we add these 216 expert\xadcorrected paral\xad lel texts into the training set and retrain mod\xad els , equal or slightly better performance is ob\xad served , which demonstrates indicates the po\xad tential of human\xadin\xadthe\xadloop learning . 1","Method":["backbone trans\xad lation models","smt","mod\xad els","human\xadin\xadthe\xadloop learning","quality estimation","statistical and neural translation models","chrentranslate","nmt","online ma\xad chine","user feedback interfaces"],"Task":["translation","quality estimation","transla\xad tion","cherokee - english machine translation demo","word alignment visualiza\xad tion","re\xad liability"]},{"url":"https://www.semanticscholar.org/paper/3d309aa1629ef9ca43e252eb6bf539286ed872f9","title":"Haitian Creole : How to Build and Ship an MT Engine from Scratch in 4 days , 17 hours , & 30 minutes","abstract":"We describe the effort of the Microsoft Translator team to develop a Haitian Creole statistical machine translation engine from scratch in a matter of days . Haitian Creole presents a number of difficulties for devleoping an SMT system , principal among these is the lack of significant amounts of parallel training data and an inconsistent orthography , both of which lead to data sparseness . We demonstrate , however , that it is possible to build a translation engine of reasonable quality over very little data by engaging with the native language community and reducing data sparseness in creative ways . As such , we show that MT as a technology and as a service can be deployed rapidly in crisis situations . ","Method":["mt engine","haitian creole","mt","haitian creole statistical machine translation engine","translation engine","smt system"],"Task":["translator"]},{"url":"https://www.semanticscholar.org/paper/f14cb1828e314a669304b0c37bc78d6b9073f6dd","title":"Measuring Societal Biases in Text Corpora via First - Order Co - occurrence","abstract":"Text corpora are used to study societal biases , typically through statistical models such as word embeddings . The bias of a word towards a concept is typically estimated using vectors similarity , measuring whether the word and concept words share other words in their contexts . We argue that this second - order relationship introduces unrelated concepts into the measure , which causes an imprecise measurement of the bias . We propose instead to measure bias using the direct normalized co - occurrence associations between the word and the representative concept words , a first - order measure , by reconstructing the co - occurrence estimates inherent in the word embedding models . To study our novel corpus bias measurement method , we calculate the correlation of the gender bias values estimated from the text to the actual gender bias statistics of the U . S . job market , provided by two recent collections . The results show a consistently higher correlation when using the proposed first - order measure with a variety of word embedding models , as well as a more severe degree of bias , especially to female in a few specific occupations . ","Method":["word embeddings","statistical models","word embedding models","co - occurrence estimates","first - order co - occurrence","corpus bias measurement method","first - order measure"],"Task":["measuring societal biases"]},{"url":"https://www.semanticscholar.org/paper/176420f394014d9e67d5cff1e1d430541fa0f55f","title":"When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?","abstract":"Social biases are encoded in word embeddings . This presents a unique opportunity to study society historically and at scale , and a unique danger when embeddings are used in downstream applications . Here , we investigate the extent to which publicly - available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods . We find that biases found in word embeddings do , on average , closely mirror survey data across seventeen dimensions of social meaning . However , we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e . g . gender) than others (e . g . race) , and that we can be highly confident that embedding - based measures reflect survey data only for the most salient biases . ","Method":["word embeddings","embedding - based measures","survey methods"],"Task":["downstream applications"]},{"url":"https://www.semanticscholar.org/paper/75d0cb419d7d58e81c2975758a36a11544a9f930","title":"Language from police body camera footage shows racial disparities in officer respect","abstract":"Significance Police officers speak significantly less respectfully to black than to white community members in everyday traffic stops , even after controlling for officer race , infraction severity , stop location , and stop outcome . This paper presents a systematic analysis of officer body - worn camera footage , using computational linguistic techniques to automatically measure the respect level that officers display to community members . This work demonstrates that body camera footage can be used as a rich source of data rather than merely archival evidence , and paves the way for developing powerful language - based tools for studying and potentially improving police\u2013community relations . Using footage from body - worn cameras , we analyze the respectfulness of police officer language toward white and black community members during routine traffic stops . We develop computational linguistic methods that extract levels of respect automatically from transcripts , informed by a thin - slicing study of participant ratings of officer utterances . We find that officers speak with consistently less respect toward black versus white community members , even after controlling for the race of the officer , the severity of the infraction , the location of the stop , and the outcome of the stop . Such disparities in common , everyday interactions between police and the communities they serve have important implications for procedural justice and the building of police\u2013community trust . ","Method":["computational linguistic methods","computational linguistic techniques","language - based tools"],"Task":["police\u2013community trust","police\u2013community relations","procedural justice"]},{"url":"https://www.semanticscholar.org/paper/b5d7a19bd0bae10917a8e294960fdacf224d64fe","title":"Word embeddings quantify 100 years of gender and ethnic stereotypes","abstract":"Significance Word embeddings are a popular machine - learning method that represents each English word by a vector , such that the geometry between these vectors captures semantic relations between the corresponding words . We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change . As specific applications , we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910 . Our framework opens up a fruitful intersection between machine learning and quantitative social science . Word embeddings are a powerful machine - learning framework that represents each English word by a vector . The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words . In this paper , we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States . We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time . The embedding captures societal shifts\u2014e . g . , the women\u2019s movement in the 1960s and Asian immigration into the United States\u2014and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time . Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science . ","Method":["word embeddings","machine - learning framework","machine - learning method"],"Task":["word embeddings","quantitative social science","gender and ethnic stereotypes","machine learning","temporal analysis of word embedding"]},{"url":"https://www.semanticscholar.org/paper/3d505c5eff8752ac1805ef546d683bfa40aec4b1","title":"Tie - breaker : Using language models to quantify gender bias in sports journalism","abstract":"Gender bias is an increasingly important issue in sports journalism . In this work , we propose a language - model - based approach to quantify differences in questions posed to female vs . male athletes , and apply it to tennis post - match interviews . We find that journalists ask male players questions that are generally more focused on the game when compared with the questions they ask their female counterparts . We also provide a fine - grained analysis of the extent to which the salience of this bias depends on various factors , such as question type , game outcome or player rank . ","Method":["language - model - based approach","tie - breaker","language models"],"Task":["gender bias","sports journalism","sports journalism gender bias","tennis post - match interviews"]},{"url":"https://www.semanticscholar.org/paper/a1280728623e8fd605284b2b7cf536579b9e2cbf","title":"Entity - Centric Contextual Affective Analysis","abstract":"While contextualized word representations have improved state - of - the - art benchmarks in many NLP tasks , their potential usefulness for social - oriented tasks remains largely unexplored . We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people . We evaluate our methodology quantitatively , on held - out affect lexicons , and qualitatively , through case examples . We find that contextualized word representations do encode meaningful affect information , but they are heavily biased towards their training data , which limits their usefulness to in - domain analyses . We ultimately use our method to examine differences in portrayals of men and women . ","Method":["contextualized word embeddings","entity - centric contextual affective analysis","contextualized word representations"],"Task":["social - oriented tasks","nlp tasks","in - domain analyses"]},{"url":"https://www.semanticscholar.org/paper/070b4a707748e289618880ffbe4762e4e3fc7860","title":"Racism is a Virus : Anti - Asian Hate and Counterhate in Social Media during the COVID - 19 Crisis","abstract":"The spread of COVID - 19 has sparked racism , hate , and xenophobia in social media targeted at Chinese and broader Asian communities . However , little is known about how racial hate spreads during a pandemic and the role of counterhate speech in mitigating the spread . Here we study the evolution and spread of anti - Asian hate speech through the lens of Twitter . We create COVID - HATE , the largest dataset of anti - Asian hate and counterhate spanning three months , containing over 30 million tweets , and a social network with over 87 million nodes . By creating a novel hand - labeled dataset of 2 , 400 tweets , we train a text classifier to identify hate and counterhate tweets that achieves an average AUROC of 0 . 852 . We identify 891 , 204 hate and 200 , 198 counterhate tweets in COVID - HATE . Using this data to conduct longitudinal analysis , we find that while hateful users are less engaged in the COVID - 19 discussions prior to their first anti - Asian tweet , they become more vocal and engaged afterwards compared to counterhate users . We find that bots comprise 10 . 4% of hateful users and are more vocal and hateful compared to non - bot users . Comparing bot accounts , we show that hateful bots are more successful in attracting followers compared to counterhate bots . Analysis of the social network reveals that hateful and counterhate users interact and engage extensively with one another , instead of living in isolated polarized communities . Furthermore , we find that hate is contagious and nodes are highly likely to become hateful after being exposed to hateful content . Importantly , our analysis reveals that counterhate messages can discourage users from turning hateful in the first place . Overall , this work presents a comprehensive overview of anti - Asian hate and counterhate content during a pandemic . The COVID - HATE dataset is available at this http URL . ","Method":["hateful bots","text classifier"],"Task":["social network","longitudinal analysis","covid - 19 crisis"]},{"url":"https://www.semanticscholar.org/paper/7b5b2a9ad37d1a6c3c8916965b1958eef0a27a6a","title":"Automatically Inferring Gender Associations from Language","abstract":"In this paper , we pose the question : do people talk about women and men in different ways? We introduce two datasets and a novel integration of approaches for automatically inferring gender associations from language , discovering coherent word clusters , and labeling the clusters for the semantic concepts they represent . The datasets allow us to compare how people write about women and men in two different settings \u2013 one set draws from celebrity news and the other from student reviews of computer science professors . We demonstrate that there are large - scale differences in the ways that people talk about women and men and that these differences vary across domains . Human evaluations show that our methods significantly outperform strong baselines . ","Method":["NOT DETECTED"],"Task":["automatically inferring gender associations","coherent word clusters"]},{"url":"https://www.semanticscholar.org/paper/6ba951771892f01206f1dd7244f14243e3885109","title":"Contextual Affective Analysis : A Case Study of People Portrayals in Online #MeToo Stories","abstract":"In October 2017 , numerous women accused producer Harvey Weinstein of sexual harassment . Their stories encouraged other women to voice allegations of sexual harassment against many high profile men , including politicians , actors , and producers . These events are broadly referred to as the #MeToo movement , named for the use of the hashtag \\"#metoo\\" on social media platforms like Twitter and Facebook . The movement has widely been referred to as \\"empowering\\" because it has amplified the voices of previously unheard women over those of traditionally powerful men . In this work , we investigate dynamics of sentiment , power and agency in online media coverage of these events . Using a corpus of online media articles about the #MeToo movement , we present a contextual affective analysis - - - an entity - centric approach that uses contextualized lexicons to examine how people are portrayed in media articles . We show that while these articles are sympathetic towards women who have experienced sexual harassment , they consistently present men as most powerful , even after sexual assault allegations . While we focus on media coverage of the #MeToo movement , our method for contextual affective analysis readily generalizes to other domains . ","Method":["contextual affective analysis","entity - centric approach"],"Task":["contextual affective analysis","people portrayals"]},{"url":"https://www.semanticscholar.org/paper/995e477360908175d0b1184f6a0aace9d864bc5a","title":"Girls Rule , Boys Drool : Extracting Semantic and Affective Stereotypes from Twitter","abstract":"Social identities carry widely agreed upon meanings , called stereotypes , that have important effects on social processes . In the present work , we develop a method to extract the stereotypes of Twitter users . Our method is grounded in two distinct strands of theory , one that represents stereotypes as identities\' affective meanings and the other that represents stereotypes as semantic relationships between identities . After validating our approach via a prediction task , we apply the model to a dataset of 45 thousand Twitter users who actively tweeted about the Michael Brown and Eric Garner tragedies . Our work provides unique insights into the stereotypes of these users , as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel , parsimonious way . ","Method":["sociological and psychological theory"],"Task":["extracting semantic and affective stereotypes","quantifying stereotypes","prediction task"]},{"url":"https://www.semanticscholar.org/paper/5e888bfd9b492a3b08f3cc2eb7c617fedf5bd811","title":"Relating Linguistic Gender Bias , Gender Values , and Gender Gaps : An International Analysis","abstract":"Recent research in machine learning has shown that many machine - learned language models contain pervasive racial and gender biases , rooting from biases in their textual training data . While these biases produce sub - optimal parsing and inferences , they may help us characterize and predict statistical gender gaps and gender values in the culture(s) that produced the training text , thereby helping us understand cultural context through big data . This paper presents an approach to (1) quantify gender bias in word embeddings (i . e . , vector - based lexical semantics) , (2) correlate gender biases with survey responses and statistical gender gaps in education , politics , economics , and health , and (3) integrate numerical biases and statistics to model different cultures\u2019 survey results more accurately than either in isolation . We validate this approach using 2018 Twitter data spanning 99 countries , 18 Global Gender Gap statistics from the World Economic Forum , and 8 international survey results from the World Value Survey . Integrating these heterogeneous data across cultures is an important step toward building computational models to understand group bias . ","Method":["computational models","language models","vector - based lexical semantics)"],"Task":["health","group bias","inferences","machine learning","linguistic gender bias","parsing"]},{"url":"https://www.semanticscholar.org/paper/abeee58b9fb5761133636ef117ef1a87203ad7ab","title":"Automation , Algorithms , and Politics| Talking to Bots : Symbiotic Agency and the Case of Tay","abstract":"In 2016 , Microsoft launched Tay , an experimental artificial intelligence chat bot . Learning from interactions with Twitter users , Tay was shut down after one day because of its obscene and inflammatory tweets . This article uses the case of Tay to re - examine theories of agency . How did users view the personality and actions of an artificial intelligence chat bot when interacting with Tay on Twitter? Using phenomenological research methods and pragmatic approaches to agency , we look at what people said about Tay to study how they imagine and interact with emerging technologies and to show the limitations of our current theories of agency for describing communication in these settings . We show how different qualities of agency , different expectations for technologies , and different capacities for affordance emerge in the interactions between people and artificial intelligence . We argue that a perspective of \u201csymbiotic agency\u201d\u2014informed by the imagined affordances of emerging technology\u2014is required to really understand the collapse of Tay . ","Method":["pragmatic approaches","phenomenological research methods"],"Task":["communication","artificial intelligence chat bot","automation"]},{"url":"https://www.semanticscholar.org/paper/e3fd3b1be871da6e048adaef4a4e201af282fe8e","title":"Empathy Is All You Need : How a Conversational Agent Should Respond to Verbal Abuse","abstract":"With the popularity of AI - infused systems , conversational agents (CAs) are becoming essential in diverse areas , offering new functionality and convenience , but simultaneously , suffering misuse and verbal abuse . We examine whether conversational agents\' response styles under varying abuse types influence those emotions found to mitigate peoples\' aggressive behaviors , involving three verbal abuse types (Insult , Threat , Swearing) and three response styles (Avoidance , Empathy , Counterattacking) . Ninety - eight participants were assigned to one of the abuse type conditions , interacted with the three spoken (voice - based) CAs in turn , and reported their feelings about guiltiness , anger , and shame after each session . The results show that the agent\'s response style has a significant effect on user emotions . Participants were less angry and more guilty with the empathy agent than the other two agents . Furthermore , we investigated the current status of commercial CAs\' responses to verbal abuse . Our study findings have direct implications for the design of conversational agents . ","Method":["ai - infused systems","conversational agents","conversational agent","empathy agent"],"Task":["NOT DETECTED"]},{"url":"https://www.semanticscholar.org/paper/ef5fa2e95fc853defb902b58d8e4e4fe95a01c75","title":"Shirtless and Dangerous : Quantifying Linguistic Signals of Gender Bias in an Online Fiction Writing Community","abstract":"Imagine a princess asleep in a castle , waiting for her prince to slay the dragon and rescue her . Tales like the famous Sleeping Beauty clearly divide up gender roles . But what about more modern stories , borne of a generation increasingly aware of social constructs like sexism and racism? Do these stories tend to reinforce gender stereotypes , or counter them? In this paper , we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction . We apply this technique across 1 . 8 billion words of fiction from the Wattpad online writing community , investigating gender representation in stories , how male and female characters behave and are described , and how authors\' use of gender stereotypes is associated with the community\'s ratings . We find that male over - representation and traditional gender stereotypes (e . g . , dominant men and submissive women) are common throughout nearly every genre in our corpus . However , only some of these stereotypes , like sexual or violent men , are associated with highly rated stories . Finally , despite women often being the target of negative stereotypes , female authors are equally likely to write such stereotypes as men . ","Method":["natural language processing","crowdsourced lexicon of stereotypes"],"Task":["gender biases in fiction","gender representation","quantifying linguistic signals of gender bias"]},{"url":"https://www.semanticscholar.org/paper/f34c73c75a640f59c11472bf6c9786aeb774856a","title":"Let\'s Talk About Race : Identity , Chatbots , and AI","abstract":"Why is it so hard for chatbots to talk about race? This work explores how the biased contents of databases , the syntactic focus of natural language processing , and the opaque nature of deep learning algorithms cause chatbots difficulty in handling race - talk . In each of these areas , the tensions between race and chatbots create new opportunities for people and machines . By making the abstract and disparate qualities of this problem space tangible , we can develop chatbots that are more capable of handling race - talk in its many forms . Our goal is to provide the HCI community with ways to begin addressing the question , how can chatbots handle race - talk in new and improved ways?","Method":["natural language processing","chatbots","deep learning algorithms"],"Task":["ai","race - talk"]},{"url":"https://www.semanticscholar.org/paper/983ad7c704d0f9a1560af322e4807e5be7799895","title":"#MeToo Alexa : How Conversational Systems Respond to Sexual Harassment","abstract":"Conversational AI systems , such as Amazon\u2019s Alexa , are rapidly developing from purely transactional systems to social chatbots , which can respond to a wide variety of user requests . In this article , we establish how current state - of - the - art conversational systems react to inappropriate requests , such as bullying and sexual harassment on the part of the user , by collecting and analysing the novel #MeTooAlexa corpus . Our results show that commercial systems mainly avoid answering , while rule - based chatbots show a variety of behaviours and often deflect . Data - driven systems , on the other hand , are often non - coherent , but also run the risk of being interpreted as flirtatious and sometimes react with counter - aggression . This includes our own system , trained on \u201cclean\u201d data , which suggests that inappropriate system behaviour is not caused by data bias . ","Method":["data - driven systems","amazon\u2019s alexa","conversational ai systems","#metoo alexa","conversational systems","rule - based chatbots","transactional systems"],"Task":["sexual harassment","answering"]},{"url":"https://www.semanticscholar.org/paper/7b14a165c6b7c1dc2c6c44727e623b94d834fb09","title":"Social Bias Frames : Reasoning about Social and Power Implications of Language","abstract":"Warning : this paper contains content that may be offensive or upsetting . Language has the power to reinforce stereotypes and project social biases onto others . At the core of the challenge is that it is rarely what is stated explicitly , but rather the implied meanings , that frame people\u2019s judgments about others . For example , given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women , \u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified . \u201d Most semantic formalisms , to date , do not capture such pragmatic implications in which people express social biases and power differentials in language . We introduce Social Bias Frames , a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others . In addition , we introduce the Social Bias Inference Corpus to support large - scale modelling and evaluation with 150k structured annotations of social media posts , covering over 34k implications about a thousand demographic groups . We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text . We find that while state - of - the - art neural models are effective at high - level categorization of whether a given statement projects unwanted social bias (80% F1) , they are not effective at spelling out more detailed explanations in terms of Social Bias Frames . Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications . ","Method":["semantic formalisms","commonsense reasoning","social bias frames","conceptual formalism","neural models","structured pragmatic inference"],"Task":["social implications","categorization","large - scale modelling","evaluation","reasoning about social and power implications of language warning"]},{"url":"https://www.semanticscholar.org/paper/87eb23f934a0e6293ee8ee9b147fe0d456e65c96","title":"Data Statements for NLP : Toward Mitigating System Bias and Enabling Better Science","abstract":"In this position paper , we propose data statements as a practice that NLP technologists , in both research and development , can adopt to begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations . We present a form data statements can take and explore the implications of adopting them as part of our regular practice . We argue that they will help alleviate issues related to exclusion and bias in language technology; lead to better precision in claims about how NLP research can generalize and thus better engineering results; protect companies from public embarrassment; and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others . ","Method":["language technology","nlp technologists"],"Task":["nlp","language technology;","mitigating system bias","nlp research","scientific and ethical issues","development"]},{"url":"https://www.semanticscholar.org/paper/0df347f5e3118fac7c351917e3a497899b071d1e","title":"Datasheets for Datasets","abstract":"The machine learning community currently has no standardized process for documenting datasets , which can lead to severe consequences in high - stakes domains . To address this gap , we propose datasheets for datasets . In the electronics industry , every component , no matter how simple or complex , is accompanied with a datasheet that describes its operating characteristics , test results , recommended uses , and other information . By analogy , we propose that every dataset be accompanied with a datasheet that documents its motivation , composition , collection process , recommended uses , and so on . Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers , and encourage the machine learning community to prioritize transparency and accountability . ","Method":["NOT DETECTED"],"Task":["dataset creators","machine learning community","electronics industry","collection process","documenting datasets"]},{"url":"https://www.semanticscholar.org/paper/221732318e3cc45aa7bc2f48435706f3e5839ddc","title":"Beyond the Belmont Principles : Ethical Challenges , Practices , and Beliefs in the Online Data Research Community","abstract":"Pervasive information streams that document people and their routines have been a boon to social computing research . But the ethics of collecting and analyzing available& - but potentially sensitive - online data present challenges to researchers . In response to increasing public and scholarly debate over the ethics of online data research , this paper analyzes the current state of practice among researchers using online data . Qualitative and quantitative responses from a survey of 263 online data researchers document beliefs and practices around which social computing researchers are converging , as well as areas of ongoing disagreement . The survey also reveals that these disagreements are not correlated with disciplinary , methodological , or workplace affiliations . The paper concludes by reflecting on changing ethical practices in the digital age , and discusses a set of emergent best practices for ethical social computing research . ","Method":["NOT DETECTED"],"Task":["online data research","ethical social computing research","social computing research"]},{"url":"https://www.semanticscholar.org/paper/0c68d7d153bb56e4637d6aee051d87580e05fd5b","title":"Detecting East Asian Prejudice on Social Media","abstract":"During COVID - 19 concerns have heightened about the spread of aggressive and hateful language online , especially hostility directed against East Asia and East Asian people . We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes : Hostility against East Asia , Criticism of East Asia , Meta - discussions of East Asian prejudice , and a neutral class . The classifier achieves a macro - F1 score of 0 . 83 . We then conduct an in - depth ground - up error analysis and show that the model struggles with edge cases and ambiguous content . We provide the 20 , 000 tweet training dataset (annotated by experienced analysts) , which also contains several secondary categories and additional flags . We also provide the 40 , 000 original annotations (before adjudication) , the full codebook , annotations for COVID - 19 relevance and East Asian relevance and stance for 1 , 000 hashtags , and the final model . ","Method":["machine learning classifier"],"Task":["stance"]},{"url":"https://www.semanticscholar.org/paper/1ece7c00d2eb6fca5443ff8e15f05a2b8b5985c2","title":"Don\u2019t quote me : reverse identification of research participants in social media studies","abstract":"We investigated if participants in social media surveillance studies could be reverse identified by reviewing all articles published on PubMed in 2015 or 2016 with the words \u201cTwitter\u201d and either \u201cread , \u201d \u201ccoded , \u201d or \u201ccontent\u201d in the title or abstract . Seventy - two percent (95% CI : 63\u201380) of articles quoted at least one participant\u2019s tweet and searching for the quoted content led to the participant 84% (95% CI : 74\u201391) of the time . Twenty - one percent (95% CI : 13\u201329) of articles disclosed a participant\u2019s Twitter username thereby making the participant immediately identifiable . Only one article reported obtaining consent to disclose identifying information and institutional review board (IRB) involvement was mentioned in only 40% (95% CI : 31\u201350) of articles , of which 17% (95% CI : 10\u201325) received IRB - approval and 23% (95% CI : 16\u201332) were deemed exempt . Biomedical publications are routinely including identifiable information by quoting tweets or revealing usernames which , in turn , violates ICMJE ethical standards governing scientific ethics , even though said content is scientifically unnecessary . We propose that authors convey aggregate findings without revealing participants\u2019 identities , editors refuse to publish reports that reveal a participant\u2019s identity , and IRBs attend to these privacy issues when reviewing studies involving social media data . These strategies together will ensure participants are protected going forward . ","Method":["NOT DETECTED"],"Task":["social media surveillance studies","reverse identification of research participants"]},{"url":"https://www.semanticscholar.org/paper/afe97d05e5b320d2af500cdae1c588f4cc0d14d2","title":"Towards an Ethical Framework for Publishing Twitter Data in Social Research : Taking into Account Users\u2019 Views , Online Context and Algorithmic Estimation","abstract":"New and emerging forms of data , including posts harvested from social media sites such as Twitter , have become part of the sociologist\u2019s data diet . In particular , some researchers see an advantage in the perceived \u2018public\u2019 nature of Twitter posts , representing them in publications without seeking informed consent . While such practice may not be at odds with Twitter\u2019s terms of service , we argue there is a need to interpret these through the lens of social science research methods that imply a more reflexive ethical approach than provided in \u2018legal\u2019 accounts of the permissible use of these data in research publications . To challenge some existing practice in Twitter - based research , this article brings to the fore : (1) views of Twitter users through analysis of online survey data; (2) the effect of context collapse and online disinhibition on the behaviours of users; and (3) the publication of identifiable sensitive classifications derived from algorithms . ","Method":["ethical framework","social science research methods","algorithmic estimation","reflexive ethical approach"],"Task":["sociologist\u2019s data diet","identifiable sensitive classifications","twitter - based research","publishing twitter data","social research"]},{"url":"https://www.semanticscholar.org/paper/5f827b963939c96968a03318b4c2b011e1871eaf","title":"Writer Profiling Without the Writer\'s Text","abstract":"Social network users may wish to preserve their anonymity online by masking their identity and not using language associated with any particular demographics or personality . However , they have no control over the language in incoming communications . We show that linguistic cues in public comments directed at a user are sufficient for an accurate inference of that user\u2019s gender , age , religion , diet , and even personality traits . Moreover , we show that directed communication is even more predictive of a user\u2019s profile than the user\u2019s own language . We then conduct a nuanced analysis of what types of social relationships are most predictive of users\u2019 attributes , and propose new strategies on how individuals can modulate their online social relationships and incoming communications to preserve their anonymity . ","Method":["NOT DETECTED"],"Task":["writer profiling","directed communication"]},{"url":"https://www.semanticscholar.org/paper/f298649194c1be9cb55574c047756ae7e8a62d6b","title":"\u201cParticipant\u201d Perceptions of Twitter Research Ethics","abstract":"Social computing systems such as Twitter present new research sites that have provided billions of data points to researchers . However , the availability of public social media data has also presented ethical challenges . As the research community works to create ethical norms , we should be considering users\u2019 concerns as well . With this in mind , we report on an exploratory survey of Twitter users\u2019 perceptions of the use of tweets in research . Within our survey sample , few users were previously aware that their public tweets could be used by researchers , and the majority felt that researchers should not be able to use tweets without consent . However , we find that these attitudes are highly contextual , depending on factors such as how the research is conducted or disseminated , who is conducting it , and what the study is about . The findings of this study point to potential best practices for researchers conducting observation and analysis of public data . ","Method":["twitter","social computing systems"],"Task":["twitter research ethics","observation and analysis of public data"]},{"url":"https://www.semanticscholar.org/paper/520ec00dc35475e0554dbb72f27bd2eeb6f4191d","title":"The Secret Sharer : Evaluating and Testing Unintended Memorization in Neural Networks","abstract":"This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training - data sequences are unintentionally memorized by generative sequence models - - - a common type of machine - learning model . Because such models are sometimes trained on sensitive data (e . g . , the text of users\' private messages) , this methodology can benefit privacy by allowing deep - learning practitioners to select means of training that minimize such memorization . \\nIn experiments , we show that unintended memorization is a persistent , hard - to - avoid issue that can have serious consequences . Specifically , for models trained without consideration of memorization , we describe new , efficient procedures that can extract unique , secret sequences , such as credit card numbers . We show that our testing strategy is a practical and easy - to - use first line of defense , e . g . , by describing its application to quantitatively limit data exposure in Google\'s Smart Compose , a commercial text - completion neural network trained on millions of users\' email messages . ","Method":["text - completion neural network","generative sequence models","machine - learning model","testing strategy","memorization","deep - learning practitioners","unintended memorization in neural networks","secret sharer"],"Task":["data exposure","training"]},{"url":"https://www.semanticscholar.org/paper/df2df1749b93ba86328ec7b86ff7e8d30029e3f5","title":"Garbage in , garbage out? : do machine learning application papers in social computing report where human - labeled training data comes from?","abstract":"Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose , from hiring crowdworkers to the paper\'s authors labeling the data themselves . Such a task is quite similar to (or a form of) structured content analysis , which is a longstanding methodology in the social sciences and humanities , with many established best practices . In this paper , we investigate to what extent a sample of machine learning application papers in social computing - - - specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data - - - give specific details about whether such best practices were followed . Our team conducted multiple rounds of structured content analysis of each paper , making determinations such as : Does the paper report who the labelers were , what their qualifications were , whether they independently labeled the same items , whether inter - rater reliability metrics were disclosed , what level of training and/or instructions were given to labelers , whether compensation for crowdworkers is disclosed , and if the training data is publicly available . We find a wide divergence in whether such practices were followed and documented . Much of machine learning research and education focuses on what is done once a \\"gold standard\\" of training data is available , but we discuss issues around the equally - important aspect of whether such data is reliable in the first place . ","Method":["NOT DETECTED"],"Task":["ml classification task","machine learning application papers","education","social computing","machine learning research","machine learning projects","structured content analysis","social sciences","humanities"]},{"url":"https://www.semanticscholar.org/paper/129cbad01be98ee88a930e31898cb76be79c41c1","title":"How NOT To Evaluate Your Dialogue System : An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation","abstract":"We investigate evaluation metrics for dialogue response generation systems where supervised labels , such as task completion , are not available . Recent works in response generation have adopted metrics from machine translation to compare a model\'s generated response to a single target response . We show that these metrics correlate very weakly with human judgements in the non - technical Twitter domain , and not at all in the technical Ubuntu domain . We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics , and provide recommendations for future development of better automatic evaluation metrics for dialogue systems . ","Method":["dialogue system"],"Task":["dialogue response generation","machine translation","dialogue systems","response generation","dialogue response generation systems"]},{"url":"https://www.semanticscholar.org/paper/5966d7c7f60898d610812e24c64d4d57855ad86a","title":"Semantics derived automatically from language corpora necessarily contain human biases","abstract":"Artificial intelligence and machine learning are in a period of astounding growth . However , there are concerns that these technologies may be used , either with or without intention , to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions . Here we show for the first time that human - like semantic biases result from the application of standard machine learning to ordinary language - - - the same sort of language humans are exposed to every day . We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well - known psychological studies . We replicate these using a widely used , purely statistical machine - learning model - - - namely , the GloVe word embedding - - - trained on a corpus of text from the Web . Our results indicate that language itself contains recoverable and accurate imprints of our historic biases , whether these are morally neutral as towards insects or flowers , problematic as towards race or gender , or even simply veridical , reflecting the status quo for the distribution of gender with respect to careers or first names . These regularities are captured by machine learning along with the rest of semantics . In addition to our empirical findings concerning language , we also contribute new methods for evaluating bias in text , the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT) . Our results have implications not only for AI and machine learning , but also for the fields of psychology , sociology , and human ethics , since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here . ","Method":["machine learning","word embedding association test","glove word embedding","implicit association test","psychological studies","word embedding factual association test","statistical machine - learning model"],"Task":["sociology","ai","psychology","human ethics","machine learning","artificial intelligence","evaluating bias in text"]},{"url":"https://www.semanticscholar.org/paper/1935a5e3937753dc7db90126a221f11009c17984","title":"Algorithms of Oppression : How Search Engines Reinforce Racism","abstract":"Read and considered thoughtfully , Safiya Umoja Noble\u2019s Algorithms of Oppression : How Search Engines Reinforce Racism is devastating . It reduces to rubble the notion that technology is neutral and ideology - free . Noble\u2019s crushing the neutrality myth does several things . First , this act lays foundations for her argument : only if you recognize and understand that technology is built with , and integrates , bias , can you then be open to her primary thesis : search engines advance discriminatory and often racist content . Second , it banishes a convenient response for many self - identified meritocratic Silicon Valley \u201cwinners\u201d and their supporters . Postreading , some individuals may retain their beliefs in a neutral and ideology - free technology in spite of the overwhelming evidence and citations Noble brings to bear . Effective countering of Noble\u2019s claims is unlikely to occur . For professionals working in technology , information , argumentation , and/or rhetorical studies , Algorithms of Oppression is refreshing . Agonistic towards structural racism and its defenses , single - minded in its evidentiary presentation , collaborative in its acknowledgement of others\u2019 scholarship and research , Noble models many academic , critical , and social moves . Technology scholars and writers will find in Algorithms of Oppression a masterful mentor text on how to be an activist researcher scholar . Noble also makes this enjoyable reading . It is uncommon to find academic books that can simultaneously be read , used , and applied by academics and non - academics alike . ","Method":["search engines","algorithms of oppression","neutrality myth"],"Task":["rhetorical studies","argumentation","technology","algorithms of oppression","information"]},{"url":"https://www.semanticscholar.org/paper/31a848022de5933029435a2c8304c2bd12537b0d","title":"The trouble with using provider assessments for rating clinical performance : it\'s a matter of bias . ","abstract":"The International Association for the Study of Pain has referred to pain as the fifth vital sign , and acute pain management after surgery has been shown to be a key factor in quality of recovery . In addition , the establishment of pain management benchmarks by the Joint Commission on the Accreditation of Healthcare Organizations more than a decade ago has resulted in a greater awareness of patients\u2019 right to optimal pain control by health care practitioners and administrators . Postoperative pain control has become a priority for hospitals across the United States . Optimization of postoperative pain management has been demonstrated through the implementation of protocols that include multimodal analgesic regimens , surgicalspecific treatment pathways , implementation of a 24 - hour anesthesiology pain service , and pain - specific training for physicians and nurses involved in postoperative care . 1 Importantly , pain as assessed by the numeric rating scale (NRS) , for which 0 = no pain and 10 = maximal pain , has been shown to be significantly reduced after the implementation of postoperative analgesia protocol . These data suggest that NRS pain scores might be a useful metric to evaluate the quality of anesthesia care provided after the transition from surgery to recovery . Wanderer et al . 2 from the Vanderbilt University have applied this principle in a research report in the current edition of Anesthesia & Analgesia in an attempt to use rank ordering by initial postanesthesia recovery unit NRS pain scores , as collected by nurses in a clinical setting , to compare supervising anesthesiologists when adjusted for confounding factors . The analysis included 26 680 cases using electronic documentation and excluded physician and nurse providers who had not cared for \u2265100 patients . When admission postanesthesia recovery unit scores were compared among anesthesiologists after adjusting for patient and surgical factors , only 6 . 4% of the 69 supervising anesthesiologists were found to differ in median postanesthesia recovery room admission pain scores . This finding clearly demonstrates that as presently assessed , initial postanesthesia recovery scores are a poor metric for identifying differences among supervising anesthesiologists , and the authors correctly concluded that they should not be included in the assessment of anesthesia care performance . Interestingly , the study also found that 16 of 66 (24%) recovery room nurses elicited median pain scores less than the median value for the entire group , and 33 nurses (50%) had elicited median pain scores significantly higher than the median for the entire group . These differences translated into a range of odds ratios from 0 . 16 (95% confidence interval , 0 . 11\u20132 . 4) for the lowest to 2 . 95 (95% confidence interval , 2 . 43\u20133 . 59) for the highest nurse compared with the nurse who ranked the median value for the overall group . In fact , NRS pain assessments using the 0 to 10 NRS pain score were found to depend more on the nurse making the assessment than patient age , gender and race , preoperative use of opioids , American Society of Anesthesiologists physical status , or procedure . This finding should not be interpreted to suggest dishonest recordings of NRS values by nurses (86%\u201390% of nurses accurately record pain ratings provided by patients) , but that personal opinions , knowledge , and attitudes toward pain strongly influence assessments and management . 3 Wanderer et al . discuss the use of nonstandard item descriptors rather than \u201cno pain\u201d equals 0 and \u201cthe worst possible pain\u201d equals 10 as anchors for the NRS pain scales as a possible explanation for the variability in the nurse assessor\u2013recorded NRS . They cited a recent systematic review that identified 24 distinct anchoring phrases in 54 included studies . 4 The use of anchors during decision making can create a form of cognitive bias because individuals tend to rely heavily on these points when making decision adjustments . Factors that have been shown to affect the influence of anchoring on decision making include happier moods and conscientious personalities , certainly desirable attributes in postanesthesia care nurses . These are factors that patients are likely to perceive , and substituting anchors could clearly influence the perceived value reported by patients . 5 The method of presentation of the NRS score range by the evaluator can be used to influence the choice made by the decision maker . This method is called the framing effect and is another type of cognitive bias . 6 The presenter in this situation is referred to as the choice architect . This practice is not an uncommon phenomenon when using Likert scales because the differences between scores in the range are not The Trouble with Using Provider Assessments for Rating Clinical Performance : It\u2019s a Matter of Bias","Method":["multimodal analgesic regimens","24 - hour anesthesiology pain service","provider assessments","postoperative analgesia protocol","decision making","pain - specific training"],"Task":["rating clinical performance","acute pain management","assessment of anesthesia care performance","accreditation of healthcare organizations","study of pain","recovery","pain control","optimization of postoperative pain management","anesthesia care","decision adjustments","postoperative pain control","provider assessments","management","analgesia","decision making","the","postoperative care"]},{"url":"https://www.semanticscholar.org/paper/d3aca13c966bb22eed7086baeb287a64bc18c152","title":"Comparing Spoken Dialog Corpora Collected with Recruited Subjects versus Real Users","abstract":"Empirical spoken dialog research often involves the collection and analysis of a dialog corpus . However , it is not well understood whether and how a corpus of dialogs collected using recruited subjects differs from a corpus of dialogs obtained from real users . In this paper we use Let\u2019s Go Lab , a platform for experimenting with a deployed spoken dialog bus information system , to address this question . Our first corpus is collected by recruiting subjects to call Let\u2019s Go in a standard laboratory setting , while our second corpus consists of calls from real users calling Let\u2019s Go during its operating hours . We quantitatively characterize the two collected corpora using previously proposed measures from the spoken dialog literature , then discuss the statistically significant similarities and differences between the two corpora with respect to these measures . For example , we find that recruited subjects talk more and speak faster , while real users ask for more help and more frequently interrupt the system . In contrast , we find no difference with respect to dialog structure . ","Method":["NOT DETECTED"],"Task":["spoken dialog research"]},{"url":"https://www.semanticscholar.org/paper/e8fa186444d98a39ee9139b1f5dd0c7618caef8f","title":"Privacy - preserving Neural Representations of Text","abstract":"This article deals with adversarial attacks towards deep learning systems for Natural Language Processing (NLP) , in the context of privacy protection . We study a specific type of attack : an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text . Such scenario may arise in situations when the computation of a neural network is shared across multiple devices , e . g . some hidden representation is computed by a user\u2019s device and sent to a cloud - based model . We measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations . Finally , we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations . ","Method":["defense methods","privacy - preserving neural representations of text","adversarial attacks","cloud - based model","neural representations","deep learning systems","neural network","hidden representations","neural text classifier","hidden representation"],"Task":["natural language processing","privacy protection"]},{"url":"https://www.semanticscholar.org/paper/a24d72bd0d08d515cb3e26f94131d33ad6c861db","title":"Ethical Challenges in Data - Driven Dialogue Systems","abstract":"The use of dialogue systems as a medium for human - machine interaction is an increasingly prevalent paradigm . A growing number of dialogue systems use conversation strategies that are learned from large datasets . There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data - driven training process . Here , we highlight potential ethical issues that arise in dialogue systems research , including : implicit biases in data - driven systems , the rise of adversarial examples , potential sources of privacy violations , safety concerns , special considerations for reinforcement learning systems , and reproducibility concerns . We also suggest areas stemming from these issues that deserve further investigation . Through this initial survey , we hope to spur research leading to robust , safe , and ethically sound dialogue systems . ","Method":["data - driven systems","reinforcement learning systems","dialogue systems","data - driven training process","conversation strategies"],"Task":["reproducibility concerns","data - driven dialogue systems","human - machine interaction","dialogue systems research","dialogue systems","ethical challenges"]},{"url":"https://www.semanticscholar.org/paper/ccf6a69a7f33bcf052aa7def176d3b9de495beb7","title":"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings","abstract":"The blind application of machine learning runs the risk of amplifying biases present in data . Such a danger is facing us with word embedding , a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks . We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent . This raises concerns because their widespread use , as we describe , often tends to amplify these biases . Geometrically , gender bias is first shown to be captured by a direction in the word embedding . Second , gender neutral words are shown to be linearly separable from gender definition words in the word embedding . Using these properties , we provide a methodology for modifying an embedding to remove gender stereotypes , such as the association between the words receptionist and female , while maintaining desired associations such as between the words queen and female . Using crowd - worker evaluation as well as standard benchmarks , we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks . The resulting embeddings can be used in applications without amplifying gender bias . ","Method":["word embeddings","machine learning","word embedding"],"Task":["word embeddings","amplifying gender bias","blind application","crowd - worker evaluation","analogy tasks","machine learning and natural language processing tasks"]},{"url":"https://www.semanticscholar.org/paper/94cf3f2c4410fcb06a90abebd99f7113c69e1ed9","title":"Lipstick on a Pig : Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them","abstract":"Word embeddings are widely used in NLP for a vast range of tasks . It was shown that word embeddings derived from text corpora reflect gender biases in society , causing serious concern . Several recent works tackle this problem , and propose methods for significantly reducing this gender bias in word embeddings , demonstrating convincing results . However , we argue that this removal is superficial . While the bias is indeed substantially reduced according to the provided bias definition , the actual effect is mostly hiding the bias , not removing it . The gender bias information is still reflected in the distances between \u201cgender - neutralized\u201d words in the debiased embeddings , and can be recovered from them . We present a series of experiments to support this claim , for two debiasing methods . We conclude that existing bias removal techniques are insufficient , and should not be trusted for providing gender - neutral modeling . ","Method":["bias removal techniques","debiasing methods"],"Task":["gender - neutral modeling","nlp"]},{"url":"https://www.semanticscholar.org/paper/5e9c85235210b59a16bdd84b444a904ae271f7e7","title":"On Measuring Social Biases in Sentence Encoders","abstract":"The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human - like implicit biases based on gender , race , and other social constructs (Caliskan et al . , 2017) . Meanwhile , research on learning reusable text representations has begun to explore sentence - level texts , with some sentence encoders seeing enthusiastic adoption . Accordingly , we extend the Word Embedding Association Test to measure bias in sentence encoders . We then test several sentence encoders , including state - of - the - art methods such as ELMo and BERT , for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level . We observe mixed results including suspicious patterns of sensitivity that suggest the test\u2019s assumptions may not hold in general . We conclude by proposing directions for future work on measuring bias in sentence encoders . ","Method":["word embedding association test","word2vec word embeddings","bert","sentence encoders","elmo","glove"],"Task":["reusable text representations","measuring social biases","measuring bias in sentence encoders","bias in sentence encoders"]},{"url":"https://www.semanticscholar.org/paper/039b1c1210c437f3b3ce6e0275ee2137bf5b951c","title":"Assessing Social and Intersectional Biases in Contextualized Word Representations","abstract":"Social bias in machine learning has drawn significant attention , with work ranging from demonstrations of bias in a multitude of applications , curating definitions of fairness for different contexts , to developing algorithms to mitigate bias . In natural language processing , gender bias has been shown to exist in context - free word embeddings . Recently , contextual word representations have outperformed word embeddings in several downstream NLP tasks . These word representations are conditioned on their context within a sentence , and can also be used to encode the entire sentence . In this paper , we analyze the extent to which state - of - the - art models for contextual word representations , such as BERT and GPT - 2 , encode biases with respect to gender , race , and intersectional identities . Towards this , we propose assessing bias at the contextual word level . This novel approach captures the contextual effects of bias missing in context - free word embeddings , yet avoids confounding effects that underestimate bias at the sentence encoding level . We demonstrate evidence of bias at the corpus level , find varying evidence of bias in embedding association tests , show in particular that racial bias is strongly encoded in contextual word models , and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities . Further , evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level , confirming the need for our novel approach . ","Method":["word embeddings","contextualized word representations","contextual word representations","gpt - 2","word representations","bert","contextual word models"],"Task":["gender bias","embedding association tests","natural language processing","machine learning","contextual word representations","nlp tasks","context - free word embeddings","social bias","assessing social and intersectional biases"]},{"url":"https://www.semanticscholar.org/paper/3259d52ae00e65b98391e7e6a2f672dfee721bf8","title":"Quantifying Social Biases in Contextual Word Representations","abstract":"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks . Since they are optimized to capture the statistical properties of training data , they tend to pick up on and amplify social stereotypes present in the data as well . In this study , we (1) propose a template - based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study , evaluating gender bias in a downstream task of Gender Pronoun Resolution . Although our case study focuses on gender bias , the proposed technique is generalizable to unveiling other biases , including in multiclass settings , such as racial and religious biases . ","Method":["contextual word embeddings","cosine based method;","bert","template - based method"],"Task":["downstream task","gender bias","capturing social biases","contextual word representations","nlp tasks","multiclass settings","quantifying social biases","gender pronoun resolution","bert;"]},{"url":"https://www.semanticscholar.org/paper/d08392eee17f809d32d7d37e9345383f41271164","title":"Sorting Things Out : Classification and Its Consequences","abstract":"What do a seventeenth - century mortality table (whose causes of death include \\"fainted in a bath , \\" \\"frighted , \\" and \\"itch\\"); the identification of South Africans during apartheid as European , Asian , colored , or black; and the separation of machine - from hand - washables have in common? All are examples of classification - - the scaffolding of information infrastructures . In Sorting Things Out , Geoffrey C . Bowker and Susan Leigh Star explore the role of categories and standards in shaping the modern world . In a clear and lively style , they investigate a variety of classification systems , including the International Classification of Diseases , the Nursing Interventions Classification , race classification under apartheid in South Africa , and the classification of viruses and of tuberculosis . The authors emphasize the role of invisibility in the process by which classification orders human interaction . They examine how categories are made and kept invisible , and how people can change this invisibility when necessary . They also explore systems of classification as part of the built information environment . Much as an urban historian would review highway permits and zoning decisions to tell a city\'s story , the authors review archives of classification design to understand how decisions have been made . Sorting Things Out has a moral agenda , for each standard and category valorizes some point of view and silences another . Standards and classifications produce advantage or suffering . Jobs are made and lost; some regions benefit at the expense of others . How these choices are made and how we think about that process are at the moral and political core of this work . The book is an important empirical source for understanding the building of information infrastructures . ","Method":["classification systems"],"Task":["race classification","classification of viruses and of tuberculosis","building of information infrastructures","international classification of diseases","classification","nursing interventions classification","classification design","scaffolding of information infrastructures"]},{"url":"https://www.semanticscholar.org/paper/d47a682723f710395454687319bb55635e653105","title":"Language (Technology) is Power : A Critical Survey of \u201cBias\u201d in NLP","abstract":"We survey 146 papers analyzing \u201cbias\u201d in NLP systems , finding that their motivations are often vague , inconsistent , and lacking in normative reasoning , despite the fact that analyzing \u201cbias\u201d is an inherently normative process . We further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of NLP . Based on these findings , we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in NLP systems . These recommendations rest on a greater recognition of the relationships between language and social hierarchies , encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d - - - i . e . , what kinds of system behaviors are harmful , in what ways , to whom , and why , as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by NLP systems , while interrogating and reimagining the power relations between technologists and such communities . ","Method":["quantitative techniques","nlp","normative process"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/493fac37cea49afb98c52c2f5dd75c303a325b25","title":"Mitigating Gender Bias in Natural Language Processing : Literature Review","abstract":"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity , it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes . Although NLP models have shown success in modeling various applications , they propagate and may even amplify gender bias found in text corpora . While the study of bias in artificial intelligence is not new , methods to mitigate gender bias in NLP are relatively nascent . In this paper , we review contemporary studies on recognizing and mitigating gender bias in NLP . We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias . Furthermore , we discuss the advantages and drawbacks of existing gender debiasing methods . Finally , we discuss future studies for recognizing and mitigating gender bias in NLP . ","Method":["nlp models","machine learning","representation bias","gender debiasing methods"],"Task":["gender bias","natural language processing","mitigating gender bias","nlp","artificial intelligence","recognizing and mitigating gender bias","bias","recognizing gender bias"]},{"url":"https://www.semanticscholar.org/paper/9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7","title":"Gender Bias in Coreference Resolution","abstract":"We present an empirical study of gender bias in coreference resolution systems . We first introduce a novel , Winograd schema - style set of minimal pair sentences that differ only by pronoun gender . With these \u201cWinogender schemas , \u201d we evaluate and confirm systematic gender bias in three publicly - available coreference resolution systems , and correlate this bias with real - world and textual gender statistics . ","Method":["coreference resolution systems"],"Task":["gender bias","coreference resolution"]},{"url":"https://www.semanticscholar.org/paper/0be19fd9896e5d40222c690cc3ff553adc7c0e27","title":"Gender Bias in Coreference Resolution : Evaluation and Debiasing Methods","abstract":"In this paper , we introduce a new benchmark for co - reference resolution focused on gender bias , WinoBias . Our corpus contains Winograd - schema style sentences with entities corresponding to people referred by their occupation (e . g . the nurse , the doctor , the carpenter) . We demonstrate that a rule - based , a feature - rich , and a neural coreference system all link gendered pronouns to pro - stereotypical entities with higher accuracy than anti - stereotypical entities , by an average difference of 21 . 1 in F1 score . Finally , we demonstrate a data - augmentation approach that , in combination with existing word - embedding debiasing techniques , removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets . ","Method":["debiasing methods","word - embedding debiasing techniques","feature - rich","winobias","data - augmentation approach","rule - based","neural coreference system"],"Task":["gender bias","co - reference resolution","coreference resolution"]},{"url":"https://www.semanticscholar.org/paper/5d4af8c9321168f9ba7a501f33fb019fa2deaa22","title":"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems","abstract":"Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases . Past work on examining inappropriate biases has largely focused on just individual systems . Further , there is no benchmark dataset for examining inappropriate biases in systems . Here for the first time , we present the Equity Evaluation Corpus (EEC) , which consists of 8 , 640 English sentences carefully chosen to tease out biases towards certain races and genders . We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task , SemEval - 2018 Task 1 \u2018Affect in Tweets\u2019 . We find that several of the systems show statistically significant bias; that is , they consistently provide slightly higher sentiment intensity predictions for one race or one gender . We make the EEC freely available . ","Method":["automatic machine learning systems","sentiment analysis systems"],"Task":["automatic sentiment analysis systems","gender and race bias","inappropriate biases in systems"]},{"url":"https://www.semanticscholar.org/paper/e85a50b523915b5fba3e3f1fdb743650f7d21bed","title":"Women\u2019s Syntactic Resilience and Men\u2019s Grammatical Luck : Gender - Bias in Part - of - Speech Tagging and Dependency Parsing","abstract":"Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender , but models for part - of - speech tagging and dependency parsing have still not adapted to account for these differences . To address this , we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles\u2019 authors , and build taggers and parsers trained on this data that show performance differences in text written by men and women . Further analyses reveal numerous part - of - speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data . The results underscore the importance of accounting for gendered differences in syntactic tasks , and outline future venues for developing more accurate taggers and parsers . We release our data to the research community . ","Method":["parsers","taggers"],"Task":["part - of - speech tagging","syntactic tasks","dependency parsing","prediction"]},{"url":"https://www.semanticscholar.org/paper/c590d2c8c2fb6ce5d32ee9165ab24171165f2b70","title":"Assessing gender bias in machine translation : a case study with Google Translate","abstract":"Recently there has been a growing concern in academia , industrial research laboratories and the mainstream commercial media about the phenomenon dubbed as machine bias , where trained statistical models\u2014unbeknownst to their creators\u2014grow to reflect controversial societal asymmetries , such as gender or racial bias . A significant number of Artificial Intelligence tools have recently been suggested to be harmfully biased toward some minority , with reports of racist criminal behavior predictors , Apple\u2019s Iphone X failing to differentiate between two distinct Asian people and the now infamous case of Google photos\u2019 mistakenly classifying black people as gorillas . Although a systematic study of such biases can be difficult , we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in AI . In this paper , we start with a comprehensive list of job positions from the U . S . Bureau of Labor Statistics (BLS) and used it in order to build sentences in constructions like \u201cHe/She is an Engineer\u201d (where \u201cEngineer\u201d is replaced by the job position of interest) in 12 different gender neutral languages such as Hungarian , Chinese , Yoruba , and several others . We translate these sentences into English using the Google Translate API , and collect statistics about the frequency of female , male and gender neutral pronouns in the translated output . We then show that Google Translate exhibits a strong tendency toward male defaults , in particular for fields typically associated to unbalanced gender distribution or stereotypes such as STEM (Science , Technology , Engineering and Mathematics) jobs . We ran these statistics against BLS\u2019 data for the frequency of female participation in each job position , in which we show that Google Translate fails to reproduce a real - world distribution of female workers . In summary , we provide experimental evidence that even if one does not expect in principle a 50 : 50 pronominal gender distribution , Google Translate yields male defaults much more frequently than what would be expected from demographic data alone . We believe that our study can shed further light on the phenomenon of machine bias and are hopeful that it will ignite a debate about the need to augment current statistical translation tools with debiasing techniques\u2014which can already be found in the scientific literature . ","Method":["google translate","statistical models\u2014unbeknownst","debiasing techniques\u2014which","racist criminal behavior predictors","statistical translation tools","google translate api","artificial intelligence tools"],"Task":["assessing gender bias","gender bias","ai","translation","machine translation","machine bias"]},{"url":"https://www.semanticscholar.org/paper/008e9001ea78e9654b5c43aeb818ea6cb06ea934","title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations","abstract":"The successful application of neural methods to machine translation has realized huge quality advances for the community . With these improvements , many have noted outstanding challenges , including the modeling and treatment of gendered language . While previous studies have identified issues using synthetic examples , we develop a novel technique to mine examples from real world data to explore challenges for deployed systems . We use our method to compile an evaluation benchmark spanning examples for four languages from three language families , which we publicly release to facilitate research . The examples in our benchmark expose where model representations are gendered , and the unintended consequences these gendered representations can have in downstream application . ","Method":["gendered representations","neural methods","model representations"],"Task":["machine translation","automatically identifying gender issues","downstream application","modeling and treatment of gendered language"]},{"url":"https://www.semanticscholar.org/paper/00059087c954c1af6ece33115315e3e0ecc2f2c2","title":"Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem","abstract":"Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men . In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality , particularly when the target language has grammatical gender . The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al , 2019) Ideally we would reduce system bias by simply debiasing all data prior to training , but achieving this effectively is itself a challenge . Rather than attempt to create a \u2018balanced\u2019 dataset , we use transfer learning on a small set of trusted , gender - balanced examples . This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch . A known pitfall of transfer learning on new domains is \u2018catastrophic forgetting\u2019 , which we address at adaptation and inference time . During adaptation we show that Elastic Weight Consolidation allows a performance trade - off between general translation quality and bias reduction . At inference time we propose a lattice - rescoring scheme which outperforms all systems evaluated in Stanovsky et al , 2019 on WinoMT with no degradation of general test set BLEU . We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability . ","Method":["elastic weight consolidation","transfer learning","lattice - rescoring scheme"],"Task":["gender bias","adaptation and inference time","nlp tasks","bias reduction","adaptation","domain adaptation problem","gender debiasing","neural machine translation","reducing gender bias"]},{"url":"https://www.semanticscholar.org/paper/40a6e8d8f253882c585f163b7333842d60ed6f14","title":"Toward Gender - Inclusive Coreference Resolution","abstract":"Correctly resolving textual mentions of people fundamentally entails making inferences about those people . Such inferences raise the risk of systemic biases in coreference resolution systems , including biases that can harm binary and non - binary trans and cis stakeholders . To better understand such biases , we foreground nuanced conceptualizations of gender from sociology and sociolinguistics , and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems . Through these studies , conducted on English text , we confirm that without acknowledging and building systems that recognize the complexity of gender , we build systems that lead to many potential harms . ","Method":["sociology","coreference resolution systems"],"Task":["resolving textual mentions of people","coreference resolution","interrogating bias","crowd annotations","gender - inclusive coreference resolution"]},{"url":"https://www.semanticscholar.org/paper/7b8318894cbeca32f1ae55780a0903445a3f4ac6","title":"Man is to Person as Woman is to Location : Measuring Gender Bias in Named Entity Recognition","abstract":"In this paper , we study the bias in named entity recognition (NER) models - - - specifically , the difference in the ability to recognize male and female names as PERSON entity types . We evaluate NER models on a dataset containing 139 years of U . S . census baby names and find that relatively more female names , as opposed to male names , are not recognized as PERSON entities . The result of this analysis yields a new benchmark for gender bias evaluation in named entity recognition systems . The data and code for the application of this benchmark is publicly available for researchers to use . ","Method":["NOT DETECTED"],"Task":["measuring gender bias","gender bias evaluation","named entity recognition systems","named entity recognition","ner"]},{"url":"https://www.semanticscholar.org/paper/57032c1e327c88a53ab41c17e91bf1406f9ef5c9","title":"Mind the GAP : A Balanced Corpus of Gendered Ambiguous Pronouns","abstract":"Coreference resolution is an important task for natural language understanding , and the resolution of ambiguous pronouns a longstanding challenge . Nonetheless , existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models . Furthermore , we find gender bias in existing corpora and systems favoring masculine entities . To address this , we present and release GAP , a gender - balanced labeled corpus of 8 , 908 ambiguous pronoun\u2013name pairs sampled to provide diverse coverage of challenges posed by real - world text . We explore a range of baselines that demonstrate the complexity of the challenge , the best achieving just 66 . 9% F1 . We show that syntactic structure and continuous neural models provide promising , complementary cues for approaching the challenge . ","Method":["continuous neural models","gap"],"Task":["coreference resolution","natural language understanding","resolution of ambiguous pronouns"]},{"url":"https://www.semanticscholar.org/paper/a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd","title":"Measuring Bias in Contextualized Word Representations","abstract":"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks . Since they are optimized to capture the statistical properties of training data , they tend to pick up on and amplify social stereotypes present in the data as well . In this study , we (1) propose a template - based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study , evaluating gender bias in a downstream task of Gender Pronoun Resolution . Although our case study focuses on gender bias , the proposed technique is generalizable to unveiling other biases , including in multiclass settings , such as racial and religious biases . ","Method":["contextual word embeddings","bert","template - based method","cosine based method;","bert;"],"Task":["downstream task","gender bias","capturing social biases","contextualized word representations","nlp tasks","multiclass settings","measuring bias","gender pronoun resolution"]},{"url":"https://www.semanticscholar.org/paper/0f4bcebc95548a7286106b67bf1115802f093469","title":"Mitigating Gender Bias Amplification in Distribution by Posterior Regularization","abstract":"Advanced machine learning techniques have boosted the performance of natural language processing . Nevertheless , recent studies , e . g . , (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it . However , their analysis is conducted only on models\u2019 top predictions . In this paper , we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels . We further propose a bias mitigation approach based on posterior regularization . With little performance loss , our method can almost remove the bias amplification in the distribution . Our study sheds the light on understanding the bias amplification . ","Method":["bias mitigation approach","posterior regularization","machine learning techniques"],"Task":["natural language processing","mitigating gender bias amplification in distribution","gender bias amplification issue","bias amplification"]},{"url":"https://www.semanticscholar.org/paper/a20ecabd83e0962329448d8af5025b8061c4ba36","title":"Social Bias in Elicited Natural Language Inferences","abstract":"We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data . The SNLI human - elicitation protocol makes it prone to amplifying bias and stereotypical associations , which we demonstrate statistically (using pointwise mutual information) and with qualitative examples . ","Method":["NOT DETECTED"],"Task":["bias and stereotyping","social bias"]},{"url":"https://www.semanticscholar.org/paper/219b7266ae848937da170c5510b2bfc66d17859a","title":"Racial disparities in automated speech recognition","abstract":"Significance Automated speech recognition (ASR) systems are now used in a variety of applications to convert spoken language to text , from virtual assistants , to closed captioning , to hands - free computing . By analyzing a large corpus of sociolinguistic interviews with white and African American speakers , we demonstrate large racial disparities in the performance of five popular commercial ASR systems . Our results point to hurdles faced by African Americans in using increasingly widespread tools driven by speech recognition technology . More generally , our work illustrates the need to audit emerging machine - learning systems to ensure they are broadly inclusive . Automated speech recognition (ASR) systems , which use sophisticated machine - learning algorithms to convert spoken language to text , have become increasingly widespread , powering popular virtual assistants , facilitating automated closed captioning , and enabling digital dictation platforms for health care . Over the last several years , the quality of these systems has dramatically improved , due both to advances in deep learning and to the collection of large - scale datasets used to train the systems . There is concern , however , that these tools do not work equally well for all subgroups of the population . Here , we examine the ability of five state - of - the - art ASR systems\u2014developed by Amazon , Apple , Google , IBM , and Microsoft\u2014to transcribe structured interviews conducted with 42 white speakers and 73 black speakers . In total , this corpus spans five US cities and consists of 19 . 8 h of audio matched on the age and gender of the speaker . We found that all five ASR systems exhibited substantial racial disparities , with an average word error rate (WER) of 0 . 35 for black speakers compared with 0 . 19 for white speakers . We trace these disparities to the underlying acoustic models used by the ASR systems as the race gap was equally large on a subset of identical phrases spoken by black and white individuals in our corpus . We conclude by proposing strategies\u2014such as using more diverse training datasets that include African American Vernacular English\u2014to reduce these performance differences and ensure speech recognition technology is inclusive . ","Method":["deep learning","asr systems","machine - learning algorithms","asr systems\u2014developed","machine - learning systems","speech recognition technology","acoustic models"],"Task":["automated speech recognition","digital dictation platforms","racial disparities","closed captioning","automated closed captioning","health care","hands - free computing","asr","virtual assistants"]},{"url":"https://www.semanticscholar.org/paper/1080dc00733e010fdd6a9b999506a0d4d864519d","title":"Effects of Talker Dialect , Gender & Race on Accuracy of Bing Speech and YouTube Automatic Captions","abstract":"This project compares the accuracy of two automatic speech recognition (ASR) systems\u2013Bing Speech and YouTube\u2019s automatic captions\u2013across gender , race and four dialects of American English . The dialects included were chosen for their acoustic dissimilarity . Bing Speech had differences in word error rate (WER) between dialects and ethnicities , but they were not statistically reliable . YouTube\u2019s automatic captions , however , did have statistically different WERs between dialects and races . The lowest average error rates were for General American and white talkers , respectively . Neither system had a reliably different WER between genders , which had been previously reported for YouTube\u2019s automatic captions [1] . However , the higher error rate non - white talkers is worrying , as it may reduce the utility of these systems for talkers of color . ","Method":["automatic speech recognition"],"Task":["NOT DETECTED"]},{"url":"https://www.semanticscholar.org/paper/21e59098bb5e36175f653d5142442d061669d07f","title":"Race as a Bundle of Sticks : Designs that Estimate Effects of Seemingly Immutable Characteristics","abstract":"Although understanding the role of race , ethnicity , and identity is central to political science , methodological debates persist about whether it is possible to estimate the effect of something immutable . At the heart of the debate is an older theoretical question : Is race best understood under an essentialist or constructivist framework? In contrast to the \u201cimmutable characteristics\u201d or essentialist approach , we argue that race should be operationalized as a \u201cbundle of sticks\u201d that can be disaggregated into elements . With elements of race , causal claims may be possible using two designs : (a) studies that measure the effect of exposure to a racial cue and (b) studies that exploit within - group variation to measure the effect of some manipulable element . These designs can reconcile scholarship on race and causation and offer a clear framework for future research . ","Method":["essentialist approach","essentialist or constructivist framework?"],"Task":["political science"]},{"url":"https://www.semanticscholar.org/paper/8417424bf9fe7a67f06f15c487403e953ab24a96","title":"Men Also Like Shopping : Reducing Gender Bias Amplification using Corpus - level Constraints","abstract":"Language is increasingly being used to de - fine rich visual recognition problems with supporting image collections sourced from the web . Structured prediction models are used in these tasks to take advantage of correlations between co - occurring labels and visual input but risk inadvertently encoding social biases found in web corpora . In this work , we study data and models associated with multilabel object classification and visual semantic role labeling . We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias . For example , the activity cooking is over 33% more likely to involve females than males in a training set , and a trained model further amplifies the disparity to 68% at test time . We propose to inject corpus - level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference . Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47 . 5% and 40 . 5% for multilabel classification and visual semantic role labeling , respectively\u3002","Method":["lagrangian relaxation","structured prediction models"],"Task":["visual semantic role labeling","multilabel classification","recognition task","gender bias amplification","visual recognition problems","collective inference","multilabel object classification"]},{"url":"https://www.semanticscholar.org/paper/0ab2fb6c850bd1c5882deb4984d37b4ccbee580c","title":"Towards Understanding Gender Bias in Relation Extraction","abstract":"Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction . While much attention has been dedicated towards improvements in accuracy , there have been no attempts in the literature to evaluate social biases exhibited in NRE systems . In this paper , we create WikiGenderBias , a distantly supervised dataset composed of over 45 , 000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems . We find that when extracting spouse - of and hypernym (i . e . , occupation) relations , an NRE system performs differently when the gender of the target entity is different . However , such disparity does not appear when extracting relations such as birthDate or birthPlace . We also analyze how existing bias mitigation techniques , such as name anonymization , word embedding debiasing , and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases . Unfortunately , due to NRE models rely heavily on surface level cues , we find that existing bias mitigation approaches have a negative effect on NRE . Our analysis lays groundwork for future quantifying and mitigating bias in NRE . ","Method":["name anonymization","bias mitigation approaches","nre system","word embedding debiasing","nre","data augmentation","bias mitigation techniques"],"Task":["gender bias","relation extraction","neural relation extraction","automated knowledge base construction","nre","mitigating bias","relation extraction systems"]},{"url":"https://www.semanticscholar.org/paper/59e94c9f21937643678ff494901f3d8b22af4e2f","title":"Racial Disparity in Natural Language Processing : A Case Study of Social Media African - American English","abstract":"We highlight an important frontier in algorithmic fairness : disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups . For example , current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males . We conduct an empirical analysis of racial disparity in language identification for tweets written in African - American English , and discuss implications of disparity in NLP . ","Method":["natural language processing algorithms"],"Task":["natural language processing","nlp","language identification","algorithmic fairness","racial disparity"]},{"url":"https://www.semanticscholar.org/paper/187608bf94b2dccd25d1266ed925abf7b55dbb2e","title":"Re - imagining Algorithmic Fairness in India and Beyond","abstract":"Conventional algorithmic fairness is West - centric , as seen in its subgroups , values , and methods . In this paper , we de - center algorithmic fairness and analyse AI power in India . Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India , we find that several assumptions of algorithmic fairness are challenged . We find that in India , data is not always reliable due to socio - economic factors , ML makers appear to follow double standards , and AI evokes unquestioning aspiration . We contend that localising model fairness alone can be window dressing in India , where the distance between models and oppressed communities is large . Instead , we re - imagine algorithmic fairness in India and provide a roadmap to re - contextualise data and models , empower oppressed communities , and enable Fair - ML ecosystems . ","Method":["discourse analysis of algorithmic deployments","ai","model fairness","algorithmic fairness","fair - ml ecosystems"],"Task":["re - imagining algorithmic fairness"]},{"url":"https://www.semanticscholar.org/paper/6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots : Can Language Models Be Too Big? \ud83e\udd9c","abstract":"The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models , especially for English . BERT , its variants , GPT - 2/3 , and others , most recently Switch - C , have pushed the boundaries of the possible both through architectural innovations and through sheer size . Using these pretrained models and the methodology of fine - tuning them for specific tasks , researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English . In this paper , we take a step back and ask : How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first , investing resources into curating and carefully documenting datasets rather than ingesting everything on the web , carrying out pre - development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values , and encouraging research directions beyond ever larger language models . ","Method":["pretrained models","switch - c","language models","bert","gpt - 2/3"],"Task":["stochastic parrots","nlp"]},{"url":"https://www.semanticscholar.org/paper/fb73b93de3734a996829caf31e4310e0054e9c6b","title":"Green AI","abstract":"Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets . ","Method":["NOT DETECTED"],"Task":["ai research","deep learning study","green ai"]},{"url":"https://www.semanticscholar.org/paper/13f25c69973373e616c48688d06a6b6ae2736ef0","title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning","abstract":"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research . We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions , as well as generating standardized online appendices . Utilizing this framework , we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning . Finally , based on case studies using our framework , we propose strategies for mitigation of carbon emissions and reduction of energy consumption . By making accounting easier , we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms . ","Method":["leaderboard","energy efficient reinforcement learning algorithms"],"Task":["machine learning experiments","tracking realtime energy consumption and carbon emissions","reduction of energy consumption","energy efficient algorithms","machine learning","systematic reporting of the energy and carbon footprints","mitigation of carbon emissions","machine learning research","accurate reporting of energy and carbon usage"]},{"url":"https://www.semanticscholar.org/paper/3cc2f69951cd24fe61be4cf32d62afbac297bc2b","title":"Social Biases in NLP Models as Barriers for Persons with Disabilities","abstract":"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models . In particular , representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained . In this paper , we present evidence of such undesirable biases towards mentions of disability in two different English language models : toxicity prediction and sentiment analysis . Next , we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability . We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance , gun violence , homelessness , and drug addiction are over - represented in texts discussing mental illness . ","Method":["nlp models","english language models","ml models","neural embeddings"],"Task":["sentiment analysis","equitable and inclusive nlp technologies","nlp","toxicity prediction"]},{"url":"https://www.semanticscholar.org/paper/10391eed628dfece8a9136f76c5df53b5704422d","title":"Social Chemistry 101 : Learning to Reason about Social and Moral Norms","abstract":"Social norms - - - the unspoken commonsense rules about acceptable social behavior - - - are crucial in understanding the underlying causes and intents of people\'s actions in narratives . For example , underlying an action such as \\"wanting to call cops on my neighbors\\" are social norms that inform our conduct , such as \\"It is expected that you report crimes . \\" \\nWe present Social Chemistry , a new conceptual formalism to study people\'s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language . We introduce Social - Chem - 101 , a large - scale corpus that catalogs 292k rules - of - thumb such as \\"it is rude to run a blender at 5am\\" as the basic conceptual units . Each rule - of - thumb is further broken down with 12 different dimensions of people\'s judgments , including social judgments of good and bad , moral foundations , expected cultural pressure , and assumed legality , which together amount to over 4 . 5 million annotations of categorical labels and free - text descriptions . \\nComprehensive empirical results based on state - of - the - art neural models demonstrate that computational modeling of social norms is a promising research direction . Our model framework , Neural Norm Transformer , learns and generalizes Social - Chem - 101 to successfully reason about previously unseen situations , generating relevant (and potentially novel) attribute - aware social rules - of - thumb . ","Method":["social - chem - 101","social chemistry 101","conceptual formalism","neural norm transformer","neural models","social chemistry"],"Task":["attribute - aware social rules - of - thumb","computational modeling of social norms"]},{"url":"https://www.semanticscholar.org/paper/8755c15fe073c6af03664b2a74aafef1fed5f198","title":"BERT has a Moral Compass : Improvements of ethical and moral values of machines","abstract":"Allowing machines to choose whether to kill humans would be devastating for world peace and security . But how do we equip machines with the ability to learn ethical or even moral choices? Jentzsch et al . (2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about \\"right\\" and \\"wrong\\" conduct by calculating a moral bias score on a sentence level using sentence embeddings . The machine learned that it is objectionable to kill living beings , but it is fine to kill time; It is essential to eat , yet one might not eat dirt; it is important to spread information , yet one should not spread misinformation . However , the evaluated moral bias was restricted to simple actions - - one verb - - and a ranking of actions with surrounding context . Recently BERT - - - and variants such as RoBERTa and SBERT - - - has set a new state - of - the - art performance for a wide range of NLP tasks . But has BERT also a better moral compass? In this paper , we discuss and show that this is indeed the case . Thus , recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine . We argue that through an advanced semantic representation of text , BERT allows one to get better insights of moral and ethical values implicitly represented in text . This enables the Moral Choice Machine (MCM) to extract more accurate imprints of moral choices and ethical values . ","Method":["sentence embeddings","machine learning","semantic representation of text","moral choice machine","language representations","bert"],"Task":["world peace","nlp tasks"]},{"url":"https://www.semanticscholar.org/paper/2a1573cfa29a426c695e2caf6de0167a12b788ef","title":"Open Problems in Cooperative AI","abstract":"Problems of cooperation - - in which agents seek ways to jointly improve their welfare - - are ubiquitous and important . They can be found at scales ranging from our daily routines - - such as driving on highways , scheduling meetings , and working collaboratively - - to our global challenges - - such as peace , commerce , and pandemic preparedness . Arguably , the success of the human species is rooted in our ability to cooperate . Since machines powered by artificial intelligence are playing an ever greater role in our lives , it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation . \\nWe see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems , which we term Cooperative AI . The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems . Central goals include building machine agents with the capabilities needed for cooperation , building tools to foster cooperation in populations of (machine and/or human) agents , and otherwise conducting AI research for insight relevant to problems of cooperation . This research integrates ongoing work on multi - agent systems , game theory and social choice , human - machine interaction and alignment , natural - language processing , and the construction of social tools and platforms . However , Cooperative AI is not the union of these existing areas , but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas . We see opportunity to more explicitly focus on the problem of cooperation , to construct unified theory and vocabulary , and to build bridges with adjacent communities working on cooperation , including in the natural , social , and behavioural sciences . ","Method":["artificial intelligence","machine agents"],"Task":["peace","ai","social tools and platforms","cooperative ai","commerce","open problems","insight","natural , social , and behavioural sciences","cooperation","artificial intelligence","game theory","multi - agent systems","social choice","pandemic preparedness","human - machine interaction","scheduling meetings","natural - language processing","alignment"]},{"url":"https://www.semanticscholar.org/paper/ced289065723368bca48636edf71eeed50f40a39","title":"Existential Risk Prevention as Global Priority","abstract":"risks are those that threaten the entire future of humanity . Many theories of value imply that even relatively small reductions in net existential risk have enormous expected value . Despite their importance , issues surrounding human - extinction risks and related hazards remain poorly understood . In this article , I clarify the concept of existential risk and develop an improved classification scheme . I discuss the relation between existential risks and basic issues in axiology , and show how existential risk reduction (via the maxipok rule) can serve as a strongly action - guiding principle for utilitarian concerns . I also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability . Policy Implications \u2022 Existential risk is a concept that can focus long - term global efforts and sustainability concerns . \u2022 The biggest existential risks are anthropogenic and related to potential future technologies . \u2022 A moral case can be made that existential risk reduction is strictly more important than any other global public good . \u2022 Sustainability should be reconceptualised in dynamic terms , as aiming for a sustainable trajectory rather than a sus - tainable state . \u2022 Some small existential risks can be mitigated today directly (e . g . asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity\'s ability to deal with the larger existential risks that will arise later in this century . This will require collective wisdom , technology foresight , and the ability when necessary to mobilise a strong global coordi - nated response to anticipated existential risks . \u2022 Perhaps the most cost - effective way to reduce existential risks today is to fund analysis of a wide range of existen - tial risks and potential mitigation strategies , with a long - term perspective . ","Method":["maxipok rule)","classification scheme","mitigation strategies","existential risk reduction","action - guiding principle"],"Task":["existential risk prevention","sustainability concerns","utilitarian concerns","human - extinction risks","sustainability","existential risk reduction","long - term global efforts","global priority risks","policy implications","axiology"]},{"url":"https://www.semanticscholar.org/paper/8763723e27cc1d4aad166b5e1d9cb0fc8c8043dd","title":"Participatory Problem Formulation for Fairer Machine Learning Through Community Based System Dynamics","abstract":"Recent research on algorithmic fairness has highlighted that the problem formulation phase of ML system development can be a key source of bias that has significant downstream impacts on ML system fairness outcomes . However , very little attention has been paid to methods for improving the fairness efficacy of this critical phase of ML system development . Current practice neither accounts for the dynamic complexity of high - stakes domains nor incorporates the perspectives of vulnerable stakeholders . In this paper we introduce community based system dynamics (CBSD) as an approach to enable the participation of typically excluded stakeholders in the problem formulation phase of the ML system development process and facilitate the deep problem understanding required to mitigate bias during this crucial stage . ","Method":["problem formulation phase","community based system dynamics"],"Task":["ml system development","algorithmic fairness","ml system fairness outcomes","deep problem understanding","problem formulation phase","ml system development process","fairer machine learning","participatory problem formulation"]},{"url":"https://www.semanticscholar.org/paper/40141f0933b5111b089049e226dc8d969b0a7fca","title":"A Research Framework for Understanding Education - Occupation Alignment with NLP Techniques","abstract":"Understanding the gaps between job requirements and university curricula is crucial for improving student success and institutional effectiveness in higher education . In this context , natural language processing (NLP) can be leveraged to generate granular insights into where the gaps are and how they change . This paper proposes a three - dimensional research framework that combines NLP techniques with economic and educational research to quantify the alignment between course syllabi and job postings . We elaborate on key technical details of the framework and further discuss its potential positive impacts on practice , including unveiling the inequalities in and long - term consequences of education - occupation alignment to inform policymakers , and fostering information systems to support students , institutions and employers in the school - to - work pipeline . ","Method":["nlp techniques","three - dimensional research framework","information systems"],"Task":["natural language processing","school - to - work pipeline","education - occupation alignment","higher education","economic and educational research","student success"]},{"url":"https://www.semanticscholar.org/paper/8d9a678c56b9085de65024aa2f6b406ccad97390","title":"A Grounded Well - being Conversational Agent with Multiple Interaction Modes : Preliminary Results","abstract":"Technologies for enhancing well - being , healthcare vigilance and monitoring are on the rise . However , despite patient interest , such technologies suffer from low adoption . One hypothesis for this limited adoption is loss of human interaction that is central to doctor - patient encounters . In this paper we seek to address this limitation via a conversational agent that adopts one aspect of in - person doctor - patient interactions : A human avatar to facilitate medical grounded question answering . This is akin to the in - person scenario where the doctor may point to the human body or the patient may point to their own body to express their conditions . Additionally , our agent has multiple interaction modes , that may give more options for the patient to use the agent , not just for medical question answering , but also to engage in conversations about general topics and current events . Both the avatar , and the multiple interaction modes could help improve adherence . We present a high level overview of the design of our agent , Marie Bot Wellbeing . We also report implementation details of our early prototype , and present preliminary results . ","Method":["grounded well - being conversational agent","conversational agent","bot"],"Task":["in - person doctor - patient interactions","in - person scenario","medical question answering","doctor - patient encounters","well - being","healthcare vigilance","medical grounded question answering","monitoring"]},{"url":"https://www.semanticscholar.org/paper/934dbfbb33cbec11fc825db56ac85a48fc52158f","title":"A Speech - enabled Fixed - phrase Translator for Healthcare Accessibility","abstract":"In this overview article we describe an application designed to enable communication between health practitioners and patients who do not share a common language , in situations where professional interpreters are not available . Built on the principle of a fixed phrase translator , the application implements different natural language processing (NLP) technologies , such as speech recognition , neural machine translation and text - to - speech to improve usability . Its design allows easy portability to new domains and integration of different types of output for multiple target audiences . Even though BabelDr is far from solving the problem of miscommunication between patients and doctors , it is a clear example of NLP in a real world application designed to help minority groups to communicate in a medical context . It also gives some insights into the relevant criteria for the development of such an application . ","Method":["babeldr","natural language processing","fixed phrase translator","speech - enabled fixed - phrase translator","nlp"],"Task":["machine translation","text - to - speech","speech recognition","real world application","healthcare accessibility","communication between health practitioners"]},{"url":"https://www.semanticscholar.org/paper/54db327cd53fe043449c9f242d3fc34c593a70ef","title":"Analyzing Stereotypes in Generative Text Inference Tasks","abstract":"Stereotypes are inferences drawn about people based on their demographic attributes , which may result in harms to users when a system is deployed . In generative language - inference tasks , given a premise , a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference) . Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes . In our work , we study how stereotypes manifest when the potential targets of stereotypes are situated in real - life , neutral contexts . We collect human judgments on the presence of stereotypes in generated inferences , and compare how perceptions of stereotypes vary due to annotator positionality . ","Method":["NOT DETECTED"],"Task":["generated inferences","generative language - inference tasks","nlp","language inference)","generative text inference tasks","analyzing stereotypes","inference)"]},{"url":"https://www.semanticscholar.org/paper/7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c","title":"Demographic Dialectal Variation in Social Media : A Case Study of African - American English","abstract":"Though dialectal language is increasingly abundant on social media , few resources exist for developing NLP tools to handle such language . We conduct a case study of dialectal language in online conversational text by investigating African - American English (AAE) on Twitter . We propose a distantly supervised model to identify AAE - like language from demographics associated with geo - located messages , and we verify that this language follows well - known AAE linguistic phenomena . In addition , we analyze the quality of existing language identification and dependency parsing tools on AAE - like text , demonstrating that they perform poorly on such text compared to text associated with white speakers . We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE - like language . ","Method":["aae","distantly supervised model","nlp tools","ensemble classifier","language identification","dependency parsing tools"],"Task":["language identification"]},{"url":"https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP","abstract":"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data . These models have obtained notable gains in accuracy across many NLP tasks . However , these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption . As a result these models are costly to train and develop , both financially , due to the cost of hardware and electricity or cloud compute time , and environmentally , due to the carbon footprint required to fuel modern tensor processing hardware . In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP . Based on these findings , we propose actionable recommendations to reduce costs and improve equity in NLP research and practice . ","Method":["tensor processing hardware","large networks","neural network models","neural networks"],"Task":["energy and policy considerations","deep learning","nlp"]},{"url":"https://www.semanticscholar.org/paper/d6a25d8726c5484bb224a3350528aae9fcaae65f","title":"Automatic Sentence Simplification in Low Resource Settings for Urdu","abstract":"To build automated simplification systems , corpora of complex sentences and their simplified versions is the first step to understand sentence complexity and enable the development of automatic text simplification systems . We present a lexical and syntactically simplified Urdu simplification corpus with a detailed analysis of the various simplification operations and human evaluation of corpus quality . We further analyze our corpora using text readability measures and present a comparison of the original , lexical simplified and syntactically simplified corpora . In addition , we compare our corpus with other existing simplification corpora by building simplification systems and evaluating these systems using BLEU and SARI scores . Our system achieves the highest BLEU score and comparable SARI score in comparison to other systems . We release our simplification corpora for the benefit of the research community . ","Method":["simplification systems","simplification operations"],"Task":["automatic sentence simplification","automated simplification systems","urdu","low resource settings","automatic text simplification systems"]},{"url":"https://www.semanticscholar.org/paper/4975c64466149c72f31489fadbbbff4e85d7b3f3","title":"Cartography of Natural Language Processing for Social Good (NLP4SG) : Searching for Definitions , Statistics and White Spots","abstract":"The range of works that can be considered as developing NLP for social good (NLP4SG) is enormous . While many of them target the identification of hate speech or fake news , there are others that address , e . g . , text simplification to alleviate consequences of dyslexia , or coaching strategies to fight depression . However , so far , there is no clear picture of what areas are targeted by NLP4SG , who are the actors , which are the main scenarios and what are the topics that have been left aside . In order to obtain a clearer view in this respect , we first propose a working definition of NLP4SG and identify some primary aspects that are crucial for NLP4SG , including , e . g . , areas , ethics , privacy and bias . Then , we draw upon a corpus of around 50 , 000 articles downloaded from the ACL Anthology . Based on a list of keywords retrieved from the literature and revised in view of the task , we select from this corpus articles that can be considered to be on NLP4SG according to our definition and analyze them in terms of trends along the time line , etc . The result is a map of the current NLP4SG research and insights concerning the white spots on this map . ","Method":["coaching strategies","nlp4sg","nlp"],"Task":["identification of hate speech","social good","nlp4sg","cartography of natural language processing","text simplification"]},{"url":"https://www.semanticscholar.org/paper/c39fc494cb84b260eaa33cceda2b76512f3701a5","title":"Breaking Down Walls of Text : How Can NLP Benefit Consumer Privacy?","abstract":"Privacy plays a crucial role in preserving democratic ideals and personal autonomy . The dominant legal approach to privacy in many jurisdictions is the \u201cNotice and Choice\u201d paradigm , where privacy policies are the primary instrument used to convey information to users . However , privacy policies are long and complex documents that are difficult for users to read and comprehend . We discuss how language technologies can play an important role in addressing this information gap , reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies : consumers , enterprises , and regulators . Our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy , limit privacy harms , and rally research efforts from the community towards addressing an issue with large social impact . We highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies . ","Method":["language technologies","legal approach","nlp"],"Task":["consumer privacy? privacy","preserving democratic ideals","privacy"]},{"url":"https://www.semanticscholar.org/paper/9995132dda17b36e5513c8e98d58ff992d0ba79a","title":"Are we human , or are we users? The role of natural language processing in human - centric news recommenders that nudge users to diverse content","abstract":"In this position paper , we present a research agenda and ideas for facilitating exposure to diverse viewpoints in news recommendation . Recommending news from diverse viewpoints is important to prevent potential filter bubble effects in news consumption , and stimulate a healthy democratic debate . To account for the complexity that is inherent to humans as citizens in a democracy , we anticipate (among others) individual - level differences in acceptance of diversity . We connect this idea to techniques in Natural Language Processing , where distributional language models would allow us to place different users and news articles in a multidimensional space based on semantic content , where diversity is operationalized as distance and variance . In this way , we can model individual \u201clatitudes of diversity\u201d for different users , and thus personalize viewpoint diversity in support of a healthy public debate . In addition , we identify technical , ethical and conceptual issues related to our presented ideas . Our investigation describes how NLP can play a central role in diversifying news recommendations . ","Method":["distributional language models","natural language processing","nlp"],"Task":["democratic debate","natural language processing","filter bubble effects","human - centric news recommenders","exposure","diversifying news recommendations","news recommendation","news consumption"]},{"url":"https://www.semanticscholar.org/paper/03c046041bc509f2cc9671ee71a78642275b77c3","title":"Challenges for Information Extraction from Dialogue in Criminal Law","abstract":"Information extraction and question answering have the potential to introduce a new paradigm for how machine learning is applied to criminal law . Existing approaches generally use tabular data for predictive metrics . An alternative approach is needed for matters of equitable justice , where individuals are judged on a case - by - case basis , in a process involving verbal or written discussion and interpretation of case factors . Such discussions are individualized , but they nonetheless rely on underlying facts . Information extraction can play an important role in surfacing these facts , which are still important to understand . We analyze unsupervised , weakly supervised , and pre - trained models\u2019 ability to extract such factual information from the free - form dialogue of California parole hearings . With a few exceptions , most F1 scores are below 0 . 85 . We use this opportunity to highlight some opportunities for further research for information extraction and question answering . We encourage new developments in NLP to enable analysis and review of legal cases to be done in a post - hoc , not predictive , manner . ","Method":["unsupervised","pre - trained models\u2019","weakly supervised"],"Task":["criminal law","question answering","nlp","dialogue","machine learning","information extraction","analysis and review of legal cases","predictive , manner","equitable justice"]},{"url":"https://www.semanticscholar.org/paper/50718d6bd163967b8353de4c854ed866b2b56c2f","title":"Conversations Gone Alright : Quantifying and Predicting Prosocial Outcomes in Online Conversations","abstract":"Online conversations can go in many directions : some turn out poorly due to antisocial behavior , while others turn out positively to the benefit of all . Research on improving online spaces has focused primarily on detecting and reducing antisocial behavior . Yet we know little about positive outcomes in online conversations and how to increase them\u2014is a prosocial outcome simply the lack of antisocial behavior or something more? Here , we examine how conversational features lead to prosocial outcomes within online discussions . We introduce a series of new theory - inspired metrics to define prosocial outcomes such as mentoring and esteem enhancement . Using a corpus of 26M Reddit conversations , we show that these outcomes can be forecasted from the initial comment of an online conversation , with the best model providing a relative 24% improvement over human forecasting performance at ranking conversations for predicted outcome . Our results indicate that platforms can use these early cues in their algorithmic ranking of early conversations to prioritize better outcomes . ","Method":["NOT DETECTED"],"Task":["detecting and reducing antisocial behavior","online spaces","mentoring","algorithmic ranking of early conversations","quantifying and predicting prosocial outcomes"]},{"url":"https://www.semanticscholar.org/paper/52a14b391f994d83759787500a9bda865acdb3c5","title":"Recommender systems and their ethical challenges","abstract":"This article presents the first , systematic analysis of the ethical challenges posed by recommender systems through a literature review . The article identifies six areas of concern , and maps them onto a proposed taxonomy of different kinds of ethical impact . The analysis uncovers a gap in the literature : currently user - centred approaches do not consider the interests of a variety of other stakeholders\u2014as opposed to just the receivers of a recommendation\u2014in assessing the ethical impacts of a recommender system . ","Method":["user - centred approaches","recommender systems","recommender system"],"Task":["ethical challenges"]},{"url":"https://www.semanticscholar.org/paper/ff3b0be4fb7debf1312c92381577292288755674","title":"Conversational receptiveness : Improving engagement with opposing views","abstract":"Abstract We examine \u201cconversational receptiveness\u201d \u2013 the use of language to communicate one\u2019s willingness to thoughtfully engage with opposing views . We develop an interpretable machine - learning algorithm to identify the linguistic profile of receptiveness (Studies 1A - B) . We then show that in contentious policy discussions , government executives who were rated as more receptive - according to our algorithm and their partners , but not their own self - evaluations - were considered better teammates , advisors , and workplace representatives (Study 2) . Furthermore , using field data from a setting where conflict management is endemic to productivity , we show that conversational receptiveness at the beginning of a conversation forestalls conflict escalation at the end . Specifically , Wikipedia editors who write more receptive posts are less prone to receiving personal attacks from disagreeing editors (Study 3) . We develop a \u201creceptiveness recipe\u201d intervention based on our algorithm . We find that writers who follow the recipe are seen as more desirable partners for future collaboration and their messages are seen as more persuasive (Study 4) . Overall , we find that conversational receptiveness is reliably measurable , has meaningful relational consequences , and can be substantially improved using our intervention (183 words) . ","Method":["interpretable machine - learning algorithm","conversational receptiveness","recipe\u201d intervention"],"Task":["contentious policy discussions","conflict management"]},{"url":"https://www.semanticscholar.org/paper/e511b338559a1df846059068ce7cc64c7066be4c","title":"Empathy and Hope : Resource Transfer to Model Inter - country Social Media Dynamics","abstract":"The ongoing COVID - 19 pandemic resulted in significant ramifications for international relations ranging from travel restrictions , global ceasefires , and international vaccine production and sharing agreements . Amidst a wave of infections in India that resulted in a systemic breakdown of healthcare infrastructure , a social welfare organization based in Pakistan offered to procure medical - grade oxygen to assist India - a nation which was involved in four wars with Pakistan in the past few decades . In this paper , we focus on Pakistani Twitter users\u2019 response to the ongoing healthcare crisis in India . While #IndiaNeedsOxygen and #PakistanStandsWithIndia featured among the top - trending hashtags in Pakistan , divisive hashtags such as #EndiaSaySorryToKashmir simultaneously started trending . Against the backdrop of a contentious history including four wars , divisive content of this nature , especially when a country is facing an unprecedented healthcare crisis , fuels further deterioration of relations . In this paper , we define a new task of detecting supportive content and demonstrate that existing NLP for social impact tools can be effectively harnessed for such tasks within a quick turnaround time . We also release the first publicly available data set at the intersection of geopolitical relations and a raging pandemic in the context of India and Pakistan . ","Method":["social impact tools","social welfare organization","nlp"],"Task":["inter - country social media dynamics","international relations","detecting supportive content","international vaccine production and sharing agreements","resource transfer","healthcare infrastructure","covid - 19 pandemic"]},{"url":"https://www.semanticscholar.org/paper/022b80b663c51563a1c6772c12ada3c79f5d798d","title":"Guiding Principles for Participatory Design - inspired Natural Language Processing","abstract":"We introduce 9 guiding principles to integrate Participatory Design (PD) methods in the development of Natural Language Processing (NLP) systems . The adoption of PD methods by NLP will help to alleviate issues concerning the development of more democratic , fairer , less - biased technologies to process natural language data . This short paper is the outcome of an ongoing dialogue between designers and NLP experts and adopts a non - standard format following previous work by Traum (2000); Bender (2013); Abzianidze and Bos (2019) . Every section is a guiding principle . While principles 1\u20133 illustrate assumptions and methods that inform community - based PD practices , we used two fictional design scenarios (Encinas and Blythe , 2018) , which build on top of situations familiar to the authors , to elicit the identification of the other 6 . Principles 4\u20136 describes the impact of PD methods on the design of NLP systems , targeting two critical aspects : data collection & annotation , and the deployment & evaluation . Finally , principles 7\u20139 guide a new reflexivity of the NLP research with respect to its context , actors and participants , and aims . We hope this guide will offer inspiration and a road - map to develop a new generation of PD - inspired NLP . ","Method":["biased technologies","guiding principles","community - based pd practices","nlp","participatory design (pd) methods","pd methods"],"Task":["natural language processing","nlp","deployment","data collection","evaluation","participatory design - inspired natural language processing","annotation"]},{"url":"https://www.semanticscholar.org/paper/203bdaca3986b51f8d011422c04ff1489e425ce5","title":"Detecting Hashtag Hijacking for Hashtag Activism","abstract":"Social media has changed the way we engage in social activities . On Twitter , users can participate in social movements using hashtags such as #MeToo; this is known as hashtag activism . However , while these hashtags can help reshape social norms , they can also be used maliciously by spammers or troll communities for other purposes , such as signal boosting unrelated content , making a dent in a movement , or sharing hate speech . We present a Tweet - level hashtag hijacking detection framework focusing on hashtag activism . Our weakly - supervised framework uses bootstrapping to update itself as new Tweets are posted . Our experiments show that the system adapts to new topics in a social movement , as well as new hijacking strategies , maintaining strong performance over time . ","Method":["weakly - supervised framework","hashtags","hijacking strategies","bootstrapping","tweet - level hashtag hijacking detection framework"],"Task":["hashtag activism","detecting hashtag hijacking","hashtag activism social media","signal boosting unrelated content"]},{"url":"https://www.semanticscholar.org/paper/41ebff09aff17c37efdab8c1d7051cbf150970f8","title":"Dialogue Act Classification for Augmentative and Alternative Communication","abstract":"Augmentative and Alternative Communication (AAC) devices and applications are intended to make it easier for individuals with complex communication needs to participate in conversations . However , these devices have low adoption and retention rates . We review prior work with text recommendation systems that have not been successful in mitigating these problems . To address these gaps , we propose applying Dialogue Act classification to AAC conversations . We evaluated the performance of a state of the art model on a limited AAC dataset that was trained on both AAC and non - AAC datasets . The one trained on AAC (accuracy = 38 . 6%) achieved better performance than that trained on a non - AAC corpus (accuracy = 34 . 1%) . These results reflect the need to incorporate representative datasets in later experiments . We discuss the need to collect more labeled AAC datasets and propose areas of future work . ","Method":["aac","text recommendation systems"],"Task":["aac conversations","dialogue act classification","augmentative and alternative communication"]},{"url":"https://www.semanticscholar.org/paper/d393f2a793930a6e38321340185756860f43c62c","title":"Improving Policing with Natural Language Processing","abstract":"This article explores the potential for Natural Language Processing (NLP) to enable a more effective , prevention focused and less confrontational policing model that has hitherto been too resource consuming to implement at scale . Problem - Oriented Policing (POP) is a potential replacement , at least in part , for traditional policing which adopts a reactive approach , relying heavily on the criminal justice system . By contrast , POP seeks to prevent crime by manipulating the underlying conditions that allow crimes to be committed . Identifying these underlying conditions requires a detailed understanding of crime events - tacit knowledge that is often held by police officers but which can be challenging to derive from structured police data . One potential source of insight exists in unstructured free text data commonly collected by police for the purposes of investigation or administration . Yet police agencies do not typically have the skills or resources to analyse these data at scale . In this article we argue that NLP offers the potential to unlock these unstructured data and by doing so allow police to implement more POP initiatives . However we caution that using NLP models without adequate knowledge may either allow or perpetuate bias within the data potentially leading to unfavourable outcomes . ","Method":["natural language processing","nlp","nlp models","reactive approach","criminal justice system","pop"],"Task":["natural language processing","problem - oriented policing","improving policing","prevention focused and less confrontational policing model","policing","investigation or administration"]},{"url":"https://www.semanticscholar.org/paper/2c5b31a02133dea21cf94fde67c8948115441432","title":"Methods for Detoxification of Texts for the Russian Language","abstract":"We introduce the first study of automatic detoxification of Russian texts to combat offensive language . Such a kind of textual style transfer can be used , for instance , for processing toxic content in social media . While much work has been done for the English language in this field , it has never been solved for the Russian language yet . We test two types of models \u2013 unsupervised approach based on BERT architecture that performs local corrections and supervised approach based on pretrained language GPT - 2 model \u2013 and compare them with several baselines . In addition , we describe evaluation setup providing training datasets and metrics for automatic evaluation . The results show that the tested approaches can be successfully used for detoxification , although there is room for improvement . ","Method":["bert architecture","unsupervised approach","pretrained language gpt - 2 model","supervised approach"],"Task":["automatic detoxification of russian texts","textual style transfer","processing toxic content","detoxification of texts","automatic evaluation","local corrections","detoxification"]},{"url":"https://www.semanticscholar.org/paper/5c7b8f26ae23c457d85da9d0f19b28e588e9b9a7","title":"How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact","abstract":"Recent years have seen many breakthroughs in natural language processing (NLP) , transitioning it from a mostly theoretical field to one with many real - world applications . Noting the rising number of applications of other machine learning and AI techniques with pervasive societal impact , we anticipate the rising importance of developing NLP technologies for social good . Inspired by theories in moral philosophy and global priorities research , we aim to promote a guideline for social good in the context of NLP . We lay the foundations via the moral philosophy definition of social good , propose a framework to evaluate the direct and indirect real - world impact of NLP tasks , and adopt the methodology of global priorities research to identify priority causes for NLP research . Finally , we use our theoretical framework to provide some practical guidelines for future NLP research for social good . 1","Method":["machine learning","nlp technologies","ai techniques","nlp?"],"Task":["natural language processing","social good","nlp","nlp tasks","nlp research","global priorities research"]},{"url":"https://www.semanticscholar.org/paper/9e1616dcabf4d04d14d642fcb7963c461cf13d41","title":"NLP for Consumer Protection : Battling Illegal Clauses in German Terms and Conditions in Online Shopping","abstract":"Online shopping is an ever more important part of the global consumer economy , not just in times of a pandemic . When we place an order online as consumers , we regularly agree to the so - called \u201cTerms and Conditions\u201d (T&C) , a contract unilaterally drafted by the seller . Often , consumers do not read these contracts and unwittingly agree to unfavourable and often void terms . Government and non - government organisations (NGOs) for consumer protection battle such terms on behalf of consumers , who often hesitate to take on legal actions themselves . However , the growing number of online shops and a lack of funding makes it increasingly difficult for such organisations to monitor the market effectively . This paper describes how Natural Language Processing (NLP) can be applied to support consumer advocates in their efforts to protect consumers . Together with two NGOs from Germany , we developed an NLP - based application that legally assesses clauses in T&C from German online shops under the European Union\u2019s (EU) jurisdiction . We report that we could achieve an accuracy of 0 . 9 in the detection of void clauses by fine - tuning a pre - trained German BERT model . The approach is currently used by two NGOs and has already helped to challenge void clauses in T&C . ","Method":["german bert model","nlp"],"Task":["online shopping","natural language processing","detection of void clauses","battling illegal clauses","consumer protection","consumer advocates"]},{"url":"https://www.semanticscholar.org/paper/473b70bb3c531b2d740fa6d652956e2733b53243","title":"Towards Knowledge - Grounded Counter Narrative Generation for Hate Speech","abstract":"Tackling online hatred using informed textual responses \u2013 called counter narratives \u2013 has been brought under the spotlight recently . Accordingly , a research line has emerged to automatically generate counter narratives in order to facilitate the direct intervention in the hate discussion and to prevent hate content from further spreading . Still , current neural approaches tend to produce generic/repetitive responses and lack grounded and up - to - date evidence such as facts , statistics , or examples . Moreover , these models can create plausible but not necessarily true arguments . In this paper we present the first complete knowledgebound counter narrative generation pipeline , grounded in an external knowledge repository that can provide more informative content to fight online hatred . Together with our approach , we present a series of experiments that show its feasibility to produce suitable and informative counter narratives in in - domain and cross - domain settings . ","Method":["neural approaches","knowledgebound counter narrative generation pipeline"],"Task":["hate speech","direct intervention","hate discussion","knowledge - grounded counter narrative generation","online hatred","cross - domain settings"]},{"url":"https://www.semanticscholar.org/paper/f29d5cb8f405903fc8af7a5d7ab4bf7d65796e95","title":"Use of Formal Ethical Reviews in NLP Literature : Historical Trends and Current Practices","abstract":"Ethical aspects of research in language technologies have received much attention recently . It is a standard practice to get a study involving human subjects reviewed and approved by a professional ethics committee/board of the institution . How commonly do we see mention of ethical approvals in NLP research? What types of research or aspects of studies are usually subject to such reviews? With the rising concerns and discourse around the ethics of NLP , do we also observe a rise in formal ethical reviews of NLP studies? And , if so , would this imply that there is a heightened awareness of ethical issues that was previously lacking? We aim to address these questions by conducting a detailed quantitative and qualitative analysis of the ACL Anthology , as well as comparing the trends in our field to those of other related disciplines , such as cognitive science , machine learning , data mining , and systems . ","Method":["formal ethical reviews","cognitive science"],"Task":["systems","nlp","machine learning","language technologies","data mining"]},{"url":"https://www.semanticscholar.org/paper/23a812dde149021bab611ca4395279b9d41031f1","title":"Using Word Embeddings to Analyze Teacher Evaluations : An Application to a Filipino Education Non - Profit Organization","abstract":"Analysis of teacher evaluations is crucial to the development of robust educational programs , particularly through the validation of desirable qualities being reflected on in the text . This research applies Natural Language Processing techniques on a real - world dataset from a Filipino education non - profit to explore insights from analyzing evaluations written by Teacher Fellows who assess their own progress . Prior to this research , only qualitative assessment had been conducted on the text . Inspired by the use of word embedding similarities to capture semantic alignment , we utilize GloVe embeddings to determine to what extent these evaluations reflect concepts critical to measuring the competency of Teacher Fellows and upholding the organization\u2019s Vision and Mission . As Fellows\u2019 quantitative ratings improved , so too did their demonstration of competency in the text . Further , Teacher Fellow language was consistent with the organization\u2019s Vision and Mission . This research therefore showcases the possibilities of NLP in education , improving our understanding of Teacher Fellow evaluations , which can lead to advances in program operations and education efforts . ","Method":["word embeddings","word embedding similarities","glove embeddings","natural language processing techniques"],"Task":["robust educational programs","nlp","education","teacher evaluations","analysis of teacher evaluations","teacher fellow evaluations","semantic alignment","filipino education non - profit organization"]},{"url":"https://www.semanticscholar.org/paper/d4eb2ca9694f34d63abe6d27bd2d958992431017","title":"Theano : A Greek - speaking conversational agent for COVID - 19","abstract":"Conversational Agents (CAs) can be a proxy for disseminating information and providing support to the public , especially in times of crisis . CAs can scale to reach larger numbers of end - users than human operators , while they can offer information interactively and engagingly . In this work , we present Theano , a Greek - speaking virtual assistant for COVID - 19 . Theano presents users with COVID - 19 statistics and facts and informs users about the best health practices as well as the latest COVID - 19 related guidelines . Additionally , Theano provides support to end - users by helping them self - assess their symptoms and redirecting them to first - line health workers . The relevant , localized information that Theano provides , makes it a valuable tool for combating COVID - 19 in Greece . Theano has already conversed with different users in more than 170 different conversations through a web interface as a chatbot and over the phone as a voice bot . ","Method":["theano","greek - speaking conversational agent","cas","greek - speaking virtual assistant","conversational agents"],"Task":["covid - 19","disseminating information"]},{"url":"https://www.semanticscholar.org/paper/100e0f3dcd319266b2772f0841dad388b45cce3f","title":"Restatement and Question Generation for Counsellor Chatbot","abstract":"Amidst rising mental health needs in society , virtual agents are increasingly deployed in counselling . In order to give pertinent advice , counsellors must first gain an understanding of the issues at hand by eliciting sharing from the counsellee . It is thus important for the counsellor chatbot to encourage the user to open up and talk . One way to sustain the conversation flow is to acknowledge the counsellee\u2019s key points by restating them , or probing them further with questions . This paper applies models from two closely related NLP tasks \u2014 summarization and question generation \u2014 to restatement and question generation in the counselling context . We conducted experiments on a manually annotated dataset of Cantonese post - reply pairs on topics related to loneliness , academic anxiety and test anxiety . We obtained the best performance in both restatement and question generation by fine - tuning BertSum , a state - of - the - art summarization model , with the in - domain manual dataset augmented with a large - scale , automatically mined open - domain dataset . ","Method":["virtual agents","bertsum"],"Task":["counselling context","restatement","summarization","nlp tasks","restatement and question generation","counselling","question generation","counsellor chatbot"]}]'),o=JSON.parse('[{"url":"https://www.semanticscholar.org/paper/e21ab639b0308a71aeac643a12f59a32603d091d","title":"UPSTAGE : Unsupervised Context Augmentation for Utterance Classification in Patient - Provider Communication","abstract":"Conversations between patients and providers in clinical settings provide a source of natural language data that may reflect and correlate with the patients\u2019 experience and response to the treatment they are receiving . When analyzing utterances in such conversations , it is not sufficient to consider each sentence in isolation , since its context may play a role in determining its semantic meaning . Recently , contextual information in natural language documents has been modeled using various techniques , such as recurrent neural networks with latent variables , or neural networks with attention mechanisms . In this paper , we present UnsuPerviSed conText AuGmEntation (Upstage) , a classification framework that relies on both local and global contextual information from different sources . Upstage uses transformer models with pretrained language models and joint sentence representation to solve the task of classifying health topics in patient - provider conversations . In addition , Upstage leverages unlabeled corpora for pretraining and data augmentation to provide additional context , which leads to improved classification performance . ","Method":["upstage"],"Task":["classification","utterance classification"]},{"url":"https://www.semanticscholar.org/paper/d77719ba4be4196c9a918ce8dff0edf5aa3c04e7","title":"A Review of Challenges and Opportunities in Machine Learning for Health . ","abstract":"Modern electronic health records (EHRs) provide data to answer clinically meaningful questions . The growing data in EHRs makes healthcare ripe for the use of machine learning . However , learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies . For example , diseases in EHRs are poorly labeled , conditions can encompass multiple underlying endotypes , and healthy individuals are underrepresented . This article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare . ","Method":["machine learning"],"Task":["machine learning"]},{"url":"https://www.semanticscholar.org/paper/e8d330f11df9c69f38b78a7cc4b1333ebecf7c55","title":"Ethical Machine Learning in Health Care","abstract":"The use of machine learning (ML) in healthcare raises numerous ethical concerns , especially as models can amplify existing health inequities . Here , we outline ethical considerations for equitable ML in the advancement of healthcare . Specifically , we frame ethics of ML in healthcare through the lens of social justice . We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health , ranging from problem selection to postdeployment considerations . We close by summarizing recommendations to address these challenges . ","Method":["ml"],"Task":["ethical machine learning"]},{"url":"https://www.semanticscholar.org/paper/4fd75d18b231f73e9c878cb6029900e4b0e327bb","title":"Intimate Partner Violence and Injury Prediction From Radiology Reports","abstract":"Intimate partner violence (IPV) is an urgent , prevalent , and under - detected public health issue . We present machine learning models to assess patients for IPV and injury . We train the predictive algorithms on radiology reports with 1) IPV labels based on entry to a violence prevention program and 2) injury labels provided by emergency radiology fellowship - trained physicians . Our dataset includes 34 , 642 radiology reports and 1479 patients of IPV victims and control patients . Our best model predicts IPV a median of 3 . 08 years before violence prevention program entry with a sensitivity of 64% and a specificity of 95% . We conduct error analysis to determine for which patients our model has especially high or low performance and discuss next steps for a deployed clinical risk model . ","Method":["error analysis"],"Task":["intimate partner violence","intimate partner violence and injury prediction"]},{"url":"https://www.semanticscholar.org/paper/e7bf950be4cf8309d0df68d18fa09e77f5b2511a","title":"De - identification of patient notes with recurrent neural networks","abstract":"Objective\\nPatient notes in electronic health records (EHRs) may contain critical information for medical investigations . However , the vast majority of medical investigators can only access de - identified notes , in order to protect the confidentiality of patients . In the United States , the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information that needs to be removed to de - identify patient notes . Manual de - identification is impractical given the size of electronic health record databases , the limited number of researchers with access to non - de - identified notes , and the frequent mistakes of human annotators . A reliable automated de - identification system would consequently be of high value . \\n\\n\\nMaterials and Methods\\nWe introduce the first de - identification system based on artificial neural networks (ANNs) , which requires no handcrafted features or rules , unlike existing systems . We compare the performance of the system with state - of - the - art systems on two datasets : the i2b2 2014 de - identification challenge dataset , which is the largest publicly available de - identification dataset , and the MIMIC de - identification dataset , which we assembled and is twice as large as the i2b2 2014 dataset . \\n\\n\\nResults\\nOur ANN model outperforms the state - of - the - art systems . It yields an F1 - score of 97 . 85 on the i2b2 2014 dataset , with a recall of 97 . 38 and a precision of 98 . 32 , and an F1 - score of 99 . 23 on the MIMIC de - identification dataset , with a recall of 99 . 25 and a precision of 99 . 21 . \\n\\n\\nConclusion\\nOur findings support the use of ANNs for de - identification of patient notes , as they show better performance than previously published systems while requiring no manual feature engineering . ","Method":["recurrent neural networks"],"Task":["mimic de - identification dataset","manual de - identification"]},{"url":"https://www.semanticscholar.org/paper/defeaddec92b53fc2e9787cc3652e3d8be812193","title":"Segment convolutional neural networks (Seg - CNNs) for classifying relations in clinical notes","abstract":"We propose Segment Convolutional Neural Networks (Seg - CNNs) for classifying relations from clinical notes . Seg - CNNs use only word - embedding features without manual feature engineering . Unlike typical CNN models , relations between 2 concepts are identified by simultaneously learning separate representations for text segments in a sentence : preceding , concept1 , middle , concept2 , and succeeding . We evaluate Seg - CNN on the i2b2/VA relation classification challenge dataset . We show that Seg - CNN achieves a state - of - the - art micro - average F - measure of 0 . 742 for overall evaluation , 0 . 686 for classifying medical problem - treatment relations , 0 . 820 for medical problem - test relations , and 0 . 702 for medical problem - medical problem relations . We demonstrate the benefits of learning segment - level representations . We show that medical domain word embeddings help improve relation classification . Seg - CNNs can be trained quickly for the i2b2/VA dataset on a graphics processing unit (GPU) platform . These results support the use of CNNs computed over segments of text for classifying medical relations , as they show state - of - the - art performance while requiring no manual feature engineering . ","Method":["- cnns)","seg - cnns","segment convolutional neural networks","seg - cnn"],"Task":["classifying relations","relation classification"]},{"url":"https://www.semanticscholar.org/paper/4554bc45be85cca73e3f94220c8cb056cf6ebf74","title":"Fast , Structured Clinical Documentation via Contextual Autocomplete","abstract":"We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi - structured clinical documentation . We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data . By constraining our architecture to shallow neural networks , we are able to make these suggestions in real time . Furthermore , as our algorithm is used to write a note , we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies , making notes more structured and readable for physicians , patients , and future algorithms . To our knowledge , this system is the only machine learning - based documentation utility for clinical notes deployed in a live hospital setting , and it reduces keystroke burden of clinical concepts by 67% in real environments . ","Method":["learned autocompletion mechanism"],"Task":["fast , structured clinical documentation"]},{"url":"https://www.semanticscholar.org/paper/4a10dffca6dcce9c570cb75aa4d76522c34a2fd4","title":"CORD - 19 : The COVID - 19 Open Research Dataset","abstract":"The COVID - 19 Open Research Dataset (CORD - 19) is a growing resource of scientific papers on COVID - 19 and related historical coronavirus research . CORD - 19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers . Since its release , CORD - 19 has been downloaded over 200K times and has served as the basis of many COVID - 19 text mining and discovery systems . In this article , we describe the mechanics of dataset construction , highlighting challenges and key design decisions , provide an overview of how CORD - 19 has been used , and describe several shared tasks built around the dataset . We hope this resource will continue to bring together the computing community , biomedical experts , and policy makers in the search for effective treatments and management policies for COVID - 19 . ","Method":["cord - 19"],"Task":["covid - 19"]},{"url":"https://www.semanticscholar.org/paper/3f8d4444cd124e21d4fa47b514c8267a3d5d9649","title":"Can AI Help Reduce Disparities in General Medical and Mental Health Care?","abstract":"Background\\nAs machine learning becomes increasingly common in health care applications , concerns have been raised about bias in these systems\' data , algorithms , and recommendations . Simply put , as health care improves for some , it might not improve for all . \\n\\n\\nMethods\\nTwo case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (ICU) mortality and 30 - day psychiatric readmission with respect to race , gender , and insurance payer type as a proxy for socioeconomic status . \\n\\n\\nResults\\nClinical note topics and psychiatric note topics were heterogenous with respect to race , gender , and insurance payer type , which reflects known clinical findings . Differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for ICU mortality and with respect to insurance policy for psychiatric 30 - day readmission . \\n\\n\\nConclusions\\nThis analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care . ","Method":["ai"],"Task":["icu mortality"]},{"url":"https://www.semanticscholar.org/paper/2d40f76055a7c28dd30c8b8f60c31724f8991cd8","title":"The Ivory Tower Lost : How College Students Respond Differently than the General Public to the COVID - 19 Pandemic","abstract":"In the United States , the country with the highest confirmed COVID - 19 infection cases , a nationwide social distancing protocol has been implemented by the President . Following the closure of the University of Washington on March 7th , more than 1000 colleges and universities in the United States have cancelled in - person classes and campus activities , impacting millions of students . This paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people\'s opinions on social media . We discover several topics embedded in a large number of COVID - 19 tweets that represent the most central issues related to the pandemic , which are of great concerns for both college students and the general public . Moreover , we find significant differences between these two groups of Twitter users with respect to the sentiments they expressed towards the COVID - 19 issues . To our best knowledge , this is the first social media - based study which focuses on the college student community\'s demographics and responses to prevalent social issues during a major crisis . ","Method":["social distancing protocol"],"Task":["covid - 19 pandemic"]},{"url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","title":"HumAID : Human - Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination , especially during time - critical events such as natural disasters . Despite its significantly large volume , social media content is often too noisy for direct use in any application . Therefore , it is important to filter , categorize , and concisely summarize the available content to facilitate effective consumption and decision - making . To address such issues automatic classification systems have been developed using supervised modeling approaches , thanks to the earlier efforts on creating labeled datasets . However , existing datasets are limited in different aspects (e . g . , size , contains duplicates) and less suitable to support more advanced and data - hungry deep learning models . In this paper , we present a new large - scale dataset with \u223c77K human - labeled tweets , sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019 . Moreover , we propose a data collection and sampling pipeline , which is important for social media data sampling for human annotation . We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies . The dataset and associated resources are publicly available at https : //crisisnlp . qcri . org/humaid_dataset . html . ","Method":["deep learning"],"Task":["classification","multiclass classification"]},{"url":"https://www.semanticscholar.org/paper/2a29f7cc10fc18ce6e031e0cf6fc307a6b85d8e9","title":"CrisisMMD : Multimodal Twitter Datasets from Natural Disasters","abstract":"During natural and man - made disasters , people use social media platforms such as Twitter to post textual and multime - dia content to report updates about injured or dead people , infrastructure damage , and missing or found people among other information types . Studies have revealed that this on - line information , if processed timely and effectively , is ex - tremely useful for humanitarian organizations to gain situational awareness and plan relief operations . In addition to the analysis of textual content , recent studies have shown that imagery content on social media can boost disaster response significantly . Despite extensive research that mainly focuses on textual content to extract useful information , limited work has focused on the use of imagery content or the combination of both content types . One of the reasons is the lack of labeled imagery data in this domain . Therefore , in this paper , we aim to tackle this limitation by releasing a large multi - modal dataset collected from Twitter during different natural disasters . We provide three types of annotations , which are useful to address a number of crisis response and management tasks for different humanitarian organizations . ","Method":["crisismmd"],"Task":["disaster response"]},{"url":"https://www.semanticscholar.org/paper/561ede166947a8bedb8be9acff182913156e06c6","title":"Domain Adaptation with Adversarial Training and Graph Embeddings","abstract":"The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data . However , obtaining labeled data is a big challenge in many real - world problems . In such scenarios , a DNN model can leverage labeled and unlabeled data from a related domain , but it has to deal with the shift in data distributions between the source and the target domains . In this paper , we study the problem of classifying social media posts during a crisis event (e . g . , Earthquake) . For that , we use labeled and unlabeled data from past similar events (e . g . , Flood) and unlabeled data for the current event . We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi - supervised learning to leverage unlabeled data within a single unified deep learning framework . Our experiments with two real - world crisis datasets collected from Twitter demonstrate significant improvements over several baselines . ","Method":["adversarial learning"],"Task":["classifying social media posts"]},{"url":"https://www.semanticscholar.org/paper/579914dac924d6b4a17889ea7b0dd744a62bbd74","title":"IBC - C : A Dataset for Armed Conflict Event Analysis","abstract":"We describe the Iraq Body Count Corpus (IBC - C) dataset , the first substantial armed conflict - related dataset which can be used for conflict analysis . IBC - C provides a ground - truth dataset for conflict specific named entity recognition , slot filling , and event de - duplication . IBC - C is constructed using data collected by the Iraq Body Count project which has been recording casualties resulting from the ongoing war in Iraq since 2003 . We describe the dataset\u2019s creation , how it can be used for the above three tasks and provide initial baseline results for the first task (named entity recognition) using Hidden Markov Models , Conditional Random Fields , and Recursive Neural Networks . ","Method":["ibc - c"],"Task":["conflict specific named entity recognition","entity recognition)"]},{"url":"https://www.semanticscholar.org/paper/3004f78d84752f3ffe8e54fa265b2ee85dac9728","title":"Text as Data for Conflict Research : A Literature Survey","abstract":"Computer - aided text analysis (CATA) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology , political science , communication studies , and computer science . The chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres . This includes both conflict as it is verbalized in the news media , in political speeches , and other public documents and conflict as it occurs in online spaces (social media platforms , forums) and that is largely confined to such spaces (e . g . , flaming and trolling) . Particular emphasis is placed on research that aims to find commonalities between online and offline conflict , and that systematically investigates the dynamics of group behavior . Both work using inductive computational procedures , such as topic modeling , and supervised machine learning approaches are assessed , as are more traditional forms of content analysis , such as dictionaries . Finally , cross - validation is highlighted as a crucial step in CATA , in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research . ","Method":["cata"],"Task":["computer - aided text analysis"]},{"url":"https://www.semanticscholar.org/paper/42c63d952f0cce7f89738e818fc9ab4e723bf7b6","title":"One - to - X Analogical Reasoning on Word Embeddings : a Case for Diachronic Armed Conflict Prediction from News Texts","abstract":"We extend the well - known word analogy task to a one - to - X formulation , including one - to - none cases , when no correct answer exists . The task is cast as a relation discovery problem and applied to historical armed conflicts datasets , attempting to predict new relations of type \u2018location : armed - group\u2019 based on data about past events . As the source of semantic information , we use diachronic word embedding models trained on English news texts . A simple technique to improve diachronic performance in such task is demonstrated , using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora . Finally , we publish a ready - to - use test set for one - to - X analogy evaluation on historical armed conflicts data . ","Method":["one - to - x analogical reasoning"],"Task":["diachronic armed conflict prediction","word analogy task"]},{"url":"https://www.semanticscholar.org/paper/636d4c0b0fe6919abe6eb546907d28ed39bf56e6","title":"Using Natural Language Processing for Automatic Detection of Plagiarism","abstract":"Current plagiarism detection tools are mostly limited to comparisons of suspicious plagiarised texts and potential original texts at string level . In this study the aim is to improve the accuracy of plagiarism detection by incorporating Natural Language Processing (NLP) techniques into existing approaches . We propose a framework for external plagiarism detection in which a number of NLP techniques are applied to process a set of suspicious and original documents , not only to analyse strings but also the structure of the text , using resources to account for text relations . Initial results obtained with a corpus of plagiarised short paragraphs have showed that NLP techniques improve the accuracy of existing approaches . ","Method":["natural language processing"],"Task":["external plagiarism detection","plagiarism detection","automatic detection of plagiarism"]},{"url":"https://www.semanticscholar.org/paper/aea14f23a951975f605a981d003386e46bf8acfe","title":"A Neural Approach to Automated Essay Scoring","abstract":"Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays . The performance of such systems is tightly bound to the quality of the underlying features . However , it is laborious to manually design the most informative features for such a system . In this paper , we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score , without any feature engineering . We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models . The results show that our best system , which is based on long short - term memory networks , outperforms a strong baseline by 5 . 6% in terms of quadratic weighted Kappa , without requiring any feature engineering . ","Method":["long short - term memory networks"],"Task":["automated essay scoring"]},{"url":"https://www.semanticscholar.org/paper/0eb5872733e643f43a0c1a7ff78953dfea74dfea","title":"Automated Scoring : Beyond Natural Language Processing","abstract":"In this position paper , we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy . Automated scoring systems warrant significant cross - discipline collaboration of which natural language processing and machine learning are just two of many important components . Such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other . Our position is that it is essential for us as NLP researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate , fair , unbiased , and useful . ","Method":["automated scoring systems"],"Task":["automated scoring"]},{"url":"https://www.semanticscholar.org/paper/0cdeb238355617a640471987af36d8e09cd905c0","title":"Event Data on Armed Conflict and Security : New Perspectives , Old Challenges , and Some Solutions","abstract":"This article presents the Event Data on Conflict and Security (EDACS) dataset , discusses the inherent problems of georeferenced conflict data , and shows how these challenges are met within EDACS . Based on an event data approach , EDACS contributes to the growing number of novel georeferenced datasets that allow researchers to identify causal pathways of violence and the dynamics of (transboundary) violence through spatiotemporal disaggregation . However , the unreflected use of any of these datasets will give researchers unjustified confidence in their findings , as the pitfalls are many and propagating errors can result in misleading conclusions . To identify and handle the different challenges to overall event data quality , we argue in favor of transparency in the data collection and coding process , to empower analysts to challenge the data and avoid cascading errors . In particular , we investigate how the choice of news sources , the handling of geographic precision , and the use of auxiliary data can bias event data . We demonstrate how the EDACS dataset design enables the analyst to deal with these issues by providing a set of variables indicating the news sources , possible sources of bias , and detailed information on geographic precision . This allows for a flexible use of the data based on individual analytical requirements . ","Method":["edacs"],"Task":["event data on conflict and security"]},{"url":"https://www.semanticscholar.org/paper/2ca0403eabc3893fed255fc119a927d83a1af739","title":"Tracing armed conflicts with diachronic word embedding models","abstract":"Recent studies have shown that word embedding models can be used to trace time - related (diachronic) semantic shifts in particular words . In this paper , we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year - to - year basis , using a dataset from the conflict research field as the gold standard and the Gigaword news corpus as the training data . The results show that much work still remains in extracting \u2018cultural\u2019 semantic shifts from diachronic word embedding models . At the same time , we present a new task complete with an evaluation set and introduce the \u2018anchor words\u2019 method which outperforms previous approaches on this set . ","Method":["\u2018anchor words\u2019 method"],"Task":["tracing armed conflicts"]},{"url":"https://www.semanticscholar.org/paper/4ed75f35ae7343cd906f311390f572a58e36805c","title":"Enriching textbooks through data mining","abstract":"Textbooks play an important role in any educational system . Unfortunately , many textbooks produced in developing countries are not written well and they often lack adequate coverage of important concepts . We propose a technological solution to address this problem based on enriching textbooks with authoritative web content . We augment textbooks at the section level for key concepts discussed in the section . We use ideas from data mining for identifying the concepts that need augmentation as well as to determine the links to the authoritative content that should be used for augmentation . Our evaluation , employing textbooks from India , shows that we are able to enrich textbooks on different subjects and across different grades with high quality augmentations using automated techniques . ","Method":["technological solution"],"Task":["enriching textbooks"]},{"url":"https://www.semanticscholar.org/paper/bc6a4304afdf59783638a6a0d9fc2c2acb6e5b67","title":"Educational Question Answering Motivated by Question - Specific Concept Maps","abstract":"Question answering (QA) is the automated process of answering general questions submitted by humans in natural language . QA has previously been explored within the educational context to facilitate learning , however the majority of works have focused on text - based answering . As an alternative , this paper proposes an approach to return answers as a concept map , which further encourages meaningful learning and knowledge organisation . Additionally , this paper investigates whether adapting the returned concept map to the specific question context provides further learning benefit . A randomised experiment was conducted with a sample of 59 Computer Science undergraduates , obtaining statistically significant results on learning gain when students are provided with the question - specific concept maps . Further , time spent on studying the concept maps were positively correlated with the learning gain . ","Method":["question - specific concept maps"],"Task":["educational question answering","question answering","qa","answering general questions"]},{"url":"https://www.semanticscholar.org/paper/911e61212eb26325d3fe58f454f36f4e70c54c7a","title":"Characterizing Stage - aware Writing Assistance for Collaborative Document Authoring","abstract":"Writing is a complex non - linear process that begins with a mental model of intent , and progresses through an outline of ideas , to words on paper (and their subsequent refinement) . Despite past research in understanding writing , Web - scale consumer and enterprise collaborative digital writing environments are yet to greatly benefit from intelligent systems that understand the stages of document evolution , providing opportune assistance based on authors\' situated actions and context . In this paper , we present three studies that explore temporal stages of document authoring . We first survey information workers at a large technology company about their writing habits and preferences , concluding that writers do in fact conceptually progress through several distinct phases while authoring documents . We also explore , qualitatively , how writing stages are linked to document lifespan . We supplement these qualitative findings with an analysis of the longitudinal user interaction logs of a popular digital writing platform over several million documents . Finally , as a first step towards facilitating an intelligent digital writing assistant , we conduct a preliminary investigation into the utility of user interaction log data for predicting the temporal stage of a document . Our results support the benefit of tools tailored to writing stages , identify primary tasks associated with these stages , and show that it is possible to predict stages from anonymous interaction logs . Together , these results argue for the benefit and feasibility of more tailored digital writing assistance . ","Method":["mental model of intent"],"Task":["stage - aware writing assistance","digital writing assistance","writing","collaborative document authoring writing","intelligent digital writing assistant"]},{"url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre - structured database or a collection of natural language documents . It presents only the requested information instead of searching full documents like search engine . As information in day to day life is increasing , so to retrieve the exact fragment of information even for a simple query requires large and expensive resources . This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques . ","Method":["question answering system"],"Task":["question answering","question answering system"]},{"url":"https://www.semanticscholar.org/paper/f8868be52c8c0d64920e5f67870c01ba48601608","title":"Natural Language Processing and Language Learning","abstract":"As a relatively young field of research and development started by work on cryptanalysis and machine translation around 50 years ago , Natural Language Processing (NLP) is concerned with the automated processing of human language . It addresses the analysis and generation of written and spoken language , though speech processing is often regarded as a separate subfield . NLP emphasizes processing and applications and as such can be seen as the applied side of Computational Linguistics , the interdisciplinary field of research concerned with formal analysis and modeling of language and its applications at the intersection of Linguistics , Computer Science , and Psychology . In terms of the language aspects dealt with in NLP , traditionally lexical , morphological and syntactic aspects of language were at the center of attention , but aspects of meaning , discourse , and the relation to the extra - linguistic context have become increasingly prominent in the last decade . A good introduction and overview of the field is provided in Jurafsky & Martin (2009) . ","Method":["NOT DETECTED"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/311381feeb6346bfcb2ba622bd8f713261a4075d","title":"Modeling the Relationship between User Comments and Edits in Document Revision","abstract":"Management of collaborative documents can be difficult , given the profusion of edits and comments that multiple authors make during a document\u2019s evolution . Reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux . A number of authoring tasks , such as categorizing and summarizing edits , detecting completed to - dos , and visually rearranging comments could benefit from such a contribution . Thus , in this paper we explore the relationship between comments and edits by defining two novel , related tasks : Comment Ranking and Edit Anchoring . We begin by collecting a dataset with more than half a million comment - edit pairs based on Wikipedia revision histories . We then propose a hierarchical multi - layer deep neural - network to model the relationship between edits and comments . Our architecture tackles both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions such as additions and deletions , while also accounting for document context . In a number of evaluation settings , our experimental results show that our approach outperforms several strong baselines significantly . We are able to achieve a precision@1 of 71 . 0% and a precision@3 of 94 . 4% for Comment Ranking , while we achieve 74 . 4% accuracy on Edit Anchoring . ","Method":["hierarchical multi - layer deep neural - network"],"Task":["edit anchoring"]},{"url":"https://www.semanticscholar.org/paper/d114af5d3dcb3792bd9faec1476feba5aaf3617f","title":"A Multimodal Human - Computer Interaction System and Its Application in Smart Learning Environments","abstract":"A multimodal human - computer interaction system is composed of the comprehensive usage of various input and output channels . For the information input , apart from the traditional keyboard typing , mouse clicking , screen touching , the latest speech and face recognition technology can be used . For the output , the traditional screen display , the latest speech and facial expression synthesis and gesture generation can be used . After literature review of related works , this paper at first presents such a system , MMISE (Multimodal Interaction System for Education) , about its architecture and working mechanism , POOOIIM (Pedagogical Objective Oriented Output , Input and Implementation Mechanism) illustrated with practical examples . Then this paper introduces this system\u2019s pilot applications in the epidemic time of novel coronavirus in 2020 . ","Method":["multimodal human - computer interaction system"],"Task":["coronavirus"]},{"url":"https://www.semanticscholar.org/paper/9b0c9d241269b98c80f65a14d5d65263d0688d70","title":"What Makes a Good Counselor? Learning to Distinguish between High - quality and Low - quality Counseling Conversations","abstract":"The quality of a counseling intervention relies highly on the active collaboration between clients and counselors . In this paper , we explore several linguistic aspects of the collaboration process occurring during counseling conversations . Specifically , we address the differences between high - quality and low - quality counseling . Our approach examines participants\u2019 turn - by - turn interaction , their linguistic alignment , the sentiment expressed by speakers during the conversation , as well as the different topics being discussed . Our results suggest important language differences in low - and high - quality counseling , which we further use to derive linguistic features able to capture the differences between the two groups . These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88% . ","Method":["automatic classifiers"],"Task":["counseling intervention"]},{"url":"https://www.semanticscholar.org/paper/aa2bbf5a1485cc90401e4a35c04463fcb4e632f0","title":"Quantifying the Effects of COVID - 19 on Mental Health Support Forums","abstract":"The COVID - 19 pandemic , like many of the disease outbreaks that have preceded it , is likely to have a profound effect on mental health . Understanding its impact can inform strategies for mitigating negative consequences . In this work , we seek to better understand the effects of COVID - 19 on mental health by examining discussions within mental health support communities on Reddit . First , we quantify the rate at which COVID - 19 is discussed in each community , or subreddit , in order to understand levels of preoccupation with the pandemic . Next , we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen . Finally , we analyze how COVID - 19 has influenced language use and topics of discussion within each subreddit . ","Method":["covid - 19"],"Task":["covid - 19"]},{"url":"https://www.semanticscholar.org/paper/81d13af7eb78725492854bb9860b57af2b2a1778","title":"Data Mining and Student e - Learning Profiles","abstract":"Data mining techniques have been applied to educational research in various ways . In this paper , I presented the application of sequential data mining algorithms to analyze computer logs to profile learners in terms of their learning tactic use and motivation in a web - based learning environment (gStudy) . The data mining algorithms are employed to discover patterns which characterize learners either across session or groups based on their study tactic choice and goal orientation . The use of this method is illustrated through a sequential pattern analysis of gStudy log files generated by university students . ","Method":["sequential pattern analysis"],"Task":["student e - learning profiles"]},{"url":"https://www.semanticscholar.org/paper/065332576fd9b264a5a338549239bb39e6733819","title":"Inferring Social Media Users\u2019 Mental Health Status from Multimodal Information","abstract":"Worldwide , an increasing number of people are suffering from mental health disorders such as depression and anxiety . In the United States alone , one in every four adults suffers from a mental health condition , which makes mental health a pressing concern . In this paper , we explore the use of multimodal cues present in social media posts to predict users\u2019 mental health status . Specifically , we focus on identifying social media activity that either indicates a mental health condition or its onset . We collect posts from Flickr and apply a multimodal approach that consists of jointly analyzing language , visual , and metadata cues and their relation to mental health . We conduct several classification experiments aiming to discriminate between (1) healthy users and users affected by a mental health illness; and (2) healthy users and users prone to mental illness . Our experimental results indicate that using multiple modalities can improve the performance of this classification task as compared to the use of one modality at a time , and can provide important cues into a user\u2019s mental status . ","Method":["multimodal approach"],"Task":["classification"]},{"url":"https://www.semanticscholar.org/paper/c278eb333c78313a3762b5b2f07ace7443582b94","title":"Expressive Interviewing : A Conversational System for Coping with COVID - 19","abstract":"The ongoing COVID - 19 pandemic has raised concerns for many regarding personal and public health implications , financial security and economic stability . Alongside many other unprecedented challenges , there are increasing concerns over social isolation and mental health . We introduce \\\\textit{Expressive Interviewing} - - an interview - style conversational system that draws on ideas from motivational interviewing and expressive writing . Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID - 19 has impacted their lives . We present relevant aspects of the system\'s design and implementation as well as quantitative and qualitative analyses of user interactions with the system . In addition , we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID - 19 issues . ","Method":["interviewing}"],"Task":["covid - 19"]},{"url":"https://www.semanticscholar.org/paper/35112c6bfa715b2bc86fc5b6c331f8a1a0b07add","title":"Understanding and Predicting Empathic Behavior in Counseling Therapy","abstract":"Counselor empathy is associated with better outcomes in psychology and behavioral counseling . In this paper , we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters . Particularly , we analyze aspects such as participants\u2019 engagement , participants\u2019 verbal and nonverbal accommodation , as well as topics being discussed during the conversation , with the final goal of identifying linguistic and acoustic markers of counselor empathy . We also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80% . ","Method":["counselor empathy classifiers"],"Task":["counselor empathy"]},{"url":"https://www.semanticscholar.org/paper/c151f144c2c0e8d3b176edaf2ce5369c7707bd31","title":"Large - scale Analysis of Counseling Conversations : An Application of Natural Language Processing to Mental Health","abstract":"Mental illness is one of the most pressing public health issues of our time . While counseling and psychotherapy can be effective treatments , our knowledge about how to conduct successful counseling conversations has been limited due to lack of large - scale data with labeled outcomes of the conversations . In this paper , we present a large - scale , quantitative study on the discourse of text - message - based counseling conversations . We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes . Applying techniques such as sequence - based conversation models , language model comparisons , message clustering , and psycholinguistics - inspired word frequency analyses , we discover actionable conversation strategies that are associated with better conversation outcomes . ","Method":["natural language processing"],"Task":["mental illness"]},{"url":"https://www.semanticscholar.org/paper/9de154d3c886177380062be7c8d50304a335752f","title":"Fermi at SemEval - 2019 Task 6 : Identifying and Categorizing Offensive Language in Social Media using Sentence Embeddings","abstract":"This paper describes our system (Fermi) for Task 6 : OffensEval : Identifying and Categorizing Offensive Language in Social Media of SemEval - 2019 . We participated in all the three sub - tasks within Task 6 . We evaluate multiple sentence embeddings in conjunction with various supervised machine learning algorithms and evaluate the performance of simple yet effective embedding - ML combination algorithms . Our team Fermi\u2019s model achieved an F1 - score of 64 . 40% , 62 . 00% and 62 . 60% for sub - task A , B and C respectively on the official leaderboard . Our model for sub - task C which uses pre - trained ELMo embeddings for transforming the input and uses SVM (RBF kernel) for training , scored third position on the official leaderboard . Through the paper we provide a detailed description of the approach , as well as the results obtained for the task . ","Method":["fermi\u2019s"],"Task":["offenseval"]},{"url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters . Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI) . We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition , transcription , expert data annotations , and reliability assessments . The dataset contains a total of 22 , 719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes . The reliability analysis showed that annotators achieved excellent agreement at session level , with Intraclass Correlation Coefficient (ICC) scores in the range of 0 . 75 to 1 , and fair to good agreement at utterance level , with Cohen\u2019s Kappa scores ranging from 0 . 31 to 0 . 64 . Behavioral interventions are a promising approach to address public health issues such as smoking cessation , increasing physical activity , and reducing substance abuse , among others (Resnicow et al . , 2002) . In particular , Motivational Interviewing (MI) , a client centered psychotherapy style , has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al . , 2009; Apodaca et al . , 2014; Barnett et al . , 2014; Catley et al . , 2012) . Despite its potential benefits in combating addiction and in providing broader disease prevention and management , implementing MI counseling at larger scale or in other domains is limited by the need for human - based evaluations . Currently , this requires a human either watching or listening to video - tapes and then providing evaluative feedback . Recently , computational approaches have been proposed to aid the MI evaluation process (Atkins et al . , 2014; Xiao et al . , 2014; Klonek et al . , 2015) . However , learning resources for this task are not readily available . Having such resources will enable the application of data - driven strategies for the automatic coding of counseling behaviors , thus providing researchers with automatic means for the evaluation of MI . Moreover , this can also be useful to explore how MI works by relating MI behaviors to health outcomes , and to provide counselors with evaluative feedback that helps them improve their MI skills . In this paper , we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4 . 0 (MITI) , which is the current gold standard for MI - based psychology interventions . The dataset is derived from 277 MI sessions containing a total of 22 , 719 coded utterances . 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative , goal - oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick , 2013) . MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol , tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Method":["mi","motivational interviewing","motivational interviewing treatment integrity 4 . 0"],"Task":["mi"]},{"url":"https://www.semanticscholar.org/paper/55925bc2522a1d4cb007ac78273f7c2efafa3916","title":"Predicting Counselor Behaviors in Motivational Interviewing Encounters","abstract":"As the number of people receiving psycho - therapeutic treatment increases , the automatic evaluation of counseling practice arises as an important challenge in the clinical domain . In this paper , we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients . In particular , we present a model towards the automation of Motivational Interviewing (MI) coding , which is the current gold standard to evaluate MI counseling . First , we build a dataset of hand labeled MI encounters; second , we use text - based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third , we develop an automatic system to predict these behaviors . We introduce a new set of features based on semantic information and syntactic patterns , and show that they lead to accuracy figures of up to 90% , which represent a significant improvement with respect to features used in the past . ","Method":["automatic system"],"Task":["mi counseling"]},{"url":"https://www.semanticscholar.org/paper/c2fd13dab7a581f2f875826b204d063f42b63a8b","title":"Happiness Entailment : Automating Suggestions for Well - Being","abstract":"Understanding what makes people happy is a central topic in psychology . Prior work has mostly focused on developing self - reporting assessment tools for individuals and relies on experts to analyze the periodic reported assessments . One of the goals of the analysis is to understand what actions are necessary to encourage modifications in the behaviors of the individuals to improve their overall well - being . In this paper , we outline a complementary approach; on the assumption that the user journals her happy moments as short texts , a system can analyze these texts and propose sustainable suggestions for the user that may lead to an overall improvement in her well - being . We prototype one necessary component of such a system , the Happiness Entailment Recognition (HER)module , which takes as input a short text describing an event , a candidate suggestion , and outputs a determination about whether the suggestion is more likely to be good for this user based on the event described . This component is implemented as a neural network model with two encoders , one for the user input and one for the candidate actionable suggestion , with additional layers to capture psychologically significant features in the happy moment and suggestion . Our model achieves an AU - ROC of 0 . 831 and outperforms our baseline as well as the current state - of - the - art Textual Entailment model from AllenNLP by more than 48% of improvements , confirming the uniqueness and complexity of the HER task . ","Method":["happiness entailment recognition"],"Task":["happiness entailment"]},{"url":"https://www.semanticscholar.org/paper/64c68fa52491c4a815f21917e068e4c19dd404b0","title":"FERMI at SemEval - 2019 Task 5 : Using Sentence embeddings to Identify Hate Speech Against Immigrants and Women in Twitter","abstract":"This paper describes our system (Fermi) for Task 5 of SemEval - 2019 : HatEval : Multilingual Detection of Hate Speech Against Immigrants and Women on Twitter . We participated in the subtask A for English and ranked first in the evaluation on the test set . We evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding - ML combination algorithms . Our team - Fermi\u2019s model achieved an accuracy of 65 . 00% for English language in task A . Our models , which use pretrained Universal Encoder sentence embeddings for transforming the input and SVM (with RBF kernel) for classification , scored first position (among 68) in the leaderboard on the test set for Subtask A in English language . In this paper we provide a detailed description of the approach , as well as the results obtained in the task . ","Method":["team - fermi\u2019s model"],"Task":["hateval"]},{"url":"https://www.semanticscholar.org/paper/7cb5bc9f8bc023f1f2c94e2e83012f1d35e4e01d","title":"Ingredients for Happiness : Modeling constructs via semi - supervised content driven inductive transfer","abstract":"Modeling affect via understanding the social constructs behind them is an important task in devising robust and accurate systems for socially relevant scenarios . In the CL - Aff Shared Task (part of Affective Content Analysis workshop @ AAAI 2019) , the organizers released a dataset of \u2018happy\u2019 moments , called the HappyDB corpus . The task is to detect two social constructs : the agency (i . e . , whether the author is in control of the happy moment) and the social characteristics (i . e . , whether anyone else other than the author was also involved in the happy moment) . We employ an inductive transfer learning technique where we utilize a pre - trained language model and fine - tune it on the target task for both the binary classification tasks . At first , we use a language model pre - trained on the huge WikiText - 103 corpus . This step utilizes an AWDLSTM with three hidden layers for training the language model . In the second step , we fine - tune the pre - trained language model on both the labeled and unlabeled instances from the HappyDB dataset . Finally , we train a classifier on top of the language model for each of the identification tasks . Our experiments using 10 - fold cross validation on the corpus show that we achieve a high accuracy of \u223c93% for detection of the social characteristic and \u223c87% for agency of the author , showing significant gains over other baselines . We also show that using the unlabeled dataset for fine - tuning the language model in the second step improves our accuracy by 1 - 2% across detection of both the constructs . ","Method":["awdlstm"],"Task":["detection","affective content analysis"]},{"url":"https://www.semanticscholar.org/paper/c239aaa2a87ea63fae666b2051f1923afaa45fc2","title":"HappyDB : A Corpus of 100 , 000 Crowdsourced Happy Moments","abstract":"The science of happiness is an area of positive psychology concerned with understanding what behaviors make people happy in a sustainable fashion . Recently , there has been interest in developing technologies that help incorporate the findings of the science of happiness into users\' daily lives by steering them towards behaviors that increase happiness . With the goal of building technology that can understand how people express their happy moments in text , we crowd - sourced HappyDB , a corpus of 100 , 000 happy moments that we make publicly available . This paper describes HappyDB and its properties , and outlines several important NLP problems that can be studied with the help of the corpus . We also apply several state - of - the - art analysis techniques to analyze HappyDB . Our results demonstrate the need for deeper NLP techniques to be developed which makes HappyDB an exciting resource for follow - on research . ","Method":["analysis techniques"],"Task":["happiness"]},{"url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","title":"HopeEDI : A Multilingual Hope Speech Detection Dataset for Equality , Diversity , and Inclusion","abstract":"Over the past few years , systems have been developed to control online content and eliminate abusive , offensive or hate speech content . However , people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech . Therefore , it is imperative that research should take a positive reinforcement approach towards online content that is encouraging , positive and supportive contents . Until now , most studies have focused on solving this problem of negativity in the English language , though the problem is much more than just harmful content . Furthermore , it is multilingual as well . Thus , we have constructed a Hope Speech dataset for Equality , Diversity and Inclusion (HopeEDI) containing user - generated comments from the social media platform YouTube with 28 , 451 , 20 , 198 and 10 , 705 comments in English , Tamil and Malayalam , respectively , manually labelled as containing hope speech or not . To our knowledge , this is the first research of its kind to annotate hope speech for equality , diversity and inclusion in a multilingual setting . We determined that the inter - annotator agreement of our dataset using Krippendorff\u2019s alpha . Further , we created several baselines to benchmark the resulting dataset and the results have been expressed using precision , recall and F1 - score . The dataset is publicly available for the research community . We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness . ","Method":["hopeedi"],"Task":["inclusion"]},{"url":"https://www.semanticscholar.org/paper/32bf7ad3fdda71036b48f7dc85cad407674277b7","title":"Women worry about family , men about the economy : Gender differences in emotional responses to COVID - 19","abstract":"Among the critical challenges around the COVID - 19 pandemic is dealing with the potentially detrimental effects on people\'s mental health . Designing appropriate interventions and identifying the concerns of those most at risk requires methods that can extract worries , concerns and emotional responses from text data . We examine gender differences and the effect of document length on worries about the ongoing COVID - 19 situation . Our findings suggest that i) short texts do not offer as adequate insights into psychological processes as longer texts . We further find ii) marked gender differences in topics concerning emotional responses . Women worried more about their loved ones and severe health concerns while men were more occupied with effects on the economy and society . This paper adds to the understanding of general gender differences in language found elsewhere , and shows that the current unique circumstances likely amplified these effects . We close this paper with a call for more high - quality datasets due to the limitations of Tweet - sized data . ","Method":["NOT DETECTED"],"Task":["covid - 19 pandemic"]},{"url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent , its causes , and the necessary responses\u2014is intense and of global importance . Yet , in the natural language processing (NLP) community , this domain has so far received little attention . In contrast , it is of enormous prominence in various social science disciplines , and some of that work follows the \u201dtext - as - data\u201d paradigm , seeking to employ quantitative methods for analyzing large amounts of CC - related text . Other research is qualitative in nature and studies details , nuances , actors , and motivations within CC discourses . Coming from both NLP and Political Science , and reviewing key works in both disciplines , we discuss how social science approaches to CC debates can inform advances in text - mining/NLP , and how , in return , NLP can support policy - makers and activists in making sense of large - scale and complex CC discourses across multiple genres , channels , topics , and communities . This is paramount for their ability to make rapid and meaningful impact on the discourse , and for shaping the necessary policy change . ","Method":["nlp"],"Task":["cc debates"]},{"url":"https://www.semanticscholar.org/paper/a9d09338a3a0d9102c1e623e6ad434a446c0bbfe","title":"Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism","abstract":"Neutralisation techniques , e . g . denial of responsibility and denial of victim , are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view . We first draw on social science to introduce the problem to the community of nlp , present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change , and experiment with supervised and semi - supervised BERT - based models . ","Method":["neutralisation techniques"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/086e6733e5fda70bbce0c7545bd06d5634918a60","title":"CLIMATE - FEVER : A Dataset for Verification of Real - World Climate Claims","abstract":"We introduce CLIMATE - FEVER , a new publicly available dataset for verification of climate change - related claims . By providing a dataset for the research community , we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate - specific claims , addressing the underlying language understanding challenges , and ultimately help alleviate the impact of misinformation on climate change . We adapt the methodology of FEVER [1] , the largest dataset of artificially designed claims , to real - life claims collected from the Internet . While during this process , we could rely on the expertise of renowned climate scientists , it turned out to be no easy task . We discuss the surprising , subtle complexity of modeling real - world climate - related claims within the \\\\textsc{fever} framework , which we believe provides a valuable challenge for general natural language understanding . We hope that our work will mark the beginning of a new exciting long - term joint effort by the climate science and AI community . ","Method":["\\\\textsc{fever} framework"],"Task":["climate science and ai community"]},{"url":"https://www.semanticscholar.org/paper/52ef2bea83eba1575fcfd02c82b4228f7aab0bd6","title":"Cheap Talk and Cherry - Picking : What ClimateBert has to say on Corporate Climate Risk Disclosures","abstract":"Disclosure of climate - related financial risks greatly helps investors assess companies\' preparedness for climate change . Voluntary disclosures such as those based on the recommendations of the Task Force for Climate - related Financial Disclosures (TCFD) are being hailed as an effective measure for better climate risk management . We ask whether this expectation is justified . We do so with the help of a deep neural language model , which we christen ClimateBert . We train ClimateBert on thousands of sentences related to climate - risk disclosures aligned with the TCFD recommendations . In analyzing the disclosures of TCFD - supporting firms , ClimateBert comes to the sobering conclusion that the firms\' TCFD support is mostly cheap talk and that firms cherry - pick to report primarily non - material climate risk information . From our analysis , we conclude that the only way out of this dilemma is to turn voluntary reporting into regulatory disclosures . ","Method":["climatebert"],"Task":["disclosure of climate - related financial risks","corporate climate risk disclosures"]},{"url":"https://www.semanticscholar.org/paper/998039a4876edc440e0cabb0bc42239b0eb29644","title":"Tackling Climate Change with Machine Learning","abstract":"Climate change is one of the greatest challenges facing humanity , and we , as machine learning experts , may wonder how we can help . Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate . From smart grids to disaster management , we identify high impact problems where existing gaps can be filled by machine learning , in collaboration with other fields . Our recommendations encompass exciting research questions as well as promising business opportunities . We call on the machine learning community to join the global effort against climate change . ","Method":["machine learning"],"Task":["climate change"]},{"url":"https://www.semanticscholar.org/paper/f071e7c19fc18fc4d4ab69ce13f0c4e92cfb08c4","title":"Learning Twitter User Sentiments on Climate Change with Limited Labeled Data","abstract":"While it is well - documented that climate change accepters and deniers have become increasingly polarized in the United States over time , there has been no large - scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences . On the sub - population of Twitter users , we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the U . S . in 2018 . We begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic - level results in line with prior research . We then apply RNNs to conduct a cohort - level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change . However , this effect does not hold for the 2018 blizzard and wildfires studied , implying that Twitter users\' opinions on climate change are fairly ingrained on this subset of natural disasters . ","Method":["rnns"],"Task":["learning twitter user sentiments"]},{"url":"https://www.semanticscholar.org/paper/bbc6486ad37365b77b9d0de8894f595d70af49ac","title":"Ask BERT : How Regulatory Disclosure of Transition and Physical Climate Risks affects the CDS Term Structure","abstract":"We use BERT , an AI - based algorithm for language understanding , to decipher regulatory climate - risk disclosures and measure their impact on the credit default swap (CDS) market . Risk disclosures can either increase or decrease credit spreads , depending on whether disclosure reveals new risks or sharpens the signal and decreases the uncertainty . Training BERT to differentiate between transition and physical climate risks , we find that disclosing transition risks increases CDS spreads , especially after the Paris Climate Agreement of 2015 , while disclosing physical climate risks leads to a decrease in CDS spreads . These impacts are statistically and economically highly significant . ","Method":["bert"],"Task":["credit default swap"]},{"url":"https://www.semanticscholar.org/paper/9b1653e3b57016958d10ff8531475eb0483d156c","title":"Social Privacy in Networked Publics : Teens\u2019 Attitudes , Practices , and Strategies","abstract":"This paper examines how teens understand privacy in highly public networked environments like Facebook and Twitter . We describe both teens\u2019 practices , their privacy strategies , and the structural conditions in which they are embedded , highlighting the ways in which privacy , as it plays out in everyday life , is related more to agency and the ability to control a social situation than particular properties of information . Finally , we discuss the implications of teens\u2019 practices and strategies , revealing the importance of social norms as a regulatory force . (This paper was presented at Oxford Internet Institute\u2019s \u201cA Decade in Internet Time : Symposium on the Dynamics of the Internet and Society\u201d on September 22 , 2011 . )","Method":["privacy strategies"],"Task":["social privacy"]},{"url":"https://www.semanticscholar.org/paper/1d7a0e8be2bbe7d148bb48a1f9de218beaea4ed5","title":"DeSMOG : Detecting Stance in Media On Global Warming","abstract":"Citing opinions is a powerful yet understudied strategy in argumentation . For example , an environmental activist might say , \u201cLeading scientists agree that global warming is a serious concern , \u201d framing a clause which affirms their own stance (\u201cthat global warming is serious\u201d) as an opinion endorsed (\\"[scientists] agree\u201d) by a reputable source (\u201cleading\u201d) . In contrast , a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt : \u201cMistaken scientists claim [ . . . ] . \\" Our work studies opinion - framing in the global warming (GW) debate , an increasingly partisan issue that has received little attention in NLP . We introduce DeSMOG , a dataset of stance - labeled GW sentences , and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other\u2019s opinions . From 56K news articles , we find that similar linguistic devices for self - affirming and opponent - doubting discourse are used across GW - accepting and skeptic media , though GW - skeptical media shows more opponent - doubt . We also find that authors often characterize sources as hypocritical , by ascribing opinions expressing the author\u2019s own view to source entities known to publicly endorse the opposing view . We release our stance dataset , model , and lexicons of framing devices for future work on opinion - framing and the automatic detection of GW stance . ","Method":["desmog"],"Task":["global warming"]},{"url":"https://www.semanticscholar.org/paper/418b9f37b07a05b38798e97f57e17a6cc1048b92","title":"You are right . I am ALARMED - But by Climate Change Counter Movement","abstract":"The world is facing the challenge of climate crisis . Despite the consensus in scientific community about anthropogenic global warming , the web is flooded with articles spreading climate misinformation . These articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change . We revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of NLP . Despite considerable work in detection of fake news , there is no misinformation dataset available that is specific to the domain . of climate change . We try to bridge this gap by scraping and releasing articles with known climate change misinformation . ","Method":["NOT DETECTED"],"Task":["detection of fake news"]},{"url":"https://www.semanticscholar.org/paper/3475876c3f5e624587181744bc5acf4223aecd00","title":"Analyzing Polarization in Social Media : Method and Application to Tweets on 21 Mass Shootings","abstract":"We provide an NLP framework to uncover four linguistic dimensions of political polarization in social media : topic choice , framing , affect and illocutionary force . We quantify these aspects with existing lexical methods , and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional LDA - based models . We apply our methods to study 4 . 4M tweets on 21 mass shootings . We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice . We identify framing devices , such as grounding and the contrasting use of the terms \u201cterrorist\u201d and \u201ccrazy\u201d , that contribute to polarization . Results pertaining to topic choice , affect and illocutionary force suggest that Republicans focus more on the shooter and event - specific facts (news) while Democrats focus more on the victims and call for policy changes . Our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them . ","Method":["nlp framework"],"Task":["framing"]},{"url":"https://www.semanticscholar.org/paper/134c8486ae58b421681656c85bbc48dc862f6f98","title":"ClimaText : A Dataset for Climate Change Topic Detection","abstract":"Climate change communication in the mass media and other textual sources may affect and shape public perception . Extracting climate change information from these sources is an important task , e . g . , for filtering content and e - discovery , sentiment analysis , automatic summarization , question - answering , and fact - checking . However , automating this process is a challenge , as climate change is a complex , fast - moving , and often ambiguous topic with scarce resources for popular text - based AI tasks . In this paper , we introduce \\\\textsc{ClimaText} , a dataset for sentence - based climate change topic detection , which we make publicly available . We explore different approaches to identify the climate change topic in various text sources . We find that popular keyword - based models are not adequate for such a complex and evolving task . Context - based algorithms like BERT \\\\cite{devlin2018bert} can detect , in addition to many trivial cases , a variety of complex and implicit topic patterns . Nevertheless , our analysis reveals a great potential for improvement in several directions , such as , e . g . , capturing the discussion on indirect effects of climate change . Hence , we hope this work can serve as a good starting point for further research on this topic . ","Method":["\\\\textsc{climatext}"],"Task":["climate change topic detection","sentence - based climate change topic detection"]},{"url":"https://www.semanticscholar.org/paper/171847842d161743f2b52dc4f85c121087ceb426","title":"Comparing Attitudes to Climate Change in the Media using sentiment analysis based on Latent Dirichlet Allocation","abstract":"News media typically present biased accounts of news stories , and different publications present different angles on the same event . In this research , we investigate how different publications differ in their approach to stories about climate change , by examining the sentiment and topics presented . To understand these attitudes , we find sentiment targets by combining Latent Dirichlet Allocation (LDA) with SentiWordNet , a general sentiment lexicon . Using LDA , we generate topics containing keywords which represent the sentiment targets , and then annotate the data using SentiWordNet before regrouping the articles based on topic similarity . Preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources . Ongoing work is investigating how systematic these attitudes are between different publications , and how these may change over time . ","Method":["sentiwordnet"],"Task":["sentiment analysis"]},{"url":"https://www.semanticscholar.org/paper/9c2cab0e72f2588e857671c1fbb13b5fba8f2586","title":"Cross - Platform Disinformation Campaigns : Lessons Learned and Next Steps","abstract":"We conducted a mixed - method , interpretative analysis of an online , cross - platform disinformation campaign targeting the White Helmets , a rescue group operating in rebel - held areas of Syria that has become the subject of a persistent effort of delegitimization . This research helps to conceptualize what a disinformation campaign is and how it works . Based on what we learned from this case study , we conclude that a comprehensive understanding of disinformation requires accounting for the spread of content across platforms and that social media platforms should increase collaboration to detect and characterize disinformation campaigns . ","Method":["mixed - method"],"Task":["disinformation"]},{"url":"https://www.semanticscholar.org/paper/7de1c0db3b3848d1a51dbfefce8b73e89a682177","title":"Classification of Moral Foundations in Microblog Political Discourse","abstract":"Previous works in computer science , as well as political and social science , have shown correlation in text between political ideologies and the moral foundations expressed within that text . Additional work has shown that policy frames , which are used by politicians to bias the public towards their stance on an issue , are also correlated with political ideology . Based on these associations , this work takes a first step towards modeling both the language and how politicians frame issues on Twitter , in order to predict the moral foundations that are used by politicians to express their stances on issues . The contributions of this work includes a dataset annotated for the moral foundations , annotation guidelines , and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans , as opposed to the unigrams of previous works , with policy frames for the prediction of the morality underlying political tweets . ","Method":["policy frames"],"Task":["microblog political discourse"]},{"url":"https://www.semanticscholar.org/paper/29584ed6d68a06fdf91440a018f6bc83a44fd177","title":"Paragraph - level Rationale Extraction through Regularization : A case study on European Court of Human Rights Cases","abstract":"Interpretability or explainability is an emerging research field in NLP . From a user - centric point of view , the goal is to build models that provide proper justification for their decisions , similar to those of humans , by requiring the models to satisfy additional constraints . To this end , we introduce a new application on legal text where , contrary to mainstream literature targeting word - level rationales , we conceive rationales as selected paragraphs in multi - paragraph structured court cases . We also release a new dataset comprising European Court of Human Rights cases , including annotations for paragraph - level rationales . We use this dataset to study the effect of already proposed rationale constraints , i . e . , sparsity , continuity , and comprehensiveness , formulated as regularizers . Our findings indicate that some of these constraints are not beneficial in paragraph - level rationale extraction , while others need re - formulation to better handle the multi - label nature of the task we consider . We also introduce a new constraint , singularity , which further improves the quality of rationales , even compared with noisy rationale supervision . Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research . ","Method":["regularization"],"Task":["paragraph - level rationale extraction"]},{"url":"https://www.semanticscholar.org/paper/08e3ce4454012d928470efa96a54aa08dd8a96b0","title":"Automatically Characterizing Targeted Information Operations Through Biases Present in Discourse on Twitter","abstract":"This paper considers the problem of automatically characterizing biases that may be associated with emerging information operations via artificial intelligence . Accurate analysis of these emerging topics usually requires laborious , manual analysis by experts to annotate millions of tweets to identify biases in new topics . We introduce adaptations of the Word Embedding Association Test [1] to a new domain : information operations . We validate our method using known information operation - related tweets from Twitter\'s Transparency Reports , and we perform a case study on the COVID - 19 pandemic to evaluate our method\'s performance on non - labeled Twitter data , demonstrating its usability in emerging domains . ","Method":["word embedding association test [1]"],"Task":["accurate analysis"]},{"url":"https://www.semanticscholar.org/paper/2547836827e0423f198320977d393f574e0fb3d6","title":"Framing and Agenda - setting in Russian News : a Computational Analysis of Intricate Political Strategies","abstract":"Amidst growing concern over media manipulation , NLP attention has focused on overt strategies like censorship and \u201cfake news\u201d . Here , we draw on two concepts from political science literature to explore subtler strategies for government media manipulation : agenda - setting (selecting what topics to cover) and framing (deciding how topics are covered) . We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction : articles mention the U . S . more frequently in the month directly following an economic downturn in Russia . We introduce embedding - based methods for cross - lingually projecting English frames to Russian , and discover that these articles emphasize U . S . moral failings and threats to the U . S . Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda - setting and framing . ","Method":["embedding - based methods"],"Task":["agenda - setting"]},{"url":"https://www.semanticscholar.org/paper/546c87abc26137359c43898559884ba9d6c5ae64","title":"Predicting the Role of Political Trolls in Social Media","abstract":"We investigate the political roles of \u201cInternet trolls\u201d in social media . Political trolls , such as the ones linked to the Russian Internet Research Agency (IRA) , have recently gained enormous attention for their ability to sway public opinion and even influence elections . Analysis of the online traces of trolls has shown different behavioral patterns , which target different slices of the population . However , this analysis is manual and labor - intensive , thus making it impractical as a first - response tool for newly - discovered troll farms . In this paper , we show how to automate this analysis by using machine learning in a realistic setting . In particular , we show how to classify trolls according to their political role \u2014left , news feed , right\u2014 by using features extracted from social media , i . e . , Twitter , in two scenarios : (i) in a traditional supervised learning scenario , where labels for trolls are available , and (ii) in a distant supervision scenario , where labels for trolls are not available , and we rely on more - commonly - available labels for news outlets mentioned by the trolls . Technically , we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph , from which we extract several types of learned representations , i . e . , embeddings , for the trolls . Experiments on the \u201cIRA Russian Troll\u201d dataset show that our methodology improves over the state - of - the - art in the first scenario , while providing a compelling case for the second scenario , which has not been explored in the literature thus far . ","Method":["machine learning"],"Task":["political trolls"]},{"url":"https://www.semanticscholar.org/paper/c0a2ee56fc80d971e920fba6179f81b515533592","title":"Historical Change in the Moral Foundations of Political Persuasion","abstract":"How have attempts at political persuasion changed over time? Using nine corpora dating back through 1789 , containing over 7 million words of speech (1 , 666 documents in total) , covering three different countries , plus the entire Google nGram corpus , we find that language relating to togetherness permanently crowded out language relating to duties and obligations in the persuasive speeches of politicians during the early 20th century . This shift is temporally predicted by a rise in Western nationalism and the mass movement of people from more rural to more urban areas and is unexplained by changes in language , private political speech , or nonmoral persuasion . We theorize that the emergence of the modern state in the 1920s had psychopolitical consequences for the ways that people understood and communicated their relationships with their government , which was then reflected in the levers of persuasion chosen by political elites . ","Method":["NOT DETECTED"],"Task":["political persuasion"]},{"url":"https://www.semanticscholar.org/paper/d89e89eac7882790b48ad73376742e7a93fea4fd","title":"Red Bots Do It Better : Comparative Analysis of Social Bot Partisan Behavior","abstract":"Recent research brought awareness of the issue of bots on social media and the significant risks of mass manipulation of public opinion in the context of political discussion . In this work , we leverage Twitter to study the discourse during the 2018 US midterm elections and analyze social bot activity and interactions with humans . We collected 2 . 6 million tweets for 42 days around the election day from nearly 1 million users . We use the collected tweets to answer three research questions : (i) Do social bots lean and behave according to a political ideology? (ii) Can we observe different strategies among liberal and conservative bots? (iii) How effective are bot strategies in engaging humans? We show that social bots can be accurately classified according to their political leaning and behave accordingly . Conservative bots share most of the topics of discussion with their human counterparts , while liberal bots show less overlap and a more inflammatory attitude . We studied bot interactions with humans and observed different strategies . Finally , we measured bots embeddedness in the social network and the extent of human engagement with each group of bots . Results show that conservative bots are more deeply embedded in the social network and more effective than liberal bots at exerting influence on humans . ","Method":["red bots"],"Task":["comparative analysis of social bot partisan behavior"]},{"url":"https://www.semanticscholar.org/paper/a12d22ff91ce159a0d3558ed5aaed115115beabd","title":"Fine - Grained Analysis of Propaganda in News Article","abstract":"Propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda . Previous work has addressed propaganda detection at document level , typically labelling all articles from a propagandistic news outlet as propaganda . Such noisy gold labels inevitably affect the quality of any learning system trained on them . A further issue with most existing systems is the lack of explainability . To overcome these limitations , we propose a novel task : performing fine - grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type . In particular , we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure . We further design a novel multi - granularity neural network , and we show that it outperforms several strong BERT - based baselines . ","Method":["multi - granularity neural network"],"Task":["fine - grained analysis of propaganda","propaganda detection"]},{"url":"https://www.semanticscholar.org/paper/9e82aba279807a6c8541c8811f049a40ebca36c1","title":"Issue Framing in Online Discussion Fora","abstract":"In online discussion fora , speakers often make arguments for or against something , say birth control , by highlighting certain aspects of the topic . In social science , this is referred to as issue framing . In this paper , we introduce a new issue frame annotated corpus of online discussions . We explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora , using a combination of multi - task and adversarial training , assuming only unlabeled training data in the target domain . ","Method":["multi - task"],"Task":["issue framing"]},{"url":"https://www.semanticscholar.org/paper/c0472b63d7d45d948dea42c4b5efffb6859541f9","title":"Technology , Autonomy , and Manipulation","abstract":"Since 2016 , when the Facebook/Cambridge Analytica scandal began to emerge , public concern has grown around the threat of \u201conline manipulation\u201d . While these worries are familiar to privacy researchers , this paper aims to make them more salient to policymakers \u2014 first , by defining \u201conline manipulation\u201d , thus enabling identification of manipulative practices; and second , by drawing attention to the specific harms online manipulation threatens . We argue that online manipulation is the use of information technology to covertly influence another person\u2019s decision - making , by targeting and exploiting their decision - making vulnerabilities . Engaging in such practices can harm individuals by diminishing their economic interests , but its deeper , more insidious harm is its challenge to individual autonomy . We explore this autonomy harm , emphasising its implications for both individuals and society , and we briefly outline some strategies for combating online manipulation and strengthening autonomy in an increasingly digital world . ","Method":["information technology"],"Task":["manipulation\u201d"]},{"url":"https://www.semanticscholar.org/paper/20a37289d0b7c90065850eb7bf96d5ac3c8a21e4","title":"Modeling Frames in Argumentation","abstract":"In argumentation , framing is used to emphasize a specific aspect of a controversial topic while concealing others . When talking about legalizing drugs , for instance , its economical aspect may be emphasized . In general , we call a set of arguments that focus on the same aspect a frame . An argumentative text has to serve the \u201cright\u201d frame(s) to convince the audience to adopt the author\u2019s stance (e . g . , being pro or con legalizing drugs) . More specifically , an author has to choose frames that fit the audience\u2019s cultural background and interests . This paper introduces frame identification , which is the task of splitting a set of arguments into non - overlapping frames . We present a fully unsupervised approach to this task , which first removes topical information and then identifies frames using clustering . For evaluation purposes , we provide a corpus with 12 , 326 debate - portal arguments , organized along the frames of the debates\u2019 topics . On this corpus , our approach outperforms different strong baselines , achieving an F1 - score of 0 . 28 . ","Method":["clustering"],"Task":["frame identification"]},{"url":"https://www.semanticscholar.org/paper/16981cc4ddefd3ea7655754fd83a2a8ff2203a8b","title":"Automatically Neutralizing Subjective Bias in Text","abstract":"Texts like news , encyclopedias , and some social media strive for objectivity . Yet bias in the form of inappropriate subjectivity - introducing attitudes via framing , presupposing truth , and casting doubt - remains ubiquitous . This kind of bias erodes our collective trust and fuels social conflict . To address this issue , we introduce a novel testbed for natural language generation : automatically bringing inappropriately subjective text into a neutral point of view (\\"neutralizing\\" biased text) . We also offer the first parallel corpus of biased language . The corpus contains 180 , 000 sentence pairs and originates from Wikipedia edits that removed various framings , presuppositions , and attitudes from biased sentences . Last , we propose two strong encoder - decoder baselines for the task . A straightforward yet opaque CONCURRENT system uses a BERT encoder to identify subjective words as part of the generation process . An interpretable and controllable MODULAR algorithm separates these steps , using (1) a BERT - based classifier to identify problematic words and (2) a novel join embedding through which the classifier can edit the hidden states of the encoder . Large - scale human evaluation across four domains (encyclopedias , news headlines , books , and political speeches) suggests that these algorithms are a first step towards the automatic identification and reduction of bias . ","Method":["join embedding"],"Task":["natural language generation"]},{"url":"https://www.semanticscholar.org/paper/a0a5669b310ceb3840a599bf4f27a077a643d613","title":"A Systematic Media Frame Analysis of 1 . 5 Million New York Times Articles from 2000 to 2017","abstract":"Framing is an indispensable narrative device for news media because even the same facts may lead to conflicting understandings if deliberate framing is employed . Therefore , identifying media framing is a crucial step to understanding how news media influence the public . Framing is , however , difficult to operationalize and detect , and thus traditional media framing studies had to rely on manual annotation , which is challenging to scale up to massive news datasets . Here , by developing a media frame classifier that achieves state - of - the - art performance , we systematically analyze the media frames of 1 . 5 million New York Times articles published from 2000 to 2017 . By examining the ebb and flow of media frames over almost two decades , we show that short - term frame abundance fluctuation closely corresponds to major events , while there also exist several long - term trends , such as the gradually increasing prevalence of the \u201cCultural identity\u201d frame . By examining specific topics and sentiments , we identify characteristics and dynamics of each frame . Finally , as a case study , we delve into the framing of mass shootings , revealing three major framing patterns . Our scalable , computational approach to massive news datasets opens up new pathways for systematic media framing studies . ","Method":["deliberate framing"],"Task":["framing","media framing studies"]},{"url":"https://www.semanticscholar.org/paper/245bcbdfff19721c226059a5d5b1ae71b67e0572","title":"FrameAxis : Characterizing Framing Bias and Intensity with Word Embedding","abstract":"We propose FrameAxis , a method of characterizing the framing of a given text by identifying the most relevant semantic axes (\\"microframes\\") defined by antonym word pairs . In contrast to the traditional framing analysis , which has been constrained by a small number of manually annotated general frames , our unsupervised approach provides much more detailed insights , by considering a host of semantic axes . Our method is capable of quantitatively teasing out framing bias - - how biased a text is in each microframe - - and framing intensity - - how much each microframe is used - - from the text , offering a nuanced characterization of framing . We evaluate our approach using SemEval datasets as well as three other datasets and human evaluations , demonstrating that FrameAxis can reliably characterize documents with relevant microframes . Our method may allow scalable and nuanced computational analyses of framing across disciplines . ","Method":["frameaxis"],"Task":["framing"]},{"url":"https://www.semanticscholar.org/paper/2cd74561109e2675634e0f4f0129fcfbc238a1d8","title":"Connotation Frames of Power and Agency in Modern Films","abstract":"The framing of an action influences how we perceive its actor . We introduce connotation frames of power and agency , a pragmatic formalism organized using frame semantic representations , to model how different levels of power and agency are implicitly projected on actors through their actions . We use the new power and agency frames to measure the subtle , but prevalent , gender bias in the portrayal of modern film characters and provide insights that deviate from the well - known Bechdel test . Our contributions include an extended lexicon of connotation frames along with a web interface that provides a comprehensive analysis through the lens of connotation frames . ","Method":["connotation frames"],"Task":["NOT DETECTED"]},{"url":"https://www.semanticscholar.org/paper/15130cdd46ff7e3a69384e6c62ab1775c4aba96f","title":"Analyzing Framing through the Casts of Characters in the News","abstract":"We present an unsupervised model for the discovery and clustering of latent \u201cpersonas\u201d (characterizations of entities) . Our model simultaneously clusters documents featuring similar collections of personas . We evaluate this model on a collection of news articles about immigration , showing that personas help predict the coarse - grained framing annotations in the Media Frames Corpus . We also introduce automated model selection as a fair and robust form of feature evaluation . ","Method":["automated model selection"],"Task":["analyzing framing"]},{"url":"https://www.semanticscholar.org/paper/92408cc19033cc4af29accef3793014ab79355c2","title":"The Media Frames Corpus : Annotations of Frames Across Issues","abstract":"We describe the first version of the Media Frames Corpus : several thousand news articles on three policy issues , annotated in terms of media framing . We motivate framing as a phenomenon of study for computational linguistics and describe our annotation process . ","Method":["annotation process"],"Task":["framing"]},{"url":"https://www.semanticscholar.org/paper/c899c6048a1bd99590972c5fb2f9fd00db8d4e18","title":"Who Falls for Online Political Manipulation?","abstract":"Social media , once hailed as a vehicle for democratization and the promotion of positive social change across the globe , are under attack for becoming a tool of political manipulation and spread of disinformation . A case in point is the alleged use of trolls by Russia to spread malicious content in Western elections . This paper examines the Russian interference campaign in the 2016 US presidential election on Twitter . Our aim is twofold : first , we test whether predicting users who spread trolls\u2019 content is feasible in order to gain insight on how to contain their influence in the future; second , we identify features that are most predictive of users who either intentionally or unintentionally play a vital role in spreading this malicious content . We collected a dataset with over 43 million election - related posts shared on Twitter between September 16 and November 9 , 2016 , by about 5 . 7 million users . This dataset includes accounts associated with the Russian trolls identified by the US Congress . Proposed models are able to very accurately identify users who spread the trolls\u2019 content (average AUC score of 96% , using 10 - fold validation) . We show that political ideology , bot likelihood scores , and some activity - related account meta data are the most predictive features of whether a user spreads trolls\u2019 content or not . ","Method":["NOT DETECTED"],"Task":["russian interference campaign"]},{"url":"https://www.semanticscholar.org/paper/2a501b074261e81b9126e80a0a308cfa5e76f8c1","title":"Linguistic Models for Analyzing and Detecting Biased Language","abstract":"Unbiased language is a requirement for reference sources like encyclopedias and scientific texts . Bias is , nonetheless , ubiquitous , making it crucial to understand its nature and linguistic realization and hence detect bias automatically . To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles . The analysis uncovers two classes of bias : framing bias , such as praising or perspective - specific words , which we link to the literature on subjectivity; and epistemological bias , related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true . We identify common linguistic cues for these classes , including factive verbs , implicatives , hedges , and subjective intensifiers . These insights help us develop features for a model to solve a new prediction task of practical importance : given a biased sentence , identify the bias - inducing word . Our linguistically - informed model performs almost as well as humans tested on the same task . ","Method":["linguistically - informed model"],"Task":["prediction task"]},{"url":"https://www.semanticscholar.org/paper/1778d1da6264ed4274fb94115ebbc3b4f34c9f7a","title":"Misinfo Belief Frames : A Case Study on Covid & Climate News","abstract":"Prior beliefs of readers impact the way in which they project meaning onto news headlines . These beliefs can influence their perception of news reliability , as well as their reaction to news , and their likelihood of spreading the misinformation through social networks . However , most prior work focuses on fact - checking veracity of news or stylometry rather than measuring impact of misinformation . We propose Misinfo Belief Frames , a formalism for understanding how readers perceive the reliability of news and the impact of misinformation . We also introduce the Misinfo Belief Frames (MBF) corpus , a dataset of 66k inferences over 23 . 5k headlines . Misinformation frames use commonsense reasoning to uncover implications of real and fake news headlines focused on global crises : the Covid - 19 pandemic and climate change . Our results using large - scale language modeling to predict misinformation frames show that machine - generated inferences can influence readers\u2019 trust in news headlines (readers\u2019 trust in news headlines was affected in 29 . 3% of cases) . This demonstrates the potential effectiveness of using generated frames to counter misinformation . ","Method":["generated frames"],"Task":["fact - checking veracity of news"]},{"url":"https://www.semanticscholar.org/paper/460986c12b9fdedc3bf9a710c429c5a84608055a","title":"Fighting the COVID - 19 Infodemic in Social Media : A Holistic Perspective and a Call to Arms","abstract":"With the outbreak of the COVID - 19 pandemic , people turned to social media to read and to share timely information including statistics , warnings , advice , and inspirational stories . Unfortunately , alongside all this useful information , there was also a new blending of medical and political misinformation and disinformation , which gave rise to the first global infodemic . While fighting this infodemic is typically thought of in terms of factuality , the problem is much broader as malicious content includes not only fake news , rumors , and conspiracy theories , but also promotion of fake cures , panic , racism , xenophobia , and mistrust in the authorities , among others . This is a complex problem that needs a holistic approach combining the perspectives of journalists , fact - checkers , policymakers , government entities , social media platforms , and society as a whole . Taking them into account we define an annotation schema and detailed annotation instructions , which reflect these perspectives . We performed initial annotations using this schema , and our initial experiments demonstrated sizable improvements over the baselines . Now , we issue a call to arms to the research community and beyond to join the fight by supporting our crowdsourcing annotation efforts . ","Method":["crowdsourcing annotation efforts"],"Task":["NOT DETECTED"]},{"url":"https://www.semanticscholar.org/paper/9129399d6b1599508709e3a72f80defd52ef89c9","title":"The online competition between pro - and anti - vaccination views","abstract":"Distrust in scientific expertise 1 \u2013 14 is dangerous . Opposition to vaccination with a future vaccine against SARS - CoV - 2 , the causal agent of COVID - 19 , for example , could amplify outbreaks 2 \u2013 4 , as happened for measles in 2019 5 , 6 . Homemade remedies 7 , 8 and falsehoods are being shared widely on the Internet , as well as dismissals of expert advice 9 \u2013 11 . There is a lack of understanding about how this distrust evolves at the system level 13 , 14 . Here we provide a map of the contention surrounding vaccines that has emerged from the global pool of around three billion Facebook users . Its core reveals a multi - sided landscape of unprecedented intricacy that involves nearly 100\xa0million individuals partitioned into highly dynamic , interconnected clusters across cities , countries , continents and languages . Although smaller in overall size , anti - vaccination clusters manage to become highly entangled with undecided clusters in the main online network , whereas pro - vaccination clusters are more peripheral . Our theoretical framework reproduces the recent explosive growth in anti - vaccination views , and predicts that these views will dominate in a decade . Insights provided by this framework can inform new policies and approaches to interrupt this shift to negative views . Our results challenge the conventional thinking about undecided individuals in issues of contention surrounding health , shed light on other issues of contention such as climate change 11 , and highlight the key role of network cluster dynamics in multi - species ecologies 15 . Insights into the interactions between pro - and anti - vaccination clusters on Facebook can enable policies and approaches that attempt to interrupt the shift to anti - vaccination views and persuade undecided individuals to adopt a pro - vaccination stance . ","Method":["NOT DETECTED"],"Task":["vaccination"]},{"url":"https://www.semanticscholar.org/paper/ed1c17451a23471afde91c109ecadc6aab8b2ba6","title":"A Survey on Multimodal Disinformation Detection","abstract":"Recent years have witnessed the proliferation of fake news , propaganda , misinformation , and disinformation online . While initially this was mostly about textual content , over time images and videos gained popularity , as they are much easier to consume , attract much more attention , and spread further than simple text . As a result , researchers started targeting different modalities and combinations thereof . As different modalities are studied in different research communities , with insufficient interaction , here we offer a survey that explores the state - of - the - art on multimodal disinformation detection covering various combinations of modalities : text , images , audio , video , network structure , and temporal information . Moreover , while some studies focused on factuality , others investigated how harmful the content is . While these two components in the definition of disinformation \u2013 (i) factuality and (ii) harmfulness , are equally important , they are typically studied in isolation . Thus , we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness , in the same framework . Finally , we discuss current challenges and future research directions . ","Method":["NOT DETECTED"],"Task":["multimodal disinformation detection","disinformation detection"]},{"url":"https://www.semanticscholar.org/paper/38d243b9f6e2c786699dbc83513fb190372cde07","title":"Automated Fact - Checking for Assisting Human Fact - Checkers","abstract":"The reporting and analysis of current events around the globe has expanded from professional , editorlead journalism all the way to citizen journalism . Politicians and other key players enjoy direct access to their audiences through social media , bypassing the filters of official cables or traditional media . However , the multiple advantages of free speech and direct communication are dimmed by the misuse of the media to spread inaccurate or misleading claims . These phenomena have led to the modern incarnation of the fact - checker \u2014 a professional whose main aim is to examine claims using available evidence to assess their veracity . As in other text forensics tasks , the amount of information available makes the work of the fact - checker more difficult . With this in mind , starting from the perspective of the professional fact - checker , we survey the available intelligent technologies that can support the human expert in the different steps of her fact - checking endeavor . These include identifying claims worth fact - checking; detecting relevant previously fact - checked claims; retrieving relevant evidence to fact - check a claim; and actually verifying a claim . In each case , we pay attention to the challenges in future work and the potential impact on real - world fact - checking . ","Method":["fact - checker"],"Task":["automated fact - checking"]},{"url":"https://www.semanticscholar.org/paper/76de89ca66f898e8211acba7392ef2d4a7e14125","title":"Fakeddit : A New Multimodal Benchmark Dataset for Fine - grained Fake News Detection","abstract":"Fake news has altered society in negative ways in politics and culture . It has adversely affected both online social network systems as well as offline communities and conversations . Using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news . However , a lack of effective , comprehensive datasets has been a problem for fake news research and detection model development . Prior fake news datasets do not provide multimodal text and image data , metadata , comment data , and fine - grained fake news categorization at the scale and breadth of our dataset . We present Fakeddit , a novel multimodal dataset consisting of over 1 million samples from multiple categories of fake news . After being processed through several stages of review , the samples are labeled according to 2 - way , 3 - way , and 6 - way classification categories through distant supervision . We construct hybrid text+image models and perform extensive experiments for multiple variations of classification , demonstrating the importance of the novel aspect of multimodality and fine - grained classification unique to Fakeddit . ","Method":["fakeddit"],"Task":["classification","fine - grained fake news categorization","fine - grained classification","fine - grained fake news detection"]},{"url":"https://www.semanticscholar.org/paper/c6a9ca56c93323c0199dd22631d1cf731bdd7ec1","title":"Automatic Detection of Fake News","abstract":"The proliferation of misleading information in everyday access media outlets such as social media feeds , news blogs , and online newspapers have made it challenging to identify trustworthy news sources , thus increasing the need for computational tools able to provide insights into the reliability of online content . In this paper , we focus on the automatic identification of fake content in online news . Our contribution is twofold . First , we introduce two novel datasets for the task of fake news detection , covering seven different news domains . We describe the collection , annotation , and validation process in detail and present several exploratory analyses on the identification of linguistic differences in fake and legitimate news content . Second , we conduct a set of learning experiments to build accurate fake news detectors , and show that we can achieve accuracies of up to 76% . In addition , we provide comparative analyses of the automatic and manual identification of fake news . ","Method":["fake news detectors"],"Task":["fake news detection","automatic identification of fake content","automatic detection of fake news"]},{"url":"https://www.semanticscholar.org/paper/03c294ad75bd1bac92217419ac25358227f6a901","title":"\\"Liar , Liar Pants on Fire\\" : A New Benchmark Dataset for Fake News Detection","abstract":"Automatic fake news detection is a challenging problem in deception detection , and it has tremendous real - world political and social impacts . However , statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets . In this paper , we present liar : a new , publicly available dataset for fake news detection . We collected a decade - long , 12 . 8K manually labeled short statements in various contexts from PolitiFact . com , which provides detailed analysis report and links to source documents for each case . This dataset can be used for fact - checking research as well . Notably , this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type . Empirically , we investigate automatic fake news detection based on surface - level linguistic patterns . We have designed a novel , hybrid convolutional neural network to integrate meta - data with text . We show that this hybrid approach can improve a text - only deep learning model . ","Method":["liar"],"Task":["fake news detection","automatic fake news detection","deception detection"]},{"url":"https://www.semanticscholar.org/paper/8415274c8fb370cbab84ad82ab2f469786ddee72","title":"Weaponized Health Communication : Twitter Bots and Russian Trolls Amplify the Vaccine Debate","abstract":"Objectives To understand how Twitter bots and trolls (\u201cbots\u201d) promote online health content . Methods We compared bots\u2019 to average users\u2019 rates of vaccine - relevant messages , which we collected online from July 2014 through September 2017 . We estimated the likelihood that users were bots , comparing proportions of polarized and antivaccine tweets across user types . We conducted a content analysis of a Twitter hashtag associated with Russian troll activity . Results Compared with average users , Russian trolls (\u03c72(1)\u2009=\u2009102 . 0; P\u2009<\u2009 . 001) , sophisticated bots (\u03c72(1)\u2009=\u200928 . 6; P\u2009<\u2009 . 001) , and \u201ccontent polluters\u201d (\u03c72(1)\u2009=\u20097 . 0; P\u2009<\u2009 . 001) tweeted about vaccination at higher rates . Whereas content polluters posted more antivaccine content (\u03c72(1)\u2009=\u200911 . 18; P\u2009<\u2009 . 001) , Russian trolls amplified both sides . Unidentifiable accounts were more polarized (\u03c72(1)\u2009=\u200912 . 1; P\u2009<\u2009 . 001) and antivaccine (\u03c72(1)\u2009=\u200935 . 9; P\u2009<\u2009 . 001) . Analysis of the Russian troll hashtag showed that its messages were more political and divisive . Conclusions Whereas bots that spread malware and unsolicited content disseminated antivaccine messages , Russian trolls promoted discord . Accounts masquerading as legitimate users create false equivalency , eroding public consensus on vaccination . Public Health Implications . Directly confronting vaccine skeptics enables bots to legitimize the vaccine debate . More research is needed to determine how best to combat bot - driven content . ","Method":["content analysis"],"Task":["bot - driven content"]},{"url":"https://www.semanticscholar.org/paper/1dad69f1fd4403aed4d3d709ab794113291d625c","title":"Fake News : Spread of Misinformation about Urological Conditions on Social Media . ","abstract":"Although there is a large amount of user - generated content about urological health issues on social media , much of this content has not been vetted for information accuracy . In this article , we review the literature on the quality and balance of information on urological health conditions on social networks . Across a wide range of benign and malignant urological conditions , studies show a substantial amount of commercial , biased and/or inaccurate information present on popular social networking sites . The healthcare community should take proactive steps to improve the quality of medical information on social networks . PATIENT SUMMARY : In this review , we examined the spread of misinformation about urological health conditions on social media . We found that a significant amount of the circulating information is commercial , biased or misinformative . ","Method":["NOT DETECTED"],"Task":["medical information"]},{"url":"https://www.semanticscholar.org/paper/7d3c2ff37d04914836f9cbd9ce54b6c97aa74a22","title":"Truth of Varying Shades : Analyzing Language in Fake News and Political Fact - Checking","abstract":"We present an analytic study on the language of news media in the context of political fact - checking and fake news detection . We compare the language of real news with that of satire , hoaxes , and propaganda to find linguistic characteristics of untrustworthy text . To probe the feasibility of automatic political fact - checking , we also present a case study based on PolitiFact . com using their factuality judgments on a 6 - point scale . Experiments show that while media fact - checking remains to be an open research question , stylistic cues can help determine the truthfulness of text . ","Method":["NOT DETECTED"],"Task":["political fact - checking"]},{"url":"https://www.semanticscholar.org/paper/29370adbf0674ad72d5e0bcc59582c22035bd0aa","title":"Coronavirus on Social Media : Analyzing Misinformation in Twitter Conversations","abstract":"The ongoing Coronavirus Disease (COVID - 19) pandemic highlights the interconnected - ness of our present - day globalized world . With social distancing policies in place , virtual communication has become an important source of (mis)information . As increasing number of people rely on social media platforms for news , identifying misinformation has emerged as a critical task in these unprecedented times . In addition to being malicious , the spread of such information poses a serious public health risk . To this end , we design a dashboard to track misinformation on popular social media news sharing platform - Twitter . Our dashboard allows visibility into the social media discussions around Coronavirus and the quality of information shared on the platform as the situation evolves . We collect streaming data using the Twitter API from March 1 , 2020 to date and provide analysis of topic clusters and social sentiments related to important emerging policies such as \\"#socialdistancing\\" and \\"#workfromhome\\" . We track emerging hashtags over time , and provide location and time sensitive analysis of sentiments . In addition , we study the challenging problem of misinformation on social media , and provide a detection method to identify false , misleading and clickbait contents from Twitter information cascades . The dashboard maintains an evolving list of detected misinformation cascades with the corresponding detection scores , accessible online athttps : //ksharmar . this http URL . ","Method":["dashboard"],"Task":["coronavirus on social media","coronavirus disease"]},{"url":"https://www.semanticscholar.org/paper/1213d98f3d0a42d575bbd2c2af0309c7f76024a1","title":"Combating Fake News : A Survey on Identification and Mitigation Techniques","abstract":"The proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news , and mitigation of its widespread impact on public opinion . While much of the earlier research was focused on identification of fake news based on its contents or by exploiting users\' engagements with the news on social media , there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society . In this survey , we describe the modern - day problem of fake news and , in particular , highlight the technical challenges associated with it . We discuss existing methods and techniques applicable to both identification and mitigation , with a focus on the significant advances in each method and their advantages and limitations . In addition , research has often been limited by the quality of existing datasets and their specific application contexts . To alleviate this problem , we comprehensively compile and summarize characteristic features of available datasets . Furthermore , we outline new directions of research to facilitate future development of effective and interdisciplinary solutions . ","Method":["identification and mitigation techniques"],"Task":["identification"]},{"url":"https://www.semanticscholar.org/paper/bc8c416f821b93795370524247c8a455c373ee6e","title":"Fake News : A Survey of Research , Detection Methods , and Opportunities","abstract":"The explosive growth in fake news and its erosion to democracy , justice , and public trust has increased the demand for fake news analysis , detection and intervention . This survey comprehensively and systematically reviews fake news research . The survey identifies and specifies fundamental theories across various disciplines , e . g . , psychology and social science , to facilitate and enhance the interdisciplinary research of fake news . Current fake news research is reviewed , summarized and evaluated . These studies focus on fake news from four perspective : (1) the false knowledge it carries , (2) its writing style , (3) its propagation patterns , and (4) the credibility of its creators and spreaders . We characterize each perspective with various analyzable and utilizable information provided by news and its spreaders , various strategies and frameworks that are adaptable , and techniques that are applicable . By reviewing the characteristics of fake news and open issues in fake news studies , we highlight some potential research tasks at the end of this survey . ","Method":["detection methods"],"Task":["fake news analysis"]},{"url":"https://www.semanticscholar.org/paper/9e8ac8df8b46c36cad3f307f85975012479b5a32","title":"Fact or Fiction : Verifying Scientific Claims","abstract":"We introduce scientific claim verification , a new task to select abstracts from the research literature containing evidence that supports or refutes a given scientific claim , and to identify rationales justifying each decision . To study this task , we construct SciFact , a dataset of 1 . 4K expert - written scientific claims paired with evidence - containing abstracts annotated with labels and rationales . We develop baseline models for SciFact , and demonstrate that these models benefit from combined training on a large dataset of claims about Wikipedia articles , together with the new SciFact data . We show that our claim verification system is able to identify plausible evidence for 23 / 36 claims relevant to COVID - 19 on the CORD - 19 corpus . Our results and experiments strongly suggest that our new task and data will support significant future research efforts . ","Method":["claim verification system"],"Task":["scifact"]},{"url":"https://www.semanticscholar.org/paper/1e3d1055fc65966bc2c64505785207d7e8d71022","title":"Rumor Cascades","abstract":"Online social networks provide a rich substrate for rumor propagation . Information received via friends tends to be trusted , and online social networks allow individuals to transmit information to many friends at once . By referencing known rumors from Snopes . com , a popular website documenting memes and urban legends , we track the propagation of thousands of rumors appearing on Facebook . From this sample we infer the rates at which rumors from different categories and of varying truth value are uploaded and reshared . We find that rumor cascades run deeper in the social network than reshare cascades in general . We then examine the effect of individual reshares receiving a comment containing a link to a Snopes article on the evolution of the cascade . We find that receiving such a comment increases the likelihood that a reshare of a rumor will be deleted . Furthermore , large cascades are able to accumulate hundreds of Snopes comments while continuing to propagate . Finally , using a dataset of rumors copied and pasted from one status update to another , we show that rumors change over time and that different variants tend to dominate different bursts in popularity . ","Method":["NOT DETECTED"],"Task":["rumor propagation"]},{"url":"https://www.semanticscholar.org/paper/6f373f4711e1285bdec23069c9503d3bf77bfaef","title":"A Benchmark Dataset of Check - worthy Factual Claims","abstract":"In this paper we present the ClaimBuster dataset of 23 , 533 statements extracted from all U . S . general election presidential debates and annotated by human coders . The ClaimBuster dataset can be leveraged in building computational methods to identify claims that are worth fact - checking from the myriad of sources of digital or traditional media . The ClaimBuster dataset is publicly available to the research community , and it can be found at this http URL . ","Method":["computational methods"],"Task":["NOT DETECTED"]},{"url":"https://www.semanticscholar.org/paper/44ecb9783e43d726fc87b7ffdd580b01e13a85cb","title":"Rumors , False Flags , and Digital Vigilantes : Misinformation on Twitter after the 2013 Boston Marathon Bombing","abstract":"The Boston Marathon bombing story unfolded on every possible carrier of information available in the spring of 2013 , including Twitter . As information spread , it was filled with rumors (unsubstantiated information) , and many of these rumors contained misinformation . Earlier studies have suggested that crowdsourced information flows can correct misinformation , and our research investigates this proposition . This exploratory research examines three rumors , later demonstrated to be false , that circulated on Twitter in the aftermath of the bombings . Our findings suggest that corrections to the misinformation emerge but are muted compared with the propagation of the misinformation . The similarities and differences we observe in the patterns of the misinformation and corrections contained within the stream over the days that followed the attacks suggest directions for possible research strategies to automatically detect misinformation . ","Method":["NOT DETECTED"],"Task":["digital vigilantes"]},{"url":"https://www.semanticscholar.org/paper/69116800a8a8195531d29c8e14cefb1c92cbb8a7","title":"Extractive and Abstractive Explanations for Fact - Checking and Evaluation of News","abstract":"In this paper , we explore the construction of natural language explanations for news claims , with the goal of assisting fact - checking and news evaluation applications . We experiment with two methods : (1) an extractive method based on Biased TextRank \u2013 a resource - effective unsupervised graph - based algorithm for content extraction; and (2) an abstractive method based on the GPT - 2 language model . We perform comparative evaluations on two misinformation datasets in the political and health news domains , and find that the extractive method shows the most promise . ","Method":["biased textrank"],"Task":["fact - checking"]},{"url":"https://www.semanticscholar.org/paper/20b2f18aaf10a9221c5edf3720d4cce7da672104","title":"That is a Known Lie : Detecting Previously Fact - Checked Claims","abstract":"The recent proliferation of \u201dfake news\u201d has triggered a number of responses , most notably the emergence of several manual fact - checking initiatives . As a result and over time , a large number of fact - checked claims have been accumulated , which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact - checked by some trusted fact - checking organization , as viral claims often come back after a while in social media , and politicians like to repeat their favorite statements , true or false , over and over again . As manual fact - checking is very time - consuming (and fully automatic fact - checking has credibility issues) , it is important to try to save this effort and to avoid wasting time on claims that have already been fact - checked . Interestingly , despite the importance of the task , it has been largely ignored by the research community so far . Here , we aim to bridge this gap . In particular , we formulate the task and we discuss how it relates to , but also differs from , previous work . We further create a specialized dataset , which we release to the research community . Finally , we present learning - to - rank experiments that demonstrate sizable improvements over state - of - the - art retrieval and textual similarity approaches . ","Method":["retrieval and textual similarity approaches"],"Task":["manual fact - checking"]},{"url":"https://www.semanticscholar.org/paper/ad7129af0644dbcafa9aa2f111cb76526ea444a1","title":"Defending Against Neural Fake News","abstract":"Recent progress in natural language generation has raised dual - use concerns . While applications like summarization and translation are positive , the underlying technology also might enable adversaries to generate neural fake news : targeted propaganda that closely mimics the style of real news . \\nModern computer security relies on careful threat modeling : identifying potential threats and vulnerabilities from an adversary\'s point of view , and exploring potential mitigations to these threats . Likewise , developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models . We thus present a model for controllable text generation called Grover . Given a headline like `Link Found Between Vaccines and Autism , \' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human - written disinformation . \\nDeveloping robust verification techniques against generators like Grover is critical . We find that best current discriminators can classify neural fake news from real , human - written , news with 73% accuracy , assuming access to a moderate level of training data . Counterintuitively , the best defense against Grover turns out to be Grover itself , with 92% accuracy , demonstrating the importance of public release of strong generators . We investigate these results further , showing that exposure bias - - and sampling strategies that alleviate its effects - - both leave artifacts that similar discriminators can pick up on . We conclude by discussing ethical issues regarding the technology , and plan to release Grover publicly , helping pave the way for better detection of neural fake news . ","Method":["grover"],"Task":["neural fake news","detection of neural fake news","natural language generation","summarization"]},{"url":"https://www.semanticscholar.org/paper/d8cb11d4be955f9869387a18967dee366eb851d9","title":"MultiFC : A Real - World Multi - Domain Dataset for Evidence - Based Fact Checking of Claims","abstract":"We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification . It is collected from 26 fact checking websites in English , paired with textual sources and rich metadata , and labelled for veracity by human expert journalists . We present an in - depth analysis of the dataset , highlighting characteristics and challenges . Further , we present results for automatic veracity prediction , both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines . Significant performance increases are achieved by encoding evidence , and by modelling metadata . Our best - performing model achieves a Macro F1 of 49 . 2% , showing that this is a challenging testbed for claim veracity prediction . ","Method":["NOT DETECTED"],"Task":["claim veracity prediction","automatic veracity prediction"]},{"url":"https://www.semanticscholar.org/paper/9aefd614e52336151966d8dca2ed0ea62a8f30af","title":"The Limitations of Stylometry for Detecting Machine - Generated Fake News","abstract":"Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation . In light of these concerns , several studies have proposed to detect machine - generated fake news by capturing their stylistic differences from human - written text . These approaches , broadly termed stylometry , have found success in source attribution and misinformation detection in human - written texts . However , in this work , we show that stylometry is limited against machine - generated misinformation . Whereas humans speak differently when trying to deceive , LMs generate stylistically consistent text , regardless of underlying motive . Thus , though stylometry can successfully prevent impersonation by identifying text provenance , it fails to distinguish legitimate LM applications from those that introduce false information . We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs , utilized in auto - completion and editing - assistance settings . 1 Our findings highlight the need for non - stylometry approaches in detecting machine - generated misinformation , and open up the discussion on the desired evaluation benchmarks . ","Method":["stylometry"],"Task":["detecting machine - generated misinformation"]},{"url":"https://www.semanticscholar.org/paper/b1d24e8e08435b7c52335485a0d635abf9bc604c","title":"FEVER : a Large - scale Dataset for Fact Extraction and VERification","abstract":"In this paper we introduce a new publicly available dataset for verification against textual sources , FEVER : Fact Extraction and VERification . It consists of 185 , 445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from . The claims are classified as Supported , Refuted or NotEnoughInfo by annotators achieving 0 . 6841 in Fleiss kappa . For the first two classes , the annotators also recorded the sentence(s) forming the necessary evidence for their judgment . To characterize the challenge of the dataset presented , we develop a pipeline approach and compare it to suitably designed oracles . The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31 . 87% , while if we ignore the evidence we achieve 50 . 91% . Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources . ","Method":["fever"],"Task":["claim verification","fever","fact extraction","verification"]},{"url":"https://www.semanticscholar.org/paper/0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World","abstract":"Language technologies contribute to promoting multilingualism and linguistic diversity around the world . However , only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications . In this paper we look at the relation between the types of languages , resources , and their representation in NLP conferences to understand the trajectory that different languages have followed over time . Our quantitative investigation underlines the disparity between languages , especially in terms of their resources , and calls into question the \u201clanguage agnostic\u201d status of current models and systems . Through this paper , we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here , so that no language is left behind . ","Method":["nlp"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/c3bcdea205ec9fb1b84d75d4767f346844082b38","title":"Stereotypes in High - Stakes Decisions : Evidence from U . S . Circuit Courts","abstract":"Attitudes towards social groups such as women and racial minorities have been shown to be important determinants of individual\u2019s decisions but are hard to measure for those in policy - making roles . We propose a way to address the challenge in the case of U . S . appellate court judges , for whom we have large corpora of written text (their published opinions) . Using the universe of published opinions in U . S . Circuit Courts 1890 - 2013 , we construct a judge - specific measure of gender - stereotyped language (gender slant) by looking at the relative co - occurrence of words identifying gender (male versus female) and words identifying gender stereotypes (career versus family) . We find that female and younger judges tend to use less stereotyped language in their opinions . In addition , the attitudes measured by gender slant matter for judicial decisions : judges with higher slant vote more conservatively on women rights\u2019 issues . These more slanted judges also influence workplace outcomes for female colleagues : they are less likely to assign opinions to female judges , they cite fewer female - authored opinions , and they are more likely to reverse lower - court decisions if the lower - court judge is a woman . Our results expose a possible use of text to detect decision - makers\u2019 stereotypes that predict behavior and disparate outcomes . \u2217Arianna Ornaghi , University of Warwick , a . ornaghi@warwick . ac . uk (corresponding author); Elliott Ash , ETH Zurich , ashe@ethz . ch; Daniel Chen , Toulouse School of Economics , daniel . chen@iast . fr . We thank Jacopo Bregolin , David Cai , Christoph Goessmann , and Ornelie Manzambi for helpful research assistance . ","Method":["NOT DETECTED"],"Task":["high - stakes decisions"]},{"url":"https://www.semanticscholar.org/paper/5e0daaeceb75ffbbe23be13d34ffae830cb4e8c4","title":"Generating Fact Checking Explanations","abstract":"Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata , social network spread , language used in claims , and , more recently , evidence supporting or denying claims . A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process \u2013 generating justifications for verdicts on claims . This paper provides the first study of how these explanations can be generated automatically based on available claim context , and how this task can be modelled jointly with veracity prediction . Our results indicate that optimising both objectives at the same time , rather than training them separately , improves the performance of a fact checking system . The results of a manual evaluation further suggest that the informativeness , coverage and overall quality of the generated explanations are also improved in the multi - task model . ","Method":["fact checking system"],"Task":["veracity prediction"]},{"url":"https://www.semanticscholar.org/paper/a4947468ceac25c11a665c4f5f95a49d6dbea3cc","title":"Towards Debiasing Fact Verification Models","abstract":"Fact verification requires validating a claim in the context of evidence . We show , however , that in the popular FEVER dataset this might not necessarily be the case . Claim - only classifiers perform competitively with top evidence - aware models . In this paper , we investigate the cause of this phenomenon , identifying strong cues for predicting labels solely based on the claim , without considering any evidence . We create an evaluation set that avoids those idiosyncrasies . The performance of FEVER - trained models significantly drops when evaluated on this test set . Therefore , we introduce a regularization method which alleviates the effect of bias in the training data , obtaining improvements on the newly created test set . This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models . ","Method":["fact verification models"],"Task":["fact verification models"]},{"url":"https://www.semanticscholar.org/paper/3a16c38294ac8899825c488490199b854e05473a","title":"Evaluating adversarial attacks against multiple fact verification systems","abstract":"Automated fact verification has been progressing owing to advancements in modeling and availability of large datasets . Due to the nature of the task , it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly . We introduce two novel scoring metrics , attack potency and system resilience which take into account the correctness of the adversarial instances , an aspect often ignored in adversarial evaluations . We consider six fact verification systems from the recent Fact Extraction and VERification (FEVER) challenge : the four best - scoring ones and two baselines . We evaluate adversarial instances generated by a recently proposed state - of - the - art method , a paraphrasing method , and rule - based attacks devised for fact verification . We find that our rule - based attacks have higher potency , and that while the rankings among the top systems changed , they exhibited higher resilience than the baselines . ","Method":["fact verification systems"],"Task":["fact verification"]},{"url":"https://www.semanticscholar.org/paper/d4eeb40b9bd06ed53a26282cd527609f71e6496f","title":"Unsupervised Discovery of Gendered Language through Latent - Variable Modeling","abstract":"Studying the ways in which language is gendered has long been an area of interest in sociolinguistics . Studies have explored , for example , the speech of male and female characters in film and the language used to describe male and female politicians . In this paper , we aim not to merely study this phenomenon qualitatively , but instead to quantify the degree to which the language used to describe men and women is different and , moreover , different in a positive or negative way . To that end , we introduce a generative latent - variable model that jointly represents adjective (or verb) choice , with its sentiment , given the natural gender of a head (or dependent) noun . We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes : Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men . ","Method":["latent - variable modeling"],"Task":["unsupervised discovery of gendered language"]},{"url":"https://www.semanticscholar.org/paper/bb53946c7da617a05bbeef47fff74012db27ee78","title":"ChrEnTranslate : Cherokee - English Machine Translation Demo with Quality Estimation and Corrective Feedback","abstract":"We introduce ChrEnTranslate , an online ma\xad chine translation demonstration system for translation between English and an endangered language Cherokee . It supports both statistical and neural translation models as well as pro\xad vides quality estimation to inform users of re\xad liability , two user feedback interfaces for ex\xad perts and common users respectively , exam\xad ple inputs to collect human translations for monolingual data , word alignment visualiza\xad tion , and relevant terms from the Cherokee\xad English dictionary . The quantitative evalu\xad ation demonstrates that our backbone trans\xad lation models achieve state\xadof\xadthe\xadart transla\xad tion performance and our quality estimation well correlates with both BLEU and human judgment . By analyzing 216 pieces of expert feedback , we find that NMT is preferable be\xad cause it copies less than SMT , and , in gen\xad eral , current models can translate fragments of the source sentence but make major mistakes . When we add these 216 expert\xadcorrected paral\xad lel texts into the training set and retrain mod\xad els , equal or slightly better performance is ob\xad served , which demonstrates indicates the po\xad tential of human\xadin\xadthe\xadloop learning . 1","Method":["online ma\xad chine","chrentranslate"],"Task":["cherokee - english machine translation demo","transla\xad tion","translation"]},{"url":"https://www.semanticscholar.org/paper/3d309aa1629ef9ca43e252eb6bf539286ed872f9","title":"Haitian Creole : How to Build and Ship an MT Engine from Scratch in 4 days , 17 hours , & 30 minutes","abstract":"We describe the effort of the Microsoft Translator team to develop a Haitian Creole statistical machine translation engine from scratch in a matter of days . Haitian Creole presents a number of difficulties for devleoping an SMT system , principal among these is the lack of significant amounts of parallel training data and an inconsistent orthography , both of which lead to data sparseness . We demonstrate , however , that it is possible to build a translation engine of reasonable quality over very little data by engaging with the native language community and reducing data sparseness in creative ways . As such , we show that MT as a technology and as a service can be deployed rapidly in crisis situations . ","Method":["haitian creole statistical machine translation engine","mt"],"Task":["translator"]},{"url":"https://www.semanticscholar.org/paper/f14cb1828e314a669304b0c37bc78d6b9073f6dd","title":"Measuring Societal Biases in Text Corpora via First - Order Co - occurrence","abstract":"Text corpora are used to study societal biases , typically through statistical models such as word embeddings . The bias of a word towards a concept is typically estimated using vectors similarity , measuring whether the word and concept words share other words in their contexts . We argue that this second - order relationship introduces unrelated concepts into the measure , which causes an imprecise measurement of the bias . We propose instead to measure bias using the direct normalized co - occurrence associations between the word and the representative concept words , a first - order measure , by reconstructing the co - occurrence estimates inherent in the word embedding models . To study our novel corpus bias measurement method , we calculate the correlation of the gender bias values estimated from the text to the actual gender bias statistics of the U . S . job market , provided by two recent collections . The results show a consistently higher correlation when using the proposed first - order measure with a variety of word embedding models , as well as a more severe degree of bias , especially to female in a few specific occupations . ","Method":["first - order co - occurrence"],"Task":["measuring societal biases"]},{"url":"https://www.semanticscholar.org/paper/176420f394014d9e67d5cff1e1d430541fa0f55f","title":"When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?","abstract":"Social biases are encoded in word embeddings . This presents a unique opportunity to study society historically and at scale , and a unique danger when embeddings are used in downstream applications . Here , we investigate the extent to which publicly - available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods . We find that biases found in word embeddings do , on average , closely mirror survey data across seventeen dimensions of social meaning . However , we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e . g . gender) than others (e . g . race) , and that we can be highly confident that embedding - based measures reflect survey data only for the most salient biases . ","Method":["word embeddings"],"Task":["downstream applications"]},{"url":"https://www.semanticscholar.org/paper/75d0cb419d7d58e81c2975758a36a11544a9f930","title":"Language from police body camera footage shows racial disparities in officer respect","abstract":"Significance Police officers speak significantly less respectfully to black than to white community members in everyday traffic stops , even after controlling for officer race , infraction severity , stop location , and stop outcome . This paper presents a systematic analysis of officer body - worn camera footage , using computational linguistic techniques to automatically measure the respect level that officers display to community members . This work demonstrates that body camera footage can be used as a rich source of data rather than merely archival evidence , and paves the way for developing powerful language - based tools for studying and potentially improving police\u2013community relations . Using footage from body - worn cameras , we analyze the respectfulness of police officer language toward white and black community members during routine traffic stops . We develop computational linguistic methods that extract levels of respect automatically from transcripts , informed by a thin - slicing study of participant ratings of officer utterances . We find that officers speak with consistently less respect toward black versus white community members , even after controlling for the race of the officer , the severity of the infraction , the location of the stop , and the outcome of the stop . Such disparities in common , everyday interactions between police and the communities they serve have important implications for procedural justice and the building of police\u2013community trust . ","Method":["computational linguistic techniques"],"Task":["police\u2013community relations"]},{"url":"https://www.semanticscholar.org/paper/b5d7a19bd0bae10917a8e294960fdacf224d64fe","title":"Word embeddings quantify 100 years of gender and ethnic stereotypes","abstract":"Significance Word embeddings are a popular machine - learning method that represents each English word by a vector , such that the geometry between these vectors captures semantic relations between the corresponding words . We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change . As specific applications , we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910 . Our framework opens up a fruitful intersection between machine learning and quantitative social science . Word embeddings are a powerful machine - learning framework that represents each English word by a vector . The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words . In this paper , we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States . We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time . The embedding captures societal shifts\u2014e . g . , the women\u2019s movement in the 1960s and Asian immigration into the United States\u2014and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time . Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science . ","Method":["word embeddings"],"Task":["word embeddings"]},{"url":"https://www.semanticscholar.org/paper/3d505c5eff8752ac1805ef546d683bfa40aec4b1","title":"Tie - breaker : Using language models to quantify gender bias in sports journalism","abstract":"Gender bias is an increasingly important issue in sports journalism . In this work , we propose a language - model - based approach to quantify differences in questions posed to female vs . male athletes , and apply it to tennis post - match interviews . We find that journalists ask male players questions that are generally more focused on the game when compared with the questions they ask their female counterparts . We also provide a fine - grained analysis of the extent to which the salience of this bias depends on various factors , such as question type , game outcome or player rank . ","Method":["tie - breaker"],"Task":["sports journalism gender bias"]},{"url":"https://www.semanticscholar.org/paper/a1280728623e8fd605284b2b7cf536579b9e2cbf","title":"Entity - Centric Contextual Affective Analysis","abstract":"While contextualized word representations have improved state - of - the - art benchmarks in many NLP tasks , their potential usefulness for social - oriented tasks remains largely unexplored . We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people . We evaluate our methodology quantitatively , on held - out affect lexicons , and qualitatively , through case examples . We find that contextualized word representations do encode meaningful affect information , but they are heavily biased towards their training data , which limits their usefulness to in - domain analyses . We ultimately use our method to examine differences in portrayals of men and women . ","Method":["entity - centric contextual affective analysis"],"Task":["nlp tasks"]},{"url":"https://www.semanticscholar.org/paper/070b4a707748e289618880ffbe4762e4e3fc7860","title":"Racism is a Virus : Anti - Asian Hate and Counterhate in Social Media during the COVID - 19 Crisis","abstract":"The spread of COVID - 19 has sparked racism , hate , and xenophobia in social media targeted at Chinese and broader Asian communities . However , little is known about how racial hate spreads during a pandemic and the role of counterhate speech in mitigating the spread . Here we study the evolution and spread of anti - Asian hate speech through the lens of Twitter . We create COVID - HATE , the largest dataset of anti - Asian hate and counterhate spanning three months , containing over 30 million tweets , and a social network with over 87 million nodes . By creating a novel hand - labeled dataset of 2 , 400 tweets , we train a text classifier to identify hate and counterhate tweets that achieves an average AUROC of 0 . 852 . We identify 891 , 204 hate and 200 , 198 counterhate tweets in COVID - HATE . Using this data to conduct longitudinal analysis , we find that while hateful users are less engaged in the COVID - 19 discussions prior to their first anti - Asian tweet , they become more vocal and engaged afterwards compared to counterhate users . We find that bots comprise 10 . 4% of hateful users and are more vocal and hateful compared to non - bot users . Comparing bot accounts , we show that hateful bots are more successful in attracting followers compared to counterhate bots . Analysis of the social network reveals that hateful and counterhate users interact and engage extensively with one another , instead of living in isolated polarized communities . Furthermore , we find that hate is contagious and nodes are highly likely to become hateful after being exposed to hateful content . Importantly , our analysis reveals that counterhate messages can discourage users from turning hateful in the first place . Overall , this work presents a comprehensive overview of anti - Asian hate and counterhate content during a pandemic . The COVID - HATE dataset is available at this http URL . ","Method":["hateful bots"],"Task":["covid - 19 crisis"]},{"url":"https://www.semanticscholar.org/paper/7b5b2a9ad37d1a6c3c8916965b1958eef0a27a6a","title":"Automatically Inferring Gender Associations from Language","abstract":"In this paper , we pose the question : do people talk about women and men in different ways? We introduce two datasets and a novel integration of approaches for automatically inferring gender associations from language , discovering coherent word clusters , and labeling the clusters for the semantic concepts they represent . The datasets allow us to compare how people write about women and men in two different settings \u2013 one set draws from celebrity news and the other from student reviews of computer science professors . We demonstrate that there are large - scale differences in the ways that people talk about women and men and that these differences vary across domains . Human evaluations show that our methods significantly outperform strong baselines . ","Method":["NOT DETECTED"],"Task":["automatically inferring gender associations"]},{"url":"https://www.semanticscholar.org/paper/6ba951771892f01206f1dd7244f14243e3885109","title":"Contextual Affective Analysis : A Case Study of People Portrayals in Online #MeToo Stories","abstract":"In October 2017 , numerous women accused producer Harvey Weinstein of sexual harassment . Their stories encouraged other women to voice allegations of sexual harassment against many high profile men , including politicians , actors , and producers . These events are broadly referred to as the #MeToo movement , named for the use of the hashtag \\"#metoo\\" on social media platforms like Twitter and Facebook . The movement has widely been referred to as \\"empowering\\" because it has amplified the voices of previously unheard women over those of traditionally powerful men . In this work , we investigate dynamics of sentiment , power and agency in online media coverage of these events . Using a corpus of online media articles about the #MeToo movement , we present a contextual affective analysis - - - an entity - centric approach that uses contextualized lexicons to examine how people are portrayed in media articles . We show that while these articles are sympathetic towards women who have experienced sexual harassment , they consistently present men as most powerful , even after sexual assault allegations . While we focus on media coverage of the #MeToo movement , our method for contextual affective analysis readily generalizes to other domains . ","Method":["contextual affective analysis"],"Task":["contextual affective analysis"]},{"url":"https://www.semanticscholar.org/paper/995e477360908175d0b1184f6a0aace9d864bc5a","title":"Girls Rule , Boys Drool : Extracting Semantic and Affective Stereotypes from Twitter","abstract":"Social identities carry widely agreed upon meanings , called stereotypes , that have important effects on social processes . In the present work , we develop a method to extract the stereotypes of Twitter users . Our method is grounded in two distinct strands of theory , one that represents stereotypes as identities\' affective meanings and the other that represents stereotypes as semantic relationships between identities . After validating our approach via a prediction task , we apply the model to a dataset of 45 thousand Twitter users who actively tweeted about the Michael Brown and Eric Garner tragedies . Our work provides unique insights into the stereotypes of these users , as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel , parsimonious way . ","Method":["sociological and psychological theory"],"Task":["prediction task"]},{"url":"https://www.semanticscholar.org/paper/5e888bfd9b492a3b08f3cc2eb7c617fedf5bd811","title":"Relating Linguistic Gender Bias , Gender Values , and Gender Gaps : An International Analysis","abstract":"Recent research in machine learning has shown that many machine - learned language models contain pervasive racial and gender biases , rooting from biases in their textual training data . While these biases produce sub - optimal parsing and inferences , they may help us characterize and predict statistical gender gaps and gender values in the culture(s) that produced the training text , thereby helping us understand cultural context through big data . This paper presents an approach to (1) quantify gender bias in word embeddings (i . e . , vector - based lexical semantics) , (2) correlate gender biases with survey responses and statistical gender gaps in education , politics , economics , and health , and (3) integrate numerical biases and statistics to model different cultures\u2019 survey results more accurately than either in isolation . We validate this approach using 2018 Twitter data spanning 99 countries , 18 Global Gender Gap statistics from the World Economic Forum , and 8 international survey results from the World Value Survey . Integrating these heterogeneous data across cultures is an important step toward building computational models to understand group bias . ","Method":["vector - based lexical semantics)"],"Task":["parsing"]},{"url":"https://www.semanticscholar.org/paper/abeee58b9fb5761133636ef117ef1a87203ad7ab","title":"Automation , Algorithms , and Politics| Talking to Bots : Symbiotic Agency and the Case of Tay","abstract":"In 2016 , Microsoft launched Tay , an experimental artificial intelligence chat bot . Learning from interactions with Twitter users , Tay was shut down after one day because of its obscene and inflammatory tweets . This article uses the case of Tay to re - examine theories of agency . How did users view the personality and actions of an artificial intelligence chat bot when interacting with Tay on Twitter? Using phenomenological research methods and pragmatic approaches to agency , we look at what people said about Tay to study how they imagine and interact with emerging technologies and to show the limitations of our current theories of agency for describing communication in these settings . We show how different qualities of agency , different expectations for technologies , and different capacities for affordance emerge in the interactions between people and artificial intelligence . We argue that a perspective of \u201csymbiotic agency\u201d\u2014informed by the imagined affordances of emerging technology\u2014is required to really understand the collapse of Tay . ","Method":["pragmatic approaches"],"Task":["artificial intelligence chat bot"]},{"url":"https://www.semanticscholar.org/paper/e3fd3b1be871da6e048adaef4a4e201af282fe8e","title":"Empathy Is All You Need : How a Conversational Agent Should Respond to Verbal Abuse","abstract":"With the popularity of AI - infused systems , conversational agents (CAs) are becoming essential in diverse areas , offering new functionality and convenience , but simultaneously , suffering misuse and verbal abuse . We examine whether conversational agents\' response styles under varying abuse types influence those emotions found to mitigate peoples\' aggressive behaviors , involving three verbal abuse types (Insult , Threat , Swearing) and three response styles (Avoidance , Empathy , Counterattacking) . Ninety - eight participants were assigned to one of the abuse type conditions , interacted with the three spoken (voice - based) CAs in turn , and reported their feelings about guiltiness , anger , and shame after each session . The results show that the agent\'s response style has a significant effect on user emotions . Participants were less angry and more guilty with the empathy agent than the other two agents . Furthermore , we investigated the current status of commercial CAs\' responses to verbal abuse . Our study findings have direct implications for the design of conversational agents . ","Method":["conversational agents"],"Task":["NOT DETECTED"]},{"url":"https://www.semanticscholar.org/paper/ef5fa2e95fc853defb902b58d8e4e4fe95a01c75","title":"Shirtless and Dangerous : Quantifying Linguistic Signals of Gender Bias in an Online Fiction Writing Community","abstract":"Imagine a princess asleep in a castle , waiting for her prince to slay the dragon and rescue her . Tales like the famous Sleeping Beauty clearly divide up gender roles . But what about more modern stories , borne of a generation increasingly aware of social constructs like sexism and racism? Do these stories tend to reinforce gender stereotypes , or counter them? In this paper , we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction . We apply this technique across 1 . 8 billion words of fiction from the Wattpad online writing community , investigating gender representation in stories , how male and female characters behave and are described , and how authors\' use of gender stereotypes is associated with the community\'s ratings . We find that male over - representation and traditional gender stereotypes (e . g . , dominant men and submissive women) are common throughout nearly every genre in our corpus . However , only some of these stereotypes , like sexual or violent men , are associated with highly rated stories . Finally , despite women often being the target of negative stereotypes , female authors are equally likely to write such stereotypes as men . ","Method":["crowdsourced lexicon of stereotypes"],"Task":["gender biases in fiction"]},{"url":"https://www.semanticscholar.org/paper/f34c73c75a640f59c11472bf6c9786aeb774856a","title":"Let\'s Talk About Race : Identity , Chatbots , and AI","abstract":"Why is it so hard for chatbots to talk about race? This work explores how the biased contents of databases , the syntactic focus of natural language processing , and the opaque nature of deep learning algorithms cause chatbots difficulty in handling race - talk . In each of these areas , the tensions between race and chatbots create new opportunities for people and machines . By making the abstract and disparate qualities of this problem space tangible , we can develop chatbots that are more capable of handling race - talk in its many forms . Our goal is to provide the HCI community with ways to begin addressing the question , how can chatbots handle race - talk in new and improved ways?","Method":["chatbots"],"Task":["ai"]},{"url":"https://www.semanticscholar.org/paper/983ad7c704d0f9a1560af322e4807e5be7799895","title":"#MeToo Alexa : How Conversational Systems Respond to Sexual Harassment","abstract":"Conversational AI systems , such as Amazon\u2019s Alexa , are rapidly developing from purely transactional systems to social chatbots , which can respond to a wide variety of user requests . In this article , we establish how current state - of - the - art conversational systems react to inappropriate requests , such as bullying and sexual harassment on the part of the user , by collecting and analysing the novel #MeTooAlexa corpus . Our results show that commercial systems mainly avoid answering , while rule - based chatbots show a variety of behaviours and often deflect . Data - driven systems , on the other hand , are often non - coherent , but also run the risk of being interpreted as flirtatious and sometimes react with counter - aggression . This includes our own system , trained on \u201cclean\u201d data , which suggests that inappropriate system behaviour is not caused by data bias . ","Method":["#metoo alexa"],"Task":["sexual harassment"]},{"url":"https://www.semanticscholar.org/paper/7b14a165c6b7c1dc2c6c44727e623b94d834fb09","title":"Social Bias Frames : Reasoning about Social and Power Implications of Language","abstract":"Warning : this paper contains content that may be offensive or upsetting . Language has the power to reinforce stereotypes and project social biases onto others . At the core of the challenge is that it is rarely what is stated explicitly , but rather the implied meanings , that frame people\u2019s judgments about others . For example , given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women , \u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified . \u201d Most semantic formalisms , to date , do not capture such pragmatic implications in which people express social biases and power differentials in language . We introduce Social Bias Frames , a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others . In addition , we introduce the Social Bias Inference Corpus to support large - scale modelling and evaluation with 150k structured annotations of social media posts , covering over 34k implications about a thousand demographic groups . We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text . We find that while state - of - the - art neural models are effective at high - level categorization of whether a given statement projects unwanted social bias (80% F1) , they are not effective at spelling out more detailed explanations in terms of Social Bias Frames . Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications . ","Method":["social bias frames"],"Task":["categorization"]},{"url":"https://www.semanticscholar.org/paper/87eb23f934a0e6293ee8ee9b147fe0d456e65c96","title":"Data Statements for NLP : Toward Mitigating System Bias and Enabling Better Science","abstract":"In this position paper , we propose data statements as a practice that NLP technologists , in both research and development , can adopt to begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations . We present a form data statements can take and explore the implications of adopting them as part of our regular practice . We argue that they will help alleviate issues related to exclusion and bias in language technology; lead to better precision in claims about how NLP research can generalize and thus better engineering results; protect companies from public embarrassment; and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others . ","Method":["nlp technologists"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/0df347f5e3118fac7c351917e3a497899b071d1e","title":"Datasheets for Datasets","abstract":"The machine learning community currently has no standardized process for documenting datasets , which can lead to severe consequences in high - stakes domains . To address this gap , we propose datasheets for datasets . In the electronics industry , every component , no matter how simple or complex , is accompanied with a datasheet that describes its operating characteristics , test results , recommended uses , and other information . By analogy , we propose that every dataset be accompanied with a datasheet that documents its motivation , composition , collection process , recommended uses , and so on . Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers , and encourage the machine learning community to prioritize transparency and accountability . ","Method":["NOT DETECTED"],"Task":["machine learning community"]},{"url":"https://www.semanticscholar.org/paper/221732318e3cc45aa7bc2f48435706f3e5839ddc","title":"Beyond the Belmont Principles : Ethical Challenges , Practices , and Beliefs in the Online Data Research Community","abstract":"Pervasive information streams that document people and their routines have been a boon to social computing research . But the ethics of collecting and analyzing available& - but potentially sensitive - online data present challenges to researchers . In response to increasing public and scholarly debate over the ethics of online data research , this paper analyzes the current state of practice among researchers using online data . Qualitative and quantitative responses from a survey of 263 online data researchers document beliefs and practices around which social computing researchers are converging , as well as areas of ongoing disagreement . The survey also reveals that these disagreements are not correlated with disciplinary , methodological , or workplace affiliations . The paper concludes by reflecting on changing ethical practices in the digital age , and discusses a set of emergent best practices for ethical social computing research . ","Method":["NOT DETECTED"],"Task":["social computing research"]},{"url":"https://www.semanticscholar.org/paper/0c68d7d153bb56e4637d6aee051d87580e05fd5b","title":"Detecting East Asian Prejudice on Social Media","abstract":"During COVID - 19 concerns have heightened about the spread of aggressive and hateful language online , especially hostility directed against East Asia and East Asian people . We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes : Hostility against East Asia , Criticism of East Asia , Meta - discussions of East Asian prejudice , and a neutral class . The classifier achieves a macro - F1 score of 0 . 83 . We then conduct an in - depth ground - up error analysis and show that the model struggles with edge cases and ambiguous content . We provide the 20 , 000 tweet training dataset (annotated by experienced analysts) , which also contains several secondary categories and additional flags . We also provide the 40 , 000 original annotations (before adjudication) , the full codebook , annotations for COVID - 19 relevance and East Asian relevance and stance for 1 , 000 hashtags , and the final model . ","Method":["machine learning classifier"],"Task":["stance"]},{"url":"https://www.semanticscholar.org/paper/1ece7c00d2eb6fca5443ff8e15f05a2b8b5985c2","title":"Don\u2019t quote me : reverse identification of research participants in social media studies","abstract":"We investigated if participants in social media surveillance studies could be reverse identified by reviewing all articles published on PubMed in 2015 or 2016 with the words \u201cTwitter\u201d and either \u201cread , \u201d \u201ccoded , \u201d or \u201ccontent\u201d in the title or abstract . Seventy - two percent (95% CI : 63\u201380) of articles quoted at least one participant\u2019s tweet and searching for the quoted content led to the participant 84% (95% CI : 74\u201391) of the time . Twenty - one percent (95% CI : 13\u201329) of articles disclosed a participant\u2019s Twitter username thereby making the participant immediately identifiable . Only one article reported obtaining consent to disclose identifying information and institutional review board (IRB) involvement was mentioned in only 40% (95% CI : 31\u201350) of articles , of which 17% (95% CI : 10\u201325) received IRB - approval and 23% (95% CI : 16\u201332) were deemed exempt . Biomedical publications are routinely including identifiable information by quoting tweets or revealing usernames which , in turn , violates ICMJE ethical standards governing scientific ethics , even though said content is scientifically unnecessary . We propose that authors convey aggregate findings without revealing participants\u2019 identities , editors refuse to publish reports that reveal a participant\u2019s identity , and IRBs attend to these privacy issues when reviewing studies involving social media data . These strategies together will ensure participants are protected going forward . ","Method":["NOT DETECTED"],"Task":["social media surveillance studies"]},{"url":"https://www.semanticscholar.org/paper/afe97d05e5b320d2af500cdae1c588f4cc0d14d2","title":"Towards an Ethical Framework for Publishing Twitter Data in Social Research : Taking into Account Users\u2019 Views , Online Context and Algorithmic Estimation","abstract":"New and emerging forms of data , including posts harvested from social media sites such as Twitter , have become part of the sociologist\u2019s data diet . In particular , some researchers see an advantage in the perceived \u2018public\u2019 nature of Twitter posts , representing them in publications without seeking informed consent . While such practice may not be at odds with Twitter\u2019s terms of service , we argue there is a need to interpret these through the lens of social science research methods that imply a more reflexive ethical approach than provided in \u2018legal\u2019 accounts of the permissible use of these data in research publications . To challenge some existing practice in Twitter - based research , this article brings to the fore : (1) views of Twitter users through analysis of online survey data; (2) the effect of context collapse and online disinhibition on the behaviours of users; and (3) the publication of identifiable sensitive classifications derived from algorithms . ","Method":["algorithmic estimation"],"Task":["social research"]},{"url":"https://www.semanticscholar.org/paper/5f827b963939c96968a03318b4c2b011e1871eaf","title":"Writer Profiling Without the Writer\'s Text","abstract":"Social network users may wish to preserve their anonymity online by masking their identity and not using language associated with any particular demographics or personality . However , they have no control over the language in incoming communications . We show that linguistic cues in public comments directed at a user are sufficient for an accurate inference of that user\u2019s gender , age , religion , diet , and even personality traits . Moreover , we show that directed communication is even more predictive of a user\u2019s profile than the user\u2019s own language . We then conduct a nuanced analysis of what types of social relationships are most predictive of users\u2019 attributes , and propose new strategies on how individuals can modulate their online social relationships and incoming communications to preserve their anonymity . ","Method":["NOT DETECTED"],"Task":["directed communication"]},{"url":"https://www.semanticscholar.org/paper/f298649194c1be9cb55574c047756ae7e8a62d6b","title":"\u201cParticipant\u201d Perceptions of Twitter Research Ethics","abstract":"Social computing systems such as Twitter present new research sites that have provided billions of data points to researchers . However , the availability of public social media data has also presented ethical challenges . As the research community works to create ethical norms , we should be considering users\u2019 concerns as well . With this in mind , we report on an exploratory survey of Twitter users\u2019 perceptions of the use of tweets in research . Within our survey sample , few users were previously aware that their public tweets could be used by researchers , and the majority felt that researchers should not be able to use tweets without consent . However , we find that these attitudes are highly contextual , depending on factors such as how the research is conducted or disseminated , who is conducting it , and what the study is about . The findings of this study point to potential best practices for researchers conducting observation and analysis of public data . ","Method":["twitter"],"Task":["twitter research ethics"]},{"url":"https://www.semanticscholar.org/paper/520ec00dc35475e0554dbb72f27bd2eeb6f4191d","title":"The Secret Sharer : Evaluating and Testing Unintended Memorization in Neural Networks","abstract":"This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training - data sequences are unintentionally memorized by generative sequence models - - - a common type of machine - learning model . Because such models are sometimes trained on sensitive data (e . g . , the text of users\' private messages) , this methodology can benefit privacy by allowing deep - learning practitioners to select means of training that minimize such memorization . \\nIn experiments , we show that unintended memorization is a persistent , hard - to - avoid issue that can have serious consequences . Specifically , for models trained without consideration of memorization , we describe new , efficient procedures that can extract unique , secret sequences , such as credit card numbers . We show that our testing strategy is a practical and easy - to - use first line of defense , e . g . , by describing its application to quantitatively limit data exposure in Google\'s Smart Compose , a commercial text - completion neural network trained on millions of users\' email messages . ","Method":["secret sharer"],"Task":["data exposure"]},{"url":"https://www.semanticscholar.org/paper/df2df1749b93ba86328ec7b86ff7e8d30029e3f5","title":"Garbage in , garbage out? : do machine learning application papers in social computing report where human - labeled training data comes from?","abstract":"Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose , from hiring crowdworkers to the paper\'s authors labeling the data themselves . Such a task is quite similar to (or a form of) structured content analysis , which is a longstanding methodology in the social sciences and humanities , with many established best practices . In this paper , we investigate to what extent a sample of machine learning application papers in social computing - - - specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data - - - give specific details about whether such best practices were followed . Our team conducted multiple rounds of structured content analysis of each paper , making determinations such as : Does the paper report who the labelers were , what their qualifications were , whether they independently labeled the same items , whether inter - rater reliability metrics were disclosed , what level of training and/or instructions were given to labelers , whether compensation for crowdworkers is disclosed , and if the training data is publicly available . We find a wide divergence in whether such practices were followed and documented . Much of machine learning research and education focuses on what is done once a \\"gold standard\\" of training data is available , but we discuss issues around the equally - important aspect of whether such data is reliable in the first place . ","Method":["NOT DETECTED"],"Task":["ml classification task"]},{"url":"https://www.semanticscholar.org/paper/129cbad01be98ee88a930e31898cb76be79c41c1","title":"How NOT To Evaluate Your Dialogue System : An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation","abstract":"We investigate evaluation metrics for dialogue response generation systems where supervised labels , such as task completion , are not available . Recent works in response generation have adopted metrics from machine translation to compare a model\'s generated response to a single target response . We show that these metrics correlate very weakly with human judgements in the non - technical Twitter domain , and not at all in the technical Ubuntu domain . We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics , and provide recommendations for future development of better automatic evaluation metrics for dialogue systems . ","Method":["dialogue system"],"Task":["dialogue response generation","dialogue response generation systems"]},{"url":"https://www.semanticscholar.org/paper/5966d7c7f60898d610812e24c64d4d57855ad86a","title":"Semantics derived automatically from language corpora necessarily contain human biases","abstract":"Artificial intelligence and machine learning are in a period of astounding growth . However , there are concerns that these technologies may be used , either with or without intention , to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions . Here we show for the first time that human - like semantic biases result from the application of standard machine learning to ordinary language - - - the same sort of language humans are exposed to every day . We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well - known psychological studies . We replicate these using a widely used , purely statistical machine - learning model - - - namely , the GloVe word embedding - - - trained on a corpus of text from the Web . Our results indicate that language itself contains recoverable and accurate imprints of our historic biases , whether these are morally neutral as towards insects or flowers , problematic as towards race or gender , or even simply veridical , reflecting the status quo for the distribution of gender with respect to careers or first names . These regularities are captured by machine learning along with the rest of semantics . In addition to our empirical findings concerning language , we also contribute new methods for evaluating bias in text , the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT) . Our results have implications not only for AI and machine learning , but also for the fields of psychology , sociology , and human ethics , since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here . ","Method":["word embedding association test"],"Task":["artificial intelligence"]},{"url":"https://www.semanticscholar.org/paper/1935a5e3937753dc7db90126a221f11009c17984","title":"Algorithms of Oppression : How Search Engines Reinforce Racism","abstract":"Read and considered thoughtfully , Safiya Umoja Noble\u2019s Algorithms of Oppression : How Search Engines Reinforce Racism is devastating . It reduces to rubble the notion that technology is neutral and ideology - free . Noble\u2019s crushing the neutrality myth does several things . First , this act lays foundations for her argument : only if you recognize and understand that technology is built with , and integrates , bias , can you then be open to her primary thesis : search engines advance discriminatory and often racist content . Second , it banishes a convenient response for many self - identified meritocratic Silicon Valley \u201cwinners\u201d and their supporters . Postreading , some individuals may retain their beliefs in a neutral and ideology - free technology in spite of the overwhelming evidence and citations Noble brings to bear . Effective countering of Noble\u2019s claims is unlikely to occur . For professionals working in technology , information , argumentation , and/or rhetorical studies , Algorithms of Oppression is refreshing . Agonistic towards structural racism and its defenses , single - minded in its evidentiary presentation , collaborative in its acknowledgement of others\u2019 scholarship and research , Noble models many academic , critical , and social moves . Technology scholars and writers will find in Algorithms of Oppression a masterful mentor text on how to be an activist researcher scholar . Noble also makes this enjoyable reading . It is uncommon to find academic books that can simultaneously be read , used , and applied by academics and non - academics alike . ","Method":["neutrality myth"],"Task":["technology"]},{"url":"https://www.semanticscholar.org/paper/31a848022de5933029435a2c8304c2bd12537b0d","title":"The trouble with using provider assessments for rating clinical performance : it\'s a matter of bias . ","abstract":"The International Association for the Study of Pain has referred to pain as the fifth vital sign , and acute pain management after surgery has been shown to be a key factor in quality of recovery . In addition , the establishment of pain management benchmarks by the Joint Commission on the Accreditation of Healthcare Organizations more than a decade ago has resulted in a greater awareness of patients\u2019 right to optimal pain control by health care practitioners and administrators . Postoperative pain control has become a priority for hospitals across the United States . Optimization of postoperative pain management has been demonstrated through the implementation of protocols that include multimodal analgesic regimens , surgicalspecific treatment pathways , implementation of a 24 - hour anesthesiology pain service , and pain - specific training for physicians and nurses involved in postoperative care . 1 Importantly , pain as assessed by the numeric rating scale (NRS) , for which 0 = no pain and 10 = maximal pain , has been shown to be significantly reduced after the implementation of postoperative analgesia protocol . These data suggest that NRS pain scores might be a useful metric to evaluate the quality of anesthesia care provided after the transition from surgery to recovery . Wanderer et al . 2 from the Vanderbilt University have applied this principle in a research report in the current edition of Anesthesia & Analgesia in an attempt to use rank ordering by initial postanesthesia recovery unit NRS pain scores , as collected by nurses in a clinical setting , to compare supervising anesthesiologists when adjusted for confounding factors . The analysis included 26 680 cases using electronic documentation and excluded physician and nurse providers who had not cared for \u2265100 patients . When admission postanesthesia recovery unit scores were compared among anesthesiologists after adjusting for patient and surgical factors , only 6 . 4% of the 69 supervising anesthesiologists were found to differ in median postanesthesia recovery room admission pain scores . This finding clearly demonstrates that as presently assessed , initial postanesthesia recovery scores are a poor metric for identifying differences among supervising anesthesiologists , and the authors correctly concluded that they should not be included in the assessment of anesthesia care performance . Interestingly , the study also found that 16 of 66 (24%) recovery room nurses elicited median pain scores less than the median value for the entire group , and 33 nurses (50%) had elicited median pain scores significantly higher than the median for the entire group . These differences translated into a range of odds ratios from 0 . 16 (95% confidence interval , 0 . 11\u20132 . 4) for the lowest to 2 . 95 (95% confidence interval , 2 . 43\u20133 . 59) for the highest nurse compared with the nurse who ranked the median value for the overall group . In fact , NRS pain assessments using the 0 to 10 NRS pain score were found to depend more on the nurse making the assessment than patient age , gender and race , preoperative use of opioids , American Society of Anesthesiologists physical status , or procedure . This finding should not be interpreted to suggest dishonest recordings of NRS values by nurses (86%\u201390% of nurses accurately record pain ratings provided by patients) , but that personal opinions , knowledge , and attitudes toward pain strongly influence assessments and management . 3 Wanderer et al . discuss the use of nonstandard item descriptors rather than \u201cno pain\u201d equals 0 and \u201cthe worst possible pain\u201d equals 10 as anchors for the NRS pain scales as a possible explanation for the variability in the nurse assessor\u2013recorded NRS . They cited a recent systematic review that identified 24 distinct anchoring phrases in 54 included studies . 4 The use of anchors during decision making can create a form of cognitive bias because individuals tend to rely heavily on these points when making decision adjustments . Factors that have been shown to affect the influence of anchoring on decision making include happier moods and conscientious personalities , certainly desirable attributes in postanesthesia care nurses . These are factors that patients are likely to perceive , and substituting anchors could clearly influence the perceived value reported by patients . 5 The method of presentation of the NRS score range by the evaluator can be used to influence the choice made by the decision maker . This method is called the framing effect and is another type of cognitive bias . 6 The presenter in this situation is referred to as the choice architect . This practice is not an uncommon phenomenon when using Likert scales because the differences between scores in the range are not The Trouble with Using Provider Assessments for Rating Clinical Performance : It\u2019s a Matter of Bias","Method":["pain - specific training"],"Task":["postoperative pain control"]},{"url":"https://www.semanticscholar.org/paper/d3aca13c966bb22eed7086baeb287a64bc18c152","title":"Comparing Spoken Dialog Corpora Collected with Recruited Subjects versus Real Users","abstract":"Empirical spoken dialog research often involves the collection and analysis of a dialog corpus . However , it is not well understood whether and how a corpus of dialogs collected using recruited subjects differs from a corpus of dialogs obtained from real users . In this paper we use Let\u2019s Go Lab , a platform for experimenting with a deployed spoken dialog bus information system , to address this question . Our first corpus is collected by recruiting subjects to call Let\u2019s Go in a standard laboratory setting , while our second corpus consists of calls from real users calling Let\u2019s Go during its operating hours . We quantitatively characterize the two collected corpora using previously proposed measures from the spoken dialog literature , then discuss the statistically significant similarities and differences between the two corpora with respect to these measures . For example , we find that recruited subjects talk more and speak faster , while real users ask for more help and more frequently interrupt the system . In contrast , we find no difference with respect to dialog structure . ","Method":["NOT DETECTED"],"Task":["spoken dialog research"]},{"url":"https://www.semanticscholar.org/paper/e8fa186444d98a39ee9139b1f5dd0c7618caef8f","title":"Privacy - preserving Neural Representations of Text","abstract":"This article deals with adversarial attacks towards deep learning systems for Natural Language Processing (NLP) , in the context of privacy protection . We study a specific type of attack : an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text . Such scenario may arise in situations when the computation of a neural network is shared across multiple devices , e . g . some hidden representation is computed by a user\u2019s device and sent to a cloud - based model . We measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations . Finally , we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations . ","Method":["privacy - preserving neural representations of text"],"Task":["natural language processing"]},{"url":"https://www.semanticscholar.org/paper/a24d72bd0d08d515cb3e26f94131d33ad6c861db","title":"Ethical Challenges in Data - Driven Dialogue Systems","abstract":"The use of dialogue systems as a medium for human - machine interaction is an increasingly prevalent paradigm . A growing number of dialogue systems use conversation strategies that are learned from large datasets . There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data - driven training process . Here , we highlight potential ethical issues that arise in dialogue systems research , including : implicit biases in data - driven systems , the rise of adversarial examples , potential sources of privacy violations , safety concerns , special considerations for reinforcement learning systems , and reproducibility concerns . We also suggest areas stemming from these issues that deserve further investigation . Through this initial survey , we hope to spur research leading to robust , safe , and ethically sound dialogue systems . ","Method":["dialogue systems"],"Task":["data - driven dialogue systems"]},{"url":"https://www.semanticscholar.org/paper/ccf6a69a7f33bcf052aa7def176d3b9de495beb7","title":"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings","abstract":"The blind application of machine learning runs the risk of amplifying biases present in data . Such a danger is facing us with word embedding , a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks . We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent . This raises concerns because their widespread use , as we describe , often tends to amplify these biases . Geometrically , gender bias is first shown to be captured by a direction in the word embedding . Second , gender neutral words are shown to be linearly separable from gender definition words in the word embedding . Using these properties , we provide a methodology for modifying an embedding to remove gender stereotypes , such as the association between the words receptionist and female , while maintaining desired associations such as between the words queen and female . Using crowd - worker evaluation as well as standard benchmarks , we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks . The resulting embeddings can be used in applications without amplifying gender bias . ","Method":["word embedding"],"Task":["word embeddings"]},{"url":"https://www.semanticscholar.org/paper/94cf3f2c4410fcb06a90abebd99f7113c69e1ed9","title":"Lipstick on a Pig : Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them","abstract":"Word embeddings are widely used in NLP for a vast range of tasks . It was shown that word embeddings derived from text corpora reflect gender biases in society , causing serious concern . Several recent works tackle this problem , and propose methods for significantly reducing this gender bias in word embeddings , demonstrating convincing results . However , we argue that this removal is superficial . While the bias is indeed substantially reduced according to the provided bias definition , the actual effect is mostly hiding the bias , not removing it . The gender bias information is still reflected in the distances between \u201cgender - neutralized\u201d words in the debiased embeddings , and can be recovered from them . We present a series of experiments to support this claim , for two debiasing methods . We conclude that existing bias removal techniques are insufficient , and should not be trusted for providing gender - neutral modeling . ","Method":["debiasing methods"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/5e9c85235210b59a16bdd84b444a904ae271f7e7","title":"On Measuring Social Biases in Sentence Encoders","abstract":"The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human - like implicit biases based on gender , race , and other social constructs (Caliskan et al . , 2017) . Meanwhile , research on learning reusable text representations has begun to explore sentence - level texts , with some sentence encoders seeing enthusiastic adoption . Accordingly , we extend the Word Embedding Association Test to measure bias in sentence encoders . We then test several sentence encoders , including state - of - the - art methods such as ELMo and BERT , for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level . We observe mixed results including suspicious patterns of sensitivity that suggest the test\u2019s assumptions may not hold in general . We conclude by proposing directions for future work on measuring bias in sentence encoders . ","Method":["sentence encoders"],"Task":["measuring bias in sentence encoders"]},{"url":"https://www.semanticscholar.org/paper/039b1c1210c437f3b3ce6e0275ee2137bf5b951c","title":"Assessing Social and Intersectional Biases in Contextualized Word Representations","abstract":"Social bias in machine learning has drawn significant attention , with work ranging from demonstrations of bias in a multitude of applications , curating definitions of fairness for different contexts , to developing algorithms to mitigate bias . In natural language processing , gender bias has been shown to exist in context - free word embeddings . Recently , contextual word representations have outperformed word embeddings in several downstream NLP tasks . These word representations are conditioned on their context within a sentence , and can also be used to encode the entire sentence . In this paper , we analyze the extent to which state - of - the - art models for contextual word representations , such as BERT and GPT - 2 , encode biases with respect to gender , race , and intersectional identities . Towards this , we propose assessing bias at the contextual word level . This novel approach captures the contextual effects of bias missing in context - free word embeddings , yet avoids confounding effects that underestimate bias at the sentence encoding level . We demonstrate evidence of bias at the corpus level , find varying evidence of bias in embedding association tests , show in particular that racial bias is strongly encoded in contextual word models , and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities . Further , evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level , confirming the need for our novel approach . ","Method":["gpt - 2","contextualized word representations"],"Task":["social bias"]},{"url":"https://www.semanticscholar.org/paper/3259d52ae00e65b98391e7e6a2f672dfee721bf8","title":"Quantifying Social Biases in Contextual Word Representations","abstract":"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks . Since they are optimized to capture the statistical properties of training data , they tend to pick up on and amplify social stereotypes present in the data as well . In this study , we (1) propose a template - based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study , evaluating gender bias in a downstream task of Gender Pronoun Resolution . Although our case study focuses on gender bias , the proposed technique is generalizable to unveiling other biases , including in multiclass settings , such as racial and religious biases . ","Method":["bert"],"Task":["contextual word representations","gender pronoun resolution","bert;"]},{"url":"https://www.semanticscholar.org/paper/d08392eee17f809d32d7d37e9345383f41271164","title":"Sorting Things Out : Classification and Its Consequences","abstract":"What do a seventeenth - century mortality table (whose causes of death include \\"fainted in a bath , \\" \\"frighted , \\" and \\"itch\\"); the identification of South Africans during apartheid as European , Asian , colored , or black; and the separation of machine - from hand - washables have in common? All are examples of classification - - the scaffolding of information infrastructures . In Sorting Things Out , Geoffrey C . Bowker and Susan Leigh Star explore the role of categories and standards in shaping the modern world . In a clear and lively style , they investigate a variety of classification systems , including the International Classification of Diseases , the Nursing Interventions Classification , race classification under apartheid in South Africa , and the classification of viruses and of tuberculosis . The authors emphasize the role of invisibility in the process by which classification orders human interaction . They examine how categories are made and kept invisible , and how people can change this invisibility when necessary . They also explore systems of classification as part of the built information environment . Much as an urban historian would review highway permits and zoning decisions to tell a city\'s story , the authors review archives of classification design to understand how decisions have been made . Sorting Things Out has a moral agenda , for each standard and category valorizes some point of view and silences another . Standards and classifications produce advantage or suffering . Jobs are made and lost; some regions benefit at the expense of others . How these choices are made and how we think about that process are at the moral and political core of this work . The book is an important empirical source for understanding the building of information infrastructures . ","Method":["classification systems"],"Task":["classification"]},{"url":"https://www.semanticscholar.org/paper/d47a682723f710395454687319bb55635e653105","title":"Language (Technology) is Power : A Critical Survey of \u201cBias\u201d in NLP","abstract":"We survey 146 papers analyzing \u201cbias\u201d in NLP systems , finding that their motivations are often vague , inconsistent , and lacking in normative reasoning , despite the fact that analyzing \u201cbias\u201d is an inherently normative process . We further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of NLP . Based on these findings , we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in NLP systems . These recommendations rest on a greater recognition of the relationships between language and social hierarchies , encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d - - - i . e . , what kinds of system behaviors are harmful , in what ways , to whom , and why , as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by NLP systems , while interrogating and reimagining the power relations between technologists and such communities . ","Method":["nlp"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/493fac37cea49afb98c52c2f5dd75c303a325b25","title":"Mitigating Gender Bias in Natural Language Processing : Literature Review","abstract":"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity , it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes . Although NLP models have shown success in modeling various applications , they propagate and may even amplify gender bias found in text corpora . While the study of bias in artificial intelligence is not new , methods to mitigate gender bias in NLP are relatively nascent . In this paper , we review contemporary studies on recognizing and mitigating gender bias in NLP . We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias . Furthermore , we discuss the advantages and drawbacks of existing gender debiasing methods . Finally , we discuss future studies for recognizing and mitigating gender bias in NLP . ","Method":["nlp models"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7","title":"Gender Bias in Coreference Resolution","abstract":"We present an empirical study of gender bias in coreference resolution systems . We first introduce a novel , Winograd schema - style set of minimal pair sentences that differ only by pronoun gender . With these \u201cWinogender schemas , \u201d we evaluate and confirm systematic gender bias in three publicly - available coreference resolution systems , and correlate this bias with real - world and textual gender statistics . ","Method":["coreference resolution systems"],"Task":["coreference resolution"]},{"url":"https://www.semanticscholar.org/paper/0be19fd9896e5d40222c690cc3ff553adc7c0e27","title":"Gender Bias in Coreference Resolution : Evaluation and Debiasing Methods","abstract":"In this paper , we introduce a new benchmark for co - reference resolution focused on gender bias , WinoBias . Our corpus contains Winograd - schema style sentences with entities corresponding to people referred by their occupation (e . g . the nurse , the doctor , the carpenter) . We demonstrate that a rule - based , a feature - rich , and a neural coreference system all link gendered pronouns to pro - stereotypical entities with higher accuracy than anti - stereotypical entities , by an average difference of 21 . 1 in F1 score . Finally , we demonstrate a data - augmentation approach that , in combination with existing word - embedding debiasing techniques , removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets . ","Method":["winobias"],"Task":["coreference resolution","co - reference resolution"]},{"url":"https://www.semanticscholar.org/paper/5d4af8c9321168f9ba7a501f33fb019fa2deaa22","title":"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems","abstract":"Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases . Past work on examining inappropriate biases has largely focused on just individual systems . Further , there is no benchmark dataset for examining inappropriate biases in systems . Here for the first time , we present the Equity Evaluation Corpus (EEC) , which consists of 8 , 640 English sentences carefully chosen to tease out biases towards certain races and genders . We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task , SemEval - 2018 Task 1 \u2018Affect in Tweets\u2019 . We find that several of the systems show statistically significant bias; that is , they consistently provide slightly higher sentiment intensity predictions for one race or one gender . We make the EEC freely available . ","Method":["sentiment analysis systems"],"Task":["automatic sentiment analysis systems"]},{"url":"https://www.semanticscholar.org/paper/e85a50b523915b5fba3e3f1fdb743650f7d21bed","title":"Women\u2019s Syntactic Resilience and Men\u2019s Grammatical Luck : Gender - Bias in Part - of - Speech Tagging and Dependency Parsing","abstract":"Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender , but models for part - of - speech tagging and dependency parsing have still not adapted to account for these differences . To address this , we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles\u2019 authors , and build taggers and parsers trained on this data that show performance differences in text written by men and women . Further analyses reveal numerous part - of - speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data . The results underscore the importance of accounting for gendered differences in syntactic tasks , and outline future venues for developing more accurate taggers and parsers . We release our data to the research community . ","Method":["taggers"],"Task":["prediction","dependency parsing","part - of - speech tagging"]},{"url":"https://www.semanticscholar.org/paper/c590d2c8c2fb6ce5d32ee9165ab24171165f2b70","title":"Assessing gender bias in machine translation : a case study with Google Translate","abstract":"Recently there has been a growing concern in academia , industrial research laboratories and the mainstream commercial media about the phenomenon dubbed as machine bias , where trained statistical models\u2014unbeknownst to their creators\u2014grow to reflect controversial societal asymmetries , such as gender or racial bias . A significant number of Artificial Intelligence tools have recently been suggested to be harmfully biased toward some minority , with reports of racist criminal behavior predictors , Apple\u2019s Iphone X failing to differentiate between two distinct Asian people and the now infamous case of Google photos\u2019 mistakenly classifying black people as gorillas . Although a systematic study of such biases can be difficult , we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in AI . In this paper , we start with a comprehensive list of job positions from the U . S . Bureau of Labor Statistics (BLS) and used it in order to build sentences in constructions like \u201cHe/She is an Engineer\u201d (where \u201cEngineer\u201d is replaced by the job position of interest) in 12 different gender neutral languages such as Hungarian , Chinese , Yoruba , and several others . We translate these sentences into English using the Google Translate API , and collect statistics about the frequency of female , male and gender neutral pronouns in the translated output . We then show that Google Translate exhibits a strong tendency toward male defaults , in particular for fields typically associated to unbalanced gender distribution or stereotypes such as STEM (Science , Technology , Engineering and Mathematics) jobs . We ran these statistics against BLS\u2019 data for the frequency of female participation in each job position , in which we show that Google Translate fails to reproduce a real - world distribution of female workers . In summary , we provide experimental evidence that even if one does not expect in principle a 50 : 50 pronominal gender distribution , Google Translate yields male defaults much more frequently than what would be expected from demographic data alone . We believe that our study can shed further light on the phenomenon of machine bias and are hopeful that it will ignite a debate about the need to augment current statistical translation tools with debiasing techniques\u2014which can already be found in the scientific literature . ","Method":["google translate"],"Task":["machine translation","translation"]},{"url":"https://www.semanticscholar.org/paper/008e9001ea78e9654b5c43aeb818ea6cb06ea934","title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations","abstract":"The successful application of neural methods to machine translation has realized huge quality advances for the community . With these improvements , many have noted outstanding challenges , including the modeling and treatment of gendered language . While previous studies have identified issues using synthetic examples , we develop a novel technique to mine examples from real world data to explore challenges for deployed systems . We use our method to compile an evaluation benchmark spanning examples for four languages from three language families , which we publicly release to facilitate research . The examples in our benchmark expose where model representations are gendered , and the unintended consequences these gendered representations can have in downstream application . ","Method":["gendered representations"],"Task":["machine translation"]},{"url":"https://www.semanticscholar.org/paper/00059087c954c1af6ece33115315e3e0ecc2f2c2","title":"Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem","abstract":"Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men . In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality , particularly when the target language has grammatical gender . The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al , 2019) Ideally we would reduce system bias by simply debiasing all data prior to training , but achieving this effectively is itself a challenge . Rather than attempt to create a \u2018balanced\u2019 dataset , we use transfer learning on a small set of trusted , gender - balanced examples . This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch . A known pitfall of transfer learning on new domains is \u2018catastrophic forgetting\u2019 , which we address at adaptation and inference time . During adaptation we show that Elastic Weight Consolidation allows a performance trade - off between general translation quality and bias reduction . At inference time we propose a lattice - rescoring scheme which outperforms all systems evaluated in Stanovsky et al , 2019 on WinoMT with no degradation of general test set BLEU . We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability . ","Method":["elastic weight consolidation"],"Task":["neural machine translation"]},{"url":"https://www.semanticscholar.org/paper/40a6e8d8f253882c585f163b7333842d60ed6f14","title":"Toward Gender - Inclusive Coreference Resolution","abstract":"Correctly resolving textual mentions of people fundamentally entails making inferences about those people . Such inferences raise the risk of systemic biases in coreference resolution systems , including biases that can harm binary and non - binary trans and cis stakeholders . To better understand such biases , we foreground nuanced conceptualizations of gender from sociology and sociolinguistics , and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems . Through these studies , conducted on English text , we confirm that without acknowledging and building systems that recognize the complexity of gender , we build systems that lead to many potential harms . ","Method":["coreference resolution systems"],"Task":["coreference resolution","gender - inclusive coreference resolution"]},{"url":"https://www.semanticscholar.org/paper/7b8318894cbeca32f1ae55780a0903445a3f4ac6","title":"Man is to Person as Woman is to Location : Measuring Gender Bias in Named Entity Recognition","abstract":"In this paper , we study the bias in named entity recognition (NER) models - - - specifically , the difference in the ability to recognize male and female names as PERSON entity types . We evaluate NER models on a dataset containing 139 years of U . S . census baby names and find that relatively more female names , as opposed to male names , are not recognized as PERSON entities . The result of this analysis yields a new benchmark for gender bias evaluation in named entity recognition systems . The data and code for the application of this benchmark is publicly available for researchers to use . ","Method":["NOT DETECTED"],"Task":["named entity recognition","ner","named entity recognition systems"]},{"url":"https://www.semanticscholar.org/paper/57032c1e327c88a53ab41c17e91bf1406f9ef5c9","title":"Mind the GAP : A Balanced Corpus of Gendered Ambiguous Pronouns","abstract":"Coreference resolution is an important task for natural language understanding , and the resolution of ambiguous pronouns a longstanding challenge . Nonetheless , existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models . Furthermore , we find gender bias in existing corpora and systems favoring masculine entities . To address this , we present and release GAP , a gender - balanced labeled corpus of 8 , 908 ambiguous pronoun\u2013name pairs sampled to provide diverse coverage of challenges posed by real - world text . We explore a range of baselines that demonstrate the complexity of the challenge , the best achieving just 66 . 9% F1 . We show that syntactic structure and continuous neural models provide promising , complementary cues for approaching the challenge . ","Method":["gap"],"Task":["coreference resolution","resolution of ambiguous pronouns"]},{"url":"https://www.semanticscholar.org/paper/a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd","title":"Measuring Bias in Contextualized Word Representations","abstract":"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks . Since they are optimized to capture the statistical properties of training data , they tend to pick up on and amplify social stereotypes present in the data as well . In this study , we (1) propose a template - based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study , evaluating gender bias in a downstream task of Gender Pronoun Resolution . Although our case study focuses on gender bias , the proposed technique is generalizable to unveiling other biases , including in multiclass settings , such as racial and religious biases . ","Method":["bert"],"Task":["gender pronoun resolution"]},{"url":"https://www.semanticscholar.org/paper/0f4bcebc95548a7286106b67bf1115802f093469","title":"Mitigating Gender Bias Amplification in Distribution by Posterior Regularization","abstract":"Advanced machine learning techniques have boosted the performance of natural language processing . Nevertheless , recent studies , e . g . , (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it . However , their analysis is conducted only on models\u2019 top predictions . In this paper , we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels . We further propose a bias mitigation approach based on posterior regularization . With little performance loss , our method can almost remove the bias amplification in the distribution . Our study sheds the light on understanding the bias amplification . ","Method":["bias mitigation approach"],"Task":["natural language processing"]},{"url":"https://www.semanticscholar.org/paper/a20ecabd83e0962329448d8af5025b8061c4ba36","title":"Social Bias in Elicited Natural Language Inferences","abstract":"We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data . The SNLI human - elicitation protocol makes it prone to amplifying bias and stereotypical associations , which we demonstrate statistically (using pointwise mutual information) and with qualitative examples . ","Method":["NOT DETECTED"],"Task":["social bias"]},{"url":"https://www.semanticscholar.org/paper/219b7266ae848937da170c5510b2bfc66d17859a","title":"Racial disparities in automated speech recognition","abstract":"Significance Automated speech recognition (ASR) systems are now used in a variety of applications to convert spoken language to text , from virtual assistants , to closed captioning , to hands - free computing . By analyzing a large corpus of sociolinguistic interviews with white and African American speakers , we demonstrate large racial disparities in the performance of five popular commercial ASR systems . Our results point to hurdles faced by African Americans in using increasingly widespread tools driven by speech recognition technology . More generally , our work illustrates the need to audit emerging machine - learning systems to ensure they are broadly inclusive . Automated speech recognition (ASR) systems , which use sophisticated machine - learning algorithms to convert spoken language to text , have become increasingly widespread , powering popular virtual assistants , facilitating automated closed captioning , and enabling digital dictation platforms for health care . Over the last several years , the quality of these systems has dramatically improved , due both to advances in deep learning and to the collection of large - scale datasets used to train the systems . There is concern , however , that these tools do not work equally well for all subgroups of the population . Here , we examine the ability of five state - of - the - art ASR systems\u2014developed by Amazon , Apple , Google , IBM , and Microsoft\u2014to transcribe structured interviews conducted with 42 white speakers and 73 black speakers . In total , this corpus spans five US cities and consists of 19 . 8 h of audio matched on the age and gender of the speaker . We found that all five ASR systems exhibited substantial racial disparities , with an average word error rate (WER) of 0 . 35 for black speakers compared with 0 . 19 for white speakers . We trace these disparities to the underlying acoustic models used by the ASR systems as the race gap was equally large on a subset of identical phrases spoken by black and white individuals in our corpus . We conclude by proposing strategies\u2014such as using more diverse training datasets that include African American Vernacular English\u2014to reduce these performance differences and ensure speech recognition technology is inclusive . ","Method":["asr systems"],"Task":["automated speech recognition","asr"]},{"url":"https://www.semanticscholar.org/paper/1080dc00733e010fdd6a9b999506a0d4d864519d","title":"Effects of Talker Dialect , Gender & Race on Accuracy of Bing Speech and YouTube Automatic Captions","abstract":"This project compares the accuracy of two automatic speech recognition (ASR) systems\u2013Bing Speech and YouTube\u2019s automatic captions\u2013across gender , race and four dialects of American English . The dialects included were chosen for their acoustic dissimilarity . Bing Speech had differences in word error rate (WER) between dialects and ethnicities , but they were not statistically reliable . YouTube\u2019s automatic captions , however , did have statistically different WERs between dialects and races . The lowest average error rates were for General American and white talkers , respectively . Neither system had a reliably different WER between genders , which had been previously reported for YouTube\u2019s automatic captions [1] . However , the higher error rate non - white talkers is worrying , as it may reduce the utility of these systems for talkers of color . ","Method":["automatic speech recognition"],"Task":["NOT DETECTED"]},{"url":"https://www.semanticscholar.org/paper/21e59098bb5e36175f653d5142442d061669d07f","title":"Race as a Bundle of Sticks : Designs that Estimate Effects of Seemingly Immutable Characteristics","abstract":"Although understanding the role of race , ethnicity , and identity is central to political science , methodological debates persist about whether it is possible to estimate the effect of something immutable . At the heart of the debate is an older theoretical question : Is race best understood under an essentialist or constructivist framework? In contrast to the \u201cimmutable characteristics\u201d or essentialist approach , we argue that race should be operationalized as a \u201cbundle of sticks\u201d that can be disaggregated into elements . With elements of race , causal claims may be possible using two designs : (a) studies that measure the effect of exposure to a racial cue and (b) studies that exploit within - group variation to measure the effect of some manipulable element . These designs can reconcile scholarship on race and causation and offer a clear framework for future research . ","Method":["essentialist or constructivist framework?"],"Task":["political science"]},{"url":"https://www.semanticscholar.org/paper/8417424bf9fe7a67f06f15c487403e953ab24a96","title":"Men Also Like Shopping : Reducing Gender Bias Amplification using Corpus - level Constraints","abstract":"Language is increasingly being used to de - fine rich visual recognition problems with supporting image collections sourced from the web . Structured prediction models are used in these tasks to take advantage of correlations between co - occurring labels and visual input but risk inadvertently encoding social biases found in web corpora . In this work , we study data and models associated with multilabel object classification and visual semantic role labeling . We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias . For example , the activity cooking is over 33% more likely to involve females than males in a training set , and a trained model further amplifies the disparity to 68% at test time . We propose to inject corpus - level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference . Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47 . 5% and 40 . 5% for multilabel classification and visual semantic role labeling , respectively\u3002","Method":["structured prediction models"],"Task":["multilabel classification","multilabel object classification"]},{"url":"https://www.semanticscholar.org/paper/0ab2fb6c850bd1c5882deb4984d37b4ccbee580c","title":"Towards Understanding Gender Bias in Relation Extraction","abstract":"Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction . While much attention has been dedicated towards improvements in accuracy , there have been no attempts in the literature to evaluate social biases exhibited in NRE systems . In this paper , we create WikiGenderBias , a distantly supervised dataset composed of over 45 , 000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems . We find that when extracting spouse - of and hypernym (i . e . , occupation) relations , an NRE system performs differently when the gender of the target entity is different . However , such disparity does not appear when extracting relations such as birthDate or birthPlace . We also analyze how existing bias mitigation techniques , such as name anonymization , word embedding debiasing , and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases . Unfortunately , due to NRE models rely heavily on surface level cues , we find that existing bias mitigation approaches have a negative effect on NRE . Our analysis lays groundwork for future quantifying and mitigating bias in NRE . ","Method":["nre"],"Task":["nre","neural relation extraction","relation extraction systems","relation extraction"]},{"url":"https://www.semanticscholar.org/paper/59e94c9f21937643678ff494901f3d8b22af4e2f","title":"Racial Disparity in Natural Language Processing : A Case Study of Social Media African - American English","abstract":"We highlight an important frontier in algorithmic fairness : disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups . For example , current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males . We conduct an empirical analysis of racial disparity in language identification for tweets written in African - American English , and discuss implications of disparity in NLP . ","Method":["natural language processing algorithms"],"Task":["racial disparity","natural language processing"]},{"url":"https://www.semanticscholar.org/paper/187608bf94b2dccd25d1266ed925abf7b55dbb2e","title":"Re - imagining Algorithmic Fairness in India and Beyond","abstract":"Conventional algorithmic fairness is West - centric , as seen in its subgroups , values , and methods . In this paper , we de - center algorithmic fairness and analyse AI power in India . Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India , we find that several assumptions of algorithmic fairness are challenged . We find that in India , data is not always reliable due to socio - economic factors , ML makers appear to follow double standards , and AI evokes unquestioning aspiration . We contend that localising model fairness alone can be window dressing in India , where the distance between models and oppressed communities is large . Instead , we re - imagine algorithmic fairness in India and provide a roadmap to re - contextualise data and models , empower oppressed communities , and enable Fair - ML ecosystems . ","Method":["fair - ml ecosystems"],"Task":["re - imagining algorithmic fairness"]},{"url":"https://www.semanticscholar.org/paper/6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots : Can Language Models Be Too Big? \ud83e\udd9c","abstract":"The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models , especially for English . BERT , its variants , GPT - 2/3 , and others , most recently Switch - C , have pushed the boundaries of the possible both through architectural innovations and through sheer size . Using these pretrained models and the methodology of fine - tuning them for specific tasks , researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English . In this paper , we take a step back and ask : How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first , investing resources into curating and carefully documenting datasets rather than ingesting everything on the web , carrying out pre - development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values , and encouraging research directions beyond ever larger language models . ","Method":["gpt - 2/3"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/fb73b93de3734a996829caf31e4310e0054e9c6b","title":"Green AI","abstract":"Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets . ","Method":["NOT DETECTED"],"Task":["green ai"]},{"url":"https://www.semanticscholar.org/paper/13f25c69973373e616c48688d06a6b6ae2736ef0","title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning","abstract":"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research . We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions , as well as generating standardized online appendices . Utilizing this framework , we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning . Finally , based on case studies using our framework , we propose strategies for mitigation of carbon emissions and reduction of energy consumption . By making accounting easier , we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms . ","Method":["leaderboard"],"Task":["machine learning"]},{"url":"https://www.semanticscholar.org/paper/3cc2f69951cd24fe61be4cf32d62afbac297bc2b","title":"Social Biases in NLP Models as Barriers for Persons with Disabilities","abstract":"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models . In particular , representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained . In this paper , we present evidence of such undesirable biases towards mentions of disability in two different English language models : toxicity prediction and sentiment analysis . Next , we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability . We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance , gun violence , homelessness , and drug addiction are over - represented in texts discussing mental illness . ","Method":["nlp models"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/10391eed628dfece8a9136f76c5df53b5704422d","title":"Social Chemistry 101 : Learning to Reason about Social and Moral Norms","abstract":"Social norms - - - the unspoken commonsense rules about acceptable social behavior - - - are crucial in understanding the underlying causes and intents of people\'s actions in narratives . For example , underlying an action such as \\"wanting to call cops on my neighbors\\" are social norms that inform our conduct , such as \\"It is expected that you report crimes . \\" \\nWe present Social Chemistry , a new conceptual formalism to study people\'s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language . We introduce Social - Chem - 101 , a large - scale corpus that catalogs 292k rules - of - thumb such as \\"it is rude to run a blender at 5am\\" as the basic conceptual units . Each rule - of - thumb is further broken down with 12 different dimensions of people\'s judgments , including social judgments of good and bad , moral foundations , expected cultural pressure , and assumed legality , which together amount to over 4 . 5 million annotations of categorical labels and free - text descriptions . \\nComprehensive empirical results based on state - of - the - art neural models demonstrate that computational modeling of social norms is a promising research direction . Our model framework , Neural Norm Transformer , learns and generalizes Social - Chem - 101 to successfully reason about previously unseen situations , generating relevant (and potentially novel) attribute - aware social rules - of - thumb . ","Method":["social - chem - 101","neural norm transformer","social chemistry 101"],"Task":["attribute - aware social rules - of - thumb"]},{"url":"https://www.semanticscholar.org/paper/8755c15fe073c6af03664b2a74aafef1fed5f198","title":"BERT has a Moral Compass : Improvements of ethical and moral values of machines","abstract":"Allowing machines to choose whether to kill humans would be devastating for world peace and security . But how do we equip machines with the ability to learn ethical or even moral choices? Jentzsch et al . (2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about \\"right\\" and \\"wrong\\" conduct by calculating a moral bias score on a sentence level using sentence embeddings . The machine learned that it is objectionable to kill living beings , but it is fine to kill time; It is essential to eat , yet one might not eat dirt; it is important to spread information , yet one should not spread misinformation . However , the evaluated moral bias was restricted to simple actions - - one verb - - and a ranking of actions with surrounding context . Recently BERT - - - and variants such as RoBERTa and SBERT - - - has set a new state - of - the - art performance for a wide range of NLP tasks . But has BERT also a better moral compass? In this paper , we discuss and show that this is indeed the case . Thus , recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine . We argue that through an advanced semantic representation of text , BERT allows one to get better insights of moral and ethical values implicitly represented in text . This enables the Moral Choice Machine (MCM) to extract more accurate imprints of moral choices and ethical values . ","Method":["bert"],"Task":["nlp tasks"]},{"url":"https://www.semanticscholar.org/paper/2a1573cfa29a426c695e2caf6de0167a12b788ef","title":"Open Problems in Cooperative AI","abstract":"Problems of cooperation - - in which agents seek ways to jointly improve their welfare - - are ubiquitous and important . They can be found at scales ranging from our daily routines - - such as driving on highways , scheduling meetings , and working collaboratively - - to our global challenges - - such as peace , commerce , and pandemic preparedness . Arguably , the success of the human species is rooted in our ability to cooperate . Since machines powered by artificial intelligence are playing an ever greater role in our lives , it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation . \\nWe see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems , which we term Cooperative AI . The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems . Central goals include building machine agents with the capabilities needed for cooperation , building tools to foster cooperation in populations of (machine and/or human) agents , and otherwise conducting AI research for insight relevant to problems of cooperation . This research integrates ongoing work on multi - agent systems , game theory and social choice , human - machine interaction and alignment , natural - language processing , and the construction of social tools and platforms . However , Cooperative AI is not the union of these existing areas , but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas . We see opportunity to more explicitly focus on the problem of cooperation , to construct unified theory and vocabulary , and to build bridges with adjacent communities working on cooperation , including in the natural , social , and behavioural sciences . ","Method":["artificial intelligence"],"Task":["cooperative ai"]},{"url":"https://www.semanticscholar.org/paper/ced289065723368bca48636edf71eeed50f40a39","title":"Existential Risk Prevention as Global Priority","abstract":"risks are those that threaten the entire future of humanity . Many theories of value imply that even relatively small reductions in net existential risk have enormous expected value . Despite their importance , issues surrounding human - extinction risks and related hazards remain poorly understood . In this article , I clarify the concept of existential risk and develop an improved classification scheme . I discuss the relation between existential risks and basic issues in axiology , and show how existential risk reduction (via the maxipok rule) can serve as a strongly action - guiding principle for utilitarian concerns . I also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability . Policy Implications \u2022 Existential risk is a concept that can focus long - term global efforts and sustainability concerns . \u2022 The biggest existential risks are anthropogenic and related to potential future technologies . \u2022 A moral case can be made that existential risk reduction is strictly more important than any other global public good . \u2022 Sustainability should be reconceptualised in dynamic terms , as aiming for a sustainable trajectory rather than a sus - tainable state . \u2022 Some small existential risks can be mitigated today directly (e . g . asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity\'s ability to deal with the larger existential risks that will arise later in this century . This will require collective wisdom , technology foresight , and the ability when necessary to mobilise a strong global coordi - nated response to anticipated existential risks . \u2022 Perhaps the most cost - effective way to reduce existential risks today is to fund analysis of a wide range of existen - tial risks and potential mitigation strategies , with a long - term perspective . ","Method":["existential risk reduction"],"Task":["existential risk prevention"]},{"url":"https://www.semanticscholar.org/paper/8763723e27cc1d4aad166b5e1d9cb0fc8c8043dd","title":"Participatory Problem Formulation for Fairer Machine Learning Through Community Based System Dynamics","abstract":"Recent research on algorithmic fairness has highlighted that the problem formulation phase of ML system development can be a key source of bias that has significant downstream impacts on ML system fairness outcomes . However , very little attention has been paid to methods for improving the fairness efficacy of this critical phase of ML system development . Current practice neither accounts for the dynamic complexity of high - stakes domains nor incorporates the perspectives of vulnerable stakeholders . In this paper we introduce community based system dynamics (CBSD) as an approach to enable the participation of typically excluded stakeholders in the problem formulation phase of the ML system development process and facilitate the deep problem understanding required to mitigate bias during this crucial stage . ","Method":["community based system dynamics"],"Task":["fairer machine learning"]},{"url":"https://www.semanticscholar.org/paper/40141f0933b5111b089049e226dc8d969b0a7fca","title":"A Research Framework for Understanding Education - Occupation Alignment with NLP Techniques","abstract":"Understanding the gaps between job requirements and university curricula is crucial for improving student success and institutional effectiveness in higher education . In this context , natural language processing (NLP) can be leveraged to generate granular insights into where the gaps are and how they change . This paper proposes a three - dimensional research framework that combines NLP techniques with economic and educational research to quantify the alignment between course syllabi and job postings . We elaborate on key technical details of the framework and further discuss its potential positive impacts on practice , including unveiling the inequalities in and long - term consequences of education - occupation alignment to inform policymakers , and fostering information systems to support students , institutions and employers in the school - to - work pipeline . ","Method":["nlp techniques"],"Task":["natural language processing"]},{"url":"https://www.semanticscholar.org/paper/8d9a678c56b9085de65024aa2f6b406ccad97390","title":"A Grounded Well - being Conversational Agent with Multiple Interaction Modes : Preliminary Results","abstract":"Technologies for enhancing well - being , healthcare vigilance and monitoring are on the rise . However , despite patient interest , such technologies suffer from low adoption . One hypothesis for this limited adoption is loss of human interaction that is central to doctor - patient encounters . In this paper we seek to address this limitation via a conversational agent that adopts one aspect of in - person doctor - patient interactions : A human avatar to facilitate medical grounded question answering . This is akin to the in - person scenario where the doctor may point to the human body or the patient may point to their own body to express their conditions . Additionally , our agent has multiple interaction modes , that may give more options for the patient to use the agent , not just for medical question answering , but also to engage in conversations about general topics and current events . Both the avatar , and the multiple interaction modes could help improve adherence . We present a high level overview of the design of our agent , Marie Bot Wellbeing . We also report implementation details of our early prototype , and present preliminary results . ","Method":["bot","grounded well - being conversational agent"],"Task":["medical question answering","medical grounded question answering"]},{"url":"https://www.semanticscholar.org/paper/934dbfbb33cbec11fc825db56ac85a48fc52158f","title":"A Speech - enabled Fixed - phrase Translator for Healthcare Accessibility","abstract":"In this overview article we describe an application designed to enable communication between health practitioners and patients who do not share a common language , in situations where professional interpreters are not available . Built on the principle of a fixed phrase translator , the application implements different natural language processing (NLP) technologies , such as speech recognition , neural machine translation and text - to - speech to improve usability . Its design allows easy portability to new domains and integration of different types of output for multiple target audiences . Even though BabelDr is far from solving the problem of miscommunication between patients and doctors , it is a clear example of NLP in a real world application designed to help minority groups to communicate in a medical context . It also gives some insights into the relevant criteria for the development of such an application . ","Method":["babeldr"],"Task":["machine translation"]},{"url":"https://www.semanticscholar.org/paper/54db327cd53fe043449c9f242d3fc34c593a70ef","title":"Analyzing Stereotypes in Generative Text Inference Tasks","abstract":"Stereotypes are inferences drawn about people based on their demographic attributes , which may result in harms to users when a system is deployed . In generative language - inference tasks , given a premise , a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference) . Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes . In our work , we study how stereotypes manifest when the potential targets of stereotypes are situated in real - life , neutral contexts . We collect human judgments on the presence of stereotypes in generated inferences , and compare how perceptions of stereotypes vary due to annotator positionality . ","Method":["NOT DETECTED"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c","title":"Demographic Dialectal Variation in Social Media : A Case Study of African - American English","abstract":"Though dialectal language is increasingly abundant on social media , few resources exist for developing NLP tools to handle such language . We conduct a case study of dialectal language in online conversational text by investigating African - American English (AAE) on Twitter . We propose a distantly supervised model to identify AAE - like language from demographics associated with geo - located messages , and we verify that this language follows well - known AAE linguistic phenomena . In addition , we analyze the quality of existing language identification and dependency parsing tools on AAE - like text , demonstrating that they perform poorly on such text compared to text associated with white speakers . We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE - like language . ","Method":["aae"],"Task":["language identification"]},{"url":"https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP","abstract":"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data . These models have obtained notable gains in accuracy across many NLP tasks . However , these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption . As a result these models are costly to train and develop , both financially , due to the cost of hardware and electricity or cloud compute time , and environmentally , due to the carbon footprint required to fuel modern tensor processing hardware . In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP . Based on these findings , we propose actionable recommendations to reduce costs and improve equity in NLP research and practice . ","Method":["large networks"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/d6a25d8726c5484bb224a3350528aae9fcaae65f","title":"Automatic Sentence Simplification in Low Resource Settings for Urdu","abstract":"To build automated simplification systems , corpora of complex sentences and their simplified versions is the first step to understand sentence complexity and enable the development of automatic text simplification systems . We present a lexical and syntactically simplified Urdu simplification corpus with a detailed analysis of the various simplification operations and human evaluation of corpus quality . We further analyze our corpora using text readability measures and present a comparison of the original , lexical simplified and syntactically simplified corpora . In addition , we compare our corpus with other existing simplification corpora by building simplification systems and evaluating these systems using BLEU and SARI scores . Our system achieves the highest BLEU score and comparable SARI score in comparison to other systems . We release our simplification corpora for the benefit of the research community . ","Method":["simplification systems"],"Task":["automatic sentence simplification"]},{"url":"https://www.semanticscholar.org/paper/4975c64466149c72f31489fadbbbff4e85d7b3f3","title":"Cartography of Natural Language Processing for Social Good (NLP4SG) : Searching for Definitions , Statistics and White Spots","abstract":"The range of works that can be considered as developing NLP for social good (NLP4SG) is enormous . While many of them target the identification of hate speech or fake news , there are others that address , e . g . , text simplification to alleviate consequences of dyslexia , or coaching strategies to fight depression . However , so far , there is no clear picture of what areas are targeted by NLP4SG , who are the actors , which are the main scenarios and what are the topics that have been left aside . In order to obtain a clearer view in this respect , we first propose a working definition of NLP4SG and identify some primary aspects that are crucial for NLP4SG , including , e . g . , areas , ethics , privacy and bias . Then , we draw upon a corpus of around 50 , 000 articles downloaded from the ACL Anthology . Based on a list of keywords retrieved from the literature and revised in view of the task , we select from this corpus articles that can be considered to be on NLP4SG according to our definition and analyze them in terms of trends along the time line , etc . The result is a map of the current NLP4SG research and insights concerning the white spots on this map . ","Method":["nlp4sg"],"Task":["identification of hate speech"]},{"url":"https://www.semanticscholar.org/paper/c39fc494cb84b260eaa33cceda2b76512f3701a5","title":"Breaking Down Walls of Text : How Can NLP Benefit Consumer Privacy?","abstract":"Privacy plays a crucial role in preserving democratic ideals and personal autonomy . The dominant legal approach to privacy in many jurisdictions is the \u201cNotice and Choice\u201d paradigm , where privacy policies are the primary instrument used to convey information to users . However , privacy policies are long and complex documents that are difficult for users to read and comprehend . We discuss how language technologies can play an important role in addressing this information gap , reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies : consumers , enterprises , and regulators . Our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy , limit privacy harms , and rally research efforts from the community towards addressing an issue with large social impact . We highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies . ","Method":["nlp"],"Task":["consumer privacy? privacy"]},{"url":"https://www.semanticscholar.org/paper/9995132dda17b36e5513c8e98d58ff992d0ba79a","title":"Are we human , or are we users? The role of natural language processing in human - centric news recommenders that nudge users to diverse content","abstract":"In this position paper , we present a research agenda and ideas for facilitating exposure to diverse viewpoints in news recommendation . Recommending news from diverse viewpoints is important to prevent potential filter bubble effects in news consumption , and stimulate a healthy democratic debate . To account for the complexity that is inherent to humans as citizens in a democracy , we anticipate (among others) individual - level differences in acceptance of diversity . We connect this idea to techniques in Natural Language Processing , where distributional language models would allow us to place different users and news articles in a multidimensional space based on semantic content , where diversity is operationalized as distance and variance . In this way , we can model individual \u201clatitudes of diversity\u201d for different users , and thus personalize viewpoint diversity in support of a healthy public debate . In addition , we identify technical , ethical and conceptual issues related to our presented ideas . Our investigation describes how NLP can play a central role in diversifying news recommendations . ","Method":["nlp"],"Task":["diversifying news recommendations"]},{"url":"https://www.semanticscholar.org/paper/03c046041bc509f2cc9671ee71a78642275b77c3","title":"Challenges for Information Extraction from Dialogue in Criminal Law","abstract":"Information extraction and question answering have the potential to introduce a new paradigm for how machine learning is applied to criminal law . Existing approaches generally use tabular data for predictive metrics . An alternative approach is needed for matters of equitable justice , where individuals are judged on a case - by - case basis , in a process involving verbal or written discussion and interpretation of case factors . Such discussions are individualized , but they nonetheless rely on underlying facts . Information extraction can play an important role in surfacing these facts , which are still important to understand . We analyze unsupervised , weakly supervised , and pre - trained models\u2019 ability to extract such factual information from the free - form dialogue of California parole hearings . With a few exceptions , most F1 scores are below 0 . 85 . We use this opportunity to highlight some opportunities for further research for information extraction and question answering . We encourage new developments in NLP to enable analysis and review of legal cases to be done in a post - hoc , not predictive , manner . ","Method":["unsupervised"],"Task":["question answering"]},{"url":"https://www.semanticscholar.org/paper/50718d6bd163967b8353de4c854ed866b2b56c2f","title":"Conversations Gone Alright : Quantifying and Predicting Prosocial Outcomes in Online Conversations","abstract":"Online conversations can go in many directions : some turn out poorly due to antisocial behavior , while others turn out positively to the benefit of all . Research on improving online spaces has focused primarily on detecting and reducing antisocial behavior . Yet we know little about positive outcomes in online conversations and how to increase them\u2014is a prosocial outcome simply the lack of antisocial behavior or something more? Here , we examine how conversational features lead to prosocial outcomes within online discussions . We introduce a series of new theory - inspired metrics to define prosocial outcomes such as mentoring and esteem enhancement . Using a corpus of 26M Reddit conversations , we show that these outcomes can be forecasted from the initial comment of an online conversation , with the best model providing a relative 24% improvement over human forecasting performance at ranking conversations for predicted outcome . Our results indicate that platforms can use these early cues in their algorithmic ranking of early conversations to prioritize better outcomes . ","Method":["NOT DETECTED"],"Task":["online spaces"]},{"url":"https://www.semanticscholar.org/paper/52a14b391f994d83759787500a9bda865acdb3c5","title":"Recommender systems and their ethical challenges","abstract":"This article presents the first , systematic analysis of the ethical challenges posed by recommender systems through a literature review . The article identifies six areas of concern , and maps them onto a proposed taxonomy of different kinds of ethical impact . The analysis uncovers a gap in the literature : currently user - centred approaches do not consider the interests of a variety of other stakeholders\u2014as opposed to just the receivers of a recommendation\u2014in assessing the ethical impacts of a recommender system . ","Method":["recommender systems"],"Task":["ethical challenges"]},{"url":"https://www.semanticscholar.org/paper/ff3b0be4fb7debf1312c92381577292288755674","title":"Conversational receptiveness : Improving engagement with opposing views","abstract":"Abstract We examine \u201cconversational receptiveness\u201d \u2013 the use of language to communicate one\u2019s willingness to thoughtfully engage with opposing views . We develop an interpretable machine - learning algorithm to identify the linguistic profile of receptiveness (Studies 1A - B) . We then show that in contentious policy discussions , government executives who were rated as more receptive - according to our algorithm and their partners , but not their own self - evaluations - were considered better teammates , advisors , and workplace representatives (Study 2) . Furthermore , using field data from a setting where conflict management is endemic to productivity , we show that conversational receptiveness at the beginning of a conversation forestalls conflict escalation at the end . Specifically , Wikipedia editors who write more receptive posts are less prone to receiving personal attacks from disagreeing editors (Study 3) . We develop a \u201creceptiveness recipe\u201d intervention based on our algorithm . We find that writers who follow the recipe are seen as more desirable partners for future collaboration and their messages are seen as more persuasive (Study 4) . Overall , we find that conversational receptiveness is reliably measurable , has meaningful relational consequences , and can be substantially improved using our intervention (183 words) . ","Method":["conversational receptiveness"],"Task":["conflict management"]},{"url":"https://www.semanticscholar.org/paper/e511b338559a1df846059068ce7cc64c7066be4c","title":"Empathy and Hope : Resource Transfer to Model Inter - country Social Media Dynamics","abstract":"The ongoing COVID - 19 pandemic resulted in significant ramifications for international relations ranging from travel restrictions , global ceasefires , and international vaccine production and sharing agreements . Amidst a wave of infections in India that resulted in a systemic breakdown of healthcare infrastructure , a social welfare organization based in Pakistan offered to procure medical - grade oxygen to assist India - a nation which was involved in four wars with Pakistan in the past few decades . In this paper , we focus on Pakistani Twitter users\u2019 response to the ongoing healthcare crisis in India . While #IndiaNeedsOxygen and #PakistanStandsWithIndia featured among the top - trending hashtags in Pakistan , divisive hashtags such as #EndiaSaySorryToKashmir simultaneously started trending . Against the backdrop of a contentious history including four wars , divisive content of this nature , especially when a country is facing an unprecedented healthcare crisis , fuels further deterioration of relations . In this paper , we define a new task of detecting supportive content and demonstrate that existing NLP for social impact tools can be effectively harnessed for such tasks within a quick turnaround time . We also release the first publicly available data set at the intersection of geopolitical relations and a raging pandemic in the context of India and Pakistan . ","Method":["nlp"],"Task":["covid - 19 pandemic"]},{"url":"https://www.semanticscholar.org/paper/022b80b663c51563a1c6772c12ada3c79f5d798d","title":"Guiding Principles for Participatory Design - inspired Natural Language Processing","abstract":"We introduce 9 guiding principles to integrate Participatory Design (PD) methods in the development of Natural Language Processing (NLP) systems . The adoption of PD methods by NLP will help to alleviate issues concerning the development of more democratic , fairer , less - biased technologies to process natural language data . This short paper is the outcome of an ongoing dialogue between designers and NLP experts and adopts a non - standard format following previous work by Traum (2000); Bender (2013); Abzianidze and Bos (2019) . Every section is a guiding principle . While principles 1\u20133 illustrate assumptions and methods that inform community - based PD practices , we used two fictional design scenarios (Encinas and Blythe , 2018) , which build on top of situations familiar to the authors , to elicit the identification of the other 6 . Principles 4\u20136 describes the impact of PD methods on the design of NLP systems , targeting two critical aspects : data collection & annotation , and the deployment & evaluation . Finally , principles 7\u20139 guide a new reflexivity of the NLP research with respect to its context , actors and participants , and aims . We hope this guide will offer inspiration and a road - map to develop a new generation of PD - inspired NLP . ","Method":["nlp"],"Task":["nlp","natural language processing","participatory design - inspired natural language processing"]},{"url":"https://www.semanticscholar.org/paper/203bdaca3986b51f8d011422c04ff1489e425ce5","title":"Detecting Hashtag Hijacking for Hashtag Activism","abstract":"Social media has changed the way we engage in social activities . On Twitter , users can participate in social movements using hashtags such as #MeToo; this is known as hashtag activism . However , while these hashtags can help reshape social norms , they can also be used maliciously by spammers or troll communities for other purposes , such as signal boosting unrelated content , making a dent in a movement , or sharing hate speech . We present a Tweet - level hashtag hijacking detection framework focusing on hashtag activism . Our weakly - supervised framework uses bootstrapping to update itself as new Tweets are posted . Our experiments show that the system adapts to new topics in a social movement , as well as new hijacking strategies , maintaining strong performance over time . ","Method":["tweet - level hashtag hijacking detection framework"],"Task":["detecting hashtag hijacking"]},{"url":"https://www.semanticscholar.org/paper/41ebff09aff17c37efdab8c1d7051cbf150970f8","title":"Dialogue Act Classification for Augmentative and Alternative Communication","abstract":"Augmentative and Alternative Communication (AAC) devices and applications are intended to make it easier for individuals with complex communication needs to participate in conversations . However , these devices have low adoption and retention rates . We review prior work with text recommendation systems that have not been successful in mitigating these problems . To address these gaps , we propose applying Dialogue Act classification to AAC conversations . We evaluated the performance of a state of the art model on a limited AAC dataset that was trained on both AAC and non - AAC datasets . The one trained on AAC (accuracy = 38 . 6%) achieved better performance than that trained on a non - AAC corpus (accuracy = 34 . 1%) . These results reflect the need to incorporate representative datasets in later experiments . We discuss the need to collect more labeled AAC datasets and propose areas of future work . ","Method":["aac"],"Task":["dialogue act classification"]},{"url":"https://www.semanticscholar.org/paper/d393f2a793930a6e38321340185756860f43c62c","title":"Improving Policing with Natural Language Processing","abstract":"This article explores the potential for Natural Language Processing (NLP) to enable a more effective , prevention focused and less confrontational policing model that has hitherto been too resource consuming to implement at scale . Problem - Oriented Policing (POP) is a potential replacement , at least in part , for traditional policing which adopts a reactive approach , relying heavily on the criminal justice system . By contrast , POP seeks to prevent crime by manipulating the underlying conditions that allow crimes to be committed . Identifying these underlying conditions requires a detailed understanding of crime events - tacit knowledge that is often held by police officers but which can be challenging to derive from structured police data . One potential source of insight exists in unstructured free text data commonly collected by police for the purposes of investigation or administration . Yet police agencies do not typically have the skills or resources to analyse these data at scale . In this article we argue that NLP offers the potential to unlock these unstructured data and by doing so allow police to implement more POP initiatives . However we caution that using NLP models without adequate knowledge may either allow or perpetuate bias within the data potentially leading to unfavourable outcomes . ","Method":["pop"],"Task":["problem - oriented policing"]},{"url":"https://www.semanticscholar.org/paper/2c5b31a02133dea21cf94fde67c8948115441432","title":"Methods for Detoxification of Texts for the Russian Language","abstract":"We introduce the first study of automatic detoxification of Russian texts to combat offensive language . Such a kind of textual style transfer can be used , for instance , for processing toxic content in social media . While much work has been done for the English language in this field , it has never been solved for the Russian language yet . We test two types of models \u2013 unsupervised approach based on BERT architecture that performs local corrections and supervised approach based on pretrained language GPT - 2 model \u2013 and compare them with several baselines . In addition , we describe evaluation setup providing training datasets and metrics for automatic evaluation . The results show that the tested approaches can be successfully used for detoxification , although there is room for improvement . ","Method":["pretrained language gpt - 2 model"],"Task":["detoxification"]},{"url":"https://www.semanticscholar.org/paper/5c7b8f26ae23c457d85da9d0f19b28e588e9b9a7","title":"How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact","abstract":"Recent years have seen many breakthroughs in natural language processing (NLP) , transitioning it from a mostly theoretical field to one with many real - world applications . Noting the rising number of applications of other machine learning and AI techniques with pervasive societal impact , we anticipate the rising importance of developing NLP technologies for social good . Inspired by theories in moral philosophy and global priorities research , we aim to promote a guideline for social good in the context of NLP . We lay the foundations via the moral philosophy definition of social good , propose a framework to evaluate the direct and indirect real - world impact of NLP tasks , and adopt the methodology of global priorities research to identify priority causes for NLP research . Finally , we use our theoretical framework to provide some practical guidelines for future NLP research for social good . 1","Method":["nlp?"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/9e1616dcabf4d04d14d642fcb7963c461cf13d41","title":"NLP for Consumer Protection : Battling Illegal Clauses in German Terms and Conditions in Online Shopping","abstract":"Online shopping is an ever more important part of the global consumer economy , not just in times of a pandemic . When we place an order online as consumers , we regularly agree to the so - called \u201cTerms and Conditions\u201d (T&C) , a contract unilaterally drafted by the seller . Often , consumers do not read these contracts and unwittingly agree to unfavourable and often void terms . Government and non - government organisations (NGOs) for consumer protection battle such terms on behalf of consumers , who often hesitate to take on legal actions themselves . However , the growing number of online shops and a lack of funding makes it increasingly difficult for such organisations to monitor the market effectively . This paper describes how Natural Language Processing (NLP) can be applied to support consumer advocates in their efforts to protect consumers . Together with two NGOs from Germany , we developed an NLP - based application that legally assesses clauses in T&C from German online shops under the European Union\u2019s (EU) jurisdiction . We report that we could achieve an accuracy of 0 . 9 in the detection of void clauses by fine - tuning a pre - trained German BERT model . The approach is currently used by two NGOs and has already helped to challenge void clauses in T&C . ","Method":["nlp"],"Task":["online shopping"]},{"url":"https://www.semanticscholar.org/paper/473b70bb3c531b2d740fa6d652956e2733b53243","title":"Towards Knowledge - Grounded Counter Narrative Generation for Hate Speech","abstract":"Tackling online hatred using informed textual responses \u2013 called counter narratives \u2013 has been brought under the spotlight recently . Accordingly , a research line has emerged to automatically generate counter narratives in order to facilitate the direct intervention in the hate discussion and to prevent hate content from further spreading . Still , current neural approaches tend to produce generic/repetitive responses and lack grounded and up - to - date evidence such as facts , statistics , or examples . Moreover , these models can create plausible but not necessarily true arguments . In this paper we present the first complete knowledgebound counter narrative generation pipeline , grounded in an external knowledge repository that can provide more informative content to fight online hatred . Together with our approach , we present a series of experiments that show its feasibility to produce suitable and informative counter narratives in in - domain and cross - domain settings . ","Method":["knowledgebound counter narrative generation pipeline"],"Task":["hate speech","online hatred"]},{"url":"https://www.semanticscholar.org/paper/f29d5cb8f405903fc8af7a5d7ab4bf7d65796e95","title":"Use of Formal Ethical Reviews in NLP Literature : Historical Trends and Current Practices","abstract":"Ethical aspects of research in language technologies have received much attention recently . It is a standard practice to get a study involving human subjects reviewed and approved by a professional ethics committee/board of the institution . How commonly do we see mention of ethical approvals in NLP research? What types of research or aspects of studies are usually subject to such reviews? With the rising concerns and discourse around the ethics of NLP , do we also observe a rise in formal ethical reviews of NLP studies? And , if so , would this imply that there is a heightened awareness of ethical issues that was previously lacking? We aim to address these questions by conducting a detailed quantitative and qualitative analysis of the ACL Anthology , as well as comparing the trends in our field to those of other related disciplines , such as cognitive science , machine learning , data mining , and systems . ","Method":["formal ethical reviews"],"Task":["nlp"]},{"url":"https://www.semanticscholar.org/paper/23a812dde149021bab611ca4395279b9d41031f1","title":"Using Word Embeddings to Analyze Teacher Evaluations : An Application to a Filipino Education Non - Profit Organization","abstract":"Analysis of teacher evaluations is crucial to the development of robust educational programs , particularly through the validation of desirable qualities being reflected on in the text . This research applies Natural Language Processing techniques on a real - world dataset from a Filipino education non - profit to explore insights from analyzing evaluations written by Teacher Fellows who assess their own progress . Prior to this research , only qualitative assessment had been conducted on the text . Inspired by the use of word embedding similarities to capture semantic alignment , we utilize GloVe embeddings to determine to what extent these evaluations reflect concepts critical to measuring the competency of Teacher Fellows and upholding the organization\u2019s Vision and Mission . As Fellows\u2019 quantitative ratings improved , so too did their demonstration of competency in the text . Further , Teacher Fellow language was consistent with the organization\u2019s Vision and Mission . This research therefore showcases the possibilities of NLP in education , improving our understanding of Teacher Fellow evaluations , which can lead to advances in program operations and education efforts . ","Method":["glove embeddings"],"Task":["filipino education non - profit organization"]},{"url":"https://www.semanticscholar.org/paper/d4eb2ca9694f34d63abe6d27bd2d958992431017","title":"Theano : A Greek - speaking conversational agent for COVID - 19","abstract":"Conversational Agents (CAs) can be a proxy for disseminating information and providing support to the public , especially in times of crisis . CAs can scale to reach larger numbers of end - users than human operators , while they can offer information interactively and engagingly . In this work , we present Theano , a Greek - speaking virtual assistant for COVID - 19 . Theano presents users with COVID - 19 statistics and facts and informs users about the best health practices as well as the latest COVID - 19 related guidelines . Additionally , Theano provides support to end - users by helping them self - assess their symptoms and redirecting them to first - line health workers . The relevant , localized information that Theano provides , makes it a valuable tool for combating COVID - 19 in Greece . Theano has already conversed with different users in more than 170 different conversations through a web interface as a chatbot and over the phone as a voice bot . ","Method":["greek - speaking conversational agent","greek - speaking virtual assistant","theano"],"Task":["covid - 19"]},{"url":"https://www.semanticscholar.org/paper/100e0f3dcd319266b2772f0841dad388b45cce3f","title":"Restatement and Question Generation for Counsellor Chatbot","abstract":"Amidst rising mental health needs in society , virtual agents are increasingly deployed in counselling . In order to give pertinent advice , counsellors must first gain an understanding of the issues at hand by eliciting sharing from the counsellee . It is thus important for the counsellor chatbot to encourage the user to open up and talk . One way to sustain the conversation flow is to acknowledge the counsellee\u2019s key points by restating them , or probing them further with questions . This paper applies models from two closely related NLP tasks \u2014 summarization and question generation \u2014 to restatement and question generation in the counselling context . We conducted experiments on a manually annotated dataset of Cantonese post - reply pairs on topics related to loneliness , academic anxiety and test anxiety . We obtained the best performance in both restatement and question generation by fine - tuning BertSum , a state - of - the - art summarization model , with the in - domain manual dataset augmented with a large - scale , automatically mined open - domain dataset . ","Method":["bertsum"],"Task":["counsellor chatbot","restatement","summarization"]}]'),r=JSON.parse('[{"title":"UPSTAGE: Unsupervised Context Augmentation for Utterance Classification in Patient-Provider Communication","abstract":"Conversations between patients and providers in clinical settings provide a source of natural language data that may reflect and correlate with the patients\u2019 experience and response to the treatment they are receiving. When analyzing utterances in such conversations, it is not sufficient to consider each sentence in isolation, since its context may play a role in determining its semantic meaning. Recently, contextual information in natural language documents has been modeled using various techniques, such as recurrent neural networks with latent variables, or neural networks with attention mechanisms. In this paper, we present UnsuPerviSed conText AuGmEntation (Upstage), a classification framework that relies on both local and global contextual information from different sources. Upstage uses transformer models with pretrained language models and joint sentence representation to solve the task of classifying health topics in patient-provider conversations. In addition, Upstage leverages unlabeled corpora for pretraining and data augmentation to provide additional context, which leads to improved classification performance.","url":"https://www.semanticscholar.org/paper/e21ab639b0308a71aeac643a12f59a32603d091d","sentence":"Title: upstage: unsupervised context augmentation for utterance classification in patient-provider communication Abstract: conversations between patients and providers in clinical settings provide a source of natural language data that may reflect and correlate with the patients\u2019 experience and response to the treatment they are receiving. when analyzing utterances in such conversations, it is not sufficient to consider each sentence in isolation, since its context may play a role in determining its semantic meaning. recently, contextual information in natural language documents has been modeled using various techniques, such as recurrent neural networks with latent variables, or neural networks with attention mechanisms. in this paper, we present unsupervised context augmentation (upstage), a classification framework that relies on both local and global contextual information from different sources. upstage uses transformer models with pretrained language models and joint sentence representation to solve the task of classifying health topics in patient-provider conversations. in addition, upstage leverages unlabeled corpora for pretraining and data augmentation to provide additional context, which leads to improved classification performance.","Task":["contextual classification"],"Method":["contextual classification"]},{"title":"A Review of Challenges and Opportunities in Machine Learning for Health.","abstract":"Modern electronic health records (EHRs) provide data to answer clinically meaningful questions. The growing data in EHRs makes healthcare ripe for the use of machine learning. However, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. For example, diseases in EHRs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. This article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.","url":"https://www.semanticscholar.org/paper/d77719ba4be4196c9a918ce8dff0edf5aa3c04e7","sentence":"Title: a review of challenges and opportunities in machine learning for health. Abstract: modern electronic health records (ehrs) provide data to answer clinically meaningful questions. the growing data in ehrs makes healthcare ripe for the use of machine learning. however, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. for example, diseases in ehrs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. this article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.","Task":["data learning"],"Method":["machine learning"]},{"title":"Ethical Machine Learning in Health Care","abstract":"The use of machine learning (ML) in healthcare raises numerous ethical concerns, especially as models can amplify existing health inequities. Here, we outline ethical considerations for equitable ML in the advancement of healthcare. Specifically, we frame ethics of ML in healthcare through the lens of social justice. We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health, ranging from problem selection to postdeployment considerations. We close by summarizing recommendations to address these challenges.","url":"https://www.semanticscholar.org/paper/e8d330f11df9c69f38b78a7cc4b1333ebecf7c55","sentence":"Title: ethical machine learning in health care Abstract: the use of machine learning (ml) in healthcare raises numerous ethical concerns, especially as models can amplify existing health inequities. here, we outline ethical considerations for equitable ml in the advancement of healthcare. specifically, we frame ethics of ml in healthcare through the lens of social justice. we describe ongoing efforts and outline challenges in a proposed pipeline of ethical ml in health, ranging from problem selection to postdeployment considerations. we close by summarizing recommendations to address these challenges.","Task":["ethical ML"],"Method":["ethical ml"]},{"title":"Intimate Partner Violence and Injury Prediction From Radiology Reports","abstract":"Intimate partner violence (IPV) is an urgent, prevalent, and under-detected public health issue. We present machine learning models to assess patients for IPV and injury. We train the predictive algorithms on radiology reports with 1) IPV labels based on entry to a violence prevention program and 2) injury labels provided by emergency radiology fellowship-trained physicians. Our dataset includes 34,642 radiology reports and 1479 patients of IPV victims and control patients. Our best model predicts IPV a median of 3.08 years before violence prevention program entry with a sensitivity of 64% and a specificity of 95%. We conduct error analysis to determine for which patients our model has especially high or low performance and discuss next steps for a deployed clinical risk model.","url":"https://www.semanticscholar.org/paper/4fd75d18b231f73e9c878cb6029900e4b0e327bb","sentence":"Title: intimate partner violence and injury prediction from radiology reports Abstract: intimate partner violence (ipv) is an urgent, prevalent, and under-detected public health issue. we present machine learning models to assess patients for ipv and injury. we train the predictive algorithms on radiology reports with 1) ipv labels based on entry to a violence prevention program and 2) injury labels provided by emergency radiology fellowship-trained physicians. our dataset includes 34,642 radiology reports and 1479 patients of ipv victims and control patients. our best model predicts ipv a median of 3.08 years before violence prevention program entry with a sensitivity of 64% and a specificity of 95%. we conduct error analysis to determine for which patients our model has especially high or low performance and discuss next steps for a deployed clinical risk model.","Task":["predictive bias"],"Method":["predictive models"]},{"title":"De-identification of patient notes with recurrent neural networks","abstract":"Objective\\nPatient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information that needs to be removed to de-identify patient notes. Manual de-identification is impractical given the size of electronic health record databases, the limited number of researchers with access to non-de-identified notes, and the frequent mistakes of human annotators. A reliable automated de-identification system would consequently be of high value.\\n\\n\\nMaterials and Methods\\nWe introduce the first de-identification system based on artificial neural networks (ANNs), which requires no handcrafted features or rules, unlike existing systems. We compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the MIMIC de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.\\n\\n\\nResults\\nOur ANN model outperforms the state-of-the-art systems. It yields an F1-score of 97.85 on the i2b2 2014 dataset, with a recall of 97.38 and a precision of 98.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with a recall of 99.25 and a precision of 99.21.\\n\\n\\nConclusion\\nOur findings support the use of ANNs for de-identification of patient notes, as they show better performance than previously published systems while requiring no manual feature engineering.","url":"https://www.semanticscholar.org/paper/e7bf950be4cf8309d0df68d18fa09e77f5b2511a","sentence":"Title: de-identification of patient notes with recurrent neural networks Abstract: objective\\npatient notes in electronic health records (ehrs) may contain critical information for medical investigations. however, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. in the united states, the health insurance portability and accountability act (hipaa) defines 18 types of protected health information that needs to be removed to de-identify patient notes. manual de-identification is impractical given the size of electronic health record databases, the limited number of researchers with access to non-de-identified notes, and the frequent mistakes of human annotators. a reliable automated de-identification system would consequently be of high value.\\n\\n\\nmaterials and methods\\nwe introduce the first de-identification system based on artificial neural networks (anns), which requires no handcrafted features or rules, unlike existing systems. we compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the mimic de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.\\n\\n\\nresults\\nour ann model outperforms the state-of-the-art systems. it yields an f1-score of 97.85 on the i2b2 2014 dataset, with a recall of 97.38 and a precision of 98.32, and an f1-score of 99.23 on the mimic de-identification dataset, with a recall of 99.25 and a precision of 99.21.\\n\\n\\nconclusion\\nour findings support the use of anns for de-identification of patient notes, as they show better performance than previously published systems while requiring no manual feature engineering.","Task":["data accuracy"],"Method":["neural networks"]},{"title":"Segment convolutional neural networks (Seg-CNNs) for classifying relations in clinical notes","abstract":"We propose Segment Convolutional Neural Networks (Seg-CNNs) for classifying relations from clinical notes. Seg-CNNs use only word-embedding features without manual feature engineering. Unlike typical CNN models, relations between 2 concepts are identified by simultaneously learning separate representations for text segments in a sentence: preceding, concept1, middle, concept2, and succeeding. We evaluate Seg-CNN on the i2b2/VA relation classification challenge dataset. We show that Seg-CNN achieves a state-of-the-art micro-average F-measure of 0.742 for overall evaluation, 0.686 for classifying medical problem-treatment relations, 0.820 for medical problem-test relations, and 0.702 for medical problem-medical problem relations. We demonstrate the benefits of learning segment-level representations. We show that medical domain word embeddings help improve relation classification. Seg-CNNs can be trained quickly for the i2b2/VA dataset on a graphics processing unit (GPU) platform. These results support the use of CNNs computed over segments of text for classifying medical relations, as they show state-of-the-art performance while requiring no manual feature engineering.","url":"https://www.semanticscholar.org/paper/defeaddec92b53fc2e9787cc3652e3d8be812193","sentence":"Title: segment convolutional neural networks (seg-cnns) for classifying relations in clinical notes Abstract: we propose segment convolutional neural networks (seg-cnns) for classifying relations from clinical notes. seg-cnns use only word-embedding features without manual feature engineering. unlike typical cnn models, relations between 2 concepts are identified by simultaneously learning separate representations for text segments in a sentence: preceding, concept1, middle, concept2, and succeeding. we evaluate seg-cnn on the i2b2/va relation classification challenge dataset. we show that seg-cnn achieves a state-of-the-art micro-average f-measure of 0.742 for overall evaluation, 0.686 for classifying medical problem-treatment relations, 0.820 for medical problem-test relations, and 0.702 for medical problem-medical problem relations. we demonstrate the benefits of learning segment-level representations. we show that medical domain word embeddings help improve relation classification. seg-cnns can be trained quickly for the i2b2/va dataset on a graphics processing unit (gpu) platform. these results support the use of cnns computed over segments of text for classifying medical relations, as they show state-of-the-art performance while requiring no manual feature engineering.","Task":["feature classification"],"Method":["semantic networks"]},{"title":"Fast, Structured Clinical Documentation via Contextual Autocomplete","abstract":"We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. By constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. Furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. To our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","url":"https://www.semanticscholar.org/paper/4554bc45be85cca73e3f94220c8cb056cf6ebf74","sentence":"Title: fast, structured clinical documentation via contextual autocomplete Abstract: we present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. we dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. by constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. to our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","Task":["clinical documentation"],"Method":["semantic annotations"]},{"title":"CORD-19: The COVID-19 Open Research Dataset","abstract":"The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.","url":"https://www.semanticscholar.org/paper/4a10dffca6dcce9c570cb75aa4d76522c34a2fd4","sentence":"Title: cord-19: the covid-19 open research dataset Abstract: the covid-19 open research dataset (cord-19) is a growing resource of scientific papers on covid-19 and related historical coronavirus research. cord-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. since its release, cord-19 has been downloaded over 200k times and has served as the basis of many covid-19 text mining and discovery systems. in this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how cord-19 has been used, and describe several shared tasks built around the dataset. we hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for covid-19.","Task":["data management"],"Method":["data management"]},{"title":"Can AI Help Reduce Disparities in General Medical and Mental Health Care?","abstract":"Background\\nAs machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems\' data, algorithms, and recommendations. Simply put, as health care improves for some, it might not improve for all.\\n\\n\\nMethods\\nTwo case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (ICU) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status.\\n\\n\\nResults\\nClinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. Differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for ICU mortality and with respect to insurance policy for psychiatric 30-day readmission.\\n\\n\\nConclusions\\nThis analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.","url":"https://www.semanticscholar.org/paper/3f8d4444cd124e21d4fa47b514c8267a3d5d9649","sentence":"Title: can ai help reduce disparities in general medical and mental health care? Abstract: background\\nas machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems\' data, algorithms, and recommendations. simply put, as health care improves for some, it might not improve for all.\\n\\n\\nmethods\\ntwo case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (icu) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status.\\n\\n\\nresults\\nclinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for icu mortality and with respect to insurance policy for psychiatric 30-day readmission.\\n\\n\\nconclusions\\nthis analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.","Task":["machine bias"],"Method":["machine bias"]},{"title":"The Ivory Tower Lost: How College Students Respond Differently than the General Public to the COVID-19 Pandemic","abstract":"In the United States, the country with the highest confirmed COVID-19 infection cases, a nationwide social distancing protocol has been implemented by the President. Following the closure of the University of Washington on March 7th, more than 1000 colleges and universities in the United States have cancelled in-person classes and campus activities, impacting millions of students. This paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people\'s opinions on social media. We discover several topics embedded in a large number of COVID-19 tweets that represent the most central issues related to the pandemic, which are of great concerns for both college students and the general public. Moreover, we find significant differences between these two groups of Twitter users with respect to the sentiments they expressed towards the COVID-19 issues. To our best knowledge, this is the first social media-based study which focuses on the college student community\'s demographics and responses to prevalent social issues during a major crisis.","url":"https://www.semanticscholar.org/paper/2d40f76055a7c28dd30c8b8f60c31724f8991cd8","sentence":"Title: the ivory tower lost: how college students respond differently than the general public to the covid-19 pandemic Abstract: in the united states, the country with the highest confirmed covid-19 infection cases, a nationwide social distancing protocol has been implemented by the president. following the closure of the university of washington on march 7th, more than 1000 colleges and universities in the united states have cancelled in-person classes and campus activities, impacting millions of students. this paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people\'s opinions on social media. we discover several topics embedded in a large number of covid-19 tweets that represent the most central issues related to the pandemic, which are of great concerns for both college students and the general public. moreover, we find significant differences between these two groups of twitter users with respect to the sentiments they expressed towards the covid-19 issues. to our best knowledge, this is the first social media-based study which focuses on the college student community\'s demographics and responses to prevalent social issues during a major crisis.","Task":["social bias"],"Method":["social bias"]},{"title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","sentence":"Title: humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks Abstract: social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. despite its significantly large volume, social media content is often too noisy for direct use in any application. therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. to address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. however, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. in this paper, we present a new large-scale dataset with \u223c77k human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. we report multiclass classification results using classic and deep learning (fasttext and transformer) based models to set the ground for future studies. the dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","Task":["data classification"],"Method":["human annotations"]},{"title":"CrisisMMD: Multimodal Twitter Datasets from Natural Disasters","abstract":"During natural and man-made disasters, people use social media platforms such as Twitter to post textual and multime- dia content to report updates about injured or dead people, infrastructure damage, and missing or found people among other information types. Studies have revealed that this on- line information, if processed timely and effectively, is ex- tremely useful for humanitarian organizations to gain situational awareness and plan relief operations. In addition to the analysis of textual content, recent studies have shown that imagery content on social media can boost disaster response significantly. Despite extensive research that mainly focuses on textual content to extract useful information, limited work has focused on the use of imagery content or the combination of both content types. One of the reasons is the lack of labeled imagery data in this domain. Therefore, in this paper, we aim to tackle this limitation by releasing a large multi-modal dataset collected from Twitter during different natural disasters. We provide three types of annotations, which are useful to address a number of crisis response and management tasks for different humanitarian organizations.","url":"https://www.semanticscholar.org/paper/2a29f7cc10fc18ce6e031e0cf6fc307a6b85d8e9","sentence":"Title: crisismmd: multimodal twitter datasets from natural disasters Abstract: during natural and man-made disasters, people use social media platforms such as twitter to post textual and multime- dia content to report updates about injured or dead people, infrastructure damage, and missing or found people among other information types. studies have revealed that this on- line information, if processed timely and effectively, is ex- tremely useful for humanitarian organizations to gain situational awareness and plan relief operations. in addition to the analysis of textual content, recent studies have shown that imagery content on social media can boost disaster response significantly. despite extensive research that mainly focuses on textual content to extract useful information, limited work has focused on the use of imagery content or the combination of both content types. one of the reasons is the lack of labeled imagery data in this domain. therefore, in this paper, we aim to tackle this limitation by releasing a large multi-modal dataset collected from twitter during different natural disasters. we provide three types of annotations, which are useful to address a number of crisis response and management tasks for different humanitarian organizations.","Task":["imagery annotation"],"Method":["imagery annotations"]},{"title":"Domain Adaptation with Adversarial Training and Graph Embeddings","abstract":"The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.","url":"https://www.semanticscholar.org/paper/561ede166947a8bedb8be9acff182913156e06c6","sentence":"Title: domain adaptation with adversarial training and graph embeddings Abstract: the success of deep neural networks (dnns) is heavily dependent on the availability of labeled data. however, obtaining labeled data is a big challenge in many real-world problems. in such scenarios, a dnn model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. in this paper, we study the problem of classifying social media posts during a crisis event (e.g., earthquake). for that, we use labeled and unlabeled data from past similar events (e.g., flood) and unlabeled data for the current event. we propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. our experiments with two real-world crisis datasets collected from twitter demonstrate significant improvements over several baselines.","Task":["domain adaptation"],"Method":["domain adaptation"]},{"title":"IBC-C : A Dataset for Armed Conflict Event Analysis","abstract":"We describe the Iraq Body Count Corpus (IBC-C) dataset, the first substantial armed conflict-related dataset which can be used for conflict analysis. IBC-C provides a ground-truth dataset for conflict specific named entity recognition, slot filling, and event de-duplication. IBC-C is constructed using data collected by the Iraq Body Count project which has been recording casualties resulting from the ongoing war in Iraq since 2003. We describe the dataset\u2019s creation, how it can be used for the above three tasks and provide initial baseline results for the first task (named entity recognition) using Hidden Markov Models, Conditional Random Fields, and Recursive Neural Networks.","url":"https://www.semanticscholar.org/paper/579914dac924d6b4a17889ea7b0dd744a62bbd74","sentence":"Title: ibc-c : a dataset for armed conflict event analysis Abstract: we describe the iraq body count corpus (ibc-c) dataset, the first substantial armed conflict-related dataset which can be used for conflict analysis. ibc-c provides a ground-truth dataset for conflict specific named entity recognition, slot filling, and event de-duplication. ibc-c is constructed using data collected by the iraq body count project which has been recording casualties resulting from the ongoing war in iraq since 2003. we describe the dataset\u2019s creation, how it can be used for the above three tasks and provide initial baseline results for the first task (named entity recognition) using hidden markov models, conditional random fields, and recursive neural networks.","Task":["event validation"],"Method":["event inference"]},{"title":"Text as Data for Conflict Research: A Literature Survey","abstract":"Computer-aided text analysis (CATA) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. The chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. This includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). Particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. Both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. Finally, cross-validation is highlighted as a crucial step in CATA, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","url":"https://www.semanticscholar.org/paper/3004f78d84752f3ffe8e54fa265b2ee85dac9728","sentence":"Title: text as data for conflict research: a literature survey Abstract: computer-aided text analysis (cata) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. the chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. this includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. finally, cross-validation is highlighted as a crucial step in cata, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","Task":["text analysis"],"Method":["text mining"]},{"title":"One-to-X Analogical Reasoning on Word Embeddings: a Case for Diachronic Armed Conflict Prediction from News Texts","abstract":"We extend the well-known word analogy task to a one-to-X formulation, including one-to-none cases, when no correct answer exists. The task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type \u2018location:armed-group\u2019 based on data about past events. As the source of semantic information, we use diachronic word embedding models trained on English news texts. A simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. Finally, we publish a ready-to-use test set for one-to-X analogy evaluation on historical armed conflicts data.","url":"https://www.semanticscholar.org/paper/42c63d952f0cce7f89738e818fc9ab4e723bf7b6","sentence":"Title: one-to-x analogical reasoning on word embeddings: a case for diachronic armed conflict prediction from news texts Abstract: we extend the well-known word analogy task to a one-to-x formulation, including one-to-none cases, when no correct answer exists. the task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type \u2018location:armed-group\u2019 based on data about past events. as the source of semantic information, we use diachronic word embedding models trained on english news texts. a simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. finally, we publish a ready-to-use test set for one-to-x analogy evaluation on historical armed conflicts data.","Task":["semantic inference"],"Method":["semantic inference"]},{"title":"Using Natural Language Processing for Automatic Detection of Plagiarism","abstract":"Current plagiarism detection tools are mostly limited to comparisons of suspicious plagiarised texts and potential original texts at string level. In this study the aim is to improve the accuracy of plagiarism detection by incorporating Natural Language Processing (NLP) techniques into existing approaches. We propose a framework for external plagiarism detection in which a number of NLP techniques are applied to process a set of suspicious and original documents, not only to analyse strings but also the structure of the text, using resources to account for text relations. Initial results obtained with a corpus of plagiarised short paragraphs have showed that NLP techniques improve the accuracy of existing approaches.","url":"https://www.semanticscholar.org/paper/636d4c0b0fe6919abe6eb546907d28ed39bf56e6","sentence":"Title: using natural language processing for automatic detection of plagiarism Abstract: current plagiarism detection tools are mostly limited to comparisons of suspicious plagiarised texts and potential original texts at string level. in this study the aim is to improve the accuracy of plagiarism detection by incorporating natural language processing (nlp) techniques into existing approaches. we propose a framework for external plagiarism detection in which a number of nlp techniques are applied to process a set of suspicious and original documents, not only to analyse strings but also the structure of the text, using resources to account for text relations. initial results obtained with a corpus of plagiarised short paragraphs have showed that nlp techniques improve the accuracy of existing approaches.","Task":["plagiarism"],"Method":["automatic corpus"]},{"title":"A Neural Approach to Automated Essay Scoring","abstract":"Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted Kappa, without requiring any feature engineering.","url":"https://www.semanticscholar.org/paper/aea14f23a951975f605a981d003386e46bf8acfe","sentence":"Title: a neural approach to automated essay scoring Abstract: traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. the performance of such systems is tightly bound to the quality of the underlying features. however, it is laborious to manually design the most informative features for such a system. in this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. we explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. the results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted kappa, without requiring any feature engineering.","Task":["feature engineering"],"Method":["neural learning"]},{"title":"Automated Scoring: Beyond Natural Language Processing","abstract":"In this position paper, we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy. Automated scoring systems warrant significant cross-discipline collaboration of which natural language processing and machine learning are just two of many important components. Such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other. Our position is that it is essential for us as NLP researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful.","url":"https://www.semanticscholar.org/paper/0eb5872733e643f43a0c1a7ff78953dfea74dfea","sentence":"Title: automated scoring: beyond natural language processing Abstract: in this position paper, we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy. automated scoring systems warrant significant cross-discipline collaboration of which natural language processing and machine learning are just two of many important components. such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other. our position is that it is essential for us as nlp researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful.","Task":["this perspectives"],"Method":["automated scoring"]},{"title":"Event Data on Armed Conflict and Security: New Perspectives, Old Challenges, and Some Solutions","abstract":"This article presents the Event Data on Conflict and Security (EDACS) dataset, discusses the inherent problems of georeferenced conflict data, and shows how these challenges are met within EDACS. Based on an event data approach, EDACS contributes to the growing number of novel georeferenced datasets that allow researchers to identify causal pathways of violence and the dynamics of (transboundary) violence through spatiotemporal disaggregation. However, the unreflected use of any of these datasets will give researchers unjustified confidence in their findings, as the pitfalls are many and propagating errors can result in misleading conclusions. To identify and handle the different challenges to overall event data quality, we argue in favor of transparency in the data collection and coding process, to empower analysts to challenge the data and avoid cascading errors. In particular, we investigate how the choice of news sources, the handling of geographic precision, and the use of auxiliary data can bias event data. We demonstrate how the EDACS dataset design enables the analyst to deal with these issues by providing a set of variables indicating the news sources, possible sources of bias, and detailed information on geographic precision. This allows for a flexible use of the data based on individual analytical requirements.","url":"https://www.semanticscholar.org/paper/0cdeb238355617a640471987af36d8e09cd905c0","sentence":"Title: event data on armed conflict and security: new perspectives, old challenges, and some solutions Abstract: this article presents the event data on conflict and security (edacs) dataset, discusses the inherent problems of georeferenced conflict data, and shows how these challenges are met within edacs. based on an event data approach, edacs contributes to the growing number of novel georeferenced datasets that allow researchers to identify causal pathways of violence and the dynamics of (transboundary) violence through spatiotemporal disaggregation. however, the unreflected use of any of these datasets will give researchers unjustified confidence in their findings, as the pitfalls are many and propagating errors can result in misleading conclusions. to identify and handle the different challenges to overall event data quality, we argue in favor of transparency in the data collection and coding process, to empower analysts to challenge the data and avoid cascading errors. in particular, we investigate how the choice of news sources, the handling of geographic precision, and the use of auxiliary data can bias event data. we demonstrate how the edacs dataset design enables the analyst to deal with these issues by providing a set of variables indicating the news sources, possible sources of bias, and detailed information on geographic precision. this allows for a flexible use of the data based on individual analytical requirements.","Task":["data bias"],"Method":["event bias"]},{"title":"Tracing armed conflicts with diachronic word embedding models","abstract":"Recent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. In this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the Gigaword news corpus as the training data. The results show that much work still remains in extracting \u2018cultural\u2019 semantic shifts from diachronic word embedding models. At the same time, we present a new task complete with an evaluation set and introduce the \u2018anchor words\u2019 method which outperforms previous approaches on this set.","url":"https://www.semanticscholar.org/paper/2ca0403eabc3893fed255fc119a927d83a1af739","sentence":"Title: tracing armed conflicts with diachronic word embedding models Abstract: recent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. in this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the gigaword news corpus as the training data. the results show that much work still remains in extracting \u2018cultural\u2019 semantic shifts from diachronic word embedding models. at the same time, we present a new task complete with an evaluation set and introduce the \u2018anchor words\u2019 method which outperforms previous approaches on this set.","Task":["attribution words"],"Method":["anchor words"]},{"title":"Enriching textbooks through data mining","abstract":"Textbooks play an important role in any educational system. Unfortunately, many textbooks produced in developing countries are not written well and they often lack adequate coverage of important concepts. We propose a technological solution to address this problem based on enriching textbooks with authoritative web content. We augment textbooks at the section level for key concepts discussed in the section. We use ideas from data mining for identifying the concepts that need augmentation as well as to determine the links to the authoritative content that should be used for augmentation. Our evaluation, employing textbooks from India, shows that we are able to enrich textbooks on different subjects and across different grades with high quality augmentations using automated techniques.","url":"https://www.semanticscholar.org/paper/4ed75f35ae7343cd906f311390f572a58e36805c","sentence":"Title: enriching textbooks through data mining Abstract: textbooks play an important role in any educational system. unfortunately, many textbooks produced in developing countries are not written well and they often lack adequate coverage of important concepts. we propose a technological solution to address this problem based on enriching textbooks with authoritative web content. we augment textbooks at the section level for key concepts discussed in the section. we use ideas from data mining for identifying the concepts that need augmentation as well as to determine the links to the authoritative content that should be used for augmentation. our evaluation, employing textbooks from india, shows that we are able to enrich textbooks on different subjects and across different grades with high quality augmentations using automated techniques.","Task":["data mining"],"Method":["data mining"]},{"title":"Educational Question Answering Motivated by Question-Specific Concept Maps","abstract":"Question answering (QA) is the automated process of answering general questions submitted by humans in natural language. QA has previously been explored within the educational context to facilitate learning, however the majority of works have focused on text-based answering. As an alternative, this paper proposes an approach to return answers as a concept map, which further encourages meaningful learning and knowledge organisation. Additionally, this paper investigates whether adapting the returned concept map to the specific question context provides further learning benefit. A randomised experiment was conducted with a sample of 59 Computer Science undergraduates, obtaining statistically significant results on learning gain when students are provided with the question-specific concept maps. Further, time spent on studying the concept maps were positively correlated with the learning gain.","url":"https://www.semanticscholar.org/paper/bc6a4304afdf59783638a6a0d9fc2c2acb6e5b67","sentence":"Title: educational question answering motivated by question-specific concept maps Abstract: question answering (qa) is the automated process of answering general questions submitted by humans in natural language. qa has previously been explored within the educational context to facilitate learning, however the majority of works have focused on text-based answering. as an alternative, this paper proposes an approach to return answers as a concept map, which further encourages meaningful learning and knowledge organisation. additionally, this paper investigates whether adapting the returned concept map to the specific question context provides further learning benefit. a randomised experiment was conducted with a sample of 59 computer science undergraduates, obtaining statistically significant results on learning gain when students are provided with the question-specific concept maps. further, time spent on studying the concept maps were positively correlated with the learning gain.","Task":["learning learning"],"Method":["concept maps"]},{"title":"Characterizing Stage-aware Writing Assistance for Collaborative Document Authoring","abstract":"Writing is a complex non-linear process that begins with a mental model of intent, and progresses through an outline of ideas, to words on paper (and their subsequent refinement). Despite past research in understanding writing, Web-scale consumer and enterprise collaborative digital writing environments are yet to greatly benefit from intelligent systems that understand the stages of document evolution, providing opportune assistance based on authors\' situated actions and context. In this paper, we present three studies that explore temporal stages of document authoring. We first survey information workers at a large technology company about their writing habits and preferences, concluding that writers do in fact conceptually progress through several distinct phases while authoring documents. We also explore, qualitatively, how writing stages are linked to document lifespan. We supplement these qualitative findings with an analysis of the longitudinal user interaction logs of a popular digital writing platform over several million documents. Finally, as a first step towards facilitating an intelligent digital writing assistant, we conduct a preliminary investigation into the utility of user interaction log data for predicting the temporal stage of a document. Our results support the benefit of tools tailored to writing stages, identify primary tasks associated with these stages, and show that it is possible to predict stages from anonymous interaction logs. Together, these results argue for the benefit and feasibility of more tailored digital writing assistance.","url":"https://www.semanticscholar.org/paper/911e61212eb26325d3fe58f454f36f4e70c54c7a","sentence":"Title: characterizing stage-aware writing assistance for collaborative document authoring Abstract: writing is a complex non-linear process that begins with a mental model of intent, and progresses through an outline of ideas, to words on paper (and their subsequent refinement). despite past research in understanding writing, web-scale consumer and enterprise collaborative digital writing environments are yet to greatly benefit from intelligent systems that understand the stages of document evolution, providing opportune assistance based on authors\' situated actions and context. in this paper, we present three studies that explore temporal stages of document authoring. we first survey information workers at a large technology company about their writing habits and preferences, concluding that writers do in fact conceptually progress through several distinct phases while authoring documents. we also explore, qualitatively, how writing stages are linked to document lifespan. we supplement these qualitative findings with an analysis of the longitudinal user interaction logs of a popular digital writing platform over several million documents. finally, as a first step towards facilitating an intelligent digital writing assistant, we conduct a preliminary investigation into the utility of user interaction log data for predicting the temporal stage of a document. our results support the benefit of tools tailored to writing stages, identify primary tasks associated with these stages, and show that it is possible to predict stages from anonymous interaction logs. together, these results argue for the benefit and feasibility of more tailored digital writing assistance.","Task":["time prediction"],"Method":["temporal stages"]},{"title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.","url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","sentence":"Title: question answering system on education acts using nlp techniques Abstract: question answering (qa) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. it presents only the requested information instead of searching full documents like search engine. as information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. this is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain qa system for handling documents related to education acts sections to retrieve more precise answers using nlp techniques.","Task":["question queries"],"Method":["questiona"]},{"title":"Natural Language Processing and Language Learning","abstract":"As a relatively young field of research and development started by work on cryptanalysis and machine translation around 50 years ago, Natural Language Processing (NLP) is concerned with the automated processing of human language. It addresses the analysis and generation of written and spoken language, though speech processing is often regarded as a separate subfield. NLP emphasizes processing and applications and as such can be seen as the applied side of Computational Linguistics, the interdisciplinary field of research concerned with formal analysis and modeling of language and its applications at the intersection of Linguistics, Computer Science, and Psychology. In terms of the language aspects dealt with in NLP, traditionally lexical, morphological and syntactic aspects of language were at the center of attention, but aspects of meaning, discourse, and the relation to the extra-linguistic context have become increasingly prominent in the last decade. A good introduction and overview of the field is provided in Jurafsky & Martin (2009).","url":"https://www.semanticscholar.org/paper/f8868be52c8c0d64920e5f67870c01ba48601608","sentence":"Title: natural language processing and language learning Abstract: as a relatively young field of research and development started by work on cryptanalysis and machine translation around 50 years ago, natural language processing (nlp) is concerned with the automated processing of human language. it addresses the analysis and generation of written and spoken language, though speech processing is often regarded as a separate subfield. nlp emphasizes processing and applications and as such can be seen as the applied side of computational linguistics, the interdisciplinary field of research concerned with formal analysis and modeling of language and its applications at the intersection of linguistics, computer science, and psychology. in terms of the language aspects dealt with in nlp, traditionally lexical, morphological and syntactic aspects of language were at the center of attention, but aspects of meaning, discourse, and the relation to the extra-linguistic context have become increasingly prominent in the last decade. a good introduction and overview of the field is provided in jurafsky & martin (2009).","Task":["language learning"],"Method":["language learning"]},{"title":"Modeling the Relationship between User Comments and Edits in Document Revision","abstract":"Management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a document\u2019s evolution. Reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. A number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. Thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: Comment Ranking and Edit Anchoring. We begin by collecting a dataset with more than half a million comment-edit pairs based on Wikipedia revision histories. We then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. Our architecture tackles both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. In a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. We are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for Comment Ranking, while we achieve 74.4% accuracy on Edit Anchoring.","url":"https://www.semanticscholar.org/paper/311381feeb6346bfcb2ba622bd8f713261a4075d","sentence":"Title: modeling the relationship between user comments and edits in document revision Abstract: management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a document\u2019s evolution. reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. a number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: comment ranking and edit anchoring. we begin by collecting a dataset with more than half a million comment-edit pairs based on wikipedia revision histories. we then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. our architecture tackles both comment ranking and edit anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. in a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. we are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for comment ranking, while we achieve 74.4% accuracy on edit anchoring.","Task":["comment ranking"],"Method":["comment filtering"]},{"title":"A Multimodal Human-Computer Interaction System and Its Application in Smart Learning Environments","abstract":"A multimodal human-computer interaction system is composed of the comprehensive usage of various input and output channels. For the information input, apart from the traditional keyboard typing, mouse clicking, screen touching, the latest speech and face recognition technology can be used. For the output, the traditional screen display, the latest speech and facial expression synthesis and gesture generation can be used. After literature review of related works, this paper at first presents such a system, MMISE (Multimodal Interaction System for Education), about its architecture and working mechanism, POOOIIM (Pedagogical Objective Oriented Output, Input and Implementation Mechanism) illustrated with practical examples. Then this paper introduces this system\u2019s pilot applications in the epidemic time of novel coronavirus in 2020.","url":"https://www.semanticscholar.org/paper/d114af5d3dcb3792bd9faec1476feba5aaf3617f","sentence":"Title: a multimodal human-computer interaction system and its application in smart learning environments Abstract: a multimodal human-computer interaction system is composed of the comprehensive usage of various input and output channels. for the information input, apart from the traditional keyboard typing, mouse clicking, screen touching, the latest speech and face recognition technology can be used. for the output, the traditional screen display, the latest speech and facial expression synthesis and gesture generation can be used. after literature review of related works, this paper at first presents such a system, mmise (multimodal interaction system for education), about its architecture and working mechanism, poooiim (pedagogical objective oriented output, input and implementation mechanism) illustrated with practical examples. then this paper introduces this system\u2019s pilot applications in the epidemic time of novel coronavirus in 2020.","Task":["learning implementation"],"Method":["smart learning"]},{"title":"What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations","abstract":"The quality of a counseling intervention relies highly on the active collaboration between clients and counselors. In this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. Specifically, we address the differences between high-quality and low-quality counseling. Our approach examines participants\u2019 turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. Our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.","url":"https://www.semanticscholar.org/paper/9b0c9d241269b98c80f65a14d5d65263d0688d70","sentence":"Title: what makes a good counselor? learning to distinguish between high-quality and low-quality counseling conversations Abstract: the quality of a counseling intervention relies highly on the active collaboration between clients and counselors. in this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. specifically, we address the differences between high-quality and low-quality counseling. our approach examines participants\u2019 turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. these features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.","Task":["language bias"],"Method":["language classification"]},{"title":"Quantifying the Effects of COVID-19 on Mental Health Support Forums","abstract":"The COVID-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. Understanding its impact can inform strategies for mitigating negative consequences. In this work, we seek to better understand the effects of COVID-19 on mental health by examining discussions within mental health support communities on Reddit. First, we quantify the rate at which COVID-19 is discussed in each community, or subreddit, in order to understand levels of preoccupation with the pandemic. Next, we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen. Finally, we analyze how COVID-19 has influenced language use and topics of discussion within each subreddit.","url":"https://www.semanticscholar.org/paper/aa2bbf5a1485cc90401e4a35c04463fcb4e632f0","sentence":"Title: quantifying the effects of covid-19 on mental health support forums Abstract: the covid-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. understanding its impact can inform strategies for mitigating negative consequences. in this work, we seek to better understand the effects of covid-19 on mental health by examining discussions within mental health support communities on reddit. first, we quantify the rate at which covid-19 is discussed in each community, or subreddit, in order to understand levels of preoccupation with the pandemic. next, we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen. finally, we analyze how covid-19 has influenced language use and topics of discussion within each subreddit.","Task":["language bias"],"Method":["language language"]},{"title":"Data Mining and Student e-Learning Profiles","abstract":"Data mining techniques have been applied to educational research in various ways. In this paper, I presented the application of sequential data mining algorithms to analyze computer logs to profile learners in terms of their learning tactic use and motivation in a web-based learning environment (gStudy). The data mining algorithms are employed to discover patterns which characterize learners either across session or groups based on their study tactic choice and goal orientation. The use of this method is illustrated through a sequential pattern analysis of gStudy log files generated by university students.","url":"https://www.semanticscholar.org/paper/81d13af7eb78725492854bb9860b57af2b2a1778","sentence":"Title: data mining and student e-learning profiles Abstract: data mining techniques have been applied to educational research in various ways. in this paper, i presented the application of sequential data mining algorithms to analyze computer logs to profile learners in terms of their learning tactic use and motivation in a web-based learning environment (gstudy). the data mining algorithms are employed to discover patterns which characterize learners either across session or groups based on their study tactic choice and goal orientation. the use of this method is illustrated through a sequential pattern analysis of gstudy log files generated by university students.","Task":["data profiling"],"Method":["sequential profiling"]},{"title":"Inferring Social Media Users\u2019 Mental Health Status from Multimodal Information","abstract":"Worldwide, an increasing number of people are suffering from mental health disorders such as depression and anxiety. In the United States alone, one in every four adults suffers from a mental health condition, which makes mental health a pressing concern. In this paper, we explore the use of multimodal cues present in social media posts to predict users\u2019 mental health status. Specifically, we focus on identifying social media activity that either indicates a mental health condition or its onset. We collect posts from Flickr and apply a multimodal approach that consists of jointly analyzing language, visual, and metadata cues and their relation to mental health. We conduct several classification experiments aiming to discriminate between (1) healthy users and users affected by a mental health illness; and (2) healthy users and users prone to mental illness. Our experimental results indicate that using multiple modalities can improve the performance of this classification task as compared to the use of one modality at a time, and can provide important cues into a user\u2019s mental status.","url":"https://www.semanticscholar.org/paper/065332576fd9b264a5a338549239bb39e6733819","sentence":"Title: inferring social media users\u2019 mental health status from multimodal information Abstract: worldwide, an increasing number of people are suffering from mental health disorders such as depression and anxiety. in the united states alone, one in every four adults suffers from a mental health condition, which makes mental health a pressing concern. in this paper, we explore the use of multimodal cues present in social media posts to predict users\u2019 mental health status. specifically, we focus on identifying social media activity that either indicates a mental health condition or its onset. we collect posts from flickr and apply a multimodal approach that consists of jointly analyzing language, visual, and metadata cues and their relation to mental health. we conduct several classification experiments aiming to discriminate between (1) healthy users and users affected by a mental health illness; and (2) healthy users and users prone to mental illness. our experimental results indicate that using multiple modalities can improve the performance of this classification task as compared to the use of one modality at a time, and can provide important cues into a user\u2019s mental status.","Task":["classification classification"],"Method":["social cues"]},{"title":"Expressive Interviewing: A Conversational System for Coping with COVID-19","abstract":"The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce \\\\textit{Expressive Interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system\'s design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues.","url":"https://www.semanticscholar.org/paper/c278eb333c78313a3762b5b2f07ace7443582b94","sentence":"Title: expressive interviewing: a conversational system for coping with covid-19 Abstract: the ongoing covid-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. we introduce \\\\textit{expressive interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. expressive interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how covid-19 has impacted their lives. we present relevant aspects of the system\'s design and implementation as well as quantitative and qualitative analyses of user interactions with the system. in addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with covid-19 issues.","Task":["expressive interviewing"],"Method":["expressive interviewing"]},{"title":"Understanding and Predicting Empathic Behavior in Counseling Therapy","abstract":"Counselor empathy is associated with better outcomes in psychology and behavioral counseling. In this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. Particularly, we analyze aspects such as participants\u2019 engagement, participants\u2019 verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. We also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80%.","url":"https://www.semanticscholar.org/paper/35112c6bfa715b2bc86fc5b6c331f8a1a0b07add","sentence":"Title: understanding and predicting empathic behavior in counseling therapy Abstract: counselor empathy is associated with better outcomes in psychology and behavioral counseling. in this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. particularly, we analyze aspects such as participants\u2019 engagement, participants\u2019 verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. we also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80%.","Task":["counselor empathy"],"Method":["counselor empathy"]},{"title":"Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health","abstract":"Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.","url":"https://www.semanticscholar.org/paper/c151f144c2c0e8d3b176edaf2ce5369c7707bd31","sentence":"Title: large-scale analysis of counseling conversations: an application of natural language processing to mental health Abstract: mental illness is one of the most pressing public health issues of our time. while counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. in this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. we develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.","Task":["discourse discourse"],"Method":["conversation discourse"]},{"title":"Fermi at SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media using Sentence Embeddings","abstract":"This paper describes our system (Fermi) for Task 6: OffensEval: Identifying and Categorizing Offensive Language in Social Media of SemEval-2019. We participated in all the three sub-tasks within Task 6. We evaluate multiple sentence embeddings in conjunction with various supervised machine learning algorithms and evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team Fermi\u2019s model achieved an F1-score of 64.40%, 62.00% and 62.60% for sub-task A, B and C respectively on the official leaderboard. Our model for sub-task C which uses pre-trained ELMo embeddings for transforming the input and uses SVM (RBF kernel) for training, scored third position on the official leaderboard. Through the paper we provide a detailed description of the approach, as well as the results obtained for the task.","url":"https://www.semanticscholar.org/paper/9de154d3c886177380062be7c8d50304a335752f","sentence":"Title: fermi at semeval-2019 task 6: identifying and categorizing offensive language in social media using sentence embeddings Abstract: this paper describes our system (fermi) for task 6: offenseval: identifying and categorizing offensive language in social media of semeval-2019. we participated in all the three sub-tasks within task 6. we evaluate multiple sentence embeddings in conjunction with various supervised machine learning algorithms and evaluate the performance of simple yet effective embedding-ml combination algorithms. our team fermi\u2019s model achieved an f1-score of 64.40%, 62.00% and 62.60% for sub-task a, b and c respectively on the official leaderboard. our model for sub-task c which uses pre-trained elmo embeddings for transforming the input and uses svm (rbf kernel) for training, scored third position on the official leaderboard. through the paper we provide a detailed description of the approach, as well as the results obtained for the task.","Task":["language classification"],"Method":["semantic learning"]},{"title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","sentence":"Title: building a motivational interviewing dataset Abstract: this paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during motivational interviewing encounters. annotations were conducted using the motivational interviewing integrity treatment (miti). we describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. the dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. the reliability analysis showed that annotators achieved excellent agreement at session level, with intraclass correlation coefficient (icc) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with cohen\u2019s kappa scores ranging from 0.31 to 0.64. behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (resnicow et al., 2002). in particular, motivational interviewing (mi), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (moyers et al., 2009; apodaca et al., 2014; barnett et al., 2014; catley et al., 2012). despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing mi counseling at larger scale or in other domains is limited by the need for human-based evaluations. currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. recently, computational approaches have been proposed to aid the mi evaluation process (atkins et al., 2014; xiao et al., 2014; klonek et al., 2015). however, learning resources for this task are not readily available. having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of mi. moreover, this can also be useful to explore how mi works by relating mi behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their mi skills. in this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the motivational interviewing treatment integrity 4.0 (miti), which is the current gold standard for mi-based psychology interventions. the dataset is derived from 277 mi sessions containing a total of 22,719 coded utterances. 1 motivational interviewing miller and rollnick define mi as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (miller and rollnick, 2013). mi has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Task":["motivational interviewing"],"Method":["motivational interviewing"]},{"title":"Predicting Counselor Behaviors in Motivational Interviewing Encounters","abstract":"As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","url":"https://www.semanticscholar.org/paper/55925bc2522a1d4cb007ac78273f7c2efafa3916","sentence":"Title: predicting counselor behaviors in motivational interviewing encounters Abstract: as the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. in this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. in particular, we present a model towards the automation of motivational interviewing (mi) coding, which is the current gold standard to evaluate mi counseling. first, we build a dataset of hand labeled mi encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. we introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","Task":["language evaluation"],"Method":["motivational interviewing"]},{"title":"Happiness Entailment: Automating Suggestions for Well-Being","abstract":"Understanding what makes people happy is a central topic in psychology. Prior work has mostly focused on developing self-reporting assessment tools for individuals and relies on experts to analyze the periodic reported assessments. One of the goals of the analysis is to understand what actions are necessary to encourage modifications in the behaviors of the individuals to improve their overall well-being. In this paper, we outline a complementary approach; on the assumption that the user journals her happy moments as short texts, a system can analyze these texts and propose sustainable suggestions for the user that may lead to an overall improvement in her well-being. We prototype one necessary component of such a system, the Happiness Entailment Recognition (HER)module, which takes as input a short text describing an event, a candidate suggestion, and outputs a determination about whether the suggestion is more likely to be good for this user based on the event described. This component is implemented as a neural network model with two encoders, one for the user input and one for the candidate actionable suggestion, with additional layers to capture psychologically significant features in the happy moment and suggestion. Our model achieves an AU-ROC of 0.831 and outperforms our baseline as well as the current state-of-the-art Textual Entailment model from AllenNLP by more than 48% of improvements, confirming the uniqueness and complexity of the HER task.","url":"https://www.semanticscholar.org/paper/c2fd13dab7a581f2f875826b204d063f42b63a8b","sentence":"Title: happiness entailment: automating suggestions for well-being Abstract: understanding what makes people happy is a central topic in psychology. prior work has mostly focused on developing self-reporting assessment tools for individuals and relies on experts to analyze the periodic reported assessments. one of the goals of the analysis is to understand what actions are necessary to encourage modifications in the behaviors of the individuals to improve their overall well-being. in this paper, we outline a complementary approach; on the assumption that the user journals her happy moments as short texts, a system can analyze these texts and propose sustainable suggestions for the user that may lead to an overall improvement in her well-being. we prototype one necessary component of such a system, the happiness entailment recognition (her)module, which takes as input a short text describing an event, a candidate suggestion, and outputs a determination about whether the suggestion is more likely to be good for this user based on the event described. this component is implemented as a neural network model with two encoders, one for the user input and one for the candidate actionable suggestion, with additional layers to capture psychologically significant features in the happy moment and suggestion. our model achieves an au-roc of 0.831 and outperforms our baseline as well as the current state-of-the-art textual entailment model from allennlp by more than 48% of improvements, confirming the uniqueness and complexity of the her task.","Task":["decision prediction"],"Method":["the feedback"]},{"title":"FERMI at SemEval-2019 Task 5: Using Sentence embeddings to Identify Hate Speech Against Immigrants and Women in Twitter","abstract":"This paper describes our system (Fermi) for Task 5 of SemEval-2019: HatEval: Multilingual Detection of Hate Speech Against Immigrants and Women on Twitter. We participated in the subtask A for English and ranked first in the evaluation on the test set. We evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team - Fermi\u2019s model achieved an accuracy of 65.00% for English language in task A. Our models, which use pretrained Universal Encoder sentence embeddings for transforming the input and SVM (with RBF kernel) for classification, scored first position (among 68) in the leaderboard on the test set for Subtask A in English language. In this paper we provide a detailed description of the approach, as well as the results obtained in the task.","url":"https://www.semanticscholar.org/paper/64c68fa52491c4a815f21917e068e4c19dd404b0","sentence":"Title: fermi at semeval-2019 task 5: using sentence embeddings to identify hate speech against immigrants and women in twitter Abstract: this paper describes our system (fermi) for task 5 of semeval-2019: hateval: multilingual detection of hate speech against immigrants and women on twitter. we participated in the subtask a for english and ranked first in the evaluation on the test set. we evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ml combination algorithms. our team - fermi\u2019s model achieved an accuracy of 65.00% for english language in task a. our models, which use pretrained universal encoder sentence embeddings for transforming the input and svm (with rbf kernel) for classification, scored first position (among 68) in the leaderboard on the test set for subtask a in english language. in this paper we provide a detailed description of the approach, as well as the results obtained in the task.","Task":["language detection"],"Method":["hate detection"]},{"title":"Ingredients for Happiness: Modeling constructs via semi-supervised content driven inductive transfer","abstract":"Modeling affect via understanding the social constructs behind them is an important task in devising robust and accurate systems for socially relevant scenarios. In the CL-Aff Shared Task (part of Affective Content Analysis workshop @ AAAI 2019), the organizers released a dataset of \u2018happy\u2019 moments, called the HappyDB corpus. The task is to detect two social constructs: the agency (i.e., whether the author is in control of the happy moment) and the social characteristics (i.e., whether anyone else other than the author was also involved in the happy moment). We employ an inductive transfer learning technique where we utilize a pre-trained language model and fine-tune it on the target task for both the binary classification tasks. At first, we use a language model pre-trained on the huge WikiText-103 corpus. This step utilizes an AWDLSTM with three hidden layers for training the language model. In the second step, we fine-tune the pre-trained language model on both the labeled and unlabeled instances from the HappyDB dataset. Finally, we train a classifier on top of the language model for each of the identification tasks. Our experiments using 10-fold cross validation on the corpus show that we achieve a high accuracy of \u223c93% for detection of the social characteristic and \u223c87% for agency of the author, showing significant gains over other baselines. We also show that using the unlabeled dataset for fine-tuning the language model in the second step improves our accuracy by 1-2% across detection of both the constructs.","url":"https://www.semanticscholar.org/paper/7cb5bc9f8bc023f1f2c94e2e83012f1d35e4e01d","sentence":"Title: ingredients for happiness: modeling constructs via semi-supervised content driven inductive transfer Abstract: modeling affect via understanding the social constructs behind them is an important task in devising robust and accurate systems for socially relevant scenarios. in the cl-aff shared task (part of affective content analysis workshop @ aaai 2019), the organizers released a dataset of \u2018happy\u2019 moments, called the happydb corpus. the task is to detect two social constructs: the agency (i.e., whether the author is in control of the happy moment) and the social characteristics (i.e., whether anyone else other than the author was also involved in the happy moment). we employ an inductive transfer learning technique where we utilize a pre-trained language model and fine-tune it on the target task for both the binary classification tasks. at first, we use a language model pre-trained on the huge wikitext-103 corpus. this step utilizes an awdlstm with three hidden layers for training the language model. in the second step, we fine-tune the pre-trained language model on both the labeled and unlabeled instances from the happydb dataset. finally, we train a classifier on top of the language model for each of the identification tasks. our experiments using 10-fold cross validation on the corpus show that we achieve a high accuracy of \u223c93% for detection of the social characteristic and \u223c87% for agency of the author, showing significant gains over other baselines. we also show that using the unlabeled dataset for fine-tuning the language model in the second step improves our accuracy by 1-2% across detection of both the constructs.","Task":["social bias"],"Method":["social learning"]},{"title":"HappyDB: A Corpus of 100, 000 Crowdsourced Happy Moments","abstract":"The science of happiness is an area of positive psychology concerned with understanding what behaviors make people happy in a sustainable fashion. Recently, there has been interest in developing technologies that help incorporate the findings of the science of happiness into users\' daily lives by steering them towards behaviors that increase happiness. With the goal of building technology that can understand how people express their happy moments in text, we crowd-sourced HappyDB, a corpus of 100,000 happy moments that we make publicly available. This paper describes HappyDB and its properties, and outlines several important NLP problems that can be studied with the help of the corpus. We also apply several state-of-the-art analysis techniques to analyze HappyDB. Our results demonstrate the need for deeper NLP techniques to be developed which makes HappyDB an exciting resource for follow-on research.","url":"https://www.semanticscholar.org/paper/c239aaa2a87ea63fae666b2051f1923afaa45fc2","sentence":"Title: happydb: a corpus of 100, 000 crowdsourced happy moments Abstract: the science of happiness is an area of positive psychology concerned with understanding what behaviors make people happy in a sustainable fashion. recently, there has been interest in developing technologies that help incorporate the findings of the science of happiness into users\' daily lives by steering them towards behaviors that increase happiness. with the goal of building technology that can understand how people express their happy moments in text, we crowd-sourced happydb, a corpus of 100,000 happy moments that we make publicly available. this paper describes happydb and its properties, and outlines several important nlp problems that can be studied with the help of the corpus. we also apply several state-of-the-art analysis techniques to analyze happydb. our results demonstrate the need for deeper nlp techniques to be developed which makes happydb an exciting resource for follow-on research.","Task":["happydb"],"Method":["happydb"]},{"title":"HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","sentence":"Title: hopeedi: a multilingual hope speech detection dataset for equality, diversity, and inclusion Abstract: over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. however, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. until now, most studies have focused on solving this problem of negativity in the english language, though the problem is much more than just harmful content. furthermore, it is multilingual as well. thus, we have constructed a hope speech dataset for equality, diversity and inclusion (hopeedi) containing user-generated comments from the social media platform youtube with 28,451, 20,198 and 10,705 comments in english, tamil and malayalam, respectively, manually labelled as containing hope speech or not. to our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. we determined that the inter-annotator agreement of our dataset using krippendorff\u2019s alpha. further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and f1-score. the dataset is publicly available for the research community. we hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","Task":["hope speech"],"Method":["hope speech"]},{"title":"Women worry about family, men about the economy: Gender differences in emotional responses to COVID-19","abstract":"Among the critical challenges around the COVID-19 pandemic is dealing with the potentially detrimental effects on people\'s mental health. Designing appropriate interventions and identifying the concerns of those most at risk requires methods that can extract worries, concerns and emotional responses from text data. We examine gender differences and the effect of document length on worries about the ongoing COVID-19 situation. Our findings suggest that i) short texts do not offer as adequate insights into psychological processes as longer texts. We further find ii) marked gender differences in topics concerning emotional responses. Women worried more about their loved ones and severe health concerns while men were more occupied with effects on the economy and society. This paper adds to the understanding of general gender differences in language found elsewhere, and shows that the current unique circumstances likely amplified these effects. We close this paper with a call for more high-quality datasets due to the limitations of Tweet-sized data.","url":"https://www.semanticscholar.org/paper/32bf7ad3fdda71036b48f7dc85cad407674277b7","sentence":"Title: women worry about family, men about the economy: gender differences in emotional responses to covid-19 Abstract: among the critical challenges around the covid-19 pandemic is dealing with the potentially detrimental effects on people\'s mental health. designing appropriate interventions and identifying the concerns of those most at risk requires methods that can extract worries, concerns and emotional responses from text data. we examine gender differences and the effect of document length on worries about the ongoing covid-19 situation. our findings suggest that i) short texts do not offer as adequate insights into psychological processes as longer texts. we further find ii) marked gender differences in topics concerning emotional responses. women worried more about their loved ones and severe health concerns while men were more occupied with effects on the economy and society. this paper adds to the understanding of general gender differences in language found elsewhere, and shows that the current unique circumstances likely amplified these effects. we close this paper with a call for more high-quality datasets due to the limitations of tweet-sized data.","Task":["gender size"],"Method":["gender data"]},{"title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","sentence":"Title: the climate change debate and natural language processing Abstract: the debate around climate change (cc)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. yet, in the natural language processing (nlp) community, this domain has so far received little attention. in contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of cc-related text. other research is qualitative in nature and studies details, nuances, actors, and motivations within cc discourses. coming from both nlp and political science, and reviewing key works in both disciplines, we discuss how social science approaches to cc debates can inform advances in text-mining/nlp, and how, in return, nlp can support policy-makers and activists in making sense of large-scale and complex cc discourses across multiple genres, channels, topics, and communities. this is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","Task":["cc attribution"],"Method":["cc analysis"]},{"title":"Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism","abstract":"Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised BERT-based models.","url":"https://www.semanticscholar.org/paper/a9d09338a3a0d9102c1e623e6ad434a446c0bbfe","sentence":"Title: automatic classification of neutralization techniques in the narrative of climate change scepticism Abstract: neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. we first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised bert-based models.","Task":["automatic classification"],"Method":["automatic classification"]},{"title":"CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims","abstract":"We introduce CLIMATE-FEVER, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of FEVER [1], the largest dataset of artificially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \\\\textsc{fever} framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and AI community.","url":"https://www.semanticscholar.org/paper/086e6733e5fda70bbce0c7545bd06d5634918a60","sentence":"Title: climate-fever: a dataset for verification of real-world climate claims Abstract: we introduce climate-fever, a new publicly available dataset for verification of climate change-related claims. by providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. we adapt the methodology of fever [1], the largest dataset of artificially designed claims, to real-life claims collected from the internet. while during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. we discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \\\\textsc{fever} framework, which we believe provides a valuable challenge for general natural language understanding. we hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and ai community.","Task":["language attribution"],"Method":["climate claims"]},{"title":"Cheap Talk and Cherry-Picking: What ClimateBert has to say on Corporate Climate Risk Disclosures","abstract":"Disclosure of climate-related financial risks greatly helps investors assess companies\' preparedness for climate change. Voluntary disclosures such as those based on the recommendations of the Task Force for Climate-related Financial Disclosures (TCFD) are being hailed as an effective measure for better climate risk management. We ask whether this expectation is justified. We do so with the help of a deep neural language model, which we christen ClimateBert. We train ClimateBert on thousands of sentences related to climate-risk disclosures aligned with the TCFD recommendations. In analyzing the disclosures of TCFD-supporting firms, ClimateBert comes to the sobering conclusion that the firms\' TCFD support is mostly cheap talk and that firms cherry-pick to report primarily non-material climate risk information. From our analysis, we conclude that the only way out of this dilemma is to turn voluntary reporting into regulatory disclosures.","url":"https://www.semanticscholar.org/paper/52ef2bea83eba1575fcfd02c82b4228f7aab0bd6","sentence":"Title: cheap talk and cherry-picking: what climatebert has to say on corporate climate risk disclosures Abstract: disclosure of climate-related financial risks greatly helps investors assess companies\' preparedness for climate change. voluntary disclosures such as those based on the recommendations of the task force for climate-related financial disclosures (tcfd) are being hailed as an effective measure for better climate risk management. we ask whether this expectation is justified. we do so with the help of a deep neural language model, which we christen climatebert. we train climatebert on thousands of sentences related to climate-risk disclosures aligned with the tcfd recommendations. in analyzing the disclosures of tcfd-supporting firms, climatebert comes to the sobering conclusion that the firms\' tcfd support is mostly cheap talk and that firms cherry-pick to report primarily non-material climate risk information. from our analysis, we conclude that the only way out of this dilemma is to turn voluntary reporting into regulatory disclosures.","Task":["thisbert"],"Method":["regulatory disclosures"]},{"title":"Tackling Climate Change with Machine Learning","abstract":"Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.","url":"https://www.semanticscholar.org/paper/998039a4876edc440e0cabb0bc42239b0eb29644","sentence":"Title: tackling climate change with machine learning Abstract: climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. from smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. our recommendations encompass exciting research questions as well as promising business opportunities. we call on the machine learning community to join the global effort against climate change.","Task":["climate learning"],"Method":["machine learning"]},{"title":"Learning Twitter User Sentiments on Climate Change with Limited Labeled Data","abstract":"While it is well-documented that climate change accepters and deniers have become increasingly polarized in the United States over time, there has been no large-scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences. On the sub-population of Twitter users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the U.S. in 2018. We begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic-level results in line with prior research. We then apply RNNs to conduct a cohort-level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change. However, this effect does not hold for the 2018 blizzard and wildfires studied, implying that Twitter users\' opinions on climate change are fairly ingrained on this subset of natural disasters.","url":"https://www.semanticscholar.org/paper/f071e7c19fc18fc4d4ab69ce13f0c4e92cfb08c4","sentence":"Title: learning twitter user sentiments on climate change with limited labeled data Abstract: while it is well-documented that climate change accepters and deniers have become increasingly polarized in the united states over time, there has been no large-scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences. on the sub-population of twitter users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the u.s. in 2018. we begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic-level results in line with prior research. we then apply rnns to conduct a cohort-level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change. however, this effect does not hold for the 2018 blizzard and wildfires studied, implying that twitter users\' opinions on climate change are fairly ingrained on this subset of natural disasters.","Task":["this bias"],"Method":["climate bias"]},{"title":"Ask BERT: How Regulatory Disclosure of Transition and Physical Climate Risks affects the CDS Term Structure","abstract":"We use BERT, an AI-based algorithm for language understanding, to decipher regulatory climate-risk disclosures and measure their impact on the credit default swap (CDS) market. Risk disclosures can either increase or decrease credit spreads, depending on whether disclosure reveals new risks or sharpens the signal and decreases the uncertainty. Training BERT to differentiate between transition and physical climate risks, we find that disclosing transition risks increases CDS spreads, especially after the Paris Climate Agreement of 2015, while disclosing physical climate risks leads to a decrease in CDS spreads. These impacts are statistically and economically highly significant.","url":"https://www.semanticscholar.org/paper/bbc6486ad37365b77b9d0de8894f595d70af49ac","sentence":"Title: ask bert: how regulatory disclosure of transition and physical climate risks affects the cds term structure Abstract: we use bert, an ai-based algorithm for language understanding, to decipher regulatory climate-risk disclosures and measure their impact on the credit default swap (cds) market. risk disclosures can either increase or decrease credit spreads, depending on whether disclosure reveals new risks or sharpens the signal and decreases the uncertainty. training bert to differentiate between transition and physical climate risks, we find that disclosing transition risks increases cds spreads, especially after the paris climate agreement of 2015, while disclosing physical climate risks leads to a decrease in cds spreads. these impacts are statistically and economically highly significant.","Task":["thisert"],"Method":["climate risk"]},{"title":"Social Privacy in Networked Publics: Teens\u2019 Attitudes, Practices, and Strategies","abstract":"This paper examines how teens understand privacy in highly public networked environments like Facebook and Twitter. We describe both teens\u2019 practices, their privacy strategies, and the structural conditions in which they are embedded, highlighting the ways in which privacy, as it plays out in everyday life, is related more to agency and the ability to control a social situation than particular properties of information. Finally, we discuss the implications of teens\u2019 practices and strategies, revealing the importance of social norms as a regulatory force.(This paper was presented at Oxford Internet Institute\u2019s \u201cA Decade in Internet Time: Symposium on the Dynamics of the Internet and Society\u201d on September 22, 2011.)","url":"https://www.semanticscholar.org/paper/9b1653e3b57016958d10ff8531475eb0483d156c","sentence":"Title: social privacy in networked publics: teens\u2019 attitudes, practices, and strategies Abstract: this paper examines how teens understand privacy in highly public networked environments like facebook and twitter. we describe both teens\u2019 practices, their privacy strategies, and the structural conditions in which they are embedded, highlighting the ways in which privacy, as it plays out in everyday life, is related more to agency and the ability to control a social situation than particular properties of information. finally, we discuss the implications of teens\u2019 practices and strategies, revealing the importance of social norms as a regulatory force.(this paper was presented at oxford internet institute\u2019s \u201ca decade in internet time: symposium on the dynamics of the internet and society\u201d on september 22, 2011.)","Task":["social privacy"],"Method":["social norms"]},{"title":"DeSMOG: Detecting Stance in Media On Global Warming","abstract":"Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, \u201cLeading scientists agree that global warming is a serious concern,\u201d framing a clause which affirms their own stance (\u201cthat global warming is serious\u201d) as an opinion endorsed (\\"[scientists] agree\u201d) by a reputable source (\u201cleading\u201d). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: \u201cMistaken scientists claim [...].\\" Our work studies opinion-framing in the global warming (GW) debate, an increasingly partisan issue that has received little attention in NLP. We introduce DeSMOG, a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other\u2019s opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GW-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author\u2019s own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.","url":"https://www.semanticscholar.org/paper/1d7a0e8be2bbe7d148bb48a1f9de218beaea4ed5","sentence":"Title: desmog: detecting stance in media on global warming Abstract: citing opinions is a powerful yet understudied strategy in argumentation. for example, an environmental activist might say, \u201cleading scientists agree that global warming is a serious concern,\u201d framing a clause which affirms their own stance (\u201cthat global warming is serious\u201d) as an opinion endorsed (\\"[scientists] agree\u201d) by a reputable source (\u201cleading\u201d). in contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: \u201cmistaken scientists claim [...].\\" our work studies opinion-framing in the global warming (gw) debate, an increasingly partisan issue that has received little attention in nlp. we introduce desmog, a dataset of stance-labeled gw sentences, and train a bert classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other\u2019s opinions. from 56k news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across gw-accepting and skeptic media, though gw-skeptical media shows more opponent-doubt. we also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author\u2019s own view to source entities known to publicly endorse the opposing view. we release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of gw stance.","Task":["false framing"],"Method":["a stance"]},{"title":"You are right. I am ALARMED - But by Climate Change Counter Movement","abstract":"The world is facing the challenge of climate crisis. Despite the consensus in scientific community about anthropogenic global warming, the web is flooded with articles spreading climate misinformation. These articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change. We revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of NLP. Despite considerable work in detection of fake news, there is no misinformation dataset available that is specific to the domain.of climate change. We try to bridge this gap by scraping and releasing articles with known climate change misinformation.","url":"https://www.semanticscholar.org/paper/418b9f37b07a05b38798e97f57e17a6cc1048b92","sentence":"Title: you are right. i am alarmed - but by climate change counter movement Abstract: the world is facing the challenge of climate crisis. despite the consensus in scientific community about anthropogenic global warming, the web is flooded with articles spreading climate misinformation. these articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change. we revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of nlp. despite considerable work in detection of fake news, there is no misinformation dataset available that is specific to the domain.of climate change. we try to bridge this gap by scraping and releasing articles with known climate change misinformation.","Task":["climate misinformation"],"Method":["climate news"]},{"title":"Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings","abstract":"We provide an NLP framework to uncover four linguistic dimensions of political polarization in social media: topic choice, framing, affect and illocutionary force. We quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional LDA-based models. We apply our methods to study 4.4M tweets on 21 mass shootings. We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice. We identify framing devices, such as grounding and the contrasting use of the terms \u201cterrorist\u201d and \u201ccrazy\u201d, that contribute to polarization. Results pertaining to topic choice, affect and illocutionary force suggest that Republicans focus more on the shooter and event-specific facts (news) while Democrats focus more on the victims and call for policy changes. Our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them.","url":"https://www.semanticscholar.org/paper/3475876c3f5e624587181744bc5acf4223aecd00","sentence":"Title: analyzing polarization in social media: method and application to tweets on 21 mass shootings Abstract: we provide an nlp framework to uncover four linguistic dimensions of political polarization in social media: topic choice, framing, affect and illocutionary force. we quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional lda-based models. we apply our methods to study 4.4m tweets on 21 mass shootings. we provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice. we identify framing devices, such as grounding and the contrasting use of the terms \u201cterrorist\u201d and \u201ccrazy\u201d, that contribute to polarization. results pertaining to topic choice, affect and illocutionary force suggest that republicans focus more on the shooter and event-specific facts (news) while democrats focus more on the victims and call for policy changes. our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them.","Task":["political polarization"],"Method":["political polarization"]},{"title":"ClimaText: A Dataset for Climate Change Topic Detection","abstract":"Climate change communication in the mass media and other textual sources may affect and shape public perception. Extracting climate change information from these sources is an important task, e.g., for filtering content and e-discovery, sentiment analysis, automatic summarization, question-answering, and fact-checking. However, automating this process is a challenge, as climate change is a complex, fast-moving, and often ambiguous topic with scarce resources for popular text-based AI tasks. In this paper, we introduce \\\\textsc{ClimaText}, a dataset for sentence-based climate change topic detection, which we make publicly available. We explore different approaches to identify the climate change topic in various text sources. We find that popular keyword-based models are not adequate for such a complex and evolving task. Context-based algorithms like BERT \\\\cite{devlin2018bert} can detect, in addition to many trivial cases, a variety of complex and implicit topic patterns. Nevertheless, our analysis reveals a great potential for improvement in several directions, such as, e.g., capturing the discussion on indirect effects of climate change. Hence, we hope this work can serve as a good starting point for further research on this topic.","url":"https://www.semanticscholar.org/paper/134c8486ae58b421681656c85bbc48dc862f6f98","sentence":"Title: climatext: a dataset for climate change topic detection Abstract: climate change communication in the mass media and other textual sources may affect and shape public perception. extracting climate change information from these sources is an important task, e.g., for filtering content and e-discovery, sentiment analysis, automatic summarization, question-answering, and fact-checking. however, automating this process is a challenge, as climate change is a complex, fast-moving, and often ambiguous topic with scarce resources for popular text-based ai tasks. in this paper, we introduce \\\\textsc{climatext}, a dataset for sentence-based climate change topic detection, which we make publicly available. we explore different approaches to identify the climate change topic in various text sources. we find that popular keyword-based models are not adequate for such a complex and evolving task. context-based algorithms like bert \\\\cite{devlin2018bert} can detect, in addition to many trivial cases, a variety of complex and implicit topic patterns. nevertheless, our analysis reveals a great potential for improvement in several directions, such as, e.g., capturing the discussion on indirect effects of climate change. hence, we hope this work can serve as a good starting point for further research on this topic.","Task":["topic detection"],"Method":["climate detection"]},{"title":"Comparing Attitudes to Climate Change in the Media using sentiment analysis based on Latent Dirichlet Allocation","abstract":"News media typically present biased accounts of news stories, and different publications present different angles on the same event. In this research, we investigate how different publications differ in their approach to stories about climate change, by examining the sentiment and topics presented. To understand these attitudes, we find sentiment targets by combining Latent Dirichlet Allocation (LDA) with SentiWordNet, a general sentiment lexicon. Using LDA, we generate topics containing keywords which represent the sentiment targets, and then annotate the data using SentiWordNet before regrouping the articles based on topic similarity. Preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources. Ongoing work is investigating how systematic these attitudes are between different publications, and how these may change over time.","url":"https://www.semanticscholar.org/paper/171847842d161743f2b52dc4f85c121087ceb426","sentence":"Title: comparing attitudes to climate change in the media using sentiment analysis based on latent dirichlet allocation Abstract: news media typically present biased accounts of news stories, and different publications present different angles on the same event. in this research, we investigate how different publications differ in their approach to stories about climate change, by examining the sentiment and topics presented. to understand these attitudes, we find sentiment targets by combining latent dirichlet allocation (lda) with sentiwordnet, a general sentiment lexicon. using lda, we generate topics containing keywords which represent the sentiment targets, and then annotate the data using sentiwordnet before regrouping the articles based on topic similarity. preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources. ongoing work is investigating how systematic these attitudes are between different publications, and how these may change over time.","Task":["bias bias"],"Method":["sentiment bias"]},{"title":"Cross-Platform Disinformation Campaigns: Lessons Learned and Next Steps","abstract":"We conducted a mixed-method, interpretative analysis of an online, cross-platform disinformation campaign targeting the White Helmets, a rescue group operating in rebel-held areas of Syria that has become the subject of a persistent effort of delegitimization. This research helps to conceptualize what a disinformation campaign is and how it works. Based on what we learned from this case study, we conclude that a comprehensive understanding of disinformation requires accounting for the spread of content across platforms and that social media platforms should increase collaboration to detect and characterize disinformation campaigns.","url":"https://www.semanticscholar.org/paper/9c2cab0e72f2588e857671c1fbb13b5fba8f2586","sentence":"Title: cross-platform disinformation campaigns: lessons learned and next steps Abstract: we conducted a mixed-method, interpretative analysis of an online, cross-platform disinformation campaign targeting the white helmets, a rescue group operating in rebel-held areas of syria that has become the subject of a persistent effort of delegitimization. this research helps to conceptualize what a disinformation campaign is and how it works. based on what we learned from this case study, we conclude that a comprehensive understanding of disinformation requires accounting for the spread of content across platforms and that social media platforms should increase collaboration to detect and characterize disinformation campaigns.","Task":["disinformation campaigns"],"Method":["disinformation campaigns"]},{"title":"Classification of Moral Foundations in Microblog Political Discourse","abstract":"Previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. Additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. Based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on Twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. The contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.","url":"https://www.semanticscholar.org/paper/7de1c0db3b3848d1a51dbfefce8b73e89a682177","sentence":"Title: classification of moral foundations in microblog political discourse Abstract: previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. the contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.","Task":["moral foundations"],"Method":["moral foundations"]},{"title":"Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases","abstract":"Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.","url":"https://www.semanticscholar.org/paper/29584ed6d68a06fdf91440a018f6bc83a44fd177","sentence":"Title: paragraph-level rationale extraction through regularization: a case study on european court of human rights cases Abstract: interpretability or explainability is an emerging research field in nlp. from a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. to this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. we also release a new dataset comprising european court of human rights cases, including annotations for paragraph-level rationales. we use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. we also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.","Task":["rationaleability"],"Method":["singularity"]},{"title":"Automatically Characterizing Targeted Information Operations Through Biases Present in Discourse on Twitter","abstract":"This paper considers the problem of automatically characterizing biases that may be associated with emerging information operations via artificial intelligence. Accurate analysis of these emerging topics usually requires laborious, manual analysis by experts to annotate millions of tweets to identify biases in new topics. We introduce adaptations of the Word Embedding Association Test [1] to a new domain: information operations. We validate our method using known information operation-related tweets from Twitter\'s Transparency Reports, and we perform a case study on the COVID-19 pandemic to evaluate our method\'s performance on non-labeled Twitter data, demonstrating its usability in emerging domains.","url":"https://www.semanticscholar.org/paper/08e3ce4454012d928470efa96a54aa08dd8a96b0","sentence":"Title: automatically characterizing targeted information operations through biases present in discourse on twitter Abstract: this paper considers the problem of automatically characterizing biases that may be associated with emerging information operations via artificial intelligence. accurate analysis of these emerging topics usually requires laborious, manual analysis by experts to annotate millions of tweets to identify biases in new topics. we introduce adaptations of the word embedding association test [1] to a new domain: information operations. we validate our method using known information operation-related tweets from twitter\'s transparency reports, and we perform a case study on the covid-19 pandemic to evaluate our method\'s performance on non-labeled twitter data, demonstrating its usability in emerging domains.","Task":["this bias"],"Method":["bias bias"]},{"title":"Framing and Agenda-setting in Russian News: a Computational Analysis of Intricate Political Strategies","abstract":"Amidst growing concern over media manipulation, NLP attention has focused on overt strategies like censorship and \u201cfake news\u201d. Here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction: articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to Russian, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.","url":"https://www.semanticscholar.org/paper/2547836827e0423f198320977d393f574e0fb3d6","sentence":"Title: framing and agenda-setting in russian news: a computational analysis of intricate political strategies Abstract: amidst growing concern over media manipulation, nlp attention has focused on overt strategies like censorship and \u201cfake news\u201d. here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). we analyze 13 years (100k articles) of the russian newspaper izvestia and identify a strategy of distraction: articles mention the u.s. more frequently in the month directly following an economic downturn in russia. we introduce embedding-based methods for cross-lingually projecting english frames to russian, and discover that these articles emphasize u.s. moral failings and threats to the u.s. our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.","Task":["media framing"],"Method":["media framing"]},{"title":"Predicting the Role of Political Trolls in Social Media","abstract":"We investigate the political roles of \u201cInternet trolls\u201d in social media. Political trolls, such as the ones linked to the Russian Internet Research Agency (IRA), have recently gained enormous attention for their ability to sway public opinion and even influence elections. Analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. However, this analysis is manual and labor-intensive, thus making it impractical as a first-response tool for newly-discovered troll farms. In this paper, we show how to automate this analysis by using machine learning in a realistic setting. In particular, we show how to classify trolls according to their political role \u2014left, news feed, right\u2014 by using features extracted from social media, i.e., Twitter, in two scenarios: (i) in a traditional supervised learning scenario, where labels for trolls are available, and (ii) in a distant supervision scenario, where labels for trolls are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. Technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e., embeddings, for the trolls. Experiments on the \u201cIRA Russian Troll\u201d dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.","url":"https://www.semanticscholar.org/paper/546c87abc26137359c43898559884ba9d6c5ae64","sentence":"Title: predicting the role of political trolls in social media Abstract: we investigate the political roles of \u201cinternet trolls\u201d in social media. political trolls, such as the ones linked to the russian internet research agency (ira), have recently gained enormous attention for their ability to sway public opinion and even influence elections. analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. however, this analysis is manual and labor-intensive, thus making it impractical as a first-response tool for newly-discovered troll farms. in this paper, we show how to automate this analysis by using machine learning in a realistic setting. in particular, we show how to classify trolls according to their political role \u2014left, news feed, right\u2014 by using features extracted from social media, i.e., twitter, in two scenarios: (i) in a traditional supervised learning scenario, where labels for trolls are available, and (ii) in a distant supervision scenario, where labels for trolls are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e., embeddings, for the trolls. experiments on the \u201cira russian troll\u201d dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.","Task":["social classification"],"Method":["social representations"]},{"title":"Historical Change in the Moral Foundations of Political Persuasion","abstract":"How have attempts at political persuasion changed over time? Using nine corpora dating back through 1789, containing over 7 million words of speech (1,666 documents in total), covering three different countries, plus the entire Google nGram corpus, we find that language relating to togetherness permanently crowded out language relating to duties and obligations in the persuasive speeches of politicians during the early 20th century. This shift is temporally predicted by a rise in Western nationalism and the mass movement of people from more rural to more urban areas and is unexplained by changes in language, private political speech, or nonmoral persuasion. We theorize that the emergence of the modern state in the 1920s had psychopolitical consequences for the ways that people understood and communicated their relationships with their government, which was then reflected in the levers of persuasion chosen by political elites.","url":"https://www.semanticscholar.org/paper/c0a2ee56fc80d971e920fba6179f81b515533592","sentence":"Title: historical change in the moral foundations of political persuasion Abstract: how have attempts at political persuasion changed over time? using nine corpora dating back through 1789, containing over 7 million words of speech (1,666 documents in total), covering three different countries, plus the entire google ngram corpus, we find that language relating to togetherness permanently crowded out language relating to duties and obligations in the persuasive speeches of politicians during the early 20th century. this shift is temporally predicted by a rise in western nationalism and the mass movement of people from more rural to more urban areas and is unexplained by changes in language, private political speech, or nonmoral persuasion. we theorize that the emergence of the modern state in the 1920s had psychopolitical consequences for the ways that people understood and communicated their relationships with their government, which was then reflected in the levers of persuasion chosen by political elites.","Task":["political persuasion"],"Method":["moral persuasion"]},{"title":"Red Bots Do It Better:Comparative Analysis of Social Bot Partisan Behavior","abstract":"Recent research brought awareness of the issue of bots on social media and the significant risks of mass manipulation of public opinion in the context of political discussion. In this work, we leverage Twitter to study the discourse during the 2018 US midterm elections and analyze social bot activity and interactions with humans. We collected 2.6 million tweets for 42 days around the election day from nearly 1 million users. We use the collected tweets to answer three research questions: (i) Do social bots lean and behave according to a political ideology? (ii) Can we observe different strategies among liberal and conservative bots? (iii) How effective are bot strategies in engaging humans? We show that social bots can be accurately classified according to their political leaning and behave accordingly. Conservative bots share most of the topics of discussion with their human counterparts, while liberal bots show less overlap and a more inflammatory attitude. We studied bot interactions with humans and observed different strategies. Finally, we measured bots embeddedness in the social network and the extent of human engagement with each group of bots. Results show that conservative bots are more deeply embedded in the social network and more effective than liberal bots at exerting influence on humans.","url":"https://www.semanticscholar.org/paper/d89e89eac7882790b48ad73376742e7a93fea4fd","sentence":"Title: red bots do it better:comparative analysis of social bot partisan behavior Abstract: recent research brought awareness of the issue of bots on social media and the significant risks of mass manipulation of public opinion in the context of political discussion. in this work, we leverage twitter to study the discourse during the 2018 us midterm elections and analyze social bot activity and interactions with humans. we collected 2.6 million tweets for 42 days around the election day from nearly 1 million users. we use the collected tweets to answer three research questions: (i) do social bots lean and behave according to a political ideology? (ii) can we observe different strategies among liberal and conservative bots? (iii) how effective are bot strategies in engaging humans? we show that social bots can be accurately classified according to their political leaning and behave accordingly. conservative bots share most of the topics of discussion with their human counterparts, while liberal bots show less overlap and a more inflammatory attitude. we studied bot interactions with humans and observed different strategies. finally, we measured bots embeddedness in the social network and the extent of human engagement with each group of bots. results show that conservative bots are more deeply embedded in the social network and more effective than liberal bots at exerting influence on humans.","Task":["bot bots"],"Method":["social bots"]},{"title":"Fine-Grained Analysis of Propaganda in News Article","abstract":"Propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect the quality of any learning system trained on them. A further issue with most existing systems is the lack of explainability. To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.","url":"https://www.semanticscholar.org/paper/a12d22ff91ce159a0d3558ed5aaed115115beabd","sentence":"Title: fine-grained analysis of propaganda in news article Abstract: propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda. previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. such noisy gold labels inevitably affect the quality of any learning system trained on them. a further issue with most existing systems is the lack of explainability. to overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. in particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. we further design a novel multi-granularity neural network, and we show that it outperforms several strong bert-based baselines.","Task":["propagandaability"],"Method":["propaganda networks"]},{"title":"Issue Framing in Online Discussion Fora","abstract":"In online discussion fora, speakers often make arguments for or against something, say birth control, by highlighting certain aspects of the topic. In social science, this is referred to as issue framing. In this paper, we introduce a new issue frame annotated corpus of online discussions. We explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora, using a combination of multi-task and adversarial training, assuming only unlabeled training data in the target domain.","url":"https://www.semanticscholar.org/paper/9e82aba279807a6c8541c8811f049a40ebca36c1","sentence":"Title: issue framing in online discussion fora Abstract: in online discussion fora, speakers often make arguments for or against something, say birth control, by highlighting certain aspects of the topic. in social science, this is referred to as issue framing. in this paper, we introduce a new issue frame annotated corpus of online discussions. we explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora, using a combination of multi-task and adversarial training, assuming only unlabeled training data in the target domain.","Task":["issue framing"],"Method":["issue framing"]},{"title":"Technology, Autonomy, and Manipulation","abstract":"Since 2016, when the Facebook/Cambridge Analytica scandal began to emerge, public concern has grown around the threat of \u201conline manipulation\u201d. While these worries are familiar to privacy researchers, this paper aims to make them more salient to policymakers \u2014 first, by defining \u201conline manipulation\u201d, thus enabling identification of manipulative practices; and second, by drawing attention to the specific harms online manipulation threatens. We argue that online manipulation is the use of information technology to covertly influence another person\u2019s decision-making, by targeting and exploiting their decision-making vulnerabilities. Engaging in such practices can harm individuals by diminishing their economic interests, but its deeper, more insidious harm is its challenge to individual autonomy. We explore this autonomy harm, emphasising its implications for both individuals and society, and we briefly outline some strategies for combating online manipulation and strengthening autonomy in an increasingly digital world.","url":"https://www.semanticscholar.org/paper/c0472b63d7d45d948dea42c4b5efffb6859541f9","sentence":"Title: technology, autonomy, and manipulation Abstract: since 2016, when the facebook/cambridge analytica scandal began to emerge, public concern has grown around the threat of \u201conline manipulation\u201d. while these worries are familiar to privacy researchers, this paper aims to make them more salient to policymakers \u2014 first, by defining \u201conline manipulation\u201d, thus enabling identification of manipulative practices; and second, by drawing attention to the specific harms online manipulation threatens. we argue that online manipulation is the use of information technology to covertly influence another person\u2019s decision-making, by targeting and exploiting their decision-making vulnerabilities. engaging in such practices can harm individuals by diminishing their economic interests, but its deeper, more insidious harm is its challenge to individual autonomy. we explore this autonomy harm, emphasising its implications for both individuals and society, and we briefly outline some strategies for combating online manipulation and strengthening autonomy in an increasingly digital world.","Task":["online manipulation"],"Method":["digital autonomy"]},{"title":"Modeling Frames in Argumentation","abstract":"In argumentation, framing is used to emphasize a specific aspect of a controversial topic while concealing others. When talking about legalizing drugs, for instance, its economical aspect may be emphasized. In general, we call a set of arguments that focus on the same aspect a frame. An argumentative text has to serve the \u201cright\u201d frame(s) to convince the audience to adopt the author\u2019s stance (e.g., being pro or con legalizing drugs). More specifically, an author has to choose frames that fit the audience\u2019s cultural background and interests. This paper introduces frame identification, which is the task of splitting a set of arguments into non-overlapping frames. We present a fully unsupervised approach to this task, which first removes topical information and then identifies frames using clustering. For evaluation purposes, we provide a corpus with 12, 326 debate-portal arguments, organized along the frames of the debates\u2019 topics. On this corpus, our approach outperforms different strong baselines, achieving an F1-score of 0.28.","url":"https://www.semanticscholar.org/paper/20a37289d0b7c90065850eb7bf96d5ac3c8a21e4","sentence":"Title: modeling frames in argumentation Abstract: in argumentation, framing is used to emphasize a specific aspect of a controversial topic while concealing others. when talking about legalizing drugs, for instance, its economical aspect may be emphasized. in general, we call a set of arguments that focus on the same aspect a frame. an argumentative text has to serve the \u201cright\u201d frame(s) to convince the audience to adopt the author\u2019s stance (e.g., being pro or con legalizing drugs). more specifically, an author has to choose frames that fit the audience\u2019s cultural background and interests. this paper introduces frame identification, which is the task of splitting a set of arguments into non-overlapping frames. we present a fully unsupervised approach to this task, which first removes topical information and then identifies frames using clustering. for evaluation purposes, we provide a corpus with 12, 326 debate-portal arguments, organized along the frames of the debates\u2019 topics. on this corpus, our approach outperforms different strong baselines, achieving an f1-score of 0.28.","Task":["frame framing"],"Method":["frame framing"]},{"title":"Automatically Neutralizing Subjective Bias in Text","abstract":"Texts like news, encyclopedias, and some social media strive for objectivity. Yet bias in the form of inappropriate subjectivity - introducing attitudes via framing, presupposing truth, and casting doubt - remains ubiquitous. This kind of bias erodes our collective trust and fuels social conflict. To address this issue, we introduce a novel testbed for natural language generation: automatically bringing inappropriately subjective text into a neutral point of view (\\"neutralizing\\" biased text). We also offer the first parallel corpus of biased language. The corpus contains 180,000 sentence pairs and originates from Wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. Last, we propose two strong encoder-decoder baselines for the task. A straightforward yet opaque CONCURRENT system uses a BERT encoder to identify subjective words as part of the generation process. An interpretable and controllable MODULAR algorithm separates these steps, using (1) a BERT-based classifier to identify problematic words and (2) a novel join embedding through which the classifier can edit the hidden states of the encoder. Large-scale human evaluation across four domains (encyclopedias, news headlines, books, and political speeches) suggests that these algorithms are a first step towards the automatic identification and reduction of bias.","url":"https://www.semanticscholar.org/paper/16981cc4ddefd3ea7655754fd83a2a8ff2203a8b","sentence":"Title: automatically neutralizing subjective bias in text Abstract: texts like news, encyclopedias, and some social media strive for objectivity. yet bias in the form of inappropriate subjectivity - introducing attitudes via framing, presupposing truth, and casting doubt - remains ubiquitous. this kind of bias erodes our collective trust and fuels social conflict. to address this issue, we introduce a novel testbed for natural language generation: automatically bringing inappropriately subjective text into a neutral point of view (\\"neutralizing\\" biased text). we also offer the first parallel corpus of biased language. the corpus contains 180,000 sentence pairs and originates from wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. last, we propose two strong encoder-decoder baselines for the task. a straightforward yet opaque concurrent system uses a bert encoder to identify subjective words as part of the generation process. an interpretable and controllable modular algorithm separates these steps, using (1) a bert-based classifier to identify problematic words and (2) a novel join embedding through which the classifier can edit the hidden states of the encoder. large-scale human evaluation across four domains (encyclopedias, news headlines, books, and political speeches) suggests that these algorithms are a first step towards the automatic identification and reduction of bias.","Task":["subjective bias"],"Method":["neutral bias"]},{"title":"A Systematic Media Frame Analysis of 1.5 Million New York Times Articles from 2000 to 2017","abstract":"Framing is an indispensable narrative device for news media because even the same facts may lead to conflicting understandings if deliberate framing is employed. Therefore, identifying media framing is a crucial step to understanding how news media influence the public. Framing is, however, difficult to operationalize and detect, and thus traditional media framing studies had to rely on manual annotation, which is challenging to scale up to massive news datasets. Here, by developing a media frame classifier that achieves state-of-the-art performance, we systematically analyze the media frames of 1.5 million New York Times articles published from 2000 to 2017. By examining the ebb and flow of media frames over almost two decades, we show that short-term frame abundance fluctuation closely corresponds to major events, while there also exist several long-term trends, such as the gradually increasing prevalence of the \u201cCultural identity\u201d frame. By examining specific topics and sentiments, we identify characteristics and dynamics of each frame. Finally, as a case study, we delve into the framing of mass shootings, revealing three major framing patterns. Our scalable, computational approach to massive news datasets opens up new pathways for systematic media framing studies.","url":"https://www.semanticscholar.org/paper/a0a5669b310ceb3840a599bf4f27a077a643d613","sentence":"Title: a systematic media frame analysis of 1.5 million new york times articles from 2000 to 2017 Abstract: framing is an indispensable narrative device for news media because even the same facts may lead to conflicting understandings if deliberate framing is employed. therefore, identifying media framing is a crucial step to understanding how news media influence the public. framing is, however, difficult to operationalize and detect, and thus traditional media framing studies had to rely on manual annotation, which is challenging to scale up to massive news datasets. here, by developing a media frame classifier that achieves state-of-the-art performance, we systematically analyze the media frames of 1.5 million new york times articles published from 2000 to 2017. by examining the ebb and flow of media frames over almost two decades, we show that short-term frame abundance fluctuation closely corresponds to major events, while there also exist several long-term trends, such as the gradually increasing prevalence of the \u201ccultural identity\u201d frame. by examining specific topics and sentiments, we identify characteristics and dynamics of each frame. finally, as a case study, we delve into the framing of mass shootings, revealing three major framing patterns. our scalable, computational approach to massive news datasets opens up new pathways for systematic media framing studies.","Task":["media framing"],"Method":["media framing"]},{"title":"FrameAxis: Characterizing Framing Bias and Intensity with Word Embedding","abstract":"We propose FrameAxis, a method of characterizing the framing of a given text by identifying the most relevant semantic axes (\\"microframes\\") defined by antonym word pairs. In contrast to the traditional framing analysis, which has been constrained by a small number of manually annotated general frames, our unsupervised approach provides much more detailed insights, by considering a host of semantic axes. Our method is capable of quantitatively teasing out framing bias -- how biased a text is in each microframe -- and framing intensity -- how much each microframe is used -- from the text, offering a nuanced characterization of framing. We evaluate our approach using SemEval datasets as well as three other datasets and human evaluations, demonstrating that FrameAxis can reliably characterize documents with relevant microframes. Our method may allow scalable and nuanced computational analyses of framing across disciplines.","url":"https://www.semanticscholar.org/paper/245bcbdfff19721c226059a5d5b1ae71b67e0572","sentence":"Title: frameaxis: characterizing framing bias and intensity with word embedding Abstract: we propose frameaxis, a method of characterizing the framing of a given text by identifying the most relevant semantic axes (\\"microframes\\") defined by antonym word pairs. in contrast to the traditional framing analysis, which has been constrained by a small number of manually annotated general frames, our unsupervised approach provides much more detailed insights, by considering a host of semantic axes. our method is capable of quantitatively teasing out framing bias -- how biased a text is in each microframe -- and framing intensity -- how much each microframe is used -- from the text, offering a nuanced characterization of framing. we evaluate our approach using semeval datasets as well as three other datasets and human evaluations, demonstrating that frameaxis can reliably characterize documents with relevant microframes. our method may allow scalable and nuanced computational analyses of framing across disciplines.","Task":["frameaxis"],"Method":["frameaxis"]},{"title":"Connotation Frames of Power and Agency in Modern Films","abstract":"The framing of an action influences how we perceive its actor. We introduce connotation frames of power and agency, a pragmatic formalism organized using frame semantic representations, to model how different levels of power and agency are implicitly projected on actors through their actions. We use the new power and agency frames to measure the subtle, but prevalent, gender bias in the portrayal of modern film characters and provide insights that deviate from the well-known Bechdel test. Our contributions include an extended lexicon of connotation frames along with a web interface that provides a comprehensive analysis through the lens of connotation frames.","url":"https://www.semanticscholar.org/paper/2cd74561109e2675634e0f4f0129fcfbc238a1d8","sentence":"Title: connotation frames of power and agency in modern films Abstract: the framing of an action influences how we perceive its actor. we introduce connotation frames of power and agency, a pragmatic formalism organized using frame semantic representations, to model how different levels of power and agency are implicitly projected on actors through their actions. we use the new power and agency frames to measure the subtle, but prevalent, gender bias in the portrayal of modern film characters and provide insights that deviate from the well-known bechdel test. our contributions include an extended lexicon of connotation frames along with a web interface that provides a comprehensive analysis through the lens of connotation frames.","Task":["gender bias"],"Method":["gender frames"]},{"title":"Analyzing Framing through the Casts of Characters in the News","abstract":"We present an unsupervised model for the discovery and clustering of latent \u201cpersonas\u201d (characterizations of entities). Our model simultaneously clusters documents featuring similar collections of personas. We evaluate this model on a collection of news articles about immigration, showing that personas help predict the coarse-grained framing annotations in the Media Frames Corpus. We also introduce automated model selection as a fair and robust form of feature evaluation.","url":"https://www.semanticscholar.org/paper/15130cdd46ff7e3a69384e6c62ab1775c4aba96f","sentence":"Title: analyzing framing through the casts of characters in the news Abstract: we present an unsupervised model for the discovery and clustering of latent \u201cpersonas\u201d (characterizations of entities). our model simultaneously clusters documents featuring similar collections of personas. we evaluate this model on a collection of news articles about immigration, showing that personas help predict the coarse-grained framing annotations in the media frames corpus. we also introduce automated model selection as a fair and robust form of feature evaluation.","Task":["framing framing"],"Method":["model framing"]},{"title":"The Media Frames Corpus: Annotations of Frames Across Issues","abstract":"We describe the first version of the Media Frames Corpus: several thousand news articles on three policy issues, annotated in terms of media framing. We motivate framing as a phenomenon of study for computational linguistics and describe our annotation process.","url":"https://www.semanticscholar.org/paper/92408cc19033cc4af29accef3793014ab79355c2","sentence":"Title: the media frames corpus: annotations of frames across issues Abstract: we describe the first version of the media frames corpus: several thousand news articles on three policy issues, annotated in terms of media framing. we motivate framing as a phenomenon of study for computational linguistics and describe our annotation process.","Task":["media framing"],"Method":["media framing"]},{"title":"Who Falls for Online Political Manipulation?","abstract":"Social media, once hailed as a vehicle for democratization and the promotion of positive social change across the globe, are under attack for becoming a tool of political manipulation and spread of disinformation. A case in point is the alleged use of trolls by Russia to spread malicious content in Western elections. This paper examines the Russian interference campaign in the 2016 US presidential election on Twitter. Our aim is twofold: first, we test whether predicting users who spread trolls\u2019 content is feasible in order to gain insight on how to contain their influence in the future; second, we identify features that are most predictive of users who either intentionally or unintentionally play a vital role in spreading this malicious content. We collected a dataset with over 43 million election-related posts shared on Twitter between September 16 and November 9, 2016, by about 5.7 million users. This dataset includes accounts associated with the Russian trolls identified by the US Congress. Proposed models are able to very accurately identify users who spread the trolls\u2019 content (average AUC score of 96%, using 10-fold validation). We show that political ideology, bot likelihood scores, and some activity-related account meta data are the most predictive features of whether a user spreads trolls\u2019 content or not.","url":"https://www.semanticscholar.org/paper/c899c6048a1bd99590972c5fb2f9fd00db8d4e18","sentence":"Title: who falls for online political manipulation? Abstract: social media, once hailed as a vehicle for democratization and the promotion of positive social change across the globe, are under attack for becoming a tool of political manipulation and spread of disinformation. a case in point is the alleged use of trolls by russia to spread malicious content in western elections. this paper examines the russian interference campaign in the 2016 us presidential election on twitter. our aim is twofold: first, we test whether predicting users who spread trolls\u2019 content is feasible in order to gain insight on how to contain their influence in the future; second, we identify features that are most predictive of users who either intentionally or unintentionally play a vital role in spreading this malicious content. we collected a dataset with over 43 million election-related posts shared on twitter between september 16 and november 9, 2016, by about 5.7 million users. this dataset includes accounts associated with the russian trolls identified by the us congress. proposed models are able to very accurately identify users who spread the trolls\u2019 content (average auc score of 96%, using 10-fold validation). we show that political ideology, bot likelihood scores, and some activity-related account meta data are the most predictive features of whether a user spreads trolls\u2019 content or not.","Task":["bot prediction"],"Method":["bot likelihood"]},{"title":"Linguistic Models for Analyzing and Detecting Biased Language","abstract":"Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. Bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective intensifiers. These insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task.","url":"https://www.semanticscholar.org/paper/2a501b074261e81b9126e80a0a308cfa5e76f8c1","sentence":"Title: linguistic models for analyzing and detecting biased language Abstract: unbiased language is a requirement for reference sources like encyclopedias and scientific texts. bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. to this end we analyze real instances of human edits designed to remove bias from wikipedia articles. the analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. we identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective intensifiers. these insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. our linguistically-informed model performs almost as well as humans tested on the same task.","Task":["bias bias"],"Method":["biased bias"]},{"title":"Misinfo Belief Frames: A Case Study on Covid & Climate News","abstract":"Prior beliefs of readers impact the way in which they project meaning onto news headlines. These beliefs can influence their perception of news reliability, as well as their reaction to news, and their likelihood of spreading the misinformation through social networks. However, most prior work focuses on fact-checking veracity of news or stylometry rather than measuring impact of misinformation. We propose Misinfo Belief Frames, a formalism for understanding how readers perceive the reliability of news and the impact of misinformation. We also introduce the Misinfo Belief Frames (MBF) corpus, a dataset of 66k inferences over 23.5k headlines. Misinformation frames use commonsense reasoning to uncover implications of real and fake news headlines focused on global crises: the Covid-19 pandemic and climate change. Our results using large-scale language modeling to predict misinformation frames show that machine-generated inferences can influence readers\u2019 trust in news headlines (readers\u2019 trust in news headlines was affected in 29.3% of cases). This demonstrates the potential effectiveness of using generated frames to counter misinformation.","url":"https://www.semanticscholar.org/paper/1778d1da6264ed4274fb94115ebbc3b4f34c9f7a","sentence":"Title: misinfo belief frames: a case study on covid & climate news Abstract: prior beliefs of readers impact the way in which they project meaning onto news headlines. these beliefs can influence their perception of news reliability, as well as their reaction to news, and their likelihood of spreading the misinformation through social networks. however, most prior work focuses on fact-checking veracity of news or stylometry rather than measuring impact of misinformation. we propose misinfo belief frames, a formalism for understanding how readers perceive the reliability of news and the impact of misinformation. we also introduce the misinfo belief frames (mbf) corpus, a dataset of 66k inferences over 23.5k headlines. misinformation frames use commonsense reasoning to uncover implications of real and fake news headlines focused on global crises: the covid-19 pandemic and climate change. our results using large-scale language modeling to predict misinformation frames show that machine-generated inferences can influence readers\u2019 trust in news headlines (readers\u2019 trust in news headlines was affected in 29.3% of cases). this demonstrates the potential effectiveness of using generated frames to counter misinformation.","Task":["misinformation frames"],"Method":["misinformation frames"]},{"title":"Fighting the COVID-19 Infodemic in Social Media: A Holistic Perspective and a Call to Arms","abstract":"With the outbreak of the COVID-19 pandemic, people turned to social media to read and to share timely information including statistics, warnings, advice, and inspirational stories. Unfortunately, alongside all this useful information, there was also a new blending of medical and political misinformation and disinformation, which gave rise to the first global infodemic. While fighting this infodemic is typically thought of in terms of factuality, the problem is much broader as malicious content includes not only fake news, rumors, and conspiracy theories, but also promotion of fake cures, panic, racism, xenophobia, and mistrust in the authorities, among others. This is a complex problem that needs a holistic approach combining the perspectives of journalists, fact-checkers, policymakers, government entities, social media platforms, and society as a whole. Taking them into account we define an annotation schema and detailed annotation instructions, which reflect these perspectives. We performed initial annotations using this schema, and our initial experiments demonstrated sizable improvements over the baselines. Now, we issue a call to arms to the research community and beyond to join the fight by supporting our crowdsourcing annotation efforts.","url":"https://www.semanticscholar.org/paper/460986c12b9fdedc3bf9a710c429c5a84608055a","sentence":"Title: fighting the covid-19 infodemic in social media: a holistic perspective and a call to arms Abstract: with the outbreak of the covid-19 pandemic, people turned to social media to read and to share timely information including statistics, warnings, advice, and inspirational stories. unfortunately, alongside all this useful information, there was also a new blending of medical and political misinformation and disinformation, which gave rise to the first global infodemic. while fighting this infodemic is typically thought of in terms of factuality, the problem is much broader as malicious content includes not only fake news, rumors, and conspiracy theories, but also promotion of fake cures, panic, racism, xenophobia, and mistrust in the authorities, among others. this is a complex problem that needs a holistic approach combining the perspectives of journalists, fact-checkers, policymakers, government entities, social media platforms, and society as a whole. taking them into account we define an annotation schema and detailed annotation instructions, which reflect these perspectives. we performed initial annotations using this schema, and our initial experiments demonstrated sizable improvements over the baselines. now, we issue a call to arms to the research community and beyond to join the fight by supporting our crowdsourcing annotation efforts.","Task":["misinformation annotation"],"Method":["annotation annotation"]},{"title":"The online competition between pro- and anti-vaccination views","abstract":"Distrust in scientific expertise 1 \u2013 14 is dangerous. Opposition to vaccination with a future vaccine against SARS-CoV-2, the causal agent of COVID-19, for example, could amplify outbreaks 2 \u2013 4 , as happened for measles in 2019 5 , 6 . Homemade remedies 7 , 8 and falsehoods are being shared widely on the Internet, as well as dismissals of expert advice 9 \u2013 11 . There is a lack of understanding about how this distrust evolves at the system level 13 , 14 . Here we provide a map of the contention surrounding vaccines that has emerged from the global pool of around three billion Facebook users. Its core reveals a multi-sided landscape of unprecedented intricacy that involves nearly 100\xa0million individuals partitioned into highly dynamic, interconnected clusters across cities, countries, continents and languages. Although smaller in overall size, anti-vaccination clusters manage to become highly entangled with undecided clusters in the main online network, whereas pro-vaccination clusters are more peripheral. Our theoretical framework reproduces the recent explosive growth in anti-vaccination views, and predicts that these views will dominate in a decade. Insights provided by this framework can inform new policies and approaches to interrupt this shift to negative views. Our results challenge the conventional thinking about undecided individuals in issues of contention surrounding health, shed light on other issues of contention such as climate change 11 , and highlight the key role of network cluster dynamics in multi-species ecologies 15 . Insights into the interactions between pro- and anti-vaccination clusters on Facebook can enable policies and approaches that attempt to interrupt the shift to anti-vaccination views and persuade undecided individuals to adopt a pro-vaccination stance.","url":"https://www.semanticscholar.org/paper/9129399d6b1599508709e3a72f80defd52ef89c9","sentence":"Title: the online competition between pro- and anti-vaccination views Abstract: distrust in scientific expertise 1 \u2013 14 is dangerous. opposition to vaccination with a future vaccine against sars-cov-2, the causal agent of covid-19, for example, could amplify outbreaks 2 \u2013 4 , as happened for measles in 2019 5 , 6 . homemade remedies 7 , 8 and falsehoods are being shared widely on the internet, as well as dismissals of expert advice 9 \u2013 11 . there is a lack of understanding about how this distrust evolves at the system level 13 , 14 . here we provide a map of the contention surrounding vaccines that has emerged from the global pool of around three billion facebook users. its core reveals a multi-sided landscape of unprecedented intricacy that involves nearly 100\xa0million individuals partitioned into highly dynamic, interconnected clusters across cities, countries, continents and languages. although smaller in overall size, anti-vaccination clusters manage to become highly entangled with undecided clusters in the main online network, whereas pro-vaccination clusters are more peripheral. our theoretical framework reproduces the recent explosive growth in anti-vaccination views, and predicts that these views will dominate in a decade. insights provided by this framework can inform new policies and approaches to interrupt this shift to negative views. our results challenge the conventional thinking about undecided individuals in issues of contention surrounding health, shed light on other issues of contention such as climate change 11 , and highlight the key role of network cluster dynamics in multi-species ecologies 15 . insights into the interactions between pro- and anti-vaccination clusters on facebook can enable policies and approaches that attempt to interrupt the shift to anti-vaccination views and persuade undecided individuals to adopt a pro-vaccination stance.","Task":["vaccine bias"],"Method":["online bias"]},{"title":"A Survey on Multimodal Disinformation Detection","abstract":"Recent years have witnessed the proliferation of fake news, propaganda, misinformation, and disinformation online. While initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract much more attention, and spread further than simple text. As a result, researchers started targeting different modalities and combinations thereof. As different modalities are studied in different research communities, with insufficient interaction, here we offer a survey that explores the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. Moreover, while some studies focused on factuality, others investigated how harmful the content is. While these two components in the definition of disinformation \u2013 (i) factuality and (ii) harmfulness, are equally important, they are typically studied in isolation. Thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. Finally, we discuss current challenges and future research directions.","url":"https://www.semanticscholar.org/paper/ed1c17451a23471afde91c109ecadc6aab8b2ba6","sentence":"Title: a survey on multimodal disinformation detection Abstract: recent years have witnessed the proliferation of fake news, propaganda, misinformation, and disinformation online. while initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract much more attention, and spread further than simple text. as a result, researchers started targeting different modalities and combinations thereof. as different modalities are studied in different research communities, with insufficient interaction, here we offer a survey that explores the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. moreover, while some studies focused on factuality, others investigated how harmful the content is. while these two components in the definition of disinformation \u2013 (i) factuality and (ii) harmfulness, are equally important, they are typically studied in isolation. thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. finally, we discuss current challenges and future research directions.","Task":["disinformation detection"],"Method":["disinformation detection"]},{"title":"Automated Fact-Checking for Assisting Human Fact-Checkers","abstract":"The reporting and analysis of current events around the globe has expanded from professional, editorlead journalism all the way to citizen journalism. Politicians and other key players enjoy direct access to their audiences through social media, bypassing the filters of official cables or traditional media. However, the multiple advantages of free speech and direct communication are dimmed by the misuse of the media to spread inaccurate or misleading claims. These phenomena have led to the modern incarnation of the fact-checker \u2014 a professional whose main aim is to examine claims using available evidence to assess their veracity. As in other text forensics tasks, the amount of information available makes the work of the fact-checker more difficult. With this in mind, starting from the perspective of the professional fact-checker, we survey the available intelligent technologies that can support the human expert in the different steps of her fact-checking endeavor. These include identifying claims worth fact-checking; detecting relevant previously fact-checked claims; retrieving relevant evidence to fact-check a claim; and actually verifying a claim. In each case, we pay attention to the challenges in future work and the potential impact on real-world fact-checking.","url":"https://www.semanticscholar.org/paper/38d243b9f6e2c786699dbc83513fb190372cde07","sentence":"Title: automated fact-checking for assisting human fact-checkers Abstract: the reporting and analysis of current events around the globe has expanded from professional, editorlead journalism all the way to citizen journalism. politicians and other key players enjoy direct access to their audiences through social media, bypassing the filters of official cables or traditional media. however, the multiple advantages of free speech and direct communication are dimmed by the misuse of the media to spread inaccurate or misleading claims. these phenomena have led to the modern incarnation of the fact-checker \u2014 a professional whose main aim is to examine claims using available evidence to assess their veracity. as in other text forensics tasks, the amount of information available makes the work of the fact-checker more difficult. with this in mind, starting from the perspective of the professional fact-checker, we survey the available intelligent technologies that can support the human expert in the different steps of her fact-checking endeavor. these include identifying claims worth fact-checking; detecting relevant previously fact-checked claims; retrieving relevant evidence to fact-check a claim; and actually verifying a claim. in each case, we pay attention to the challenges in future work and the potential impact on real-world fact-checking.","Task":["automated verification"],"Method":["automated verification"]},{"title":"Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection","abstract":"Fake news has altered society in negative ways in politics and culture. It has adversely affected both online social network systems as well as offline communities and conversations. Using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news. However, a lack of effective, comprehensive datasets has been a problem for fake news research and detection model development. Prior fake news datasets do not provide multimodal text and image data, metadata, comment data, and fine-grained fake news categorization at the scale and breadth of our dataset. We present Fakeddit, a novel multimodal dataset consisting of over 1 million samples from multiple categories of fake news. After being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision. We construct hybrid text+image models and perform extensive experiments for multiple variations of classification, demonstrating the importance of the novel aspect of multimodality and fine-grained classification unique to Fakeddit.","url":"https://www.semanticscholar.org/paper/76de89ca66f898e8211acba7392ef2d4a7e14125","sentence":"Title: fakeddit: a new multimodal benchmark dataset for fine-grained fake news detection Abstract: fake news has altered society in negative ways in politics and culture. it has adversely affected both online social network systems as well as offline communities and conversations. using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news. however, a lack of effective, comprehensive datasets has been a problem for fake news research and detection model development. prior fake news datasets do not provide multimodal text and image data, metadata, comment data, and fine-grained fake news categorization at the scale and breadth of our dataset. we present fakeddit, a novel multimodal dataset consisting of over 1 million samples from multiple categories of fake news. after being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision. we construct hybrid text+image models and perform extensive experiments for multiple variations of classification, demonstrating the importance of the novel aspect of multimodality and fine-grained classification unique to fakeddit.","Task":["this data"],"Method":["fakeddit"]},{"title":"Automatic Detection of Fake News","abstract":"The proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in online news. Our contribution is twofold. First, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. We describe the collection, annotation, and validation process in detail and present several exploratory analyses on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors, and show that we can achieve accuracies of up to 76%. In addition, we provide comparative analyses of the automatic and manual identification of fake news.","url":"https://www.semanticscholar.org/paper/c6a9ca56c93323c0199dd22631d1cf731bdd7ec1","sentence":"Title: automatic detection of fake news Abstract: the proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. in this paper, we focus on the automatic identification of fake content in online news. our contribution is twofold. first, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. we describe the collection, annotation, and validation process in detail and present several exploratory analyses on the identification of linguistic differences in fake and legitimate news content. second, we conduct a set of learning experiments to build accurate fake news detectors, and show that we can achieve accuracies of up to 76%. in addition, we provide comparative analyses of the automatic and manual identification of fake news.","Task":["automatic detection"],"Method":["automatic detection"]},{"title":"\\"Liar, Liar Pants on Fire\\": A New Benchmark Dataset for Fake News Detection","abstract":"Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present liar: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.","url":"https://www.semanticscholar.org/paper/03c294ad75bd1bac92217419ac25358227f6a901","sentence":"Title: \\"liar, liar pants on fire\\": a new benchmark dataset for fake news detection Abstract: automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. however, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. in this paper, we present liar: a new, publicly available dataset for fake news detection. we collected a decade-long, 12.8k manually labeled short statements in various contexts from politifact.com, which provides detailed analysis report and links to source documents for each case. this dataset can be used for fact-checking research as well. notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. we have designed a novel, hybrid convolutional neural network to integrate meta-data with text. we show that this hybrid approach can improve a text-only deep learning model.","Task":["language detection"],"Method":["semantic learning"]},{"title":"Weaponized Health Communication: Twitter Bots and Russian Trolls Amplify the Vaccine Debate","abstract":"Objectives To understand how Twitter bots and trolls (\u201cbots\u201d) promote online health content. Methods We compared bots\u2019 to average users\u2019 rates of vaccine-relevant messages, which we collected online from July 2014 through September 2017. We estimated the likelihood that users were bots, comparing proportions of polarized and antivaccine tweets across user types. We conducted a content analysis of a Twitter hashtag associated with Russian troll activity. Results Compared with average users, Russian trolls (\u03c72(1)\u2009=\u2009102.0; P\u2009<\u2009.001), sophisticated bots (\u03c72(1)\u2009=\u200928.6; P\u2009<\u2009.001), and \u201ccontent polluters\u201d (\u03c72(1)\u2009=\u20097.0; P\u2009<\u2009.001) tweeted about vaccination at higher rates. Whereas content polluters posted more antivaccine content (\u03c72(1)\u2009=\u200911.18; P\u2009<\u2009.001), Russian trolls amplified both sides. Unidentifiable accounts were more polarized (\u03c72(1)\u2009=\u200912.1; P\u2009<\u2009.001) and antivaccine (\u03c72(1)\u2009=\u200935.9; P\u2009<\u2009.001). Analysis of the Russian troll hashtag showed that its messages were more political and divisive. Conclusions Whereas bots that spread malware and unsolicited content disseminated antivaccine messages, Russian trolls promoted discord. Accounts masquerading as legitimate users create false equivalency, eroding public consensus on vaccination. Public Health Implications. Directly confronting vaccine skeptics enables bots to legitimize the vaccine debate. More research is needed to determine how best to combat bot-driven content.","url":"https://www.semanticscholar.org/paper/8415274c8fb370cbab84ad82ab2f469786ddee72","sentence":"Title: weaponized health communication: twitter bots and russian trolls amplify the vaccine debate Abstract: objectives to understand how twitter bots and trolls (\u201cbots\u201d) promote online health content. methods we compared bots\u2019 to average users\u2019 rates of vaccine-relevant messages, which we collected online from july 2014 through september 2017. we estimated the likelihood that users were bots, comparing proportions of polarized and antivaccine tweets across user types. we conducted a content analysis of a twitter hashtag associated with russian troll activity. results compared with average users, russian trolls (\u03c72(1)\u2009=\u2009102.0; p\u2009<\u2009.001), sophisticated bots (\u03c72(1)\u2009=\u200928.6; p\u2009<\u2009.001), and \u201ccontent polluters\u201d (\u03c72(1)\u2009=\u20097.0; p\u2009<\u2009.001) tweeted about vaccination at higher rates. whereas content polluters posted more antivaccine content (\u03c72(1)\u2009=\u200911.18; p\u2009<\u2009.001), russian trolls amplified both sides. unidentifiable accounts were more polarized (\u03c72(1)\u2009=\u200912.1; p\u2009<\u2009.001) and antivaccine (\u03c72(1)\u2009=\u200935.9; p\u2009<\u2009.001). analysis of the russian troll hashtag showed that its messages were more political and divisive. conclusions whereas bots that spread malware and unsolicited content disseminated antivaccine messages, russian trolls promoted discord. accounts masquerading as legitimate users create false equivalency, eroding public consensus on vaccination. public health implications. directly confronting vaccine skeptics enables bots to legitimize the vaccine debate. more research is needed to determine how best to combat bot-driven content.","Task":["vaccine misinformation"],"Method":["viral messaging"]},{"title":"Fake News: Spread of Misinformation about Urological Conditions on Social Media.","abstract":"Although there is a large amount of user-generated content about urological health issues on social media, much of this content has not been vetted for information accuracy. In this article, we review the literature on the quality and balance of information on urological health conditions on social networks. Across a wide range of benign and malignant urological conditions, studies show a substantial amount of commercial, biased and/or inaccurate information present on popular social networking sites. The healthcare community should take proactive steps to improve the quality of medical information on social networks. PATIENT SUMMARY: In this review, we examined the spread of misinformation about urological health conditions on social media. We found that a significant amount of the circulating information is commercial, biased or misinformative.","url":"https://www.semanticscholar.org/paper/1dad69f1fd4403aed4d3d709ab794113291d625c","sentence":"Title: fake news: spread of misinformation about urological conditions on social media. Abstract: although there is a large amount of user-generated content about urological health issues on social media, much of this content has not been vetted for information accuracy. in this article, we review the literature on the quality and balance of information on urological health conditions on social networks. across a wide range of benign and malignant urological conditions, studies show a substantial amount of commercial, biased and/or inaccurate information present on popular social networking sites. the healthcare community should take proactive steps to improve the quality of medical information on social networks. patient summary: in this review, we examined the spread of misinformation about urological health conditions on social media. we found that a significant amount of the circulating information is commercial, biased or misinformative.","Task":["false misinformation"],"Method":["fake news"]},{"title":"Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking","abstract":"We present an analytic study on the language of news media in the context of political fact-checking and fake news detection. We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.","url":"https://www.semanticscholar.org/paper/7d3c2ff37d04914836f9cbd9ce54b6c97aa74a22","sentence":"Title: truth of varying shades: analyzing language in fake news and political fact-checking Abstract: we present an analytic study on the language of news media in the context of political fact-checking and fake news detection. we compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. to probe the feasibility of automatic political fact-checking, we also present a case study based on politifact.com using their factuality judgments on a 6-point scale. experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.","Task":["linguistic language"],"Method":["semantic language"]},{"title":"Coronavirus on Social Media: Analyzing Misinformation in Twitter Conversations","abstract":"The ongoing Coronavirus Disease (COVID-19) pandemic highlights the interconnected-ness of our present-day globalized world. With social distancing policies in place, virtual communication has become an important source of (mis)information. As increasing number of people rely on social media platforms for news, identifying misinformation has emerged as a critical task in these unprecedented times. In addition to being malicious, the spread of such information poses a serious public health risk. To this end, we design a dashboard to track misinformation on popular social media news sharing platform - Twitter. Our dashboard allows visibility into the social media discussions around Coronavirus and the quality of information shared on the platform as the situation evolves. We collect streaming data using the Twitter API from March 1, 2020 to date and provide analysis of topic clusters and social sentiments related to important emerging policies such as \\"#socialdistancing\\" and \\"#workfromhome\\". We track emerging hashtags over time, and provide location and time sensitive analysis of sentiments. In addition, we study the challenging problem of misinformation on social media, and provide a detection method to identify false, misleading and clickbait contents from Twitter information cascades. The dashboard maintains an evolving list of detected misinformation cascades with the corresponding detection scores, accessible online athttps://ksharmar.this http URL.","url":"https://www.semanticscholar.org/paper/29370adbf0674ad72d5e0bcc59582c22035bd0aa","sentence":"Title: coronavirus on social media: analyzing misinformation in twitter conversations Abstract: the ongoing coronavirus disease (covid-19) pandemic highlights the interconnected-ness of our present-day globalized world. with social distancing policies in place, virtual communication has become an important source of (mis)information. as increasing number of people rely on social media platforms for news, identifying misinformation has emerged as a critical task in these unprecedented times. in addition to being malicious, the spread of such information poses a serious public health risk. to this end, we design a dashboard to track misinformation on popular social media news sharing platform - twitter. our dashboard allows visibility into the social media discussions around coronavirus and the quality of information shared on the platform as the situation evolves. we collect streaming data using the twitter api from march 1, 2020 to date and provide analysis of topic clusters and social sentiments related to important emerging policies such as \\"#socialdistancing\\" and \\"#workfromhome\\". we track emerging hashtags over time, and provide location and time sensitive analysis of sentiments. in addition, we study the challenging problem of misinformation on social media, and provide a detection method to identify false, misleading and clickbait contents from twitter information cascades. the dashboard maintains an evolving list of detected misinformation cascades with the corresponding detection scores, accessible online athttps://ksharmar.this http url.","Task":["misinformation misinformation"],"Method":["Twitter filtering"]},{"title":"Combating Fake News: A Survey on Identification and Mitigation Techniques","abstract":"The proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news, and mitigation of its widespread impact on public opinion. While much of the earlier research was focused on identification of fake news based on its contents or by exploiting users\' engagements with the news on social media, there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society. In this survey, we describe the modern-day problem of fake news and, in particular, highlight the technical challenges associated with it. We discuss existing methods and techniques applicable to both identification and mitigation, with a focus on the significant advances in each method and their advantages and limitations. In addition, research has often been limited by the quality of existing datasets and their specific application contexts. To alleviate this problem, we comprehensively compile and summarize characteristic features of available datasets. Furthermore, we outline new directions of research to facilitate future development of effective and interdisciplinary solutions.","url":"https://www.semanticscholar.org/paper/1213d98f3d0a42d575bbd2c2af0309c7f76024a1","sentence":"Title: combating fake news: a survey on identification and mitigation techniques Abstract: the proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news, and mitigation of its widespread impact on public opinion. while much of the earlier research was focused on identification of fake news based on its contents or by exploiting users\' engagements with the news on social media, there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society. in this survey, we describe the modern-day problem of fake news and, in particular, highlight the technical challenges associated with it. we discuss existing methods and techniques applicable to both identification and mitigation, with a focus on the significant advances in each method and their advantages and limitations. in addition, research has often been limited by the quality of existing datasets and their specific application contexts. to alleviate this problem, we comprehensively compile and summarize characteristic features of available datasets. furthermore, we outline new directions of research to facilitate future development of effective and interdisciplinary solutions.","Task":["fake news"],"Method":["attribution analysis"]},{"title":"The spread of true and false news online","abstract":"Lies spread faster than the truth There is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. To understand how false news spreads, Vosoughi et al. used a data set of rumor cascades on Twitter from 2006 to 2017. About 126,000 rumors were spread by \u223c3 million people. False news reached more people than the truth; the top 1% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. Falsehood also diffused faster than the truth. The degree of novelty and the emotional reactions of recipients may be responsible for the differences observed. Science, this issue p. 1146 A large-scale analysis of tweets reveals that false rumors spread further and faster than the truth. We investigated the differential diffusion of all of the verified true and false news stories distributed on Twitter from 2006 to 2017. The data comprise ~126,000 stories tweeted by ~3 million people more than 4.5 million times. We classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98% agreement on the classifications. Falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. We found that false news was more novel than true news, which suggests that people were more likely to share novel information. Whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. Contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.","url":"https://www.semanticscholar.org/paper/a1e58f89f57f57fad3c77cd558444ad5ad64b525","sentence":"Title: the spread of true and false news online Abstract: lies spread faster than the truth there is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. to understand how false news spreads, vosoughi et al. used a data set of rumor cascades on twitter from 2006 to 2017. about 126,000 rumors were spread by \u223c3 million people. false news reached more people than the truth; the top 1% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. falsehood also diffused faster than the truth. the degree of novelty and the emotional reactions of recipients may be responsible for the differences observed. science, this issue p. 1146 a large-scale analysis of tweets reveals that false rumors spread further and faster than the truth. we investigated the differential diffusion of all of the verified true and false news stories distributed on twitter from 2006 to 2017. the data comprise ~126,000 stories tweeted by ~3 million people more than 4.5 million times. we classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98% agreement on the classifications. falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. we found that false news was more novel than true news, which suggests that people were more likely to share novel information. whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.","Task":["false diffusion"],"Method":["false diffusion"]},{"title":"Fake News: A Survey of Research, Detection Methods, and Opportunities","abstract":"The explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news analysis, detection and intervention. This survey comprehensively and systematically reviews fake news research. The survey identifies and specifies fundamental theories across various disciplines, e.g., psychology and social science, to facilitate and enhance the interdisciplinary research of fake news. Current fake news research is reviewed, summarized and evaluated. These studies focus on fake news from four perspective: (1) the false knowledge it carries, (2) its writing style, (3) its propagation patterns, and (4) the credibility of its creators and spreaders. We characterize each perspective with various analyzable and utilizable information provided by news and its spreaders, various strategies and frameworks that are adaptable, and techniques that are applicable. By reviewing the characteristics of fake news and open issues in fake news studies, we highlight some potential research tasks at the end of this survey.","url":"https://www.semanticscholar.org/paper/bc8c416f821b93795370524247c8a455c373ee6e","sentence":"Title: fake news: a survey of research, detection methods, and opportunities Abstract: the explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news analysis, detection and intervention. this survey comprehensively and systematically reviews fake news research. the survey identifies and specifies fundamental theories across various disciplines, e.g., psychology and social science, to facilitate and enhance the interdisciplinary research of fake news. current fake news research is reviewed, summarized and evaluated. these studies focus on fake news from four perspective: (1) the false knowledge it carries, (2) its writing style, (3) its propagation patterns, and (4) the credibility of its creators and spreaders. we characterize each perspective with various analyzable and utilizable information provided by news and its spreaders, various strategies and frameworks that are adaptable, and techniques that are applicable. by reviewing the characteristics of fake news and open issues in fake news studies, we highlight some potential research tasks at the end of this survey.","Task":["fake news"],"Method":["fake news"]},{"title":"Fact or Fiction: Verifying Scientific Claims","abstract":"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that supports or refutes a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that these models benefit from combined training on a large dataset of claims about Wikipedia articles, together with the new SciFact data. We show that our claim verification system is able to identify plausible evidence for 23 / 36 claims relevant to COVID-19 on the CORD-19 corpus. Our results and experiments strongly suggest that our new task and data will support significant future research efforts.","url":"https://www.semanticscholar.org/paper/9e8ac8df8b46c36cad3f307f85975012479b5a32","sentence":"Title: fact or fiction: verifying scientific claims Abstract: we introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that supports or refutes a given scientific claim, and to identify rationales justifying each decision. to study this task, we construct scifact, a dataset of 1.4k expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. we develop baseline models for scifact, and demonstrate that these models benefit from combined training on a large dataset of claims about wikipedia articles, together with the new scifact data. we show that our claim verification system is able to identify plausible evidence for 23 / 36 claims relevant to covid-19 on the cord-19 corpus. our results and experiments strongly suggest that our new task and data will support significant future research efforts.","Task":["claim verification"],"Method":["claim verification"]},{"title":"Rumor Cascades","abstract":"Online social networks provide a rich substrate for rumor propagation. Information received via friends tends to be trusted, and online social networks allow individuals to transmit information to many friends at once. By referencing known rumors from Snopes.com, a popular website documenting memes and urban legends, we track the propagation of thousands of rumors appearing on Facebook. From this sample we infer the rates at which rumors from different categories and of varying truth value are uploaded and reshared. We find that rumor cascades run deeper in the social network than reshare cascades in general. We then examine the effect of individual reshares receiving a comment containing a link to a Snopes article on the evolution of the cascade. We find that receiving such a comment increases the likelihood that a reshare of a rumor will be deleted. Furthermore, large cascades are able to accumulate hundreds of Snopes comments while continuing to propagate. Finally, using a dataset of rumors copied and pasted from one status update to another, we show that rumors change over time and that different variants tend to dominate different bursts in popularity.","url":"https://www.semanticscholar.org/paper/1e3d1055fc65966bc2c64505785207d7e8d71022","sentence":"Title: rumor cascades Abstract: online social networks provide a rich substrate for rumor propagation. information received via friends tends to be trusted, and online social networks allow individuals to transmit information to many friends at once. by referencing known rumors from snopes.com, a popular website documenting memes and urban legends, we track the propagation of thousands of rumors appearing on facebook. from this sample we infer the rates at which rumors from different categories and of varying truth value are uploaded and reshared. we find that rumor cascades run deeper in the social network than reshare cascades in general. we then examine the effect of individual reshares receiving a comment containing a link to a snopes article on the evolution of the cascade. we find that receiving such a comment increases the likelihood that a reshare of a rumor will be deleted. furthermore, large cascades are able to accumulate hundreds of snopes comments while continuing to propagate. finally, using a dataset of rumors copied and pasted from one status update to another, we show that rumors change over time and that different variants tend to dominate different bursts in popularity.","Task":["rumor propagation"],"Method":["rumor cascade"]},{"title":"A Benchmark Dataset of Check-worthy Factual Claims","abstract":"In this paper we present the ClaimBuster dataset of 23,533 statements extracted from all U.S. general election presidential debates and annotated by human coders. The ClaimBuster dataset can be leveraged in building computational methods to identify claims that are worth fact-checking from the myriad of sources of digital or traditional media. The ClaimBuster dataset is publicly available to the research community, and it can be found at this http URL.","url":"https://www.semanticscholar.org/paper/6f373f4711e1285bdec23069c9503d3bf77bfaef","sentence":"Title: a benchmark dataset of check-worthy factual claims Abstract: in this paper we present the claimbuster dataset of 23,533 statements extracted from all u.s. general election presidential debates and annotated by human coders. the claimbuster dataset can be leveraged in building computational methods to identify claims that are worth fact-checking from the myriad of sources of digital or traditional media. the claimbuster dataset is publicly available to the research community, and it can be found at this http url.","Task":["claimbuster"],"Method":["claimbuster"]},{"title":"Rumors, False Flags, and Digital Vigilantes: Misinformation on Twitter after the 2013 Boston Marathon Bombing","abstract":"The Boston Marathon bombing story unfolded on every possible carrier of information available in the spring of 2013, including Twitter. As information spread, it was filled with rumors (unsubstantiated information), and many of these rumors contained misinformation. Earlier studies have suggested that crowdsourced information flows can correct misinformation, and our research investigates this proposition. This exploratory research examines three rumors, later demonstrated to be false, that circulated on Twitter in the aftermath of the bombings. Our findings suggest that corrections to the misinformation emerge but are muted compared with the propagation of the misinformation. The similarities and differences we observe in the patterns of the misinformation and corrections contained within the stream over the days that followed the attacks suggest directions for possible research strategies to automatically detect misinformation.","url":"https://www.semanticscholar.org/paper/44ecb9783e43d726fc87b7ffdd580b01e13a85cb","sentence":"Title: rumors, false flags, and digital vigilantes: misinformation on twitter after the 2013 boston marathon bombing Abstract: the boston marathon bombing story unfolded on every possible carrier of information available in the spring of 2013, including twitter. as information spread, it was filled with rumors (unsubstantiated information), and many of these rumors contained misinformation. earlier studies have suggested that crowdsourced information flows can correct misinformation, and our research investigates this proposition. this exploratory research examines three rumors, later demonstrated to be false, that circulated on twitter in the aftermath of the bombings. our findings suggest that corrections to the misinformation emerge but are muted compared with the propagation of the misinformation. the similarities and differences we observe in the patterns of the misinformation and corrections contained within the stream over the days that followed the attacks suggest directions for possible research strategies to automatically detect misinformation.","Task":["false misinformation"],"Method":["automated flags"]},{"title":"Social Media and Fake News in the 2016 Election","abstract":"Following the 2016 U.S. presidential election, many have expressed concern about the effects of false stories (\u201cfake news\u201d), circulated largely through social media. We discuss the economics of fake news and present new data on its consumption prior to the election. Drawing on web browsing data, archives of fact-checking websites, and results from a new online survey, we find: (i) social media was an important but not dominant source of election news, with 14 percent of Americans calling social media their \u201cmost important\u201d source; (ii) of the known false news stories that appeared in the three months before the election, those favoring Trump were shared a total of 30 million times on Facebook, while those favoring Clinton were shared 8 million times; (iii) the average American adult saw on the order of one or perhaps several fake news stories in the months around the election, with just over half of those who recalled seeing them believing them; and (iv) people are much more likely to believe stories that favor their preferred candidate, especially if they have ideologically segregated social media networks.","url":"https://www.semanticscholar.org/paper/6f78b5608fed43f106da192f12e09d9edbd2fce0","sentence":"Title: social media and fake news in the 2016 election Abstract: following the 2016 u.s. presidential election, many have expressed concern about the effects of false stories (\u201cfake news\u201d), circulated largely through social media. we discuss the economics of fake news and present new data on its consumption prior to the election. drawing on web browsing data, archives of fact-checking websites, and results from a new online survey, we find: (i) social media was an important but not dominant source of election news, with 14 percent of americans calling social media their \u201cmost important\u201d source; (ii) of the known false news stories that appeared in the three months before the election, those favoring trump were shared a total of 30 million times on facebook, while those favoring clinton were shared 8 million times; (iii) the average american adult saw on the order of one or perhaps several fake news stories in the months around the election, with just over half of those who recalled seeing them believing them; and (iv) people are much more likely to believe stories that favor their preferred candidate, especially if they have ideologically segregated social media networks.","Task":["fake news"],"Method":["false news"]},{"title":"Extractive and Abstractive Explanations for Fact-Checking and Evaluation of News","abstract":"In this paper, we explore the construction of natural language explanations for news claims, with the goal of assisting fact-checking and news evaluation applications. We experiment with two methods: (1) an extractive method based on Biased TextRank \u2013 a resource-effective unsupervised graph-based algorithm for content extraction; and (2) an abstractive method based on the GPT-2 language model. We perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise.","url":"https://www.semanticscholar.org/paper/69116800a8a8195531d29c8e14cefb1c92cbb8a7","sentence":"Title: extractive and abstractive explanations for fact-checking and evaluation of news Abstract: in this paper, we explore the construction of natural language explanations for news claims, with the goal of assisting fact-checking and news evaluation applications. we experiment with two methods: (1) an extractive method based on biased textrank \u2013 a resource-effective unsupervised graph-based algorithm for content extraction; and (2) an abstractive method based on the gpt-2 language model. we perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise.","Task":["content extraction"],"Method":["content language"]},{"title":"That is a Known Lie: Detecting Previously Fact-Checked Claims","abstract":"The recent proliferation of \u201dfake news\u201d has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. As manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. Interestingly, despite the importance of the task, it has been largely ignored by the research community so far. Here, we aim to bridge this gap. In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. We further create a specialized dataset, which we release to the research community. Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.","url":"https://www.semanticscholar.org/paper/20b2f18aaf10a9221c5edf3720d4cce7da672104","sentence":"Title: that is a known lie: detecting previously fact-checked claims Abstract: the recent proliferation of \u201dfake news\u201d has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. as a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. as manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. interestingly, despite the importance of the task, it has been largely ignored by the research community so far. here, we aim to bridge this gap. in particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. we further create a specialized dataset, which we release to the research community. finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.","Task":["semantic bias"],"Method":["semantic ranking"]},{"title":"Defending Against Neural Fake News","abstract":"Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \\nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary\'s point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,\' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \\nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.","url":"https://www.semanticscholar.org/paper/ad7129af0644dbcafa9aa2f111cb76526ea444a1","sentence":"Title: defending against neural fake news Abstract: recent progress in natural language generation has raised dual-use concerns. while applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \\nmodern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary\'s point of view, and exploring potential mitigations to these threats. likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. we thus present a model for controllable text generation called grover. given a headline like `link found between vaccines and autism,\' grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \\ndeveloping robust verification techniques against generators like grover is critical. we find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. counterintuitively, the best defense against grover turns out to be grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. we investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. we conclude by discussing ethical issues regarding the technology, and plan to release grover publicly, helping pave the way for better detection of neural fake news.","Task":["neuralver"],"Method":["neural generators"]},{"title":"MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims","abstract":"We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.","url":"https://www.semanticscholar.org/paper/d8cb11d4be955f9869387a18967dee366eb851d9","sentence":"Title: multifc: a real-world multi-domain dataset for evidence-based fact checking of claims Abstract: we contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. it is collected from 26 fact checking websites in english, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. we present an in-depth analysis of the dataset, highlighting characteristics and challenges. further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. significant performance increases are achieved by encoding evidence, and by modelling metadata. our best-performing model achieves a macro f1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.","Task":["data accuracy"],"Method":["predictive verification"]},{"title":"The Limitations of Stylometry for Detecting Machine-Generated Fake News","abstract":"Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that stylometry is limited against machine-generated misinformation. Whereas humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs, utilized in auto-completion and editing-assistance settings.1 Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.","url":"https://www.semanticscholar.org/paper/9aefd614e52336151966d8dca2ed0ea62a8f30af","sentence":"Title: the limitations of stylometry for detecting machine-generated fake news Abstract: recent developments in neural language models (lms) have raised concerns about their potential misuse for automatically spreading misinformation. in light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. these approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. however, in this work, we show that stylometry is limited against machine-generated misinformation. whereas humans speak differently when trying to deceive, lms generate stylistically consistent text, regardless of underlying motive. thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate lm applications from those that introduce false information. we create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of lms, utilized in auto-completion and editing-assistance settings.1 our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.","Task":["stylometry"],"Method":["stylometry"]},{"title":"FEVER: a Large-scale Dataset for Fact Extraction and VERification","abstract":"In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.","url":"https://www.semanticscholar.org/paper/b1d24e8e08435b7c52335485a0d635abf9bc604c","sentence":"Title: fever: a large-scale dataset for fact extraction and verification Abstract: in this paper we introduce a new publicly available dataset for verification against textual sources, fever: fact extraction and verification. it consists of 185,445 claims generated by altering sentences extracted from wikipedia and subsequently verified without knowledge of the sentence they were derived from. the claims are classified as supported, refuted or notenoughinfo by annotators achieving 0.6841 in fleiss kappa. for the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. to characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. the best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. thus we believe that fever is a challenging testbed that will help stimulate progress on claim verification against textual sources.","Task":["claim verification"],"Method":["fever verification"]},{"title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World","abstract":"Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the \u201clanguage agnostic\u201d status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.","url":"https://www.semanticscholar.org/paper/0e141942fa265142f41a2a26eb17b6005d3af29e","sentence":"Title: the state and fate of linguistic diversity and inclusion in the nlp world Abstract: language technologies contribute to promoting multilingualism and linguistic diversity around the world. however, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. in this paper we look at the relation between the types of languages, resources, and their representation in nlp conferences to understand the trajectory that different languages have followed over time. our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the \u201clanguage agnostic\u201d status of current models and systems. through this paper, we attempt to convince the acl community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.","Task":["language diversity"],"Method":["language languages"]},{"title":"Stereotypes in High-Stakes Decisions: Evidence from U.S. Circuit Courts","abstract":"Attitudes towards social groups such as women and racial minorities have been shown to be important determinants of individual\u2019s decisions but are hard to measure for those in policy-making roles. We propose a way to address the challenge in the case of U.S. appellate court judges, for whom we have large corpora of written text (their published opinions). Using the universe of published opinions in U.S. Circuit Courts 1890-2013, we construct a judge-specific measure of gender-stereotyped language (gender slant) by looking at the relative co-occurrence of words identifying gender (male versus female) and words identifying gender stereotypes (career versus family). We find that female and younger judges tend to use less stereotyped language in their opinions. In addition, the attitudes measured by gender slant matter for judicial decisions: judges with higher slant vote more conservatively on women rights\u2019 issues. These more slanted judges also influence workplace outcomes for female colleagues: they are less likely to assign opinions to female judges, they cite fewer female-authored opinions, and they are more likely to reverse lower-court decisions if the lower-court judge is a woman. Our results expose a possible use of text to detect decision-makers\u2019 stereotypes that predict behavior and disparate outcomes. \u2217Arianna Ornaghi, University of Warwick, a.ornaghi@warwick.ac.uk (corresponding author); Elliott Ash, ETH Zurich, ashe@ethz.ch; Daniel Chen, Toulouse School of Economics, daniel.chen@iast.fr. We thank Jacopo Bregolin, David Cai, Christoph Goessmann, and Ornelie Manzambi for helpful research assistance.","url":"https://www.semanticscholar.org/paper/c3bcdea205ec9fb1b84d75d4767f346844082b38","sentence":"Title: stereotypes in high-stakes decisions: evidence from u.s. circuit courts Abstract: attitudes towards social groups such as women and racial minorities have been shown to be important determinants of individual\u2019s decisions but are hard to measure for those in policy-making roles. we propose a way to address the challenge in the case of u.s. appellate court judges, for whom we have large corpora of written text (their published opinions). using the universe of published opinions in u.s. circuit courts 1890-2013, we construct a judge-specific measure of gender-stereotyped language (gender slant) by looking at the relative co-occurrence of words identifying gender (male versus female) and words identifying gender stereotypes (career versus family). we find that female and younger judges tend to use less stereotyped language in their opinions. in addition, the attitudes measured by gender slant matter for judicial decisions: judges with higher slant vote more conservatively on women rights\u2019 issues. these more slanted judges also influence workplace outcomes for female colleagues: they are less likely to assign opinions to female judges, they cite fewer female-authored opinions, and they are more likely to reverse lower-court decisions if the lower-court judge is a woman. our results expose a possible use of text to detect decision-makers\u2019 stereotypes that predict behavior and disparate outcomes. \u2217arianna ornaghi, university of warwick, a.ornaghi@warwick.ac.uk (corresponding author); elliott ash, eth zurich, ashe@ethz.ch; daniel chen, toulouse school of economics, daniel.chen@iast.fr. we thank jacopo bregolin, david cai, christoph goessmann, and ornelie manzambi for helpful research assistance.","Task":["gender bias"],"Method":["gender stereotypes"]},{"title":"Generating Fact Checking Explanations","abstract":"Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process \u2013 generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.","url":"https://www.semanticscholar.org/paper/5e0daaeceb75ffbbe23be13d34ffae830cb4e8c4","sentence":"Title: generating fact checking explanations Abstract: most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. a crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process \u2013 generating justifications for verdicts on claims. this paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. the results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.","Task":["thisability"],"Method":["automated explanations"]},{"title":"Towards Debiasing Fact Verification Models","abstract":"Fact verification requires validating a claim in the context of evidence. We show, however, that in the popular FEVER dataset this might not necessarily be the case. Claim-only classifiers perform competitively with top evidence-aware models. In this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. We create an evaluation set that avoids those idiosyncrasies. The performance of FEVER-trained models significantly drops when evaluated on this test set. Therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.","url":"https://www.semanticscholar.org/paper/a4947468ceac25c11a665c4f5f95a49d6dbea3cc","sentence":"Title: towards debiasing fact verification models Abstract: fact verification requires validating a claim in the context of evidence. we show, however, that in the popular fever dataset this might not necessarily be the case. claim-only classifiers perform competitively with top evidence-aware models. in this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. we create an evaluation set that avoids those idiosyncrasies. the performance of fever-trained models significantly drops when evaluated on this test set. therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. this work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.","Task":["bias bias"],"Method":["causal bias"]},{"title":"Evaluating adversarial attacks against multiple fact verification systems","abstract":"Automated fact verification has been progressing owing to advancements in modeling and availability of large datasets. Due to the nature of the task, it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly. We introduce two novel scoring metrics, attack potency and system resilience which take into account the correctness of the adversarial instances, an aspect often ignored in adversarial evaluations. We consider six fact verification systems from the recent Fact Extraction and VERification (FEVER) challenge: the four best-scoring ones and two baselines. We evaluate adversarial instances generated by a recently proposed state-of-the-art method, a paraphrasing method, and rule-based attacks devised for fact verification. We find that our rule-based attacks have higher potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines.","url":"https://www.semanticscholar.org/paper/3a16c38294ac8899825c488490199b854e05473a","sentence":"Title: evaluating adversarial attacks against multiple fact verification systems Abstract: automated fact verification has been progressing owing to advancements in modeling and availability of large datasets. due to the nature of the task, it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly. we introduce two novel scoring metrics, attack potency and system resilience which take into account the correctness of the adversarial instances, an aspect often ignored in adversarial evaluations. we consider six fact verification systems from the recent fact extraction and verification (fever) challenge: the four best-scoring ones and two baselines. we evaluate adversarial instances generated by a recently proposed state-of-the-art method, a paraphrasing method, and rule-based attacks devised for fact verification. we find that our rule-based attacks have higher potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines.","Task":["system resilience"],"Method":["system resilience"]},{"title":"Unsupervised Discovery of Gendered Language through Latent-Variable Modeling","abstract":"Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.","url":"https://www.semanticscholar.org/paper/d4eeb40b9bd06ed53a26282cd527609f71e6496f","sentence":"Title: unsupervised discovery of gendered language through latent-variable modeling Abstract: studying the ways in which language is gendered has long been an area of interest in sociolinguistics. studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. in this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. to that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. we find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.","Task":["gender stereotypes"],"Method":["gender gender"]},{"title":"ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback","abstract":"We introduce ChrEnTranslate, an online ma\xad chine translation demonstration system for translation between English and an endangered language Cherokee. It supports both statistical and neural translation models as well as pro\xad vides quality estimation to inform users of re\xad liability, two user feedback interfaces for ex\xad perts and common users respectively, exam\xad ple inputs to collect human translations for monolingual data, word alignment visualiza\xad tion, and relevant terms from the Cherokee\xad English dictionary. The quantitative evalu\xad ation demonstrates that our backbone trans\xad lation models achieve state\xadof\xadthe\xadart transla\xad tion performance and our quality estimation well correlates with both BLEU and human judgment. By analyzing 216 pieces of expert feedback, we find that NMT is preferable be\xad cause it copies less than SMT, and, in gen\xad eral, current models can translate fragments of the source sentence but make major mistakes. When we add these 216 expert\xadcorrected paral\xad lel texts into the training set and retrain mod\xad els, equal or slightly better performance is ob\xad served, which demonstrates indicates the po\xad tential of human\xadin\xadthe\xadloop learning.1","url":"https://www.semanticscholar.org/paper/bb53946c7da617a05bbeef47fff74012db27ee78","sentence":"Title: chrentranslate: cherokee-english machine translation demo with quality estimation and corrective feedback Abstract: we introduce chrentranslate, an online ma\xad chine translation demonstration system for translation between english and an endangered language cherokee. it supports both statistical and neural translation models as well as pro\xad vides quality estimation to inform users of re\xad liability, two user feedback interfaces for ex\xad perts and common users respectively, exam\xad ple inputs to collect human translations for monolingual data, word alignment visualiza\xad tion, and relevant terms from the cherokee\xad english dictionary. the quantitative evalu\xad ation demonstrates that our backbone trans\xad lation models achieve state\xadof\xadthe\xadart transla\xad tion performance and our quality estimation well correlates with both bleu and human judgment. by analyzing 216 pieces of expert feedback, we find that nmt is preferable be\xad cause it copies less than smt, and, in gen\xad eral, current models can translate fragments of the source sentence but make major mistakes. when we add these 216 expert\xadcorrected paral\xad lel texts into the training set and retrain mod\xad els, equal or slightly better performance is ob\xad served, which demonstrates indicates the po\xad tential of human\xadin\xadthe\xadloop learning.1","Task":["corrective feedback"],"Method":["corrective feedback"]},{"title":"Haitian Creole: How to Build and Ship an MT Engine from Scratch in 4 days, 17 hours, & 30 minutes","abstract":"We describe the effort of the Microsoft Translator team to develop a Haitian Creole statistical machine translation engine from scratch in a matter of days. Haitian Creole presents a number of difficulties for devleoping an SMT system, principal among these is the lack of significant amounts of parallel training data and an inconsistent orthography, both of which lead to data sparseness. We demonstrate, however, that it is possible to build a translation engine of reasonable quality over very little data by engaging with the native language community and reducing data sparseness in creative ways. As such, we show that MT as a technology and as a service can be deployed rapidly in crisis situations.","url":"https://www.semanticscholar.org/paper/3d309aa1629ef9ca43e252eb6bf539286ed872f9","sentence":"Title: haitian creole: how to build and ship an mt engine from scratch in 4 days, 17 hours, & 30 minutes Abstract: we describe the effort of the microsoft translator team to develop a haitian creole statistical machine translation engine from scratch in a matter of days. haitian creole presents a number of difficulties for devleoping an smt system, principal among these is the lack of significant amounts of parallel training data and an inconsistent orthography, both of which lead to data sparseness. we demonstrate, however, that it is possible to build a translation engine of reasonable quality over very little data by engaging with the native language community and reducing data sparseness in creative ways. as such, we show that mt as a technology and as a service can be deployed rapidly in crisis situations.","Task":["data translation"],"Method":["translation translation"]},{"title":"Measuring Societal Biases in Text Corpora via First-Order Co-occurrence","abstract":"Text corpora are used to study societal biases, typically through statistical models such as word embeddings. The bias of a word towards a concept is typically estimated using vectors similarity, measuring whether the word and concept words share other words in their contexts. We argue that this second-order relationship introduces unrelated concepts into the measure, which causes an imprecise measurement of the bias. We propose instead to measure bias using the direct normalized co-occurrence associations between the word and the representative concept words, a first-order measure, by reconstructing the co-occurrence estimates inherent in the word embedding models. To study our novel corpus bias measurement method, we calculate the correlation of the gender bias values estimated from the text to the actual gender bias statistics of the U.S. job market, provided by two recent collections. The results show a consistently higher correlation when using the proposed first-order measure with a variety of word embedding models, as well as a more severe degree of bias, especially to female in a few specific occupations.","url":"https://www.semanticscholar.org/paper/f14cb1828e314a669304b0c37bc78d6b9073f6dd","sentence":"Title: measuring societal biases in text corpora via first-order co-occurrence Abstract: text corpora are used to study societal biases, typically through statistical models such as word embeddings. the bias of a word towards a concept is typically estimated using vectors similarity, measuring whether the word and concept words share other words in their contexts. we argue that this second-order relationship introduces unrelated concepts into the measure, which causes an imprecise measurement of the bias. we propose instead to measure bias using the direct normalized co-occurrence associations between the word and the representative concept words, a first-order measure, by reconstructing the co-occurrence estimates inherent in the word embedding models. to study our novel corpus bias measurement method, we calculate the correlation of the gender bias values estimated from the text to the actual gender bias statistics of the u.s. job market, provided by two recent collections. the results show a consistently higher correlation when using the proposed first-order measure with a variety of word embedding models, as well as a more severe degree of bias, especially to female in a few specific occupations.","Task":["gender bias"],"Method":["gender bias"]},{"title":"When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?","abstract":"Social biases are encoded in word embeddings. This presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. Here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods. We find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning. However, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g. gender) than others (e.g. race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.","url":"https://www.semanticscholar.org/paper/176420f394014d9e67d5cff1e1d430541fa0f55f","sentence":"Title: when do word embeddings accurately reflect surveys on our beliefs about people? Abstract: social biases are encoded in word embeddings. this presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods. we find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning. however, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g. gender) than others (e.g. race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.","Task":["this bias"],"Method":["implicit bias"]},{"title":"Language from police body camera footage shows racial disparities in officer respect","abstract":"Significance Police officers speak significantly less respectfully to black than to white community members in everyday traffic stops, even after controlling for officer race, infraction severity, stop location, and stop outcome. This paper presents a systematic analysis of officer body-worn camera footage, using computational linguistic techniques to automatically measure the respect level that officers display to community members. This work demonstrates that body camera footage can be used as a rich source of data rather than merely archival evidence, and paves the way for developing powerful language-based tools for studying and potentially improving police\u2013community relations. Using footage from body-worn cameras, we analyze the respectfulness of police officer language toward white and black community members during routine traffic stops. We develop computational linguistic methods that extract levels of respect automatically from transcripts, informed by a thin-slicing study of participant ratings of officer utterances. We find that officers speak with consistently less respect toward black versus white community members, even after controlling for the race of the officer, the severity of the infraction, the location of the stop, and the outcome of the stop. Such disparities in common, everyday interactions between police and the communities they serve have important implications for procedural justice and the building of police\u2013community trust.","url":"https://www.semanticscholar.org/paper/75d0cb419d7d58e81c2975758a36a11544a9f930","sentence":"Title: language from police body camera footage shows racial disparities in officer respect Abstract: significance police officers speak significantly less respectfully to black than to white community members in everyday traffic stops, even after controlling for officer race, infraction severity, stop location, and stop outcome. this paper presents a systematic analysis of officer body-worn camera footage, using computational linguistic techniques to automatically measure the respect level that officers display to community members. this work demonstrates that body camera footage can be used as a rich source of data rather than merely archival evidence, and paves the way for developing powerful language-based tools for studying and potentially improving police\u2013community relations. using footage from body-worn cameras, we analyze the respectfulness of police officer language toward white and black community members during routine traffic stops. we develop computational linguistic methods that extract levels of respect automatically from transcripts, informed by a thin-slicing study of participant ratings of officer utterances. we find that officers speak with consistently less respect toward black versus white community members, even after controlling for the race of the officer, the severity of the infraction, the location of the stop, and the outcome of the stop. such disparities in common, everyday interactions between police and the communities they serve have important implications for procedural justice and the building of police\u2013community trust.","Task":["police bias"],"Method":["implicit respect"]},{"title":"Word embeddings quantify 100 years of gender and ethnic stereotypes","abstract":"Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts\u2014e.g., the women\u2019s movement in the 1960s and Asian immigration into the United States\u2014and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.","url":"https://www.semanticscholar.org/paper/b5d7a19bd0bae10917a8e294960fdacf224d64fe","sentence":"Title: word embeddings quantify 100 years of gender and ethnic stereotypes Abstract: significance word embeddings are a popular machine-learning method that represents each english word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. we demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. as specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the united states evolved during the 20th and 21st centuries starting from 1910. our framework opens up a fruitful intersection between machine learning and quantitative social science. word embeddings are a powerful machine-learning framework that represents each english word by a vector. the geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. in this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the united states. we integrate word embeddings trained on 100 y of text data with the us census to show that changes in the embedding track closely with demographic and occupation shifts over time. the embedding captures societal shifts\u2014e.g., the women\u2019s movement in the 1960s and asian immigration into the united states\u2014and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.","Task":["semantic bias"],"Method":["semantic bias"]},{"title":"Tie-breaker: Using language models to quantify gender bias in sports journalism","abstract":"Gender bias is an increasingly important issue in sports journalism. In this work, we propose a language-model-based approach to quantify differences in questions posed to female vs. male athletes, and apply it to tennis post-match interviews. We find that journalists ask male players questions that are generally more focused on the game when compared with the questions they ask their female counterparts. We also provide a fine-grained analysis of the extent to which the salience of this bias depends on various factors, such as question type, game outcome or player rank.","url":"https://www.semanticscholar.org/paper/3d505c5eff8752ac1805ef546d683bfa40aec4b1","sentence":"Title: tie-breaker: using language models to quantify gender bias in sports journalism Abstract: gender bias is an increasingly important issue in sports journalism. in this work, we propose a language-model-based approach to quantify differences in questions posed to female vs. male athletes, and apply it to tennis post-match interviews. we find that journalists ask male players questions that are generally more focused on the game when compared with the questions they ask their female counterparts. we also provide a fine-grained analysis of the extent to which the salience of this bias depends on various factors, such as question type, game outcome or player rank.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Entity-Centric Contextual Affective Analysis","abstract":"While contextualized word representations have improved state-of-the-art benchmarks in many NLP tasks, their potential usefulness for social-oriented tasks remains largely unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women.","url":"https://www.semanticscholar.org/paper/a1280728623e8fd605284b2b7cf536579b9e2cbf","sentence":"Title: entity-centric contextual affective analysis Abstract: while contextualized word representations have improved state-of-the-art benchmarks in many nlp tasks, their potential usefulness for social-oriented tasks remains largely unexplored. we show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. we evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. we find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. we ultimately use our method to examine differences in portrayals of men and women.","Task":["gender bias"],"Method":["affect representations"]},{"title":"Racism is a Virus: Anti-Asian Hate and Counterhate in Social Media during the COVID-19 Crisis","abstract":"The spread of COVID-19 has sparked racism, hate, and xenophobia in social media targeted at Chinese and broader Asian communities. However, little is known about how racial hate spreads during a pandemic and the role of counterhate speech in mitigating the spread. Here we study the evolution and spread of anti-Asian hate speech through the lens of Twitter. We create COVID-HATE, the largest dataset of anti-Asian hate and counterhate spanning three months, containing over 30 million tweets, and a social network with over 87 million nodes. By creating a novel hand-labeled dataset of 2,400 tweets, we train a text classifier to identify hate and counterhate tweets that achieves an average AUROC of 0.852. We identify 891,204 hate and 200,198 counterhate tweets in COVID-HATE. Using this data to conduct longitudinal analysis, we find that while hateful users are less engaged in the COVID-19 discussions prior to their first anti-Asian tweet, they become more vocal and engaged afterwards compared to counterhate users. We find that bots comprise 10.4% of hateful users and are more vocal and hateful compared to non-bot users. Comparing bot accounts, we show that hateful bots are more successful in attracting followers compared to counterhate bots. Analysis of the social network reveals that hateful and counterhate users interact and engage extensively with one another, instead of living in isolated polarized communities. Furthermore, we find that hate is contagious and nodes are highly likely to become hateful after being exposed to hateful content. Importantly, our analysis reveals that counterhate messages can discourage users from turning hateful in the first place. Overall, this work presents a comprehensive overview of anti-Asian hate and counterhate content during a pandemic. The COVID-HATE dataset is available at this http URL.","url":"https://www.semanticscholar.org/paper/070b4a707748e289618880ffbe4762e4e3fc7860","sentence":"Title: racism is a virus: anti-asian hate and counterhate in social media during the covid-19 crisis Abstract: the spread of covid-19 has sparked racism, hate, and xenophobia in social media targeted at chinese and broader asian communities. however, little is known about how racial hate spreads during a pandemic and the role of counterhate speech in mitigating the spread. here we study the evolution and spread of anti-asian hate speech through the lens of twitter. we create covid-hate, the largest dataset of anti-asian hate and counterhate spanning three months, containing over 30 million tweets, and a social network with over 87 million nodes. by creating a novel hand-labeled dataset of 2,400 tweets, we train a text classifier to identify hate and counterhate tweets that achieves an average auroc of 0.852. we identify 891,204 hate and 200,198 counterhate tweets in covid-hate. using this data to conduct longitudinal analysis, we find that while hateful users are less engaged in the covid-19 discussions prior to their first anti-asian tweet, they become more vocal and engaged afterwards compared to counterhate users. we find that bots comprise 10.4% of hateful users and are more vocal and hateful compared to non-bot users. comparing bot accounts, we show that hateful bots are more successful in attracting followers compared to counterhate bots. analysis of the social network reveals that hateful and counterhate users interact and engage extensively with one another, instead of living in isolated polarized communities. furthermore, we find that hate is contagious and nodes are highly likely to become hateful after being exposed to hateful content. importantly, our analysis reveals that counterhate messages can discourage users from turning hateful in the first place. overall, this work presents a comprehensive overview of anti-asian hate and counterhate content during a pandemic. the covid-hate dataset is available at this http url.","Task":["social bots"],"Method":["social bots"]},{"title":"Automatically Inferring Gender Associations from Language","abstract":"In this paper, we pose the question: do people talk about women and men in different ways? We introduce two datasets and a novel integration of approaches for automatically inferring gender associations from language, discovering coherent word clusters, and labeling the clusters for the semantic concepts they represent. The datasets allow us to compare how people write about women and men in two different settings \u2013 one set draws from celebrity news and the other from student reviews of computer science professors. We demonstrate that there are large-scale differences in the ways that people talk about women and men and that these differences vary across domains. Human evaluations show that our methods significantly outperform strong baselines.","url":"https://www.semanticscholar.org/paper/7b5b2a9ad37d1a6c3c8916965b1958eef0a27a6a","sentence":"Title: automatically inferring gender associations from language Abstract: in this paper, we pose the question: do people talk about women and men in different ways? we introduce two datasets and a novel integration of approaches for automatically inferring gender associations from language, discovering coherent word clusters, and labeling the clusters for the semantic concepts they represent. the datasets allow us to compare how people write about women and men in two different settings \u2013 one set draws from celebrity news and the other from student reviews of computer science professors. we demonstrate that there are large-scale differences in the ways that people talk about women and men and that these differences vary across domains. human evaluations show that our methods significantly outperform strong baselines.","Task":["gender bias"],"Method":["gender clusters"]},{"title":"Contextual Affective Analysis: A Case Study of People Portrayals in Online #MeToo Stories","abstract":"In October 2017, numerous women accused producer Harvey Weinstein of sexual harassment. Their stories encouraged other women to voice allegations of sexual harassment against many high profile men, including politicians, actors, and producers. These events are broadly referred to as the #MeToo movement, named for the use of the hashtag \\"#metoo\\" on social media platforms like Twitter and Facebook. The movement has widely been referred to as \\"empowering\\" because it has amplified the voices of previously unheard women over those of traditionally powerful men. In this work, we investigate dynamics of sentiment, power and agency in online media coverage of these events. Using a corpus of online media articles about the #MeToo movement, we present a contextual affective analysis---an entity-centric approach that uses contextualized lexicons to examine how people are portrayed in media articles. We show that while these articles are sympathetic towards women who have experienced sexual harassment, they consistently present men as most powerful, even after sexual assault allegations. While we focus on media coverage of the #MeToo movement, our method for contextual affective analysis readily generalizes to other domains.","url":"https://www.semanticscholar.org/paper/6ba951771892f01206f1dd7244f14243e3885109","sentence":"Title: contextual affective analysis: a case study of people portrayals in online #metoo stories Abstract: in october 2017, numerous women accused producer harvey weinstein of sexual harassment. their stories encouraged other women to voice allegations of sexual harassment against many high profile men, including politicians, actors, and producers. these events are broadly referred to as the #metoo movement, named for the use of the hashtag \\"#metoo\\" on social media platforms like twitter and facebook. the movement has widely been referred to as \\"empowering\\" because it has amplified the voices of previously unheard women over those of traditionally powerful men. in this work, we investigate dynamics of sentiment, power and agency in online media coverage of these events. using a corpus of online media articles about the #metoo movement, we present a contextual affective analysis---an entity-centric approach that uses contextualized lexicons to examine how people are portrayed in media articles. we show that while these articles are sympathetic towards women who have experienced sexual harassment, they consistently present men as most powerful, even after sexual assault allegations. while we focus on media coverage of the #metoo movement, our method for contextual affective analysis readily generalizes to other domains.","Task":["gender representation"],"Method":["contextual representation"]},{"title":"Girls Rule, Boys Drool: Extracting Semantic and Affective Stereotypes from Twitter","abstract":"Social identities carry widely agreed upon meanings, called stereotypes, that have important effects on social processes. In the present work, we develop a method to extract the stereotypes of Twitter users. Our method is grounded in two distinct strands of theory, one that represents stereotypes as identities\' affective meanings and the other that represents stereotypes as semantic relationships between identities. After validating our approach via a prediction task, we apply the model to a dataset of 45 thousand Twitter users who actively tweeted about the Michael Brown and Eric Garner tragedies. Our work provides unique insights into the stereotypes of these users, as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel, parsimonious way.","url":"https://www.semanticscholar.org/paper/995e477360908175d0b1184f6a0aace9d864bc5a","sentence":"Title: girls rule, boys drool: extracting semantic and affective stereotypes from twitter Abstract: social identities carry widely agreed upon meanings, called stereotypes, that have important effects on social processes. in the present work, we develop a method to extract the stereotypes of twitter users. our method is grounded in two distinct strands of theory, one that represents stereotypes as identities\' affective meanings and the other that represents stereotypes as semantic relationships between identities. after validating our approach via a prediction task, we apply the model to a dataset of 45 thousand twitter users who actively tweeted about the michael brown and eric garner tragedies. our work provides unique insights into the stereotypes of these users, as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel, parsimonious way.","Task":["twitter stereotypes"],"Method":["social stereotypes"]},{"title":"Relating Linguistic Gender Bias, Gender Values, and Gender Gaps: An International Analysis","abstract":"Recent research in machine learning has shown that many machine-learned language models contain pervasive racial and gender biases, rooting from biases in their textual training data. While these biases produce sub-optimal parsing and inferences, they may help us characterize and predict statistical gender gaps and gender values in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach to (1) quantify gender bias in word embeddings (i.e., vector-based lexical semantics), (2) correlate gender biases with survey responses and statistical gender gaps in education, politics, economics, and health, and (3) integrate numerical biases and statistics to model different cultures\u2019 survey results more accurately than either in isolation. We validate this approach using 2018 Twitter data spanning 99 countries, 18 Global Gender Gap statistics from the World Economic Forum, and 8 international survey results from the World Value Survey. Integrating these heterogeneous data across cultures is an important step toward building computational models to understand group bias.","url":"https://www.semanticscholar.org/paper/5e888bfd9b492a3b08f3cc2eb7c617fedf5bd811","sentence":"Title: relating linguistic gender bias, gender values, and gender gaps: an international analysis Abstract: recent research in machine learning has shown that many machine-learned language models contain pervasive racial and gender biases, rooting from biases in their textual training data. while these biases produce sub-optimal parsing and inferences, they may help us characterize and predict statistical gender gaps and gender values in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. this paper presents an approach to (1) quantify gender bias in word embeddings (i.e., vector-based lexical semantics), (2) correlate gender biases with survey responses and statistical gender gaps in education, politics, economics, and health, and (3) integrate numerical biases and statistics to model different cultures\u2019 survey results more accurately than either in isolation. we validate this approach using 2018 twitter data spanning 99 countries, 18 global gender gap statistics from the world economic forum, and 8 international survey results from the world value survey. integrating these heterogeneous data across cultures is an important step toward building computational models to understand group bias.","Task":["gender bias"],"Method":["group bias"]},{"title":"Automation, Algorithms, and Politics| Talking to Bots: Symbiotic Agency and the Case of Tay","abstract":"In 2016, Microsoft launched Tay, an experimental artificial intelligence chat bot. Learning from interactions with Twitter users, Tay was shut down after one day because of its obscene and inflammatory tweets. This article uses the case of Tay to re-examine theories of agency. How did users view the personality and actions of an artificial intelligence chat bot when interacting with Tay on Twitter? Using phenomenological research methods and pragmatic approaches to agency, we look at what people said about Tay to study how they imagine and interact with emerging technologies and to show the limitations of our current theories of agency for describing communication in these settings. We show how different qualities of agency, different expectations for technologies, and different capacities for affordance emerge in the interactions between people and artificial intelligence. We argue that a perspective of \u201csymbiotic agency\u201d\u2014informed by the imagined affordances of emerging technology\u2014is required to really understand the collapse of Tay.","url":"https://www.semanticscholar.org/paper/abeee58b9fb5761133636ef117ef1a87203ad7ab","sentence":"Title: automation, algorithms, and politics| talking to bots: symbiotic agency and the case of tay Abstract: in 2016, microsoft launched tay, an experimental artificial intelligence chat bot. learning from interactions with twitter users, tay was shut down after one day because of its obscene and inflammatory tweets. this article uses the case of tay to re-examine theories of agency. how did users view the personality and actions of an artificial intelligence chat bot when interacting with tay on twitter? using phenomenological research methods and pragmatic approaches to agency, we look at what people said about tay to study how they imagine and interact with emerging technologies and to show the limitations of our current theories of agency for describing communication in these settings. we show how different qualities of agency, different expectations for technologies, and different capacities for affordance emerge in the interactions between people and artificial intelligence. we argue that a perspective of \u201csymbiotic agency\u201d\u2014informed by the imagined affordances of emerging technology\u2014is required to really understand the collapse of tay.","Task":["thisay"],"Method":["symb agency"]},{"title":"Empathy Is All You Need: How a Conversational Agent Should Respond to Verbal Abuse","abstract":"With the popularity of AI-infused systems, conversational agents (CAs) are becoming essential in diverse areas, offering new functionality and convenience, but simultaneously, suffering misuse and verbal abuse. We examine whether conversational agents\' response styles under varying abuse types influence those emotions found to mitigate peoples\' aggressive behaviors, involving three verbal abuse types (Insult, Threat, Swearing) and three response styles (Avoidance, Empathy, Counterattacking). Ninety-eight participants were assigned to one of the abuse type conditions, interacted with the three spoken (voice-based) CAs in turn, and reported their feelings about guiltiness, anger, and shame after each session. The results show that the agent\'s response style has a significant effect on user emotions. Participants were less angry and more guilty with the empathy agent than the other two agents. Furthermore, we investigated the current status of commercial CAs\' responses to verbal abuse. Our study findings have direct implications for the design of conversational agents.","url":"https://www.semanticscholar.org/paper/e3fd3b1be871da6e048adaef4a4e201af282fe8e","sentence":"Title: empathy is all you need: how a conversational agent should respond to verbal abuse Abstract: with the popularity of ai-infused systems, conversational agents (cas) are becoming essential in diverse areas, offering new functionality and convenience, but simultaneously, suffering misuse and verbal abuse. we examine whether conversational agents\' response styles under varying abuse types influence those emotions found to mitigate peoples\' aggressive behaviors, involving three verbal abuse types (insult, threat, swearing) and three response styles (avoidance, empathy, counterattacking). ninety-eight participants were assigned to one of the abuse type conditions, interacted with the three spoken (voice-based) cas in turn, and reported their feelings about guiltiness, anger, and shame after each session. the results show that the agent\'s response style has a significant effect on user emotions. participants were less angry and more guilty with the empathy agent than the other two agents. furthermore, we investigated the current status of commercial cas\' responses to verbal abuse. our study findings have direct implications for the design of conversational agents.","Task":["verbal abuse"],"Method":["empathy empathy"]},{"title":"Shirtless and Dangerous: Quantifying Linguistic Signals of Gender Bias in an Online Fiction Writing Community","abstract":"Imagine a princess asleep in a castle, waiting for her prince to slay the dragon and rescue her. Tales like the famous Sleeping Beauty clearly divide up gender roles. But what about more modern stories, borne of a generation increasingly aware of social constructs like sexism and racism? Do these stories tend to reinforce gender stereotypes, or counter them? In this paper, we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction. We apply this technique across 1.8 billion words of fiction from the Wattpad online writing community, investigating gender representation in stories, how male and female characters behave and are described, and how authors\' use of gender stereotypes is associated with the community\'s ratings. We find that male over-representation and traditional gender stereotypes (e.g., dominant men and submissive women) are common throughout nearly every genre in our corpus. However, only some of these stereotypes, like sexual or violent men, are associated with highly rated stories. Finally, despite women often being the target of negative stereotypes, female authors are equally likely to write such stereotypes as men.","url":"https://www.semanticscholar.org/paper/ef5fa2e95fc853defb902b58d8e4e4fe95a01c75","sentence":"Title: shirtless and dangerous: quantifying linguistic signals of gender bias in an online fiction writing community Abstract: imagine a princess asleep in a castle, waiting for her prince to slay the dragon and rescue her. tales like the famous sleeping beauty clearly divide up gender roles. but what about more modern stories, borne of a generation increasingly aware of social constructs like sexism and racism? do these stories tend to reinforce gender stereotypes, or counter them? in this paper, we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction. we apply this technique across 1.8 billion words of fiction from the wattpad online writing community, investigating gender representation in stories, how male and female characters behave and are described, and how authors\' use of gender stereotypes is associated with the community\'s ratings. we find that male over-representation and traditional gender stereotypes (e.g., dominant men and submissive women) are common throughout nearly every genre in our corpus. however, only some of these stereotypes, like sexual or violent men, are associated with highly rated stories. finally, despite women often being the target of negative stereotypes, female authors are equally likely to write such stereotypes as men.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Let\'s Talk About Race: Identity, Chatbots, and AI","abstract":"Why is it so hard for chatbots to talk about race? This work explores how the biased contents of databases, the syntactic focus of natural language processing, and the opaque nature of deep learning algorithms cause chatbots difficulty in handling race-talk. In each of these areas, the tensions between race and chatbots create new opportunities for people and machines. By making the abstract and disparate qualities of this problem space tangible, we can develop chatbots that are more capable of handling race-talk in its many forms. Our goal is to provide the HCI community with ways to begin addressing the question, how can chatbots handle race-talk in new and improved ways?","url":"https://www.semanticscholar.org/paper/f34c73c75a640f59c11472bf6c9786aeb774856a","sentence":"Title: let\'s talk about race: identity, chatbots, and ai Abstract: why is it so hard for chatbots to talk about race? this work explores how the biased contents of databases, the syntactic focus of natural language processing, and the opaque nature of deep learning algorithms cause chatbots difficulty in handling race-talk. in each of these areas, the tensions between race and chatbots create new opportunities for people and machines. by making the abstract and disparate qualities of this problem space tangible, we can develop chatbots that are more capable of handling race-talk in its many forms. our goal is to provide the hci community with ways to begin addressing the question, how can chatbots handle race-talk in new and improved ways?","Task":["racebots"],"Method":["racebots"]},{"title":"#MeToo Alexa: How Conversational Systems Respond to Sexual Harassment","abstract":"Conversational AI systems, such as Amazon\u2019s Alexa, are rapidly developing from purely transactional systems to social chatbots, which can respond to a wide variety of user requests. In this article, we establish how current state-of-the-art conversational systems react to inappropriate requests, such as bullying and sexual harassment on the part of the user, by collecting and analysing the novel #MeTooAlexa corpus. Our results show that commercial systems mainly avoid answering, while rule-based chatbots show a variety of behaviours and often deflect. Data-driven systems, on the other hand, are often non-coherent, but also run the risk of being interpreted as flirtatious and sometimes react with counter-aggression. This includes our own system, trained on \u201cclean\u201d data, which suggests that inappropriate system behaviour is not caused by data bias.","url":"https://www.semanticscholar.org/paper/983ad7c704d0f9a1560af322e4807e5be7799895","sentence":"Title: #metoo alexa: how conversational systems respond to sexual harassment Abstract: conversational ai systems, such as amazon\u2019s alexa, are rapidly developing from purely transactional systems to social chatbots, which can respond to a wide variety of user requests. in this article, we establish how current state-of-the-art conversational systems react to inappropriate requests, such as bullying and sexual harassment on the part of the user, by collecting and analysing the novel #metooalexa corpus. our results show that commercial systems mainly avoid answering, while rule-based chatbots show a variety of behaviours and often deflect. data-driven systems, on the other hand, are often non-coherent, but also run the risk of being interpreted as flirtatious and sometimes react with counter-aggression. this includes our own system, trained on \u201cclean\u201d data, which suggests that inappropriate system behaviour is not caused by data bias.","Task":["data bias"],"Method":["data bias"]},{"title":"Social Bias Frames: Reasoning about Social and Power Implications of Language","abstract":"Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people\u2019s judgments about others. For example, given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women,\u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified.\u201d Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.","url":"https://www.semanticscholar.org/paper/7b14a165c6b7c1dc2c6c44727e623b94d834fb09","sentence":"Title: social bias frames: reasoning about social and power implications of language Abstract: warning: this paper contains content that may be offensive or upsetting. language has the power to reinforce stereotypes and project social biases onto others. at the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people\u2019s judgments about others. for example, given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women,\u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified.\u201d most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. we introduce social bias frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. in addition, we introduce the social bias inference corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. we then establish baseline approaches that learn to recover social bias frames from unstructured text. we find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% f1), they are not effective at spelling out more detailed explanations in terms of social bias frames. our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.","Task":["implicit framing"],"Method":["social frames"]},{"title":"Data Statements for NLP: Toward Mitigating System Bias and Enabling Better Science","abstract":"In this position paper, we propose data statements as a practice that NLP technologists, in both research and development, can adopt to begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form data statements can take and explore the implications of adopting them as part of our regular practice. We argue that they will help alleviate issues related to exclusion and bias in language technology; lead to better precision in claims about how NLP research can generalize and thus better engineering results; protect companies from public embarrassment; and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.","url":"https://www.semanticscholar.org/paper/87eb23f934a0e6293ee8ee9b147fe0d456e65c96","sentence":"Title: data statements for nlp: toward mitigating system bias and enabling better science Abstract: in this position paper, we propose data statements as a practice that nlp technologists, in both research and development, can adopt to begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. we present a form data statements can take and explore the implications of adopting them as part of our regular practice. we argue that they will help alleviate issues related to exclusion and bias in language technology; lead to better precision in claims about how nlp research can generalize and thus better engineering results; protect companies from public embarrassment; and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.","Task":["system bias"],"Method":["data statements"]},{"title":"Datasheets for Datasets","abstract":"The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.","url":"https://www.semanticscholar.org/paper/0df347f5e3118fac7c351917e3a497899b071d1e","sentence":"Title: datasheets for datasets Abstract: the machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. to address this gap, we propose datasheets for datasets. in the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. by analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.","Task":["dataset transparency"],"Method":["dataset datasets"]},{"title":"Beyond the Belmont Principles: Ethical Challenges, Practices, and Beliefs in the Online Data Research Community","abstract":"Pervasive information streams that document people and their routines have been a boon to social computing research. But the ethics of collecting and analyzing available&-but potentially sensitive-online data present challenges to researchers. In response to increasing public and scholarly debate over the ethics of online data research, this paper analyzes the current state of practice among researchers using online data. Qualitative and quantitative responses from a survey of 263 online data researchers document beliefs and practices around which social computing researchers are converging, as well as areas of ongoing disagreement. The survey also reveals that these disagreements are not correlated with disciplinary, methodological, or workplace affiliations. The paper concludes by reflecting on changing ethical practices in the digital age, and discusses a set of emergent best practices for ethical social computing research.","url":"https://www.semanticscholar.org/paper/221732318e3cc45aa7bc2f48435706f3e5839ddc","sentence":"Title: beyond the belmont principles: ethical challenges, practices, and beliefs in the online data research community Abstract: pervasive information streams that document people and their routines have been a boon to social computing research. but the ethics of collecting and analyzing available&-but potentially sensitive-online data present challenges to researchers. in response to increasing public and scholarly debate over the ethics of online data research, this paper analyzes the current state of practice among researchers using online data. qualitative and quantitative responses from a survey of 263 online data researchers document beliefs and practices around which social computing researchers are converging, as well as areas of ongoing disagreement. the survey also reveals that these disagreements are not correlated with disciplinary, methodological, or workplace affiliations. the paper concludes by reflecting on changing ethical practices in the digital age, and discusses a set of emergent best practices for ethical social computing research.","Task":["ethical ethics"],"Method":["ethical ethics"]},{"title":"Detecting East Asian Prejudice on Social Media","abstract":"During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The classifier achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.","url":"https://www.semanticscholar.org/paper/0c68d7d153bb56e4637d6aee051d87580e05fd5b","sentence":"Title: detecting east asian prejudice on social media Abstract: during covid-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against east asia and east asian people. we report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from twitter into four classes: hostility against east asia, criticism of east asia, meta-discussions of east asian prejudice, and a neutral class. the classifier achieves a macro-f1 score of 0.83. we then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. we provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. we also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for covid-19 relevance and east asian relevance and stance for 1,000 hashtags, and the final model.","Task":["ambiguous bias"],"Method":["semantic bias"]},{"title":"Don\u2019t quote me: reverse identification of research participants in social media studies","abstract":"We investigated if participants in social media surveillance studies could be reverse identified by reviewing all articles published on PubMed in 2015 or 2016 with the words \u201cTwitter\u201d and either \u201cread,\u201d \u201ccoded,\u201d or \u201ccontent\u201d in the title or abstract. Seventy-two percent (95% CI: 63\u201380) of articles quoted at least one participant\u2019s tweet and searching for the quoted content led to the participant 84% (95% CI: 74\u201391) of the time. Twenty-one percent (95% CI: 13\u201329) of articles disclosed a participant\u2019s Twitter username thereby making the participant immediately identifiable. Only one article reported obtaining consent to disclose identifying information and institutional review board (IRB) involvement was mentioned in only 40% (95% CI: 31\u201350) of articles, of which 17% (95% CI: 10\u201325) received IRB-approval and 23% (95% CI:16\u201332) were deemed exempt. Biomedical publications are routinely including identifiable information by quoting tweets or revealing usernames which, in turn, violates ICMJE ethical standards governing scientific ethics, even though said content is scientifically unnecessary. We propose that authors convey aggregate findings without revealing participants\u2019 identities, editors refuse to publish reports that reveal a participant\u2019s identity, and IRBs attend to these privacy issues when reviewing studies involving social media data. These strategies together will ensure participants are protected going forward.","url":"https://www.semanticscholar.org/paper/1ece7c00d2eb6fca5443ff8e15f05a2b8b5985c2","sentence":"Title: don\u2019t quote me: reverse identification of research participants in social media studies Abstract: we investigated if participants in social media surveillance studies could be reverse identified by reviewing all articles published on pubmed in 2015 or 2016 with the words \u201ctwitter\u201d and either \u201cread,\u201d \u201ccoded,\u201d or \u201ccontent\u201d in the title or abstract. seventy-two percent (95% ci: 63\u201380) of articles quoted at least one participant\u2019s tweet and searching for the quoted content led to the participant 84% (95% ci: 74\u201391) of the time. twenty-one percent (95% ci: 13\u201329) of articles disclosed a participant\u2019s twitter username thereby making the participant immediately identifiable. only one article reported obtaining consent to disclose identifying information and institutional review board (irb) involvement was mentioned in only 40% (95% ci: 31\u201350) of articles, of which 17% (95% ci: 10\u201325) received irb-approval and 23% (95% ci:16\u201332) were deemed exempt. biomedical publications are routinely including identifiable information by quoting tweets or revealing usernames which, in turn, violates icmje ethical standards governing scientific ethics, even though said content is scientifically unnecessary. we propose that authors convey aggregate findings without revealing participants\u2019 identities, editors refuse to publish reports that reveal a participant\u2019s identity, and irbs attend to these privacy issues when reviewing studies involving social media data. these strategies together will ensure participants are protected going forward.","Task":["reverse identification"],"Method":["reverse identity"]},{"title":"Towards an Ethical Framework for Publishing Twitter Data in Social Research: Taking into Account Users\u2019 Views, Online Context and Algorithmic Estimation","abstract":"New and emerging forms of data, including posts harvested from social media sites such as Twitter, have become part of the sociologist\u2019s data diet. In particular, some researchers see an advantage in the perceived \u2018public\u2019 nature of Twitter posts, representing them in publications without seeking informed consent. While such practice may not be at odds with Twitter\u2019s terms of service, we argue there is a need to interpret these through the lens of social science research methods that imply a more reflexive ethical approach than provided in \u2018legal\u2019 accounts of the permissible use of these data in research publications. To challenge some existing practice in Twitter-based research, this article brings to the fore: (1) views of Twitter users through analysis of online survey data; (2) the effect of context collapse and online disinhibition on the behaviours of users; and (3) the publication of identifiable sensitive classifications derived from algorithms.","url":"https://www.semanticscholar.org/paper/afe97d05e5b320d2af500cdae1c588f4cc0d14d2","sentence":"Title: towards an ethical framework for publishing twitter data in social research: taking into account users\u2019 views, online context and algorithmic estimation Abstract: new and emerging forms of data, including posts harvested from social media sites such as twitter, have become part of the sociologist\u2019s data diet. in particular, some researchers see an advantage in the perceived \u2018public\u2019 nature of twitter posts, representing them in publications without seeking informed consent. while such practice may not be at odds with twitter\u2019s terms of service, we argue there is a need to interpret these through the lens of social science research methods that imply a more reflexive ethical approach than provided in \u2018legal\u2019 accounts of the permissible use of these data in research publications. to challenge some existing practice in twitter-based research, this article brings to the fore: (1) views of twitter users through analysis of online survey data; (2) the effect of context collapse and online disinhibition on the behaviours of users; and (3) the publication of identifiable sensitive classifications derived from algorithms.","Task":["this bias"],"Method":["social classification"]},{"title":"Writer Profiling Without the Writer\'s Text","abstract":"Social network users may wish to preserve their anonymity online by masking their identity and not using language associated with any particular demographics or personality. However, they have no control over the language in incoming communications. We show that linguistic cues in public comments directed at a user are sufficient for an accurate inference of that user\u2019s gender, age, religion, diet, and even personality traits. Moreover, we show that directed communication is even more predictive of a user\u2019s profile than the user\u2019s own language. We then conduct a nuanced analysis of what types of social relationships are most predictive of users\u2019 attributes, and propose new strategies on how individuals can modulate their online social relationships and incoming communications to preserve their anonymity.","url":"https://www.semanticscholar.org/paper/5f827b963939c96968a03318b4c2b011e1871eaf","sentence":"Title: writer profiling without the writer\'s text Abstract: social network users may wish to preserve their anonymity online by masking their identity and not using language associated with any particular demographics or personality. however, they have no control over the language in incoming communications. we show that linguistic cues in public comments directed at a user are sufficient for an accurate inference of that user\u2019s gender, age, religion, diet, and even personality traits. moreover, we show that directed communication is even more predictive of a user\u2019s profile than the user\u2019s own language. we then conduct a nuanced analysis of what types of social relationships are most predictive of users\u2019 attributes, and propose new strategies on how individuals can modulate their online social relationships and incoming communications to preserve their anonymity.","Task":["identity profiling"],"Method":["anonymous profiling"]},{"title":"\u201cParticipant\u201d Perceptions of Twitter Research Ethics","abstract":"Social computing systems such as Twitter present new research sites that have provided billions of data points to researchers. However, the availability of public social media data has also presented ethical challenges. As the research community works to create ethical norms, we should be considering users\u2019 concerns as well. With this in mind, we report on an exploratory survey of Twitter users\u2019 perceptions of the use of tweets in research. Within our survey sample, few users were previously aware that their public tweets could be used by researchers, and the majority felt that researchers should not be able to use tweets without consent. However, we find that these attitudes are highly contextual, depending on factors such as how the research is conducted or disseminated, who is conducting it, and what the study is about. The findings of this study point to potential best practices for researchers conducting observation and analysis of public data.","url":"https://www.semanticscholar.org/paper/f298649194c1be9cb55574c047756ae7e8a62d6b","sentence":"Title: \u201cparticipant\u201d perceptions of twitter research ethics Abstract: social computing systems such as twitter present new research sites that have provided billions of data points to researchers. however, the availability of public social media data has also presented ethical challenges. as the research community works to create ethical norms, we should be considering users\u2019 concerns as well. with this in mind, we report on an exploratory survey of twitter users\u2019 perceptions of the use of tweets in research. within our survey sample, few users were previously aware that their public tweets could be used by researchers, and the majority felt that researchers should not be able to use tweets without consent. however, we find that these attitudes are highly contextual, depending on factors such as how the research is conducted or disseminated, who is conducting it, and what the study is about. the findings of this study point to potential best practices for researchers conducting observation and analysis of public data.","Task":["ethical ethics"],"Method":["research ethics"]},{"title":"The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks","abstract":"This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users\' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. \\nIn experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google\'s Smart Compose, a commercial text-completion neural network trained on millions of users\' email messages.","url":"https://www.semanticscholar.org/paper/520ec00dc35475e0554dbb72f27bd2eeb6f4191d","sentence":"Title: the secret sharer: evaluating and testing unintended memorization in neural networks Abstract: this paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. because such models are sometimes trained on sensitive data (e.g., the text of users\' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. \\nin experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. we show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in google\'s smart compose, a commercial text-completion neural network trained on millions of users\' email messages.","Task":["unintendedization"],"Method":["unintendedization"]},{"title":"Garbage in, garbage out?: do machine learning application papers in social computing report where human-labeled training data comes from?","abstract":"Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper\'s authors labeling the data themselves. Such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. In this paper, we investigate to what extent a sample of machine learning application papers in social computing --- specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data --- give specific details about whether such best practices were followed. Our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: Does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. We find a wide divergence in whether such practices were followed and documented. Much of machine learning research and education focuses on what is done once a \\"gold standard\\" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.","url":"https://www.semanticscholar.org/paper/df2df1749b93ba86328ec7b86ff7e8d30029e3f5","sentence":"Title: garbage in, garbage out?: do machine learning application papers in social computing report where human-labeled training data comes from? Abstract: many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper\'s authors labeling the data themselves. such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. in this paper, we investigate to what extent a sample of machine learning application papers in social computing --- specifically papers from arxiv and traditional publications performing an ml classification task on twitter data --- give specific details about whether such best practices were followed. our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. we find a wide divergence in whether such practices were followed and documented. much of machine learning research and education focuses on what is done once a \\"gold standard\\" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.","Task":["data reporting"],"Method":["data reporting"]},{"title":"How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation","abstract":"We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model\'s generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.","url":"https://www.semanticscholar.org/paper/129cbad01be98ee88a930e31898cb76be79c41c1","sentence":"Title: how not to evaluate your dialogue system: an empirical study of unsupervised evaluation metrics for dialogue response generation Abstract: we investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. recent works in response generation have adopted metrics from machine translation to compare a model\'s generated response to a single target response. we show that these metrics correlate very weakly with human judgements in the non-technical twitter domain, and not at all in the technical ubuntu domain. we provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.","Task":["manual evaluation"],"Method":["automated evaluation"]},{"title":"Fifty years later: the significance of the Nuremberg Code.","abstract":"The Nuremberg Code 1. The voluntary consent of the human subject is absolutely essential. This means that the person involved should have legal capacity to give consent; should be so situated as to be able to exercise free power of choice, without the intervention of any element of force, fraud, deceit, duress, overreaching, or other ulterior form of constraint or coercion; and should have sufficient knowledge and comprehension of the elements of the subject matter involved as to enable him to make an understanding and enlightened decision. This latter element requires that before the acceptance of an affirmative decision by .\xa0.\xa0.","url":"https://www.semanticscholar.org/paper/dff6976f237ecff091547dd2df26937bd6b59198","sentence":"Title: fifty years later: the significance of the nuremberg code. Abstract: the nuremberg code 1. the voluntary consent of the human subject is absolutely essential. this means that the person involved should have legal capacity to give consent; should be so situated as to be able to exercise free power of choice, without the intervention of any element of force, fraud, deceit, duress, overreaching, or other ulterior form of constraint or coercion; and should have sufficient knowledge and comprehension of the elements of the subject matter involved as to enable him to make an understanding and enlightened decision. this latter element requires that before the acceptance of an affirmative decision by .\xa0.\xa0.","Task":["affirmative consent"],"Method":["voluntary consent"]},{"title":"Semantics derived automatically from language corpora necessarily contain human biases","abstract":"Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.","url":"https://www.semanticscholar.org/paper/5966d7c7f60898d610812e24c64d4d57855ad86a","sentence":"Title: semantics derived automatically from language corpora necessarily contain human biases Abstract: artificial intelligence and machine learning are in a period of astounding growth. however, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. we replicate a spectrum of standard human biases as exposed by the implicit association test and other well-known psychological studies. we replicate these using a widely used, purely statistical machine-learning model---namely, the glove word embedding---trained on a corpus of text from the web. our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. these regularities are captured by machine learning along with the rest of semantics. in addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the word embedding association test (weat) and the word embedding factual association test (wefat). our results have implications not only for ai and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.","Task":["semantic bias"],"Method":["semantic bias"]},{"title":"Algorithms of Oppression: How Search Engines Reinforce Racism","abstract":"Read and considered thoughtfully, Safiya Umoja Noble\u2019s Algorithms of Oppression: How Search Engines Reinforce Racism is devastating. It reduces to rubble the notion that technology is neutral and ideology-free. Noble\u2019s crushing the neutrality myth does several things. First, this act lays foundations for her argument: only if you recognize and understand that technology is built with, and integrates, bias, can you then be open to her primary thesis: search engines advance discriminatory and often racist content. Second, it banishes a convenient response for many self-identified meritocratic Silicon Valley \u201cwinners\u201d and their supporters. Postreading, some individuals may retain their beliefs in a neutral and ideology-free technology in spite of the overwhelming evidence and citations Noble brings to bear. Effective countering of Noble\u2019s claims is unlikely to occur. For professionals working in technology, information, argumentation, and/or rhetorical studies, Algorithms of Oppression is refreshing. Agonistic towards structural racism and its defenses, single-minded in its evidentiary presentation, collaborative in its acknowledgement of others\u2019 scholarship and research, Noble models many academic, critical, and social moves. Technology scholars and writers will find in Algorithms of Oppression a masterful mentor text on how to be an activist researcher scholar. Noble also makes this enjoyable reading. It is uncommon to find academic books that can simultaneously be read, used, and applied by academics and non-academics alike.","url":"https://www.semanticscholar.org/paper/1935a5e3937753dc7db90126a221f11009c17984","sentence":"Title: algorithms of oppression: how search engines reinforce racism Abstract: read and considered thoughtfully, safiya umoja noble\u2019s algorithms of oppression: how search engines reinforce racism is devastating. it reduces to rubble the notion that technology is neutral and ideology-free. noble\u2019s crushing the neutrality myth does several things. first, this act lays foundations for her argument: only if you recognize and understand that technology is built with, and integrates, bias, can you then be open to her primary thesis: search engines advance discriminatory and often racist content. second, it banishes a convenient response for many self-identified meritocratic silicon valley \u201cwinners\u201d and their supporters. postreading, some individuals may retain their beliefs in a neutral and ideology-free technology in spite of the overwhelming evidence and citations noble brings to bear. effective countering of noble\u2019s claims is unlikely to occur. for professionals working in technology, information, argumentation, and/or rhetorical studies, algorithms of oppression is refreshing. agonistic towards structural racism and its defenses, single-minded in its evidentiary presentation, collaborative in its acknowledgement of others\u2019 scholarship and research, noble models many academic, critical, and social moves. technology scholars and writers will find in algorithms of oppression a masterful mentor text on how to be an activist researcher scholar. noble also makes this enjoyable reading. it is uncommon to find academic books that can simultaneously be read, used, and applied by academics and non-academics alike.","Task":["this algorithms"],"Method":["algorithm algorithms"]},{"title":"The trouble with using provider assessments for rating clinical performance: it\'s a matter of bias.","abstract":"The International Association for the Study of Pain has referred to pain as the fifth vital sign, and acute pain management after surgery has been shown to be a key factor in quality of recovery. In addition, the establishment of pain management benchmarks by the Joint Commission on the Accreditation of Healthcare Organizations more than a decade ago has resulted in a greater awareness of patients\u2019 right to optimal pain control by health care practitioners and administrators. Postoperative pain control has become a priority for hospitals across the United States. Optimization of postoperative pain management has been demonstrated through the implementation of protocols that include multimodal analgesic regimens, surgicalspecific treatment pathways, implementation of a 24-hour anesthesiology pain service, and pain-specific training for physicians and nurses involved in postoperative care.1 Importantly, pain as assessed by the numeric rating scale (NRS), for which 0 = no pain and 10 = maximal pain, has been shown to be significantly reduced after the implementation of postoperative analgesia protocol. These data suggest that NRS pain scores might be a useful metric to evaluate the quality of anesthesia care provided after the transition from surgery to recovery. Wanderer et al.2 from the Vanderbilt University have applied this principle in a research report in the current edition of Anesthesia & Analgesia in an attempt to use rank ordering by initial postanesthesia recovery unit NRS pain scores, as collected by nurses in a clinical setting, to compare supervising anesthesiologists when adjusted for confounding factors. The analysis included 26 680 cases using electronic documentation and excluded physician and nurse providers who had not cared for \u2265100 patients. When admission postanesthesia recovery unit scores were compared among anesthesiologists after adjusting for patient and surgical factors, only 6.4% of the 69 supervising anesthesiologists were found to differ in median postanesthesia recovery room admission pain scores. This finding clearly demonstrates that as presently assessed, initial postanesthesia recovery scores are a poor metric for identifying differences among supervising anesthesiologists, and the authors correctly concluded that they should not be included in the assessment of anesthesia care performance. Interestingly, the study also found that 16 of 66 (24%) recovery room nurses elicited median pain scores less than the median value for the entire group, and 33 nurses (50%) had elicited median pain scores significantly higher than the median for the entire group. These differences translated into a range of odds ratios from 0.16 (95% confidence interval, 0.11\u20132.4) for the lowest to 2.95 (95% confidence interval, 2.43\u20133.59) for the highest nurse compared with the nurse who ranked the median value for the overall group. In fact, NRS pain assessments using the 0 to 10 NRS pain score were found to depend more on the nurse making the assessment than patient age, gender and race, preoperative use of opioids, American Society of Anesthesiologists physical status, or procedure. This finding should not be interpreted to suggest dishonest recordings of NRS values by nurses (86%\u201390% of nurses accurately record pain ratings provided by patients), but that personal opinions, knowledge, and attitudes toward pain strongly influence assessments and management.3 Wanderer et al. discuss the use of nonstandard item descriptors rather than \u201cno pain\u201d equals 0 and \u201cthe worst possible pain\u201d equals 10 as anchors for the NRS pain scales as a possible explanation for the variability in the nurse assessor\u2013recorded NRS. They cited a recent systematic review that identified 24 distinct anchoring phrases in 54 included studies.4 The use of anchors during decision making can create a form of cognitive bias because individuals tend to rely heavily on these points when making decision adjustments. Factors that have been shown to affect the influence of anchoring on decision making include happier moods and conscientious personalities, certainly desirable attributes in postanesthesia care nurses. These are factors that patients are likely to perceive, and substituting anchors could clearly influence the perceived value reported by patients.5 The method of presentation of the NRS score range by the evaluator can be used to influence the choice made by the decision maker. This method is called the framing effect and is another type of cognitive bias.6 The presenter in this situation is referred to as the choice architect. This practice is not an uncommon phenomenon when using Likert scales because the differences between scores in the range are not The Trouble with Using Provider Assessments for Rating Clinical Performance: It\u2019s a Matter of Bias","url":"https://www.semanticscholar.org/paper/31a848022de5933029435a2c8304c2bd12537b0d","sentence":"Title: the trouble with using provider assessments for rating clinical performance: it\'s a matter of bias. Abstract: the international association for the study of pain has referred to pain as the fifth vital sign, and acute pain management after surgery has been shown to be a key factor in quality of recovery. in addition, the establishment of pain management benchmarks by the joint commission on the accreditation of healthcare organizations more than a decade ago has resulted in a greater awareness of patients\u2019 right to optimal pain control by health care practitioners and administrators. postoperative pain control has become a priority for hospitals across the united states. optimization of postoperative pain management has been demonstrated through the implementation of protocols that include multimodal analgesic regimens, surgicalspecific treatment pathways, implementation of a 24-hour anesthesiology pain service, and pain-specific training for physicians and nurses involved in postoperative care.1 importantly, pain as assessed by the numeric rating scale (nrs), for which 0 = no pain and 10 = maximal pain, has been shown to be significantly reduced after the implementation of postoperative analgesia protocol. these data suggest that nrs pain scores might be a useful metric to evaluate the quality of anesthesia care provided after the transition from surgery to recovery. wanderer et al.2 from the vanderbilt university have applied this principle in a research report in the current edition of anesthesia & analgesia in an attempt to use rank ordering by initial postanesthesia recovery unit nrs pain scores, as collected by nurses in a clinical setting, to compare supervising anesthesiologists when adjusted for confounding factors. the analysis included 26 680 cases using electronic documentation and excluded physician and nurse providers who had not cared for \u2265100 patients. when admission postanesthesia recovery unit scores were compared among anesthesiologists after adjusting for patient and surgical factors, only 6.4% of the 69 supervising anesthesiologists were found to differ in median postanesthesia recovery room admission pain scores. this finding clearly demonstrates that as presently assessed, initial postanesthesia recovery scores are a poor metric for identifying differences among supervising anesthesiologists, and the authors correctly concluded that they should not be included in the assessment of anesthesia care performance. interestingly, the study also found that 16 of 66 (24%) recovery room nurses elicited median pain scores less than the median value for the entire group, and 33 nurses (50%) had elicited median pain scores significantly higher than the median for the entire group. these differences translated into a range of odds ratios from 0.16 (95% confidence interval, 0.11\u20132.4) for the lowest to 2.95 (95% confidence interval, 2.43\u20133.59) for the highest nurse compared with the nurse who ranked the median value for the overall group. in fact, nrs pain assessments using the 0 to 10 nrs pain score were found to depend more on the nurse making the assessment than patient age, gender and race, preoperative use of opioids, american society of anesthesiologists physical status, or procedure. this finding should not be interpreted to suggest dishonest recordings of nrs values by nurses (86%\u201390% of nurses accurately record pain ratings provided by patients), but that personal opinions, knowledge, and attitudes toward pain strongly influence assessments and management.3 wanderer et al. discuss the use of nonstandard item descriptors rather than \u201cno pain\u201d equals 0 and \u201cthe worst possible pain\u201d equals 10 as anchors for the nrs pain scales as a possible explanation for the variability in the nurse assessor\u2013recorded nrs. they cited a recent systematic review that identified 24 distinct anchoring phrases in 54 included studies.4 the use of anchors during decision making can create a form of cognitive bias because individuals tend to rely heavily on these points when making decision adjustments. factors that have been shown to affect the influence of anchoring on decision making include happier moods and conscientious personalities, certainly desirable attributes in postanesthesia care nurses. these are factors that patients are likely to perceive, and substituting anchors could clearly influence the perceived value reported by patients.5 the method of presentation of the nrs score range by the evaluator can be used to influence the choice made by the decision maker. this method is called the framing effect and is another type of cognitive bias.6 the presenter in this situation is referred to as the choice architect. this practice is not an uncommon phenomenon when using likert scales because the differences between scores in the range are not the trouble with using provider assessments for rating clinical performance: it\u2019s a matter of bias","Task":["bias bias"],"Method":["pain scores"]},{"title":"Comparing Spoken Dialog Corpora Collected with Recruited Subjects versus Real Users","abstract":"Empirical spoken dialog research often involves the collection and analysis of a dialog corpus. However, it is not well understood whether and how a corpus of dialogs collected using recruited subjects differs from a corpus of dialogs obtained from real users. In this paper we use Let\u2019s Go Lab, a platform for experimenting with a deployed spoken dialog bus information system, to address this question. Our first corpus is collected by recruiting subjects to call Let\u2019s Go in a standard laboratory setting, while our second corpus consists of calls from real users calling Let\u2019s Go during its operating hours. We quantitatively characterize the two collected corpora using previously proposed measures from the spoken dialog literature, then discuss the statistically significant similarities and differences between the two corpora with respect to these measures. For example, we find that recruited subjects talk more and speak faster, while real users ask for more help and more frequently interrupt the system. In contrast, we find no difference with respect to dialog structure.","url":"https://www.semanticscholar.org/paper/d3aca13c966bb22eed7086baeb287a64bc18c152","sentence":"Title: comparing spoken dialog corpora collected with recruited subjects versus real users Abstract: empirical spoken dialog research often involves the collection and analysis of a dialog corpus. however, it is not well understood whether and how a corpus of dialogs collected using recruited subjects differs from a corpus of dialogs obtained from real users. in this paper we use let\u2019s go lab, a platform for experimenting with a deployed spoken dialog bus information system, to address this question. our first corpus is collected by recruiting subjects to call let\u2019s go in a standard laboratory setting, while our second corpus consists of calls from real users calling let\u2019s go during its operating hours. we quantitatively characterize the two collected corpora using previously proposed measures from the spoken dialog literature, then discuss the statistically significant similarities and differences between the two corpora with respect to these measures. for example, we find that recruited subjects talk more and speak faster, while real users ask for more help and more frequently interrupt the system. in contrast, we find no difference with respect to dialog structure.","Task":["dialog structure"],"Method":["dialog structure"]},{"title":"Privacy-preserving Neural Representations of Text","abstract":"This article deals with adversarial attacks towards deep learning systems for Natural Language Processing (NLP), in the context of privacy protection. We study a specific type of attack: an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text. Such scenario may arise in situations when the computation of a neural network is shared across multiple devices, e.g. some hidden representation is computed by a user\u2019s device and sent to a cloud-based model. We measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations. Finally, we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations.","url":"https://www.semanticscholar.org/paper/e8fa186444d98a39ee9139b1f5dd0c7618caef8f","sentence":"Title: privacy-preserving neural representations of text Abstract: this article deals with adversarial attacks towards deep learning systems for natural language processing (nlp), in the context of privacy protection. we study a specific type of attack: an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text. such scenario may arise in situations when the computation of a neural network is shared across multiple devices, e.g. some hidden representation is computed by a user\u2019s device and sent to a cloud-based model. we measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations. finally, we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations.","Task":["privacy protection"],"Method":["privacy protection"]},{"title":"Ethical Challenges in Data-Driven Dialogue Systems","abstract":"The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.","url":"https://www.semanticscholar.org/paper/a24d72bd0d08d515cb3e26f94131d33ad6c861db","sentence":"Title: ethical challenges in data-driven dialogue systems Abstract: the use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. a growing number of dialogue systems use conversation strategies that are learned from large datasets. there are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. we also suggest areas stemming from these issues that deserve further investigation. through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.","Task":["implicit bias"],"Method":["ethical learning"]},{"title":"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings","abstract":"The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.","url":"https://www.semanticscholar.org/paper/ccf6a69a7f33bcf052aa7def176d3b9de495beb7","sentence":"Title: man is to computer programmer as woman is to homemaker? debiasing word embeddings Abstract: the blind application of machine learning runs the risk of amplifying biases present in data. such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. we show that even word embeddings trained on google news articles exhibit female/male gender stereotypes to a disturbing extent. this raises concerns because their widespread use, as we describe, often tends to amplify these biases. geometrically, gender bias is first shown to be captured by a direction in the word embedding. second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. the resulting embeddings can be used in applications without amplifying gender bias.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them","abstract":"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between \u201cgender-neutralized\u201d words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","url":"https://www.semanticscholar.org/paper/94cf3f2c4410fcb06a90abebd99f7113c69e1ed9","sentence":"Title: lipstick on a pig: debiasing methods cover up systematic gender biases in word embeddings but do not remove them Abstract: word embeddings are widely used in nlp for a vast range of tasks. it was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. however, we argue that this removal is superficial. while the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. the gender bias information is still reflected in the distances between \u201cgender-neutralized\u201d words in the debiased embeddings, and can be recovered from them. we present a series of experiments to support this claim, for two debiasing methods. we conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Task":["gender bias"],"Method":["gender bias"]},{"title":"On Measuring Social Biases in Sentence Encoders","abstract":"The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test\u2019s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.","url":"https://www.semanticscholar.org/paper/5e9c85235210b59a16bdd84b444a904ae271f7e7","sentence":"Title: on measuring social biases in sentence encoders Abstract: the word embedding association test shows that glove and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (caliskan et al., 2017). meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. accordingly, we extend the word embedding association test to measure bias in sentence encoders. we then test several sentence encoders, including state-of-the-art methods such as elmo and bert, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. we observe mixed results including suspicious patterns of sensitivity that suggest the test\u2019s assumptions may not hold in general. we conclude by proposing directions for future work on measuring bias in sentence encoders.","Task":["implicit bias"],"Method":["social bias"]},{"title":"Assessing Social and Intersectional Biases in Contextualized Word Representations","abstract":"Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.","url":"https://www.semanticscholar.org/paper/039b1c1210c437f3b3ce6e0275ee2137bf5b951c","sentence":"Title: assessing social and intersectional biases in contextualized word representations Abstract: social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. in natural language processing, gender bias has been shown to exist in context-free word embeddings. recently, contextual word representations have outperformed word embeddings in several downstream nlp tasks. these word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. in this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as bert and gpt-2, encode biases with respect to gender, race, and intersectional identities. towards this, we propose assessing bias at the contextual word level. this novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. we demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.","Task":["contextual bias"],"Method":["contextual bias"]},{"title":"Quantifying Social Biases in Contextual Word Representations","abstract":"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","url":"https://www.semanticscholar.org/paper/3259d52ae00e65b98391e7e6a2f672dfee721bf8","sentence":"Title: quantifying social biases in contextual word representations Abstract: contextual word embeddings such as bert have achieved state of the art performance in numerous nlp tasks. since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. in this study, we (1) propose a template-based method to quantify bias in bert; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of gender pronoun resolution. although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Sorting Things Out: Classification and Its Consequences","abstract":"What do a seventeenth-century mortality table (whose causes of death include \\"fainted in a bath,\\" \\"frighted,\\" and \\"itch\\"); the identification of South Africans during apartheid as European, Asian, colored, or black; and the separation of machine- from hand-washables have in common? All are examples of classification -- the scaffolding of information infrastructures. In Sorting Things Out, Geoffrey C. Bowker and Susan Leigh Star explore the role of categories and standards in shaping the modern world. In a clear and lively style, they investigate a variety of classification systems, including the International Classification of Diseases, the Nursing Interventions Classification, race classification under apartheid in South Africa, and the classification of viruses and of tuberculosis. The authors emphasize the role of invisibility in the process by which classification orders human interaction. They examine how categories are made and kept invisible, and how people can change this invisibility when necessary. They also explore systems of classification as part of the built information environment. Much as an urban historian would review highway permits and zoning decisions to tell a city\'s story, the authors review archives of classification design to understand how decisions have been made. Sorting Things Out has a moral agenda, for each standard and category valorizes some point of view and silences another. Standards and classifications produce advantage or suffering. Jobs are made and lost; some regions benefit at the expense of others. How these choices are made and how we think about that process are at the moral and political core of this work. The book is an important empirical source for understanding the building of information infrastructures.","url":"https://www.semanticscholar.org/paper/d08392eee17f809d32d7d37e9345383f41271164","sentence":"Title: sorting things out: classification and its consequences Abstract: what do a seventeenth-century mortality table (whose causes of death include \\"fainted in a bath,\\" \\"frighted,\\" and \\"itch\\"); the identification of south africans during apartheid as european, asian, colored, or black; and the separation of machine- from hand-washables have in common? all are examples of classification -- the scaffolding of information infrastructures. in sorting things out, geoffrey c. bowker and susan leigh star explore the role of categories and standards in shaping the modern world. in a clear and lively style, they investigate a variety of classification systems, including the international classification of diseases, the nursing interventions classification, race classification under apartheid in south africa, and the classification of viruses and of tuberculosis. the authors emphasize the role of invisibility in the process by which classification orders human interaction. they examine how categories are made and kept invisible, and how people can change this invisibility when necessary. they also explore systems of classification as part of the built information environment. much as an urban historian would review highway permits and zoning decisions to tell a city\'s story, the authors review archives of classification design to understand how decisions have been made. sorting things out has a moral agenda, for each standard and category valorizes some point of view and silences another. standards and classifications produce advantage or suffering. jobs are made and lost; some regions benefit at the expense of others. how these choices are made and how we think about that process are at the moral and political core of this work. the book is an important empirical source for understanding the building of information infrastructures.","Task":["classification classification"],"Method":["classification classification"]},{"title":"Language (Technology) is Power: A Critical Survey of \u201cBias\u201d in NLP","abstract":"We survey 146 papers analyzing \u201cbias\u201d in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \u201cbias\u201d is an inherently normative process. We further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.","url":"https://www.semanticscholar.org/paper/d47a682723f710395454687319bb55635e653105","sentence":"Title: language (technology) is power: a critical survey of \u201cbias\u201d in nlp Abstract: we survey 146 papers analyzing \u201cbias\u201d in nlp systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \u201cbias\u201d is an inherently normative process. we further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of nlp. based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in nlp systems. these recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by nlp systems, while interrogating and reimagining the power relations between technologists and such communities.","Task":["this bias"],"Method":["power power"]},{"title":"Mitigating Gender Bias in Natural Language Processing: Literature Review","abstract":"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.","url":"https://www.semanticscholar.org/paper/493fac37cea49afb98c52c2f5dd75c303a325b25","sentence":"Title: mitigating gender bias in natural language processing: literature review Abstract: as natural language processing (nlp) and machine learning (ml) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. although nlp models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. while the study of bias in artificial intelligence is not new, methods to mitigate gender bias in nlp are relatively nascent. in this paper, we review contemporary studies on recognizing and mitigating gender bias in nlp. we discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. finally, we discuss future studies for recognizing and mitigating gender bias in nlp.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Gender Bias in Coreference Resolution","abstract":"We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these \u201cWinogender schemas,\u201d we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.","url":"https://www.semanticscholar.org/paper/9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7","sentence":"Title: gender bias in coreference resolution Abstract: we present an empirical study of gender bias in coreference resolution systems. we first introduce a novel, winograd schema-style set of minimal pair sentences that differ only by pronoun gender. with these \u201cwinogender schemas,\u201d we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods","abstract":"In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.","url":"https://www.semanticscholar.org/paper/0be19fd9896e5d40222c690cc3ff553adc7c0e27","sentence":"Title: gender bias in coreference resolution: evaluation and debiasing methods Abstract: in this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, winobias. our corpus contains winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). we demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in f1 score. finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in winobias without significantly affecting their performance on existing datasets.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems","abstract":"Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 \u2018Affect in Tweets\u2019. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.","url":"https://www.semanticscholar.org/paper/5d4af8c9321168f9ba7a501f33fb019fa2deaa22","sentence":"Title: examining gender and race bias in two hundred sentiment analysis systems Abstract: automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. past work on examining inappropriate biases has largely focused on just individual systems. further, there is no benchmark dataset for examining inappropriate biases in systems. here for the first time, we present the equity evaluation corpus (eec), which consists of 8,640 english sentences carefully chosen to tease out biases towards certain races and genders. we use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, semeval-2018 task 1 \u2018affect in tweets\u2019. we find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. we make the eec freely available.","Task":["this bias"],"Method":["implicit bias"]},{"title":"Women\u2019s Syntactic Resilience and Men\u2019s Grammatical Luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing","abstract":"Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. To address this, we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles\u2019 authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. Further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. The results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. We release our data to the research community.","url":"https://www.semanticscholar.org/paper/e85a50b523915b5fba3e3f1fdb743650f7d21bed","sentence":"Title: women\u2019s syntactic resilience and men\u2019s grammatical luck: gender-bias in part-of-speech tagging and dependency parsing Abstract: several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. to address this, we annotate the wall street journal part of the penn treebank with the gender information of the articles\u2019 authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. the results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. we release our data to the research community.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Assessing gender bias in machine translation: a case study with Google Translate","abstract":"Recently there has been a growing concern in academia, industrial research laboratories and the mainstream commercial media about the phenomenon dubbed as machine bias , where trained statistical models\u2014unbeknownst to their creators\u2014grow to reflect controversial societal asymmetries, such as gender or racial bias. A significant number of Artificial Intelligence tools have recently been suggested to be harmfully biased toward some minority, with reports of racist criminal behavior predictors, Apple\u2019s Iphone X failing to differentiate between two distinct Asian people and the now infamous case of Google photos\u2019 mistakenly classifying black people as gorillas. Although a systematic study of such biases can be difficult, we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in AI. In this paper, we start with a comprehensive list of job positions from the U.S. Bureau of Labor Statistics (BLS) and used it in order to build sentences in constructions like \u201cHe/She is an Engineer\u201d (where \u201cEngineer\u201d is replaced by the job position of interest) in 12 different gender neutral languages such as Hungarian, Chinese, Yoruba, and several others. We translate these sentences into English using the Google Translate API, and collect statistics about the frequency of female, male and gender neutral pronouns in the translated output. We then show that Google Translate exhibits a strong tendency toward male defaults, in particular for fields typically associated to unbalanced gender distribution or stereotypes such as STEM (Science, Technology, Engineering and Mathematics) jobs. We ran these statistics against BLS\u2019 data for the frequency of female participation in each job position, in which we show that Google Translate fails to reproduce a real-world distribution of female workers. In summary, we provide experimental evidence that even if one does not expect in principle a 50:50 pronominal gender distribution, Google Translate yields male defaults much more frequently than what would be expected from demographic data alone. We believe that our study can shed further light on the phenomenon of machine bias and are hopeful that it will ignite a debate about the need to augment current statistical translation tools with debiasing techniques\u2014which can already be found in the scientific literature.","url":"https://www.semanticscholar.org/paper/c590d2c8c2fb6ce5d32ee9165ab24171165f2b70","sentence":"Title: assessing gender bias in machine translation: a case study with google translate Abstract: recently there has been a growing concern in academia, industrial research laboratories and the mainstream commercial media about the phenomenon dubbed as machine bias , where trained statistical models\u2014unbeknownst to their creators\u2014grow to reflect controversial societal asymmetries, such as gender or racial bias. a significant number of artificial intelligence tools have recently been suggested to be harmfully biased toward some minority, with reports of racist criminal behavior predictors, apple\u2019s iphone x failing to differentiate between two distinct asian people and the now infamous case of google photos\u2019 mistakenly classifying black people as gorillas. although a systematic study of such biases can be difficult, we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in ai. in this paper, we start with a comprehensive list of job positions from the u.s. bureau of labor statistics (bls) and used it in order to build sentences in constructions like \u201che/she is an engineer\u201d (where \u201cengineer\u201d is replaced by the job position of interest) in 12 different gender neutral languages such as hungarian, chinese, yoruba, and several others. we translate these sentences into english using the google translate api, and collect statistics about the frequency of female, male and gender neutral pronouns in the translated output. we then show that google translate exhibits a strong tendency toward male defaults, in particular for fields typically associated to unbalanced gender distribution or stereotypes such as stem (science, technology, engineering and mathematics) jobs. we ran these statistics against bls\u2019 data for the frequency of female participation in each job position, in which we show that google translate fails to reproduce a real-world distribution of female workers. in summary, we provide experimental evidence that even if one does not expect in principle a 50:50 pronominal gender distribution, google translate yields male defaults much more frequently than what would be expected from demographic data alone. we believe that our study can shed further light on the phenomenon of machine bias and are hopeful that it will ignite a debate about the need to augment current statistical translation tools with debiasing techniques\u2014which can already be found in the scientific literature.","Task":["gender translation"],"Method":["gender translation"]},{"title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations","abstract":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. While previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. We use our method to compile an evaluation benchmark spanning examples for four languages from three language families, which we publicly release to facilitate research. The examples in our benchmark expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.","url":"https://www.semanticscholar.org/paper/008e9001ea78e9654b5c43aeb818ea6cb06ea934","sentence":"Title: automatically identifying gender issues in machine translation using perturbations Abstract: the successful application of neural methods to machine translation has realized huge quality advances for the community. with these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. while previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. we use our method to compile an evaluation benchmark spanning examples for four languages from three language families, which we publicly release to facilitate research. the examples in our benchmark expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.","Task":["gender representation"],"Method":["gender models"]},{"title":"Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem","abstract":"Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a \u2018balanced\u2019 dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is \u2018catastrophic forgetting\u2019, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.","url":"https://www.semanticscholar.org/paper/00059087c954c1af6ece33115315e3e0ecc2f2c2","sentence":"Title: reducing gender bias in neural machine translation as a domain adaptation problem Abstract: training data for nlp tasks often exhibits gender bias in that fewer sentences refer to women than to men. in neural machine translation (nmt) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. the recent winomt challenge set allows us to measure this effect directly (stanovsky et al, 2019) ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. rather than attempt to create a \u2018balanced\u2019 dataset, we use transfer learning on a small set of trusted, gender-balanced examples. this approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. a known pitfall of transfer learning on new domains is \u2018catastrophic forgetting\u2019, which we address at adaptation and inference time. during adaptation we show that elastic weight consolidation allows a performance trade-off between general translation quality and bias reduction. at inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in stanovsky et al, 2019 on winomt with no degradation of general test set bleu. we demonstrate our approach translating from english into three languages with varied linguistic properties and data availability.","Task":["gender bias"],"Method":["gender translation"]},{"title":"Toward Gender-Inclusive Coreference Resolution","abstract":"Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.","url":"https://www.semanticscholar.org/paper/40a6e8d8f253882c585f163b7333842d60ed6f14","sentence":"Title: toward gender-inclusive coreference resolution Abstract: correctly resolving textual mentions of people fundamentally entails making inferences about those people. such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. to better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. through these studies, conducted on english text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.","Task":["systemic bias"],"Method":["gender bias"]},{"title":"Man is to Person as Woman is to Location: Measuring Gender Bias in Named Entity Recognition","abstract":"In this paper, we study the bias in named entity recognition (NER) models---specifically, the difference in the ability to recognize male and female names as PERSON entity types. We evaluate NER models on a dataset containing 139 years of U.S. census baby names and find that relatively more female names, as opposed to male names, are not recognized as PERSON entities. The result of this analysis yields a new benchmark for gender bias evaluation in named entity recognition systems. The data and code for the application of this benchmark is publicly available for researchers to use.","url":"https://www.semanticscholar.org/paper/7b8318894cbeca32f1ae55780a0903445a3f4ac6","sentence":"Title: man is to person as woman is to location: measuring gender bias in named entity recognition Abstract: in this paper, we study the bias in named entity recognition (ner) models---specifically, the difference in the ability to recognize male and female names as person entity types. we evaluate ner models on a dataset containing 139 years of u.s. census baby names and find that relatively more female names, as opposed to male names, are not recognized as person entities. the result of this analysis yields a new benchmark for gender bias evaluation in named entity recognition systems. the data and code for the application of this benchmark is publicly available for researchers to use.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns","abstract":"Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun\u2013name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9% F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.","url":"https://www.semanticscholar.org/paper/57032c1e327c88a53ab41c17e91bf1406f9ef5c9","sentence":"Title: mind the gap: a balanced corpus of gendered ambiguous pronouns Abstract: coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. furthermore, we find gender bias in existing corpora and systems favoring masculine entities. to address this, we present and release gap, a gender-balanced labeled corpus of 8,908 ambiguous pronoun\u2013name pairs sampled to provide diverse coverage of challenges posed by real-world text. we explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9% f1. we show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.","Task":["gender pronouns"],"Method":["gender gap"]},{"title":"Measuring Bias in Contextualized Word Representations","abstract":"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","url":"https://www.semanticscholar.org/paper/a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd","sentence":"Title: measuring bias in contextualized word representations Abstract: contextual word embeddings such as bert have achieved state of the art performance in numerous nlp tasks. since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. in this study, we (1) propose a template-based method to quantify bias in bert; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of gender pronoun resolution. although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Mitigating Gender Bias Amplification in Distribution by Posterior Regularization","abstract":"Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models\u2019 top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.","url":"https://www.semanticscholar.org/paper/0f4bcebc95548a7286106b67bf1115802f093469","sentence":"Title: mitigating gender bias amplification in distribution by posterior regularization Abstract: advanced machine learning techniques have boosted the performance of natural language processing. nevertheless, recent studies, e.g., (citation) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. however, their analysis is conducted only on models\u2019 top predictions. in this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. we further propose a bias mitigation approach based on posterior regularization. with little performance loss, our method can almost remove the bias amplification in the distribution. our study sheds the light on understanding the bias amplification.","Task":["gender bias"],"Method":["bias amplification"]},{"title":"Social Bias in Elicited Natural Language Inferences","abstract":"We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The SNLI human-elicitation protocol makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.","url":"https://www.semanticscholar.org/paper/a20ecabd83e0962329448d8af5025b8061c4ba36","sentence":"Title: social bias in elicited natural language inferences Abstract: we analyze the stanford natural language inference (snli) corpus in an investigation of bias and stereotyping in nlp data. the snli human-elicitation protocol makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.","Task":["social bias"],"Method":["social bias"]},{"title":"Racial disparities in automated speech recognition","abstract":"Significance Automated speech recognition (ASR) systems are now used in a variety of applications to convert spoken language to text, from virtual assistants, to closed captioning, to hands-free computing. By analyzing a large corpus of sociolinguistic interviews with white and African American speakers, we demonstrate large racial disparities in the performance of five popular commercial ASR systems. Our results point to hurdles faced by African Americans in using increasingly widespread tools driven by speech recognition technology. More generally, our work illustrates the need to audit emerging machine-learning systems to ensure they are broadly inclusive. Automated speech recognition (ASR) systems, which use sophisticated machine-learning algorithms to convert spoken language to text, have become increasingly widespread, powering popular virtual assistants, facilitating automated closed captioning, and enabling digital dictation platforms for health care. Over the last several years, the quality of these systems has dramatically improved, due both to advances in deep learning and to the collection of large-scale datasets used to train the systems. There is concern, however, that these tools do not work equally well for all subgroups of the population. Here, we examine the ability of five state-of-the-art ASR systems\u2014developed by Amazon, Apple, Google, IBM, and Microsoft\u2014to transcribe structured interviews conducted with 42 white speakers and 73 black speakers. In total, this corpus spans five US cities and consists of 19.8 h of audio matched on the age and gender of the speaker. We found that all five ASR systems exhibited substantial racial disparities, with an average word error rate (WER) of 0.35 for black speakers compared with 0.19 for white speakers. We trace these disparities to the underlying acoustic models used by the ASR systems as the race gap was equally large on a subset of identical phrases spoken by black and white individuals in our corpus. We conclude by proposing strategies\u2014such as using more diverse training datasets that include African American Vernacular English\u2014to reduce these performance differences and ensure speech recognition technology is inclusive.","url":"https://www.semanticscholar.org/paper/219b7266ae848937da170c5510b2bfc66d17859a","sentence":"Title: racial disparities in automated speech recognition Abstract: significance automated speech recognition (asr) systems are now used in a variety of applications to convert spoken language to text, from virtual assistants, to closed captioning, to hands-free computing. by analyzing a large corpus of sociolinguistic interviews with white and african american speakers, we demonstrate large racial disparities in the performance of five popular commercial asr systems. our results point to hurdles faced by african americans in using increasingly widespread tools driven by speech recognition technology. more generally, our work illustrates the need to audit emerging machine-learning systems to ensure they are broadly inclusive. automated speech recognition (asr) systems, which use sophisticated machine-learning algorithms to convert spoken language to text, have become increasingly widespread, powering popular virtual assistants, facilitating automated closed captioning, and enabling digital dictation platforms for health care. over the last several years, the quality of these systems has dramatically improved, due both to advances in deep learning and to the collection of large-scale datasets used to train the systems. there is concern, however, that these tools do not work equally well for all subgroups of the population. here, we examine the ability of five state-of-the-art asr systems\u2014developed by amazon, apple, google, ibm, and microsoft\u2014to transcribe structured interviews conducted with 42 white speakers and 73 black speakers. in total, this corpus spans five us cities and consists of 19.8 h of audio matched on the age and gender of the speaker. we found that all five asr systems exhibited substantial racial disparities, with an average word error rate (wer) of 0.35 for black speakers compared with 0.19 for white speakers. we trace these disparities to the underlying acoustic models used by the asr systems as the race gap was equally large on a subset of identical phrases spoken by black and white individuals in our corpus. we conclude by proposing strategies\u2014such as using more diverse training datasets that include african american vernacular english\u2014to reduce these performance differences and ensure speech recognition technology is inclusive.","Task":["racial disparities"],"Method":["racial bias"]},{"title":"Effects of Talker Dialect, Gender & Race on Accuracy of Bing Speech and YouTube Automatic Captions","abstract":"This project compares the accuracy of two automatic speech recognition (ASR) systems\u2013Bing Speech and YouTube\u2019s automatic captions\u2013across gender, race and four dialects of American English. The dialects included were chosen for their acoustic dissimilarity. Bing Speech had differences in word error rate (WER) between dialects and ethnicities, but they were not statistically reliable. YouTube\u2019s automatic captions, however, did have statistically different WERs between dialects and races. The lowest average error rates were for General American and white talkers, respectively. Neither system had a reliably different WER between genders, which had been previously reported for YouTube\u2019s automatic captions [1]. However, the higher error rate non-white talkers is worrying, as it may reduce the utility of these systems for talkers of color.","url":"https://www.semanticscholar.org/paper/1080dc00733e010fdd6a9b999506a0d4d864519d","sentence":"Title: effects of talker dialect, gender & race on accuracy of bing speech and youtube automatic captions Abstract: this project compares the accuracy of two automatic speech recognition (asr) systems\u2013bing speech and youtube\u2019s automatic captions\u2013across gender, race and four dialects of american english. the dialects included were chosen for their acoustic dissimilarity. bing speech had differences in word error rate (wer) between dialects and ethnicities, but they were not statistically reliable. youtube\u2019s automatic captions, however, did have statistically different wers between dialects and races. the lowest average error rates were for general american and white talkers, respectively. neither system had a reliably different wer between genders, which had been previously reported for youtube\u2019s automatic captions [1]. however, the higher error rate non-white talkers is worrying, as it may reduce the utility of these systems for talkers of color.","Task":["this bias"],"Method":["speech speech"]},{"title":"Race as a Bundle of Sticks: Designs that Estimate Effects of Seemingly Immutable Characteristics","abstract":"Although understanding the role of race, ethnicity, and identity is central to political science, methodological debates persist about whether it is possible to estimate the effect of something immutable. At the heart of the debate is an older theoretical question: Is race best understood under an essentialist or constructivist framework? In contrast to the \u201cimmutable characteristics\u201d or essentialist approach, we argue that race should be operationalized as a \u201cbundle of sticks\u201d that can be disaggregated into elements. With elements of race, causal claims may be possible using two designs: (a) studies that measure the effect of exposure to a racial cue and (b) studies that exploit within-group variation to measure the effect of some manipulable element. These designs can reconcile scholarship on race and causation and offer a clear framework for future research.","url":"https://www.semanticscholar.org/paper/21e59098bb5e36175f653d5142442d061669d07f","sentence":"Title: race as a bundle of sticks: designs that estimate effects of seemingly immutable characteristics Abstract: although understanding the role of race, ethnicity, and identity is central to political science, methodological debates persist about whether it is possible to estimate the effect of something immutable. at the heart of the debate is an older theoretical question: is race best understood under an essentialist or constructivist framework? in contrast to the \u201cimmutable characteristics\u201d or essentialist approach, we argue that race should be operationalized as a \u201cbundle of sticks\u201d that can be disaggregated into elements. with elements of race, causal claims may be possible using two designs: (a) studies that measure the effect of exposure to a racial cue and (b) studies that exploit within-group variation to measure the effect of some manipulable element. these designs can reconcile scholarship on race and causation and offer a clear framework for future research.","Task":["causal design"],"Method":["immutable elements"]},{"title":"Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints","abstract":"Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively\u3002","url":"https://www.semanticscholar.org/paper/8417424bf9fe7a67f06f15c487403e953ab24a96","sentence":"Title: men also like shopping: reducing gender bias amplification using corpus-level constraints Abstract: language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. in this work, we study data and models associated with multilabel object classification and visual semantic role labeling. we find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. for example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. we propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on lagrangian relaxation for collective inference. our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively\u3002","Task":["gender bias"],"Method":["gender bias"]},{"title":"Towards Understanding Gender Bias in Relation Extraction","abstract":"Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction. While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in NRE systems. In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems. We find that when extracting spouse-of and hypernym (i.e., occupation) relations, an NRE system performs differently when the gender of the target entity is different. However, such disparity does not appear when extracting relations such as birthDate or birthPlace. We also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases. Unfortunately, due to NRE models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on NRE. Our analysis lays groundwork for future quantifying and mitigating bias in NRE.","url":"https://www.semanticscholar.org/paper/0ab2fb6c850bd1c5882deb4984d37b4ccbee580c","sentence":"Title: towards understanding gender bias in relation extraction Abstract: recent developments in neural relation extraction (nre) have made significant strides towards automated knowledge base construction. while much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in nre systems. in this paper, we create wikigenderbias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems. we find that when extracting spouse-of and hypernym (i.e., occupation) relations, an nre system performs differently when the gender of the target entity is different. however, such disparity does not appear when extracting relations such as birthdate or birthplace. we also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the nre system in terms of maintaining the test performance and reducing biases. unfortunately, due to nre models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on nre. our analysis lays groundwork for future quantifying and mitigating bias in nre.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English","abstract":"We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.","url":"https://www.semanticscholar.org/paper/59e94c9f21937643678ff494901f3d8b22af4e2f","sentence":"Title: racial disparity in natural language processing: a case study of social media african-american english Abstract: we highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. for example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. we conduct an empirical analysis of racial disparity in language identification for tweets written in african-american english, and discuss implications of disparity in nlp.","Task":["language disparity"],"Method":["racial disparity"]},{"title":"Re-imagining Algorithmic Fairness in India and Beyond","abstract":"Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.","url":"https://www.semanticscholar.org/paper/187608bf94b2dccd25d1266ed925abf7b55dbb2e","sentence":"Title: re-imagining algorithmic fairness in india and beyond Abstract: conventional algorithmic fairness is west-centric, as seen in its subgroups, values, and methods. in this paper, we de-center algorithmic fairness and analyse ai power in india. based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in india, we find that several assumptions of algorithmic fairness are challenged. we find that in india, data is not always reliable due to socio-economic factors, ml makers appear to follow double standards, and ai evokes unquestioning aspiration. we contend that localising model fairness alone can be window dressing in india, where the distance between models and oppressed communities is large. instead, we re-imagine algorithmic fairness in india and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable fair-ml ecosystems.","Task":["model inequality"],"Method":["fair fairness"]},{"title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c","abstract":"The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.","url":"https://www.semanticscholar.org/paper/6d9727f1f058614cada3fe296eeebd8ec4fc512a","sentence":"Title: on the dangers of stochastic parrots: can language models be too big? \ud83e\udd9c Abstract: the past 3 years of work in nlp have been characterized by the development and deployment of ever larger language models, especially for english. bert, its variants, gpt-2/3, and others, most recently switch-c, have pushed the boundaries of the possible both through architectural innovations and through sheer size. using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for english. in this paper, we take a step back and ask: how big is too big? what are the possible risks associated with this technology and what paths are available for mitigating those risks? we provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.","Task":["language size"],"Method":["large models"]},{"title":"Green AI","abstract":"Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.","url":"https://www.semanticscholar.org/paper/fb73b93de3734a996829caf31e4310e0054e9c6b","sentence":"Title: green ai Abstract: creating efficiency in ai research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.","Task":["green efficiency"],"Method":["green efficiency"]},{"title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning","abstract":"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.","url":"https://www.semanticscholar.org/paper/13f25c69973373e616c48688d06a6b6ae2736ef0","sentence":"Title: towards the systematic reporting of the energy and carbon footprints of machine learning Abstract: accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. we introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. by making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.","Task":["energy reporting"],"Method":["energy accounting"]},{"title":"Social Biases in NLP Models as Barriers for Persons with Disabilities","abstract":"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.","url":"https://www.semanticscholar.org/paper/3cc2f69951cd24fe61be4cf32d62afbac297bc2b","sentence":"Title: social biases in nlp models as barriers for persons with disabilities Abstract: building equitable and inclusive nlp technologies demands consideration of whether and how social attitudes are represented in ml models. in particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. in this paper, we present evidence of such undesirable biases towards mentions of disability in two different english language models: toxicity prediction and sentiment analysis. next, we demonstrate that the neural embeddings that are the critical first step in most nlp pipelines similarly contain undesirable biases towards mentions of disability. we end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.","Task":["this bias"],"Method":["social bias"]},{"title":"Social Chemistry 101: Learning to Reason about Social and Moral Norms","abstract":"Social norms---the unspoken commonsense rules about acceptable social behavior---are crucial in understanding the underlying causes and intents of people\'s actions in narratives. For example, underlying an action such as \\"wanting to call cops on my neighbors\\" are social norms that inform our conduct, such as \\"It is expected that you report crimes.\\" \\nWe present Social Chemistry, a new conceptual formalism to study people\'s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as \\"it is rude to run a blender at 5am\\" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people\'s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. \\nComprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.","url":"https://www.semanticscholar.org/paper/10391eed628dfece8a9136f76c5df53b5704422d","sentence":"Title: social chemistry 101: learning to reason about social and moral norms Abstract: social norms---the unspoken commonsense rules about acceptable social behavior---are crucial in understanding the underlying causes and intents of people\'s actions in narratives. for example, underlying an action such as \\"wanting to call cops on my neighbors\\" are social norms that inform our conduct, such as \\"it is expected that you report crimes.\\" \\nwe present social chemistry, a new conceptual formalism to study people\'s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. we introduce social-chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as \\"it is rude to run a blender at 5am\\" as the basic conceptual units. each rule-of-thumb is further broken down with 12 different dimensions of people\'s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. \\ncomprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. our model framework, neural norm transformer, learns and generalizes social-chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.","Task":["norm norms"],"Method":["social chemistry"]},{"title":"BERT has a Moral Compass: Improvements of ethical and moral values of machines","abstract":"Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Jentzsch et al.(2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about \\"right\\" and \\"wrong\\" conduct by calculating a moral bias score on a sentence level using sentence embeddings. The machine learned that it is objectionable to kill living beings, but it is fine to kill time; It is essential to eat, yet one might not eat dirt; it is important to spread information, yet one should not spread misinformation. However, the evaluated moral bias was restricted to simple actions -- one verb -- and a ranking of actions with surrounding context. Recently BERT ---and variants such as RoBERTa and SBERT--- has set a new state-of-the-art performance for a wide range of NLP tasks. But has BERT also a better moral compass? In this paper, we discuss and show that this is indeed the case. Thus, recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine. We argue that through an advanced semantic representation of text, BERT allows one to get better insights of moral and ethical values implicitly represented in text. This enables the Moral Choice Machine (MCM) to extract more accurate imprints of moral choices and ethical values.","url":"https://www.semanticscholar.org/paper/8755c15fe073c6af03664b2a74aafef1fed5f198","sentence":"Title: bert has a moral compass: improvements of ethical and moral values of machines Abstract: allowing machines to choose whether to kill humans would be devastating for world peace and security. but how do we equip machines with the ability to learn ethical or even moral choices? jentzsch et al.(2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about \\"right\\" and \\"wrong\\" conduct by calculating a moral bias score on a sentence level using sentence embeddings. the machine learned that it is objectionable to kill living beings, but it is fine to kill time; it is essential to eat, yet one might not eat dirt; it is important to spread information, yet one should not spread misinformation. however, the evaluated moral bias was restricted to simple actions -- one verb -- and a ranking of actions with surrounding context. recently bert ---and variants such as roberta and sbert--- has set a new state-of-the-art performance for a wide range of nlp tasks. but has bert also a better moral compass? in this paper, we discuss and show that this is indeed the case. thus, recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine. we argue that through an advanced semantic representation of text, bert allows one to get better insights of moral and ethical values implicitly represented in text. this enables the moral choice machine (mcm) to extract more accurate imprints of moral choices and ethical values.","Task":["semantic representation"],"Method":["moral compass"]},{"title":"Open Problems in Cooperative AI","abstract":"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. \\nWe see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.","url":"https://www.semanticscholar.org/paper/2a1573cfa29a426c695e2caf6de0167a12b788ef","sentence":"Title: open problems in cooperative ai Abstract: problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. they can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. arguably, the success of the human species is rooted in our ability to cooperate. since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. \\nwe see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term cooperative ai. the objective of this research would be to study the many aspects of the problems of cooperation and to innovate in ai to contribute to solving these problems. central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting ai research for insight relevant to problems of cooperation. this research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. however, cooperative ai is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. we see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.","Task":["cooperative cooperation"],"Method":["cooperative cooperation"]},{"title":"Existential Risk Prevention as Global Priority","abstract":"risks are those that threaten the entire future of humanity. Many theories of value imply that even relatively small reductions in net existential risk have enormous expected value. Despite their importance, issues surrounding human-extinction risks and related hazards remain poorly understood. In this article, I clarify the concept of existential risk and develop an improved classification scheme. I discuss the relation between existential risks and basic issues in axiology, and show how existential risk reduction (via the maxipok rule) can serve as a strongly action-guiding principle for utilitarian concerns. I also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability. Policy Implications \u2022 Existential risk is a concept that can focus long-term global efforts and sustainability concerns. \u2022 The biggest existential risks are anthropogenic and related to potential future technologies. \u2022 A moral case can be made that existential risk reduction is strictly more important than any other global public good. \u2022 Sustainability should be reconceptualised in dynamic terms, as aiming for a sustainable trajectory rather than a sus- tainable state. \u2022 Some small existential risks can be mitigated today directly (e.g. asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity\'s ability to deal with the larger existential risks that will arise later in this century. This will require collective wisdom, technology foresight, and the ability when necessary to mobilise a strong global coordi- nated response to anticipated existential risks. \u2022 Perhaps the most cost-effective way to reduce existential risks today is to fund analysis of a wide range of existen- tial risks and potential mitigation strategies, with a long-term perspective.","url":"https://www.semanticscholar.org/paper/ced289065723368bca48636edf71eeed50f40a39","sentence":"Title: existential risk prevention as global priority Abstract: risks are those that threaten the entire future of humanity. many theories of value imply that even relatively small reductions in net existential risk have enormous expected value. despite their importance, issues surrounding human-extinction risks and related hazards remain poorly understood. in this article, i clarify the concept of existential risk and develop an improved classification scheme. i discuss the relation between existential risks and basic issues in axiology, and show how existential risk reduction (via the maxipok rule) can serve as a strongly action-guiding principle for utilitarian concerns. i also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability. policy implications \u2022 existential risk is a concept that can focus long-term global efforts and sustainability concerns. \u2022 the biggest existential risks are anthropogenic and related to potential future technologies. \u2022 a moral case can be made that existential risk reduction is strictly more important than any other global public good. \u2022 sustainability should be reconceptualised in dynamic terms, as aiming for a sustainable trajectory rather than a sus- tainable state. \u2022 some small existential risks can be mitigated today directly (e.g. asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity\'s ability to deal with the larger existential risks that will arise later in this century. this will require collective wisdom, technology foresight, and the ability when necessary to mobilise a strong global coordi- nated response to anticipated existential risks. \u2022 perhaps the most cost-effective way to reduce existential risks today is to fund analysis of a wide range of existen- tial risks and potential mitigation strategies, with a long-term perspective.","Task":["existential risk"],"Method":["existential risk"]},{"title":"Participatory Problem Formulation for Fairer Machine Learning Through Community Based System Dynamics","abstract":"Recent research on algorithmic fairness has highlighted that the problem formulation phase of ML system development can be a key source of bias that has significant downstream impacts on ML system fairness outcomes. However, very little attention has been paid to methods for improving the fairness efficacy of this critical phase of ML system development. Current practice neither accounts for the dynamic complexity of high-stakes domains nor incorporates the perspectives of vulnerable stakeholders. In this paper we introduce community based system dynamics (CBSD) as an approach to enable the participation of typically excluded stakeholders in the problem formulation phase of the ML system development process and facilitate the deep problem understanding required to mitigate bias during this crucial stage.","url":"https://www.semanticscholar.org/paper/8763723e27cc1d4aad166b5e1d9cb0fc8c8043dd","sentence":"Title: participatory problem formulation for fairer machine learning through community based system dynamics Abstract: recent research on algorithmic fairness has highlighted that the problem formulation phase of ml system development can be a key source of bias that has significant downstream impacts on ml system fairness outcomes. however, very little attention has been paid to methods for improving the fairness efficacy of this critical phase of ml system development. current practice neither accounts for the dynamic complexity of high-stakes domains nor incorporates the perspectives of vulnerable stakeholders. in this paper we introduce community based system dynamics (cbsd) as an approach to enable the participation of typically excluded stakeholders in the problem formulation phase of the ml system development process and facilitate the deep problem understanding required to mitigate bias during this crucial stage.","Task":["problem bias"],"Method":["community bias"]},{"title":"A Research Framework for Understanding Education-Occupation Alignment with NLP Techniques","abstract":"Understanding the gaps between job requirements and university curricula is crucial for improving student success and institutional effectiveness in higher education. In this context, natural language processing (NLP) can be leveraged to generate granular insights into where the gaps are and how they change. This paper proposes a three-dimensional research framework that combines NLP techniques with economic and educational research to quantify the alignment between course syllabi and job postings. We elaborate on key technical details of the framework and further discuss its potential positive impacts on practice, including unveiling the inequalities in and long-term consequences of education-occupation alignment to inform policymakers, and fostering information systems to support students, institutions and employers in the school-to-work pipeline.","url":"https://www.semanticscholar.org/paper/40141f0933b5111b089049e226dc8d969b0a7fca","sentence":"Title: a research framework for understanding education-occupation alignment with nlp techniques Abstract: understanding the gaps between job requirements and university curricula is crucial for improving student success and institutional effectiveness in higher education. in this context, natural language processing (nlp) can be leveraged to generate granular insights into where the gaps are and how they change. this paper proposes a three-dimensional research framework that combines nlp techniques with economic and educational research to quantify the alignment between course syllabi and job postings. we elaborate on key technical details of the framework and further discuss its potential positive impacts on practice, including unveiling the inequalities in and long-term consequences of education-occupation alignment to inform policymakers, and fostering information systems to support students, institutions and employers in the school-to-work pipeline.","Task":["this alignment"],"Method":["knowledge alignment"]},{"title":"A Grounded Well-being Conversational Agent with Multiple Interaction Modes: Preliminary Results","abstract":"Technologies for enhancing well-being, healthcare vigilance and monitoring are on the rise. However, despite patient interest, such technologies suffer from low adoption. One hypothesis for this limited adoption is loss of human interaction that is central to doctor-patient encounters. In this paper we seek to address this limitation via a conversational agent that adopts one aspect of in-person doctor-patient interactions: A human avatar to facilitate medical grounded question answering. This is akin to the in-person scenario where the doctor may point to the human body or the patient may point to their own body to express their conditions. Additionally, our agent has multiple interaction modes, that may give more options for the patient to use the agent, not just for medical question answering, but also to engage in conversations about general topics and current events. Both the avatar, and the multiple interaction modes could help improve adherence. We present a high level overview of the design of our agent, Marie Bot Wellbeing. We also report implementation details of our early prototype , and present preliminary results.","url":"https://www.semanticscholar.org/paper/8d9a678c56b9085de65024aa2f6b406ccad97390","sentence":"Title: a grounded well-being conversational agent with multiple interaction modes: preliminary results Abstract: technologies for enhancing well-being, healthcare vigilance and monitoring are on the rise. however, despite patient interest, such technologies suffer from low adoption. one hypothesis for this limited adoption is loss of human interaction that is central to doctor-patient encounters. in this paper we seek to address this limitation via a conversational agent that adopts one aspect of in-person doctor-patient interactions: a human avatar to facilitate medical grounded question answering. this is akin to the in-person scenario where the doctor may point to the human body or the patient may point to their own body to express their conditions. additionally, our agent has multiple interaction modes, that may give more options for the patient to use the agent, not just for medical question answering, but also to engage in conversations about general topics and current events. both the avatar, and the multiple interaction modes could help improve adherence. we present a high level overview of the design of our agent, marie bot wellbeing. we also report implementation details of our early prototype , and present preliminary results.","Task":["human interaction"],"Method":["grounded agent"]},{"title":"A Speech-enabled Fixed-phrase Translator for Healthcare Accessibility","abstract":"In this overview article we describe an application designed to enable communication between health practitioners and patients who do not share a common language, in situations where professional interpreters are not available. Built on the principle of a fixed phrase translator, the application implements different natural language processing (NLP) technologies, such as speech recognition, neural machine translation and text-to-speech to improve usability. Its design allows easy portability to new domains and integration of different types of output for multiple target audiences. Even though BabelDr is far from solving the problem of miscommunication between patients and doctors, it is a clear example of NLP in a real world application designed to help minority groups to communicate in a medical context. It also gives some insights into the relevant criteria for the development of such an application.","url":"https://www.semanticscholar.org/paper/934dbfbb33cbec11fc825db56ac85a48fc52158f","sentence":"Title: a speech-enabled fixed-phrase translator for healthcare accessibility Abstract: in this overview article we describe an application designed to enable communication between health practitioners and patients who do not share a common language, in situations where professional interpreters are not available. built on the principle of a fixed phrase translator, the application implements different natural language processing (nlp) technologies, such as speech recognition, neural machine translation and text-to-speech to improve usability. its design allows easy portability to new domains and integration of different types of output for multiple target audiences. even though babeldr is far from solving the problem of miscommunication between patients and doctors, it is a clear example of nlp in a real world application designed to help minority groups to communicate in a medical context. it also gives some insights into the relevant criteria for the development of such an application.","Task":["language communication"],"Method":["speech translation"]},{"title":"Analyzing Stereotypes in Generative Text Inference Tasks","abstract":"Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality.","url":"https://www.semanticscholar.org/paper/54db327cd53fe043449c9f242d3fc34c593a70ef","sentence":"Title: analyzing stereotypes in generative text inference tasks Abstract: stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. in generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). such tasks are therefore a fruitful setting in which to explore the degree to which nlp systems encode stereotypes. in our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. we collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality.","Task":["stereotype stereotypes"],"Method":["stereotype stereotypes"]},{"title":"Demographic Dialectal Variation in Social Media: A Case Study of African-American English","abstract":"Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language.","url":"https://www.semanticscholar.org/paper/7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c","sentence":"Title: demographic dialectal variation in social media: a case study of african-american english Abstract: though dialectal language is increasingly abundant on social media, few resources exist for developing nlp tools to handle such language. we conduct a case study of dialectal language in online conversational text by investigating african-american english (aae) on twitter. we propose a distantly supervised model to identify aae-like language from demographics associated with geo-located messages, and we verify that this language follows well-known aae linguistic phenomena. in addition, we analyze the quality of existing language identification and dependency parsing tools on aae-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. we also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing aae-like language.","Task":["language parsing"],"Method":["language language"]},{"title":"Energy and Policy Considerations for Deep Learning in NLP","abstract":"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.","url":"https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","sentence":"Title: energy and policy considerations for deep learning in nlp Abstract: recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. these models have obtained notable gains in accuracy across many nlp tasks. however, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. as a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. in this paper we bring this issue to the attention of nlp researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for nlp. based on these findings, we propose actionable recommendations to reduce costs and improve equity in nlp research and practice.","Task":["energy costs"],"Method":["cost costs"]},{"title":"Automatic Sentence Simplification in Low Resource Settings for Urdu","abstract":"To build automated simplification systems, corpora of complex sentences and their simplified versions is the first step to understand sentence complexity and enable the development of automatic text simplification systems. We present a lexical and syntactically simplified Urdu simplification corpus with a detailed analysis of the various simplification operations and human evaluation of corpus quality. We further analyze our corpora using text readability measures and present a comparison of the original, lexical simplified and syntactically simplified corpora. In addition, we compare our corpus with other existing simplification corpora by building simplification systems and evaluating these systems using BLEU and SARI scores. Our system achieves the highest BLEU score and comparable SARI score in comparison to other systems. We release our simplification corpora for the benefit of the research community.","url":"https://www.semanticscholar.org/paper/d6a25d8726c5484bb224a3350528aae9fcaae65f","sentence":"Title: automatic sentence simplification in low resource settings for urdu Abstract: to build automated simplification systems, corpora of complex sentences and their simplified versions is the first step to understand sentence complexity and enable the development of automatic text simplification systems. we present a lexical and syntactically simplified urdu simplification corpus with a detailed analysis of the various simplification operations and human evaluation of corpus quality. we further analyze our corpora using text readability measures and present a comparison of the original, lexical simplified and syntactically simplified corpora. in addition, we compare our corpus with other existing simplification corpora by building simplification systems and evaluating these systems using bleu and sari scores. our system achieves the highest bleu score and comparable sari score in comparison to other systems. we release our simplification corpora for the benefit of the research community.","Task":["semantic quality"],"Method":["semantic corpus"]},{"title":"Cartography of Natural Language Processing for Social Good (NLP4SG): Searching for Definitions, Statistics and White Spots","abstract":"The range of works that can be considered as developing NLP for social good (NLP4SG) is enormous. While many of them target the identification of hate speech or fake news, there are others that address, e.g., text simplification to alleviate consequences of dyslexia, or coaching strategies to fight depression. However, so far, there is no clear picture of what areas are targeted by NLP4SG, who are the actors, which are the main scenarios and what are the topics that have been left aside. In order to obtain a clearer view in this respect, we first propose a working definition of NLP4SG and identify some primary aspects that are crucial for NLP4SG, including, e.g., areas, ethics, privacy and bias. Then, we draw upon a corpus of around 50,000 articles downloaded from the ACL Anthology. Based on a list of keywords retrieved from the literature and revised in view of the task, we select from this corpus articles that can be considered to be on NLP4SG according to our definition and analyze them in terms of trends along the time line, etc. The result is a map of the current NLP4SG research and insights concerning the white spots on this map.","url":"https://www.semanticscholar.org/paper/4975c64466149c72f31489fadbbbff4e85d7b3f3","sentence":"Title: cartography of natural language processing for social good (nlp4sg): searching for definitions, statistics and white spots Abstract: the range of works that can be considered as developing nlp for social good (nlp4sg) is enormous. while many of them target the identification of hate speech or fake news, there are others that address, e.g., text simplification to alleviate consequences of dyslexia, or coaching strategies to fight depression. however, so far, there is no clear picture of what areas are targeted by nlp4sg, who are the actors, which are the main scenarios and what are the topics that have been left aside. in order to obtain a clearer view in this respect, we first propose a working definition of nlp4sg and identify some primary aspects that are crucial for nlp4sg, including, e.g., areas, ethics, privacy and bias. then, we draw upon a corpus of around 50,000 articles downloaded from the acl anthology. based on a list of keywords retrieved from the literature and revised in view of the task, we select from this corpus articles that can be considered to be on nlp4sg according to our definition and analyze them in terms of trends along the time line, etc. the result is a map of the current nlp4sg research and insights concerning the white spots on this map.","Task":["white spots"],"Method":["white spots"]},{"title":"Breaking Down Walls of Text: How Can NLP Benefit Consumer Privacy?","abstract":"Privacy plays a crucial role in preserving democratic ideals and personal autonomy. The dominant legal approach to privacy in many jurisdictions is the \u201cNotice and Choice\u201d paradigm, where privacy policies are the primary instrument used to convey information to users. However, privacy policies are long and complex documents that are difficult for users to read and comprehend. We discuss how language technologies can play an important role in addressing this information gap, reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies: consumers, enterprises, and regulators. Our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy, limit privacy harms, and rally research efforts from the community towards addressing an issue with large social impact. We highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies.","url":"https://www.semanticscholar.org/paper/c39fc494cb84b260eaa33cceda2b76512f3701a5","sentence":"Title: breaking down walls of text: how can nlp benefit consumer privacy? Abstract: privacy plays a crucial role in preserving democratic ideals and personal autonomy. the dominant legal approach to privacy in many jurisdictions is the \u201cnotice and choice\u201d paradigm, where privacy policies are the primary instrument used to convey information to users. however, privacy policies are long and complex documents that are difficult for users to read and comprehend. we discuss how language technologies can play an important role in addressing this information gap, reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies: consumers, enterprises, and regulators. our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy, limit privacy harms, and rally research efforts from the community towards addressing an issue with large social impact. we highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies.","Task":["language privacy"],"Method":["semantic language"]},{"title":"Are we human, or are we users? The role of natural language processing in human-centric news recommenders that nudge users to diverse content","abstract":"In this position paper, we present a research agenda and ideas for facilitating exposure to diverse viewpoints in news recommendation. Recommending news from diverse viewpoints is important to prevent potential filter bubble effects in news consumption, and stimulate a healthy democratic debate.To account for the complexity that is inherent to humans as citizens in a democracy, we anticipate (among others) individual-level differences in acceptance of diversity. We connect this idea to techniques in Natural Language Processing, where distributional language models would allow us to place different users and news articles in a multidimensional space based on semantic content, where diversity is operationalized as distance and variance. In this way, we can model individual \u201clatitudes of diversity\u201d for different users, and thus personalize viewpoint diversity in support of a healthy public debate. In addition, we identify technical, ethical and conceptual issues related to our presented ideas. Our investigation describes how NLP can play a central role in diversifying news recommendations.","url":"https://www.semanticscholar.org/paper/9995132dda17b36e5513c8e98d58ff992d0ba79a","sentence":"Title: are we human, or are we users? the role of natural language processing in human-centric news recommenders that nudge users to diverse content Abstract: in this position paper, we present a research agenda and ideas for facilitating exposure to diverse viewpoints in news recommendation. recommending news from diverse viewpoints is important to prevent potential filter bubble effects in news consumption, and stimulate a healthy democratic debate.to account for the complexity that is inherent to humans as citizens in a democracy, we anticipate (among others) individual-level differences in acceptance of diversity. we connect this idea to techniques in natural language processing, where distributional language models would allow us to place different users and news articles in a multidimensional space based on semantic content, where diversity is operationalized as distance and variance. in this way, we can model individual \u201clatitudes of diversity\u201d for different users, and thus personalize viewpoint diversity in support of a healthy public debate. in addition, we identify technical, ethical and conceptual issues related to our presented ideas. our investigation describes how nlp can play a central role in diversifying news recommendations.","Task":["n bias"],"Method":["n bias"]},{"title":"Challenges for Information Extraction from Dialogue in Criminal Law","abstract":"Information extraction and question answering have the potential to introduce a new paradigm for how machine learning is applied to criminal law. Existing approaches generally use tabular data for predictive metrics. An alternative approach is needed for matters of equitable justice, where individuals are judged on a case-by-case basis, in a process involving verbal or written discussion and interpretation of case factors. Such discussions are individualized, but they nonetheless rely on underlying facts. Information extraction can play an important role in surfacing these facts, which are still important to understand. We analyze unsupervised, weakly supervised, and pre-trained models\u2019 ability to extract such factual information from the free-form dialogue of California parole hearings. With a few exceptions, most F1 scores are below 0.85. We use this opportunity to highlight some opportunities for further research for information extraction and question answering. We encourage new developments in NLP to enable analysis and review of legal cases to be done in a post-hoc, not predictive, manner.","url":"https://www.semanticscholar.org/paper/03c046041bc509f2cc9671ee71a78642275b77c3","sentence":"Title: challenges for information extraction from dialogue in criminal law Abstract: information extraction and question answering have the potential to introduce a new paradigm for how machine learning is applied to criminal law. existing approaches generally use tabular data for predictive metrics. an alternative approach is needed for matters of equitable justice, where individuals are judged on a case-by-case basis, in a process involving verbal or written discussion and interpretation of case factors. such discussions are individualized, but they nonetheless rely on underlying facts. information extraction can play an important role in surfacing these facts, which are still important to understand. we analyze unsupervised, weakly supervised, and pre-trained models\u2019 ability to extract such factual information from the free-form dialogue of california parole hearings. with a few exceptions, most f1 scores are below 0.85. we use this opportunity to highlight some opportunities for further research for information extraction and question answering. we encourage new developments in nlp to enable analysis and review of legal cases to be done in a post-hoc, not predictive, manner.","Task":["question extraction"],"Method":["question learning"]},{"title":"Conversations Gone Alright: Quantifying and Predicting Prosocial Outcomes in Online Conversations","abstract":"Online conversations can go in many directions: some turn out poorly due to antisocial behavior, while others turn out positively to the benefit of all. Research on improving online spaces has focused primarily on detecting and reducing antisocial behavior. Yet we know little about positive outcomes in online conversations and how to increase them\u2014is a prosocial outcome simply the lack of antisocial behavior or something more? Here, we examine how conversational features lead to prosocial outcomes within online discussions. We introduce a series of new theory-inspired metrics to define prosocial outcomes such as mentoring and esteem enhancement. Using a corpus of 26M Reddit conversations, we show that these outcomes can be forecasted from the initial comment of an online conversation, with the best model providing a relative 24% improvement over human forecasting performance at ranking conversations for predicted outcome. Our results indicate that platforms can use these early cues in their algorithmic ranking of early conversations to prioritize better outcomes.","url":"https://www.semanticscholar.org/paper/50718d6bd163967b8353de4c854ed866b2b56c2f","sentence":"Title: conversations gone alright: quantifying and predicting prosocial outcomes in online conversations Abstract: online conversations can go in many directions: some turn out poorly due to antisocial behavior, while others turn out positively to the benefit of all. research on improving online spaces has focused primarily on detecting and reducing antisocial behavior. yet we know little about positive outcomes in online conversations and how to increase them\u2014is a prosocial outcome simply the lack of antisocial behavior or something more? here, we examine how conversational features lead to prosocial outcomes within online discussions. we introduce a series of new theory-inspired metrics to define prosocial outcomes such as mentoring and esteem enhancement. using a corpus of 26m reddit conversations, we show that these outcomes can be forecasted from the initial comment of an online conversation, with the best model providing a relative 24% improvement over human forecasting performance at ranking conversations for predicted outcome. our results indicate that platforms can use these early cues in their algorithmic ranking of early conversations to prioritize better outcomes.","Task":["predictive bias"],"Method":["social outcomes"]},{"title":"Recommender systems and their ethical challenges","abstract":"This article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. The article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. The analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders\u2014as opposed to just the receivers of a recommendation\u2014in assessing the ethical impacts of a recommender system.","url":"https://www.semanticscholar.org/paper/52a14b391f994d83759787500a9bda865acdb3c5","sentence":"Title: recommender systems and their ethical challenges Abstract: this article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. the article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. the analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders\u2014as opposed to just the receivers of a recommendation\u2014in assessing the ethical impacts of a recommender system.","Task":["this heterogeneity"],"Method":["ethical impact"]},{"title":"Conversational receptiveness: Improving engagement with opposing views","abstract":"Abstract We examine \u201cconversational receptiveness\u201d \u2013 the use of language to communicate one\u2019s willingness to thoughtfully engage with opposing views. We develop an interpretable machine-learning algorithm to identify the linguistic profile of receptiveness (Studies 1A-B). We then show that in contentious policy discussions, government executives who were rated as more receptive - according to our algorithm and their partners, but not their own self-evaluations - were considered better teammates, advisors, and workplace representatives (Study 2). Furthermore, using field data from a setting where conflict management is endemic to productivity, we show that conversational receptiveness at the beginning of a conversation forestalls conflict escalation at the end. Specifically, Wikipedia editors who write more receptive posts are less prone to receiving personal attacks from disagreeing editors (Study 3). We develop a \u201creceptiveness recipe\u201d intervention based on our algorithm. We find that writers who follow the recipe are seen as more desirable partners for future collaboration and their messages are seen as more persuasive (Study 4). Overall, we find that conversational receptiveness is reliably measurable, has meaningful relational consequences, and can be substantially improved using our intervention (183 words).","url":"https://www.semanticscholar.org/paper/ff3b0be4fb7debf1312c92381577292288755674","sentence":"Title: conversational receptiveness: improving engagement with opposing views Abstract: abstract we examine \u201cconversational receptiveness\u201d \u2013 the use of language to communicate one\u2019s willingness to thoughtfully engage with opposing views. we develop an interpretable machine-learning algorithm to identify the linguistic profile of receptiveness (studies 1a-b). we then show that in contentious policy discussions, government executives who were rated as more receptive - according to our algorithm and their partners, but not their own self-evaluations - were considered better teammates, advisors, and workplace representatives (study 2). furthermore, using field data from a setting where conflict management is endemic to productivity, we show that conversational receptiveness at the beginning of a conversation forestalls conflict escalation at the end. specifically, wikipedia editors who write more receptive posts are less prone to receiving personal attacks from disagreeing editors (study 3). we develop a \u201creceptiveness recipe\u201d intervention based on our algorithm. we find that writers who follow the recipe are seen as more desirable partners for future collaboration and their messages are seen as more persuasive (study 4). overall, we find that conversational receptiveness is reliably measurable, has meaningful relational consequences, and can be substantially improved using our intervention (183 words).","Task":["conflictiveness"],"Method":["receptiveness"]},{"title":"Empathy and Hope: Resource Transfer to Model Inter-country Social Media Dynamics","abstract":"The ongoing COVID-19 pandemic resulted in significant ramifications for international relations ranging from travel restrictions, global ceasefires, and international vaccine production and sharing agreements. Amidst a wave of infections in India that resulted in a systemic breakdown of healthcare infrastructure, a social welfare organization based in Pakistan offered to procure medical-grade oxygen to assist India - a nation which was involved in four wars with Pakistan in the past few decades. In this paper, we focus on Pakistani Twitter users\u2019 response to the ongoing healthcare crisis in India. While #IndiaNeedsOxygen and #PakistanStandsWithIndia featured among the top-trending hashtags in Pakistan, divisive hashtags such as #EndiaSaySorryToKashmir simultaneously started trending. Against the backdrop of a contentious history including four wars, divisive content of this nature, especially when a country is facing an unprecedented healthcare crisis, fuels further deterioration of relations. In this paper, we define a new task of detecting supportive content and demonstrate that existing NLP for social impact tools can be effectively harnessed for such tasks within a quick turnaround time. We also release the first publicly available data set at the intersection of geopolitical relations and a raging pandemic in the context of India and Pakistan.","url":"https://www.semanticscholar.org/paper/e511b338559a1df846059068ce7cc64c7066be4c","sentence":"Title: empathy and hope: resource transfer to model inter-country social media dynamics Abstract: the ongoing covid-19 pandemic resulted in significant ramifications for international relations ranging from travel restrictions, global ceasefires, and international vaccine production and sharing agreements. amidst a wave of infections in india that resulted in a systemic breakdown of healthcare infrastructure, a social welfare organization based in pakistan offered to procure medical-grade oxygen to assist india - a nation which was involved in four wars with pakistan in the past few decades. in this paper, we focus on pakistani twitter users\u2019 response to the ongoing healthcare crisis in india. while #indianeedsoxygen and #pakistanstandswithindia featured among the top-trending hashtags in pakistan, divisive hashtags such as #endiasaysorrytokashmir simultaneously started trending. against the backdrop of a contentious history including four wars, divisive content of this nature, especially when a country is facing an unprecedented healthcare crisis, fuels further deterioration of relations. in this paper, we define a new task of detecting supportive content and demonstrate that existing nlp for social impact tools can be effectively harnessed for such tasks within a quick turnaround time. we also release the first publicly available data set at the intersection of geopolitical relations and a raging pandemic in the context of india and pakistan.","Task":["social messaging"],"Method":["social engagement"]},{"title":"Guiding Principles for Participatory Design-inspired Natural Language Processing","abstract":"We introduce 9 guiding principles to integrate Participatory Design (PD) methods in the development of Natural Language Processing (NLP) systems. The adoption of PD methods by NLP will help to alleviate issues concerning the development of more democratic, fairer, less-biased technologies to process natural language data. This short paper is the outcome of an ongoing dialogue between designers and NLP experts and adopts a non-standard format following previous work by Traum (2000); Bender (2013); Abzianidze and Bos (2019). Every section is a guiding principle. While principles 1\u20133 illustrate assumptions and methods that inform community-based PD practices, we used two fictional design scenarios (Encinas and Blythe, 2018), which build on top of situations familiar to the authors, to elicit the identification of the other 6. Principles 4\u20136 describes the impact of PD methods on the design of NLP systems, targeting two critical aspects: data collection & annotation, and the deployment & evaluation. Finally, principles 7\u20139 guide a new reflexivity of the NLP research with respect to its context, actors and participants, and aims. We hope this guide will offer inspiration and a road-map to develop a new generation of PD-inspired NLP.","url":"https://www.semanticscholar.org/paper/022b80b663c51563a1c6772c12ada3c79f5d798d","sentence":"Title: guiding principles for participatory design-inspired natural language processing Abstract: we introduce 9 guiding principles to integrate participatory design (pd) methods in the development of natural language processing (nlp) systems. the adoption of pd methods by nlp will help to alleviate issues concerning the development of more democratic, fairer, less-biased technologies to process natural language data. this short paper is the outcome of an ongoing dialogue between designers and nlp experts and adopts a non-standard format following previous work by traum (2000); bender (2013); abzianidze and bos (2019). every section is a guiding principle. while principles 1\u20133 illustrate assumptions and methods that inform community-based pd practices, we used two fictional design scenarios (encinas and blythe, 2018), which build on top of situations familiar to the authors, to elicit the identification of the other 6. principles 4\u20136 describes the impact of pd methods on the design of nlp systems, targeting two critical aspects: data collection & annotation, and the deployment & evaluation. finally, principles 7\u20139 guide a new reflexivity of the nlp research with respect to its context, actors and participants, and aims. we hope this guide will offer inspiration and a road-map to develop a new generation of pd-inspired nlp.","Task":["data bias"],"Method":["a design"]},{"title":"Detecting Hashtag Hijacking for Hashtag Activism","abstract":"Social media has changed the way we engage in social activities. On Twitter, users can participate in social movements using hashtags such as #MeToo; this is known as hashtag activism. However, while these hashtags can help reshape social norms, they can also be used maliciously by spammers or troll communities for other purposes, such as signal boosting unrelated content, making a dent in a movement, or sharing hate speech. We present a Tweet-level hashtag hijacking detection framework focusing on hashtag activism. Our weakly-supervised framework uses bootstrapping to update itself as new Tweets are posted. Our experiments show that the system adapts to new topics in a social movement, as well as new hijacking strategies, maintaining strong performance over time.","url":"https://www.semanticscholar.org/paper/203bdaca3986b51f8d011422c04ff1489e425ce5","sentence":"Title: detecting hashtag hijacking for hashtag activism Abstract: social media has changed the way we engage in social activities. on twitter, users can participate in social movements using hashtags such as #metoo; this is known as hashtag activism. however, while these hashtags can help reshape social norms, they can also be used maliciously by spammers or troll communities for other purposes, such as signal boosting unrelated content, making a dent in a movement, or sharing hate speech. we present a tweet-level hashtag hijacking detection framework focusing on hashtag activism. our weakly-supervised framework uses bootstrapping to update itself as new tweets are posted. our experiments show that the system adapts to new topics in a social movement, as well as new hijacking strategies, maintaining strong performance over time.","Task":["hashtagacking"],"Method":["hashtag activism"]},{"title":"Dialogue Act Classification for Augmentative and Alternative Communication","abstract":"Augmentative and Alternative Communication (AAC) devices and applications are intended to make it easier for individuals with complex communication needs to participate in conversations. However, these devices have low adoption and retention rates. We review prior work with text recommendation systems that have not been successful in mitigating these problems. To address these gaps, we propose applying Dialogue Act classification to AAC conversations. We evaluated the performance of a state of the art model on a limited AAC dataset that was trained on both AAC and non-AAC datasets. The one trained on AAC (accuracy = 38.6%) achieved better performance than that trained on a non-AAC corpus (accuracy = 34.1%). These results reflect the need to incorporate representative datasets in later experiments. We discuss the need to collect more labeled AAC datasets and propose areas of future work.","url":"https://www.semanticscholar.org/paper/41ebff09aff17c37efdab8c1d7051cbf150970f8","sentence":"Title: dialogue act classification for augmentative and alternative communication Abstract: augmentative and alternative communication (aac) devices and applications are intended to make it easier for individuals with complex communication needs to participate in conversations. however, these devices have low adoption and retention rates. we review prior work with text recommendation systems that have not been successful in mitigating these problems. to address these gaps, we propose applying dialogue act classification to aac conversations. we evaluated the performance of a state of the art model on a limited aac dataset that was trained on both aac and non-aac datasets. the one trained on aac (accuracy = 38.6%) achieved better performance than that trained on a non-aac corpus (accuracy = 34.1%). these results reflect the need to incorporate representative datasets in later experiments. we discuss the need to collect more labeled aac datasets and propose areas of future work.","Task":["data classification"],"Method":["dialogue analysis"]},{"title":"Improving Policing with Natural Language Processing","abstract":"This article explores the potential for Natural Language Processing (NLP) to enable a more effective, prevention focused and less confrontational policing model that has hitherto been too resource consuming to implement at scale. Problem-Oriented Policing (POP) is a potential replacement, at least in part, for traditional policing which adopts a reactive approach, relying heavily on the criminal justice system. By contrast, POP seeks to prevent crime by manipulating the underlying conditions that allow crimes to be committed. Identifying these underlying conditions requires a detailed understanding of crime events - tacit knowledge that is often held by police officers but which can be challenging to derive from structured police data. One potential source of insight exists in unstructured free text data commonly collected by police for the purposes of investigation or administration. Yet police agencies do not typically have the skills or resources to analyse these data at scale. In this article we argue that NLP offers the potential to unlock these unstructured data and by doing so allow police to implement more POP initiatives. However we caution that using NLP models without adequate knowledge may either allow or perpetuate bias within the data potentially leading to unfavourable outcomes.","url":"https://www.semanticscholar.org/paper/d393f2a793930a6e38321340185756860f43c62c","sentence":"Title: improving policing with natural language processing Abstract: this article explores the potential for natural language processing (nlp) to enable a more effective, prevention focused and less confrontational policing model that has hitherto been too resource consuming to implement at scale. problem-oriented policing (pop) is a potential replacement, at least in part, for traditional policing which adopts a reactive approach, relying heavily on the criminal justice system. by contrast, pop seeks to prevent crime by manipulating the underlying conditions that allow crimes to be committed. identifying these underlying conditions requires a detailed understanding of crime events - tacit knowledge that is often held by police officers but which can be challenging to derive from structured police data. one potential source of insight exists in unstructured free text data commonly collected by police for the purposes of investigation or administration. yet police agencies do not typically have the skills or resources to analyse these data at scale. in this article we argue that nlp offers the potential to unlock these unstructured data and by doing so allow police to implement more pop initiatives. however we caution that using nlp models without adequate knowledge may either allow or perpetuate bias within the data potentially leading to unfavourable outcomes.","Task":["this bias"],"Method":["bias bias"]},{"title":"Methods for Detoxification of Texts for the Russian Language","abstract":"We introduce the first study of automatic detoxification of Russian texts to combat offensive language. Such a kind of textual style transfer can be used, for instance, for processing toxic content in social media. While much work has been done for the English language in this field, it has never been solved for the Russian language yet. We test two types of models \u2013 unsupervised approach based on BERT architecture that performs local corrections and supervised approach based on pretrained language GPT-2 model \u2013 and compare them with several baselines. In addition, we describe evaluation setup providing training datasets and metrics for automatic evaluation. The results show that the tested approaches can be successfully used for detoxification, although there is room for improvement.","url":"https://www.semanticscholar.org/paper/2c5b31a02133dea21cf94fde67c8948115441432","sentence":"Title: methods for detoxification of texts for the russian language Abstract: we introduce the first study of automatic detoxification of russian texts to combat offensive language. such a kind of textual style transfer can be used, for instance, for processing toxic content in social media. while much work has been done for the english language in this field, it has never been solved for the russian language yet. we test two types of models \u2013 unsupervised approach based on bert architecture that performs local corrections and supervised approach based on pretrained language gpt-2 model \u2013 and compare them with several baselines. in addition, we describe evaluation setup providing training datasets and metrics for automatic evaluation. the results show that the tested approaches can be successfully used for detoxification, although there is room for improvement.","Task":["languageification"],"Method":["automaticification"]},{"title":"How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact","abstract":"Recent years have seen many breakthroughs in natural language processing (NLP), transitioning it from a mostly theoretical field to one with many real-world applications. Noting the rising number of applications of other machine learning and AI techniques with pervasive societal impact, we anticipate the rising importance of developing NLP technologies for social good. Inspired by theories in moral philosophy and global priorities research, we aim to promote a guideline for social good in the context of NLP. We lay the foundations via the moral philosophy definition of social good, propose a framework to evaluate the direct and indirect real-world impact of NLP tasks, and adopt the methodology of global priorities research to identify priority causes for NLP research. Finally, we use our theoretical framework to provide some practical guidelines for future NLP research for social good.1","url":"https://www.semanticscholar.org/paper/5c7b8f26ae23c457d85da9d0f19b28e588e9b9a7","sentence":"Title: how good is nlp? a sober look at nlp tasks through the lens of social impact Abstract: recent years have seen many breakthroughs in natural language processing (nlp), transitioning it from a mostly theoretical field to one with many real-world applications. noting the rising number of applications of other machine learning and ai techniques with pervasive societal impact, we anticipate the rising importance of developing nlp technologies for social good. inspired by theories in moral philosophy and global priorities research, we aim to promote a guideline for social good in the context of nlp. we lay the foundations via the moral philosophy definition of social good, propose a framework to evaluate the direct and indirect real-world impact of nlp tasks, and adopt the methodology of global priorities research to identify priority causes for nlp research. finally, we use our theoretical framework to provide some practical guidelines for future nlp research for social good.1","Task":["social impact"],"Method":["social good"]},{"title":"NLP for Consumer Protection: Battling Illegal Clauses in German Terms and Conditions in Online Shopping","abstract":"Online shopping is an ever more important part of the global consumer economy, not just in times of a pandemic. When we place an order online as consumers, we regularly agree to the so-called \u201cTerms and Conditions\u201d (T&C), a contract unilaterally drafted by the seller. Often, consumers do not read these contracts and unwittingly agree to unfavourable and often void terms. Government and non-government organisations (NGOs) for consumer protection battle such terms on behalf of consumers, who often hesitate to take on legal actions themselves. However, the growing number of online shops and a lack of funding makes it increasingly difficult for such organisations to monitor the market effectively. This paper describes how Natural Language Processing (NLP) can be applied to support consumer advocates in their efforts to protect consumers. Together with two NGOs from Germany, we developed an NLP-based application that legally assesses clauses in T&C from German online shops under the European Union\u2019s (EU) jurisdiction. We report that we could achieve an accuracy of 0.9 in the detection of void clauses by fine-tuning a pre-trained German BERT model. The approach is currently used by two NGOs and has already helped to challenge void clauses in T&C.","url":"https://www.semanticscholar.org/paper/9e1616dcabf4d04d14d642fcb7963c461cf13d41","sentence":"Title: nlp for consumer protection: battling illegal clauses in german terms and conditions in online shopping Abstract: online shopping is an ever more important part of the global consumer economy, not just in times of a pandemic. when we place an order online as consumers, we regularly agree to the so-called \u201cterms and conditions\u201d (t&c), a contract unilaterally drafted by the seller. often, consumers do not read these contracts and unwittingly agree to unfavourable and often void terms. government and non-government organisations (ngos) for consumer protection battle such terms on behalf of consumers, who often hesitate to take on legal actions themselves. however, the growing number of online shops and a lack of funding makes it increasingly difficult for such organisations to monitor the market effectively. this paper describes how natural language processing (nlp) can be applied to support consumer advocates in their efforts to protect consumers. together with two ngos from germany, we developed an nlp-based application that legally assesses clauses in t&c from german online shops under the european union\u2019s (eu) jurisdiction. we report that we could achieve an accuracy of 0.9 in the detection of void clauses by fine-tuning a pre-trained german bert model. the approach is currently used by two ngos and has already helped to challenge void clauses in t&c.","Task":["illegal clauses"],"Method":["legal clauses"]},{"title":"Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech","abstract":"Tackling online hatred using informed textual responses \u2013 called counter narratives \u2013 has been brought under the spotlight recently. Accordingly, a research line has emerged to automatically generate counter narratives in order to facilitate the direct intervention in the hate discussion and to prevent hate content from further spreading. Still, current neural approaches tend to produce generic/repetitive responses and lack grounded and up-to-date evidence such as facts, statistics, or examples. Moreover, these models can create plausible but not necessarily true arguments. In this paper we present the first complete knowledgebound counter narrative generation pipeline, grounded in an external knowledge repository that can provide more informative content to fight online hatred. Together with our approach, we present a series of experiments that show its feasibility to produce suitable and informative counter narratives in in-domain and cross-domain settings.","url":"https://www.semanticscholar.org/paper/473b70bb3c531b2d740fa6d652956e2733b53243","sentence":"Title: towards knowledge-grounded counter narrative generation for hate speech Abstract: tackling online hatred using informed textual responses \u2013 called counter narratives \u2013 has been brought under the spotlight recently. accordingly, a research line has emerged to automatically generate counter narratives in order to facilitate the direct intervention in the hate discussion and to prevent hate content from further spreading. still, current neural approaches tend to produce generic/repetitive responses and lack grounded and up-to-date evidence such as facts, statistics, or examples. moreover, these models can create plausible but not necessarily true arguments. in this paper we present the first complete knowledgebound counter narrative generation pipeline, grounded in an external knowledge repository that can provide more informative content to fight online hatred. together with our approach, we present a series of experiments that show its feasibility to produce suitable and informative counter narratives in in-domain and cross-domain settings.","Task":["counter narratives"],"Method":["counter narratives"]},{"title":"Use of Formal Ethical Reviews in NLP Literature: Historical Trends and Current Practices","abstract":"Ethical aspects of research in language technologies have received much attention recently. It is a standard practice to get a study involving human subjects reviewed and approved by a professional ethics committee/board of the institution. How commonly do we see mention of ethical approvals in NLP research? What types of research or aspects of studies are usually subject to such reviews? With the rising concerns and discourse around the ethics of NLP, do we also observe a rise in formal ethical reviews of NLP studies? And, if so, would this imply that there is a heightened awareness of ethical issues that was previously lacking? We aim to address these questions by conducting a detailed quantitative and qualitative analysis of the ACL Anthology, as well as comparing the trends in our field to those of other related disciplines, such as cognitive science, machine learning, data mining, and systems.","url":"https://www.semanticscholar.org/paper/f29d5cb8f405903fc8af7a5d7ab4bf7d65796e95","sentence":"Title: use of formal ethical reviews in nlp literature: historical trends and current practices Abstract: ethical aspects of research in language technologies have received much attention recently. it is a standard practice to get a study involving human subjects reviewed and approved by a professional ethics committee/board of the institution. how commonly do we see mention of ethical approvals in nlp research? what types of research or aspects of studies are usually subject to such reviews? with the rising concerns and discourse around the ethics of nlp, do we also observe a rise in formal ethical reviews of nlp studies? and, if so, would this imply that there is a heightened awareness of ethical issues that was previously lacking? we aim to address these questions by conducting a detailed quantitative and qualitative analysis of the acl anthology, as well as comparing the trends in our field to those of other related disciplines, such as cognitive science, machine learning, data mining, and systems.","Task":["ethical review"],"Method":["ethical review"]},{"title":"Using Word Embeddings to Analyze Teacher Evaluations: An Application to a Filipino Education Non-Profit Organization","abstract":"Analysis of teacher evaluations is crucial to the development of robust educational programs, particularly through the validation of desirable qualities being reflected on in the text. This research applies Natural Language Processing techniques on a real-world dataset from a Filipino education non-profit to explore insights from analyzing evaluations written by Teacher Fellows who assess their own progress. Prior to this research, only qualitative assessment had been conducted on the text. Inspired by the use of word embedding similarities to capture semantic alignment, we utilize GloVe embeddings to determine to what extent these evaluations reflect concepts critical to measuring the competency of Teacher Fellows and upholding the organization\u2019s Vision and Mission. As Fellows\u2019 quantitative ratings improved, so too did their demonstration of competency in the text. Further, Teacher Fellow language was consistent with the organization\u2019s Vision and Mission. This research therefore showcases the possibilities of NLP in education, improving our understanding of Teacher Fellow evaluations, which can lead to advances in program operations and education efforts.","url":"https://www.semanticscholar.org/paper/23a812dde149021bab611ca4395279b9d41031f1","sentence":"Title: using word embeddings to analyze teacher evaluations: an application to a filipino education non-profit organization Abstract: analysis of teacher evaluations is crucial to the development of robust educational programs, particularly through the validation of desirable qualities being reflected on in the text. this research applies natural language processing techniques on a real-world dataset from a filipino education non-profit to explore insights from analyzing evaluations written by teacher fellows who assess their own progress. prior to this research, only qualitative assessment had been conducted on the text. inspired by the use of word embedding similarities to capture semantic alignment, we utilize glove embeddings to determine to what extent these evaluations reflect concepts critical to measuring the competency of teacher fellows and upholding the organization\u2019s vision and mission. as fellows\u2019 quantitative ratings improved, so too did their demonstration of competency in the text. further, teacher fellow language was consistent with the organization\u2019s vision and mission. this research therefore showcases the possibilities of nlp in education, improving our understanding of teacher fellow evaluations, which can lead to advances in program operations and education efforts.","Task":["n evaluation"],"Method":["n language"]},{"title":"Theano: A Greek-speaking conversational agent for COVID-19","abstract":"Conversational Agents (CAs) can be a proxy for disseminating information and providing support to the public, especially in times of crisis. CAs can scale to reach larger numbers of end-users than human operators, while they can offer information interactively and engagingly. In this work, we present Theano, a Greek-speaking virtual assistant for COVID-19. Theano presents users with COVID-19 statistics and facts and informs users about the best health practices as well as the latest COVID-19 related guidelines. Additionally, Theano provides support to end-users by helping them self-assess their symptoms and redirecting them to first-line health workers. The relevant, localized information that Theano provides, makes it a valuable tool for combating COVID-19 in Greece. Theano has already conversed with different users in more than 170 different conversations through a web interface as a chatbot and over the phone as a voice bot.","url":"https://www.semanticscholar.org/paper/d4eb2ca9694f34d63abe6d27bd2d958992431017","sentence":"Title: theano: a greek-speaking conversational agent for covid-19 Abstract: conversational agents (cas) can be a proxy for disseminating information and providing support to the public, especially in times of crisis. cas can scale to reach larger numbers of end-users than human operators, while they can offer information interactively and engagingly. in this work, we present theano, a greek-speaking virtual assistant for covid-19. theano presents users with covid-19 statistics and facts and informs users about the best health practices as well as the latest covid-19 related guidelines. additionally, theano provides support to end-users by helping them self-assess their symptoms and redirecting them to first-line health workers. the relevant, localized information that theano provides, makes it a valuable tool for combating covid-19 in greece. theano has already conversed with different users in more than 170 different conversations through a web interface as a chatbot and over the phone as a voice bot.","Task":["this communication"],"Method":["virtual assistant"]},{"title":"Restatement and Question Generation for Counsellor Chatbot","abstract":"Amidst rising mental health needs in society, virtual agents are increasingly deployed in counselling. In order to give pertinent advice, counsellors must first gain an understanding of the issues at hand by eliciting sharing from the counsellee. It is thus important for the counsellor chatbot to encourage the user to open up and talk. One way to sustain the conversation flow is to acknowledge the counsellee\u2019s key points by restating them, or probing them further with questions. This paper applies models from two closely related NLP tasks \u2014 summarization and question generation \u2014 to restatement and question generation in the counselling context. We conducted experiments on a manually annotated dataset of Cantonese post-reply pairs on topics related to loneliness, academic anxiety and test anxiety. We obtained the best performance in both restatement and question generation by fine-tuning BertSum, a state-of-the-art summarization model, with the in-domain manual dataset augmented with a large-scale, automatically mined open-domain dataset.","url":"https://www.semanticscholar.org/paper/100e0f3dcd319266b2772f0841dad388b45cce3f","sentence":"Title: restatement and question generation for counsellor chatbot Abstract: amidst rising mental health needs in society, virtual agents are increasingly deployed in counselling. in order to give pertinent advice, counsellors must first gain an understanding of the issues at hand by eliciting sharing from the counsellee. it is thus important for the counsellor chatbot to encourage the user to open up and talk. one way to sustain the conversation flow is to acknowledge the counsellee\u2019s key points by restating them, or probing them further with questions. this paper applies models from two closely related nlp tasks \u2014 summarization and question generation \u2014 to restatement and question generation in the counselling context. we conducted experiments on a manually annotated dataset of cantonese post-reply pairs on topics related to loneliness, academic anxiety and test anxiety. we obtained the best performance in both restatement and question generation by fine-tuning bertsum, a state-of-the-art summarization model, with the in-domain manual dataset augmented with a large-scale, automatically mined open-domain dataset.","Task":["question generation"],"Method":["question generation"]}]'),c=JSON.parse('[{"title":"UPSTAGE: Unsupervised Context Augmentation for Utterance Classification in Patient-Provider Communication","abstract":"Conversations between patients and providers in clinical settings provide a source of natural language data that may reflect and correlate with the patients\u2019 experience and response to the treatment they are receiving. When analyzing utterances in such conversations, it is not sufficient to consider each sentence in isolation, since its context may play a role in determining its semantic meaning. Recently, contextual information in natural language documents has been modeled using various techniques, such as recurrent neural networks with latent variables, or neural networks with attention mechanisms. In this paper, we present UnsuPerviSed conText AuGmEntation (Upstage), a classification framework that relies on both local and global contextual information from different sources. Upstage uses transformer models with pretrained language models and joint sentence representation to solve the task of classifying health topics in patient-provider conversations. In addition, Upstage leverages unlabeled corpora for pretraining and data augmentation to provide additional context, which leads to improved classification performance.","url":"https://www.semanticscholar.org/paper/e21ab639b0308a71aeac643a12f59a32603d091d","sentence":"Title: upstage: unsupervised context augmentation for utterance classification in patient-provider communication Abstract: conversations between patients and providers in clinical settings provide a source of natural language data that may reflect and correlate with the patients\u2019 experience and response to the treatment they are receiving. when analyzing utterances in such conversations, it is not sufficient to consider each sentence in isolation, since its context may play a role in determining its semantic meaning. recently, contextual information in natural language documents has been modeled using various techniques, such as recurrent neural networks with latent variables, or neural networks with attention mechanisms. in this paper, we present unsupervised context augmentation (upstage), a classification framework that relies on both local and global contextual information from different sources. upstage uses transformer models with pretrained language models and joint sentence representation to solve the task of classifying health topics in patient-provider conversations. in addition, upstage leverages unlabeled corpora for pretraining and data augmentation to provide additional context, which leads to improved classification performance.","Task":["up learning"],"Method":["upstage"]},{"title":"A Review of Challenges and Opportunities in Machine Learning for Health.","abstract":"Modern electronic health records (EHRs) provide data to answer clinically meaningful questions. The growing data in EHRs makes healthcare ripe for the use of machine learning. However, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. For example, diseases in EHRs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. This article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.","url":"https://www.semanticscholar.org/paper/d77719ba4be4196c9a918ce8dff0edf5aa3c04e7","sentence":"Title: a review of challenges and opportunities in machine learning for health. Abstract: modern electronic health records (ehrs) provide data to answer clinically meaningful questions. the growing data in ehrs makes healthcare ripe for the use of machine learning. however, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. for example, diseases in ehrs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. this article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.","Task":["machine learning"],"Method":["machine learning"]},{"title":"Ethical Machine Learning in Health Care","abstract":"The use of machine learning (ML) in healthcare raises numerous ethical concerns, especially as models can amplify existing health inequities. Here, we outline ethical considerations for equitable ML in the advancement of healthcare. Specifically, we frame ethics of ML in healthcare through the lens of social justice. We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health, ranging from problem selection to postdeployment considerations. We close by summarizing recommendations to address these challenges.","url":"https://www.semanticscholar.org/paper/e8d330f11df9c69f38b78a7cc4b1333ebecf7c55","sentence":"Title: ethical machine learning in health care Abstract: the use of machine learning (ml) in healthcare raises numerous ethical concerns, especially as models can amplify existing health inequities. here, we outline ethical considerations for equitable ml in the advancement of healthcare. specifically, we frame ethics of ml in healthcare through the lens of social justice. we describe ongoing efforts and outline challenges in a proposed pipeline of ethical ml in health, ranging from problem selection to postdeployment considerations. we close by summarizing recommendations to address these challenges.","Task":["machine learning"],"Method":["ethical ml"]},{"title":"Intimate Partner Violence and Injury Prediction From Radiology Reports","abstract":"Intimate partner violence (IPV) is an urgent, prevalent, and under-detected public health issue. We present machine learning models to assess patients for IPV and injury. We train the predictive algorithms on radiology reports with 1) IPV labels based on entry to a violence prevention program and 2) injury labels provided by emergency radiology fellowship-trained physicians. Our dataset includes 34,642 radiology reports and 1479 patients of IPV victims and control patients. Our best model predicts IPV a median of 3.08 years before violence prevention program entry with a sensitivity of 64% and a specificity of 95%. We conduct error analysis to determine for which patients our model has especially high or low performance and discuss next steps for a deployed clinical risk model.","url":"https://www.semanticscholar.org/paper/4fd75d18b231f73e9c878cb6029900e4b0e327bb","sentence":"Title: intimate partner violence and injury prediction from radiology reports Abstract: intimate partner violence (ipv) is an urgent, prevalent, and under-detected public health issue. we present machine learning models to assess patients for ipv and injury. we train the predictive algorithms on radiology reports with 1) ipv labels based on entry to a violence prevention program and 2) injury labels provided by emergency radiology fellowship-trained physicians. our dataset includes 34,642 radiology reports and 1479 patients of ipv victims and control patients. our best model predicts ipv a median of 3.08 years before violence prevention program entry with a sensitivity of 64% and a specificity of 95%. we conduct error analysis to determine for which patients our model has especially high or low performance and discuss next steps for a deployed clinical risk model.","Task":["machine learning"],"Method":["ipv"]},{"title":"De-identification of patient notes with recurrent neural networks","abstract":"Objective\\nPatient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information that needs to be removed to de-identify patient notes. Manual de-identification is impractical given the size of electronic health record databases, the limited number of researchers with access to non-de-identified notes, and the frequent mistakes of human annotators. A reliable automated de-identification system would consequently be of high value.\\n\\n\\nMaterials and Methods\\nWe introduce the first de-identification system based on artificial neural networks (ANNs), which requires no handcrafted features or rules, unlike existing systems. We compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the MIMIC de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.\\n\\n\\nResults\\nOur ANN model outperforms the state-of-the-art systems. It yields an F1-score of 97.85 on the i2b2 2014 dataset, with a recall of 97.38 and a precision of 98.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with a recall of 99.25 and a precision of 99.21.\\n\\n\\nConclusion\\nOur findings support the use of ANNs for de-identification of patient notes, as they show better performance than previously published systems while requiring no manual feature engineering.","url":"https://www.semanticscholar.org/paper/e7bf950be4cf8309d0df68d18fa09e77f5b2511a","sentence":"Title: de-identification of patient notes with recurrent neural networks Abstract: objective\\npatient notes in electronic health records (ehrs) may contain critical information for medical investigations. however, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. in the united states, the health insurance portability and accountability act (hipaa) defines 18 types of protected health information that needs to be removed to de-identify patient notes. manual de-identification is impractical given the size of electronic health record databases, the limited number of researchers with access to non-de-identified notes, and the frequent mistakes of human annotators. a reliable automated de-identification system would consequently be of high value.\\n\\n\\nmaterials and methods\\nwe introduce the first de-identification system based on artificial neural networks (anns), which requires no handcrafted features or rules, unlike existing systems. we compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the mimic de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.\\n\\n\\nresults\\nour ann model outperforms the state-of-the-art systems. it yields an f1-score of 97.85 on the i2b2 2014 dataset, with a recall of 97.38 and a precision of 98.32, and an f1-score of 99.23 on the mimic de-identification dataset, with a recall of 99.25 and a precision of 99.21.\\n\\n\\nconclusion\\nour findings support the use of anns for de-identification of patient notes, as they show better performance than previously published systems while requiring no manual feature engineering.","Task":["neural networks"],"Method":["anns"]},{"title":"Segment convolutional neural networks (Seg-CNNs) for classifying relations in clinical notes","abstract":"We propose Segment Convolutional Neural Networks (Seg-CNNs) for classifying relations from clinical notes. Seg-CNNs use only word-embedding features without manual feature engineering. Unlike typical CNN models, relations between 2 concepts are identified by simultaneously learning separate representations for text segments in a sentence: preceding, concept1, middle, concept2, and succeeding. We evaluate Seg-CNN on the i2b2/VA relation classification challenge dataset. We show that Seg-CNN achieves a state-of-the-art micro-average F-measure of 0.742 for overall evaluation, 0.686 for classifying medical problem-treatment relations, 0.820 for medical problem-test relations, and 0.702 for medical problem-medical problem relations. We demonstrate the benefits of learning segment-level representations. We show that medical domain word embeddings help improve relation classification. Seg-CNNs can be trained quickly for the i2b2/VA dataset on a graphics processing unit (GPU) platform. These results support the use of CNNs computed over segments of text for classifying medical relations, as they show state-of-the-art performance while requiring no manual feature engineering.","url":"https://www.semanticscholar.org/paper/defeaddec92b53fc2e9787cc3652e3d8be812193","sentence":"Title: segment convolutional neural networks (seg-cnns) for classifying relations in clinical notes Abstract: we propose segment convolutional neural networks (seg-cnns) for classifying relations from clinical notes. seg-cnns use only word-embedding features without manual feature engineering. unlike typical cnn models, relations between 2 concepts are identified by simultaneously learning separate representations for text segments in a sentence: preceding, concept1, middle, concept2, and succeeding. we evaluate seg-cnn on the i2b2/va relation classification challenge dataset. we show that seg-cnn achieves a state-of-the-art micro-average f-measure of 0.742 for overall evaluation, 0.686 for classifying medical problem-treatment relations, 0.820 for medical problem-test relations, and 0.702 for medical problem-medical problem relations. we demonstrate the benefits of learning segment-level representations. we show that medical domain word embeddings help improve relation classification. seg-cnns can be trained quickly for the i2b2/va dataset on a graphics processing unit (gpu) platform. these results support the use of cnns computed over segments of text for classifying medical relations, as they show state-of-the-art performance while requiring no manual feature engineering.","Task":["neuralNN"],"Method":["clinicalNN"]},{"title":"Fast, Structured Clinical Documentation via Contextual Autocomplete","abstract":"We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. By constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. Furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. To our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","url":"https://www.semanticscholar.org/paper/4554bc45be85cca73e3f94220c8cb056cf6ebf74","sentence":"Title: fast, structured clinical documentation via contextual autocomplete Abstract: we present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. we dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. by constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. to our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","Task":["machine learning"],"Method":["clinical topic"]},{"title":"CORD-19: The COVID-19 Open Research Dataset","abstract":"The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.","url":"https://www.semanticscholar.org/paper/4a10dffca6dcce9c570cb75aa4d76522c34a2fd4","sentence":"Title: cord-19: the covid-19 open research dataset Abstract: the covid-19 open research dataset (cord-19) is a growing resource of scientific papers on covid-19 and related historical coronavirus research. cord-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. since its release, cord-19 has been downloaded over 200k times and has served as the basis of many covid-19 text mining and discovery systems. in this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how cord-19 has been used, and describe several shared tasks built around the dataset. we hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for covid-19.","Task":["open research"],"Method":["the dataset"]},{"title":"Can AI Help Reduce Disparities in General Medical and Mental Health Care?","abstract":"Background\\nAs machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems\' data, algorithms, and recommendations. Simply put, as health care improves for some, it might not improve for all.\\n\\n\\nMethods\\nTwo case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (ICU) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status.\\n\\n\\nResults\\nClinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. Differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for ICU mortality and with respect to insurance policy for psychiatric 30-day readmission.\\n\\n\\nConclusions\\nThis analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.","url":"https://www.semanticscholar.org/paper/3f8d4444cd124e21d4fa47b514c8267a3d5d9649","sentence":"Title: can ai help reduce disparities in general medical and mental health care? Abstract: background\\nas machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems\' data, algorithms, and recommendations. simply put, as health care improves for some, it might not improve for all.\\n\\n\\nmethods\\ntwo case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (icu) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status.\\n\\n\\nresults\\nclinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for icu mortality and with respect to insurance policy for psychiatric 30-day readmission.\\n\\n\\nconclusions\\nthis analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.","Task":["machine learning"],"Method":["machine learning"]},{"title":"The Ivory Tower Lost: How College Students Respond Differently than the General Public to the COVID-19 Pandemic","abstract":"In the United States, the country with the highest confirmed COVID-19 infection cases, a nationwide social distancing protocol has been implemented by the President. Following the closure of the University of Washington on March 7th, more than 1000 colleges and universities in the United States have cancelled in-person classes and campus activities, impacting millions of students. This paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people\'s opinions on social media. We discover several topics embedded in a large number of COVID-19 tweets that represent the most central issues related to the pandemic, which are of great concerns for both college students and the general public. Moreover, we find significant differences between these two groups of Twitter users with respect to the sentiments they expressed towards the COVID-19 issues. To our best knowledge, this is the first social media-based study which focuses on the college student community\'s demographics and responses to prevalent social issues during a major crisis.","url":"https://www.semanticscholar.org/paper/2d40f76055a7c28dd30c8b8f60c31724f8991cd8","sentence":"Title: the ivory tower lost: how college students respond differently than the general public to the covid-19 pandemic Abstract: in the united states, the country with the highest confirmed covid-19 infection cases, a nationwide social distancing protocol has been implemented by the president. following the closure of the university of washington on march 7th, more than 1000 colleges and universities in the united states have cancelled in-person classes and campus activities, impacting millions of students. this paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people\'s opinions on social media. we discover several topics embedded in a large number of covid-19 tweets that represent the most central issues related to the pandemic, which are of great concerns for both college students and the general public. moreover, we find significant differences between these two groups of twitter users with respect to the sentiments they expressed towards the covid-19 issues. to our best knowledge, this is the first social media-based study which focuses on the college student community\'s demographics and responses to prevalent social issues during a major crisis.","Task":["social media"],"Method":["social topic"]},{"title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","sentence":"Title: humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks Abstract: social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. despite its significantly large volume, social media content is often too noisy for direct use in any application. therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. to address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. however, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. in this paper, we present a new large-scale dataset with \u223c77k human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. we report multiclass classification results using classic and deep learning (fasttext and transformer) based models to set the ground for future studies. the dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","Task":["deep learning"],"Method":["humaid"]},{"title":"CrisisMMD: Multimodal Twitter Datasets from Natural Disasters","abstract":"During natural and man-made disasters, people use social media platforms such as Twitter to post textual and multime- dia content to report updates about injured or dead people, infrastructure damage, and missing or found people among other information types. Studies have revealed that this on- line information, if processed timely and effectively, is ex- tremely useful for humanitarian organizations to gain situational awareness and plan relief operations. In addition to the analysis of textual content, recent studies have shown that imagery content on social media can boost disaster response significantly. Despite extensive research that mainly focuses on textual content to extract useful information, limited work has focused on the use of imagery content or the combination of both content types. One of the reasons is the lack of labeled imagery data in this domain. Therefore, in this paper, we aim to tackle this limitation by releasing a large multi-modal dataset collected from Twitter during different natural disasters. We provide three types of annotations, which are useful to address a number of crisis response and management tasks for different humanitarian organizations.","url":"https://www.semanticscholar.org/paper/2a29f7cc10fc18ce6e031e0cf6fc307a6b85d8e9","sentence":"Title: crisismmd: multimodal twitter datasets from natural disasters Abstract: during natural and man-made disasters, people use social media platforms such as twitter to post textual and multime- dia content to report updates about injured or dead people, infrastructure damage, and missing or found people among other information types. studies have revealed that this on- line information, if processed timely and effectively, is ex- tremely useful for humanitarian organizations to gain situational awareness and plan relief operations. in addition to the analysis of textual content, recent studies have shown that imagery content on social media can boost disaster response significantly. despite extensive research that mainly focuses on textual content to extract useful information, limited work has focused on the use of imagery content or the combination of both content types. one of the reasons is the lack of labeled imagery data in this domain. therefore, in this paper, we aim to tackle this limitation by releasing a large multi-modal dataset collected from twitter during different natural disasters. we provide three types of annotations, which are useful to address a number of crisis response and management tasks for different humanitarian organizations.","Task":["imagery analysis"],"Method":["thisism"]},{"title":"Domain Adaptation with Adversarial Training and Graph Embeddings","abstract":"The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.","url":"https://www.semanticscholar.org/paper/561ede166947a8bedb8be9acff182913156e06c6","sentence":"Title: domain adaptation with adversarial training and graph embeddings Abstract: the success of deep neural networks (dnns) is heavily dependent on the availability of labeled data. however, obtaining labeled data is a big challenge in many real-world problems. in such scenarios, a dnn model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. in this paper, we study the problem of classifying social media posts during a crisis event (e.g., earthquake). for that, we use labeled and unlabeled data from past similar events (e.g., flood) and unlabeled data for the current event. we propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. our experiments with two real-world crisis datasets collected from twitter demonstrate significant improvements over several baselines.","Task":["deep learning"],"Method":["domain adaptation"]},{"title":"IBC-C : A Dataset for Armed Conflict Event Analysis","abstract":"We describe the Iraq Body Count Corpus (IBC-C) dataset, the first substantial armed conflict-related dataset which can be used for conflict analysis. IBC-C provides a ground-truth dataset for conflict specific named entity recognition, slot filling, and event de-duplication. IBC-C is constructed using data collected by the Iraq Body Count project which has been recording casualties resulting from the ongoing war in Iraq since 2003. We describe the dataset\u2019s creation, how it can be used for the above three tasks and provide initial baseline results for the first task (named entity recognition) using Hidden Markov Models, Conditional Random Fields, and Recursive Neural Networks.","url":"https://www.semanticscholar.org/paper/579914dac924d6b4a17889ea7b0dd744a62bbd74","sentence":"Title: ibc-c : a dataset for armed conflict event analysis Abstract: we describe the iraq body count corpus (ibc-c) dataset, the first substantial armed conflict-related dataset which can be used for conflict analysis. ibc-c provides a ground-truth dataset for conflict specific named entity recognition, slot filling, and event de-duplication. ibc-c is constructed using data collected by the iraq body count project which has been recording casualties resulting from the ongoing war in iraq since 2003. we describe the dataset\u2019s creation, how it can be used for the above three tasks and provide initial baseline results for the first task (named entity recognition) using hidden markov models, conditional random fields, and recursive neural networks.","Task":["conflict analysis"],"Method":["conflict analysis"]},{"title":"Text as Data for Conflict Research: A Literature Survey","abstract":"Computer-aided text analysis (CATA) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. The chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. This includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). Particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. Both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. Finally, cross-validation is highlighted as a crucial step in CATA, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","url":"https://www.semanticscholar.org/paper/3004f78d84752f3ffe8e54fa265b2ee85dac9728","sentence":"Title: text as data for conflict research: a literature survey Abstract: computer-aided text analysis (cata) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. the chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. this includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. finally, cross-validation is highlighted as a crucial step in cata, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","Task":["cata"],"Method":["cata"]},{"title":"One-to-X Analogical Reasoning on Word Embeddings: a Case for Diachronic Armed Conflict Prediction from News Texts","abstract":"We extend the well-known word analogy task to a one-to-X formulation, including one-to-none cases, when no correct answer exists. The task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type \u2018location:armed-group\u2019 based on data about past events. As the source of semantic information, we use diachronic word embedding models trained on English news texts. A simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. Finally, we publish a ready-to-use test set for one-to-X analogy evaluation on historical armed conflicts data.","url":"https://www.semanticscholar.org/paper/42c63d952f0cce7f89738e818fc9ab4e723bf7b6","sentence":"Title: one-to-x analogical reasoning on word embeddings: a case for diachronic armed conflict prediction from news texts Abstract: we extend the well-known word analogy task to a one-to-x formulation, including one-to-none cases, when no correct answer exists. the task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type \u2018location:armed-group\u2019 based on data about past events. as the source of semantic information, we use diachronic word embedding models trained on english news texts. a simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. finally, we publish a ready-to-use test set for one-to-x analogy evaluation on historical armed conflicts data.","Task":["semanticonflict"],"Method":["this topic"]},{"title":"Using Natural Language Processing for Automatic Detection of Plagiarism","abstract":"Current plagiarism detection tools are mostly limited to comparisons of suspicious plagiarised texts and potential original texts at string level. In this study the aim is to improve the accuracy of plagiarism detection by incorporating Natural Language Processing (NLP) techniques into existing approaches. We propose a framework for external plagiarism detection in which a number of NLP techniques are applied to process a set of suspicious and original documents, not only to analyse strings but also the structure of the text, using resources to account for text relations. Initial results obtained with a corpus of plagiarised short paragraphs have showed that NLP techniques improve the accuracy of existing approaches.","url":"https://www.semanticscholar.org/paper/636d4c0b0fe6919abe6eb546907d28ed39bf56e6","sentence":"Title: using natural language processing for automatic detection of plagiarism Abstract: current plagiarism detection tools are mostly limited to comparisons of suspicious plagiarised texts and potential original texts at string level. in this study the aim is to improve the accuracy of plagiarism detection by incorporating natural language processing (nlp) techniques into existing approaches. we propose a framework for external plagiarism detection in which a number of nlp techniques are applied to process a set of suspicious and original documents, not only to analyse strings but also the structure of the text, using resources to account for text relations. initial results obtained with a corpus of plagiarised short paragraphs have showed that nlp techniques improve the accuracy of existing approaches.","Task":["plagiarism"],"Method":["plagiarism"]},{"title":"A Neural Approach to Automated Essay Scoring","abstract":"Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted Kappa, without requiring any feature engineering.","url":"https://www.semanticscholar.org/paper/aea14f23a951975f605a981d003386e46bf8acfe","sentence":"Title: a neural approach to automated essay scoring Abstract: traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. the performance of such systems is tightly bound to the quality of the underlying features. however, it is laborious to manually design the most informative features for such a system. in this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. we explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. the results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted kappa, without requiring any feature engineering.","Task":["neuralearning"],"Method":["this learning"]},{"title":"Automated Scoring: Beyond Natural Language Processing","abstract":"In this position paper, we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy. Automated scoring systems warrant significant cross-discipline collaboration of which natural language processing and machine learning are just two of many important components. Such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other. Our position is that it is essential for us as NLP researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful.","url":"https://www.semanticscholar.org/paper/0eb5872733e643f43a0c1a7ff78953dfea74dfea","sentence":"Title: automated scoring: beyond natural language processing Abstract: in this position paper, we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy. automated scoring systems warrant significant cross-discipline collaboration of which natural language processing and machine learning are just two of many important components. such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other. our position is that it is essential for us as nlp researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful.","Task":["automated learning"],"Method":["automated topic"]},{"title":"Event Data on Armed Conflict and Security: New Perspectives, Old Challenges, and Some Solutions","abstract":"This article presents the Event Data on Conflict and Security (EDACS) dataset, discusses the inherent problems of georeferenced conflict data, and shows how these challenges are met within EDACS. Based on an event data approach, EDACS contributes to the growing number of novel georeferenced datasets that allow researchers to identify causal pathways of violence and the dynamics of (transboundary) violence through spatiotemporal disaggregation. However, the unreflected use of any of these datasets will give researchers unjustified confidence in their findings, as the pitfalls are many and propagating errors can result in misleading conclusions. To identify and handle the different challenges to overall event data quality, we argue in favor of transparency in the data collection and coding process, to empower analysts to challenge the data and avoid cascading errors. In particular, we investigate how the choice of news sources, the handling of geographic precision, and the use of auxiliary data can bias event data. We demonstrate how the EDACS dataset design enables the analyst to deal with these issues by providing a set of variables indicating the news sources, possible sources of bias, and detailed information on geographic precision. This allows for a flexible use of the data based on individual analytical requirements.","url":"https://www.semanticscholar.org/paper/0cdeb238355617a640471987af36d8e09cd905c0","sentence":"Title: event data on armed conflict and security: new perspectives, old challenges, and some solutions Abstract: this article presents the event data on conflict and security (edacs) dataset, discusses the inherent problems of georeferenced conflict data, and shows how these challenges are met within edacs. based on an event data approach, edacs contributes to the growing number of novel georeferenced datasets that allow researchers to identify causal pathways of violence and the dynamics of (transboundary) violence through spatiotemporal disaggregation. however, the unreflected use of any of these datasets will give researchers unjustified confidence in their findings, as the pitfalls are many and propagating errors can result in misleading conclusions. to identify and handle the different challenges to overall event data quality, we argue in favor of transparency in the data collection and coding process, to empower analysts to challenge the data and avoid cascading errors. in particular, we investigate how the choice of news sources, the handling of geographic precision, and the use of auxiliary data can bias event data. we demonstrate how the edacs dataset design enables the analyst to deal with these issues by providing a set of variables indicating the news sources, possible sources of bias, and detailed information on geographic precision. this allows for a flexible use of the data based on individual analytical requirements.","Task":["event data"],"Method":["edata"]},{"title":"Tracing armed conflicts with diachronic word embedding models","abstract":"Recent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. In this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the Gigaword news corpus as the training data. The results show that much work still remains in extracting \u2018cultural\u2019 semantic shifts from diachronic word embedding models. At the same time, we present a new task complete with an evaluation set and introduce the \u2018anchor words\u2019 method which outperforms previous approaches on this set.","url":"https://www.semanticscholar.org/paper/2ca0403eabc3893fed255fc119a927d83a1af739","sentence":"Title: tracing armed conflicts with diachronic word embedding models Abstract: recent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. in this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the gigaword news corpus as the training data. the results show that much work still remains in extracting \u2018cultural\u2019 semantic shifts from diachronic word embedding models. at the same time, we present a new task complete with an evaluation set and introduce the \u2018anchor words\u2019 method which outperforms previous approaches on this set.","Task":["semantic research"],"Method":["this research"]},{"title":"Enriching textbooks through data mining","abstract":"Textbooks play an important role in any educational system. Unfortunately, many textbooks produced in developing countries are not written well and they often lack adequate coverage of important concepts. We propose a technological solution to address this problem based on enriching textbooks with authoritative web content. We augment textbooks at the section level for key concepts discussed in the section. We use ideas from data mining for identifying the concepts that need augmentation as well as to determine the links to the authoritative content that should be used for augmentation. Our evaluation, employing textbooks from India, shows that we are able to enrich textbooks on different subjects and across different grades with high quality augmentations using automated techniques.","url":"https://www.semanticscholar.org/paper/4ed75f35ae7343cd906f311390f572a58e36805c","sentence":"Title: enriching textbooks through data mining Abstract: textbooks play an important role in any educational system. unfortunately, many textbooks produced in developing countries are not written well and they often lack adequate coverage of important concepts. we propose a technological solution to address this problem based on enriching textbooks with authoritative web content. we augment textbooks at the section level for key concepts discussed in the section. we use ideas from data mining for identifying the concepts that need augmentation as well as to determine the links to the authoritative content that should be used for augmentation. our evaluation, employing textbooks from india, shows that we are able to enrich textbooks on different subjects and across different grades with high quality augmentations using automated techniques.","Task":["data mining"],"Method":["augmented textbooks"]},{"title":"Educational Question Answering Motivated by Question-Specific Concept Maps","abstract":"Question answering (QA) is the automated process of answering general questions submitted by humans in natural language. QA has previously been explored within the educational context to facilitate learning, however the majority of works have focused on text-based answering. As an alternative, this paper proposes an approach to return answers as a concept map, which further encourages meaningful learning and knowledge organisation. Additionally, this paper investigates whether adapting the returned concept map to the specific question context provides further learning benefit. A randomised experiment was conducted with a sample of 59 Computer Science undergraduates, obtaining statistically significant results on learning gain when students are provided with the question-specific concept maps. Further, time spent on studying the concept maps were positively correlated with the learning gain.","url":"https://www.semanticscholar.org/paper/bc6a4304afdf59783638a6a0d9fc2c2acb6e5b67","sentence":"Title: educational question answering motivated by question-specific concept maps Abstract: question answering (qa) is the automated process of answering general questions submitted by humans in natural language. qa has previously been explored within the educational context to facilitate learning, however the majority of works have focused on text-based answering. as an alternative, this paper proposes an approach to return answers as a concept map, which further encourages meaningful learning and knowledge organisation. additionally, this paper investigates whether adapting the returned concept map to the specific question context provides further learning benefit. a randomised experiment was conducted with a sample of 59 computer science undergraduates, obtaining statistically significant results on learning gain when students are provided with the question-specific concept maps. further, time spent on studying the concept maps were positively correlated with the learning gain.","Task":["question mapping"],"Method":["qa"]},{"title":"Characterizing Stage-aware Writing Assistance for Collaborative Document Authoring","abstract":"Writing is a complex non-linear process that begins with a mental model of intent, and progresses through an outline of ideas, to words on paper (and their subsequent refinement). Despite past research in understanding writing, Web-scale consumer and enterprise collaborative digital writing environments are yet to greatly benefit from intelligent systems that understand the stages of document evolution, providing opportune assistance based on authors\' situated actions and context. In this paper, we present three studies that explore temporal stages of document authoring. We first survey information workers at a large technology company about their writing habits and preferences, concluding that writers do in fact conceptually progress through several distinct phases while authoring documents. We also explore, qualitatively, how writing stages are linked to document lifespan. We supplement these qualitative findings with an analysis of the longitudinal user interaction logs of a popular digital writing platform over several million documents. Finally, as a first step towards facilitating an intelligent digital writing assistant, we conduct a preliminary investigation into the utility of user interaction log data for predicting the temporal stage of a document. Our results support the benefit of tools tailored to writing stages, identify primary tasks associated with these stages, and show that it is possible to predict stages from anonymous interaction logs. Together, these results argue for the benefit and feasibility of more tailored digital writing assistance.","url":"https://www.semanticscholar.org/paper/911e61212eb26325d3fe58f454f36f4e70c54c7a","sentence":"Title: characterizing stage-aware writing assistance for collaborative document authoring Abstract: writing is a complex non-linear process that begins with a mental model of intent, and progresses through an outline of ideas, to words on paper (and their subsequent refinement). despite past research in understanding writing, web-scale consumer and enterprise collaborative digital writing environments are yet to greatly benefit from intelligent systems that understand the stages of document evolution, providing opportune assistance based on authors\' situated actions and context. in this paper, we present three studies that explore temporal stages of document authoring. we first survey information workers at a large technology company about their writing habits and preferences, concluding that writers do in fact conceptually progress through several distinct phases while authoring documents. we also explore, qualitatively, how writing stages are linked to document lifespan. we supplement these qualitative findings with an analysis of the longitudinal user interaction logs of a popular digital writing platform over several million documents. finally, as a first step towards facilitating an intelligent digital writing assistant, we conduct a preliminary investigation into the utility of user interaction log data for predicting the temporal stage of a document. our results support the benefit of tools tailored to writing stages, identify primary tasks associated with these stages, and show that it is possible to predict stages from anonymous interaction logs. together, these results argue for the benefit and feasibility of more tailored digital writing assistance.","Task":["digital processing"],"Method":["writing stages"]},{"title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.","url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","sentence":"Title: question answering system on education acts using nlp techniques Abstract: question answering (qa) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. it presents only the requested information instead of searching full documents like search engine. as information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. this is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain qa system for handling documents related to education acts sections to retrieve more precise answers using nlp techniques.","Task":["question retrieval"],"Method":["educationa"]},{"title":"Natural Language Processing and Language Learning","abstract":"As a relatively young field of research and development started by work on cryptanalysis and machine translation around 50 years ago, Natural Language Processing (NLP) is concerned with the automated processing of human language. It addresses the analysis and generation of written and spoken language, though speech processing is often regarded as a separate subfield. NLP emphasizes processing and applications and as such can be seen as the applied side of Computational Linguistics, the interdisciplinary field of research concerned with formal analysis and modeling of language and its applications at the intersection of Linguistics, Computer Science, and Psychology. In terms of the language aspects dealt with in NLP, traditionally lexical, morphological and syntactic aspects of language were at the center of attention, but aspects of meaning, discourse, and the relation to the extra-linguistic context have become increasingly prominent in the last decade. A good introduction and overview of the field is provided in Jurafsky & Martin (2009).","url":"https://www.semanticscholar.org/paper/f8868be52c8c0d64920e5f67870c01ba48601608","sentence":"Title: natural language processing and language learning Abstract: as a relatively young field of research and development started by work on cryptanalysis and machine translation around 50 years ago, natural language processing (nlp) is concerned with the automated processing of human language. it addresses the analysis and generation of written and spoken language, though speech processing is often regarded as a separate subfield. nlp emphasizes processing and applications and as such can be seen as the applied side of computational linguistics, the interdisciplinary field of research concerned with formal analysis and modeling of language and its applications at the intersection of linguistics, computer science, and psychology. in terms of the language aspects dealt with in nlp, traditionally lexical, morphological and syntactic aspects of language were at the center of attention, but aspects of meaning, discourse, and the relation to the extra-linguistic context have become increasingly prominent in the last decade. a good introduction and overview of the field is provided in jurafsky & martin (2009).","Task":["language learning"],"Method":["nlp"]},{"title":"Modeling the Relationship between User Comments and Edits in Document Revision","abstract":"Management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a document\u2019s evolution. Reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. A number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. Thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: Comment Ranking and Edit Anchoring. We begin by collecting a dataset with more than half a million comment-edit pairs based on Wikipedia revision histories. We then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. Our architecture tackles both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. In a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. We are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for Comment Ranking, while we achieve 74.4% accuracy on Edit Anchoring.","url":"https://www.semanticscholar.org/paper/311381feeb6346bfcb2ba622bd8f713261a4075d","sentence":"Title: modeling the relationship between user comments and edits in document revision Abstract: management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a document\u2019s evolution. reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. a number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: comment ranking and edit anchoring. we begin by collecting a dataset with more than half a million comment-edit pairs based on wikipedia revision histories. we then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. our architecture tackles both comment ranking and edit anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. in a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. we are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for comment ranking, while we achieve 74.4% accuracy on edit anchoring.","Task":["semantic learning"],"Method":["this topic"]},{"title":"A Multimodal Human-Computer Interaction System and Its Application in Smart Learning Environments","abstract":"A multimodal human-computer interaction system is composed of the comprehensive usage of various input and output channels. For the information input, apart from the traditional keyboard typing, mouse clicking, screen touching, the latest speech and face recognition technology can be used. For the output, the traditional screen display, the latest speech and facial expression synthesis and gesture generation can be used. After literature review of related works, this paper at first presents such a system, MMISE (Multimodal Interaction System for Education), about its architecture and working mechanism, POOOIIM (Pedagogical Objective Oriented Output, Input and Implementation Mechanism) illustrated with practical examples. Then this paper introduces this system\u2019s pilot applications in the epidemic time of novel coronavirus in 2020.","url":"https://www.semanticscholar.org/paper/d114af5d3dcb3792bd9faec1476feba5aaf3617f","sentence":"Title: a multimodal human-computer interaction system and its application in smart learning environments Abstract: a multimodal human-computer interaction system is composed of the comprehensive usage of various input and output channels. for the information input, apart from the traditional keyboard typing, mouse clicking, screen touching, the latest speech and face recognition technology can be used. for the output, the traditional screen display, the latest speech and facial expression synthesis and gesture generation can be used. after literature review of related works, this paper at first presents such a system, mmise (multimodal interaction system for education), about its architecture and working mechanism, poooiim (pedagogical objective oriented output, input and implementation mechanism) illustrated with practical examples. then this paper introduces this system\u2019s pilot applications in the epidemic time of novel coronavirus in 2020.","Task":["face recognition"],"Method":["mmise"]},{"title":"What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations","abstract":"The quality of a counseling intervention relies highly on the active collaboration between clients and counselors. In this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. Specifically, we address the differences between high-quality and low-quality counseling. Our approach examines participants\u2019 turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. Our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.","url":"https://www.semanticscholar.org/paper/9b0c9d241269b98c80f65a14d5d65263d0688d70","sentence":"Title: what makes a good counselor? learning to distinguish between high-quality and low-quality counseling conversations Abstract: the quality of a counseling intervention relies highly on the active collaboration between clients and counselors. in this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. specifically, we address the differences between high-quality and low-quality counseling. our approach examines participants\u2019 turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. these features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.","Task":["language analysis"],"Method":["counseling quality"]},{"title":"Quantifying the Effects of COVID-19 on Mental Health Support Forums","abstract":"The COVID-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. Understanding its impact can inform strategies for mitigating negative consequences. In this work, we seek to better understand the effects of COVID-19 on mental health by examining discussions within mental health support communities on Reddit. First, we quantify the rate at which COVID-19 is discussed in each community, or subreddit, in order to understand levels of preoccupation with the pandemic. Next, we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen. Finally, we analyze how COVID-19 has influenced language use and topics of discussion within each subreddit.","url":"https://www.semanticscholar.org/paper/aa2bbf5a1485cc90401e4a35c04463fcb4e632f0","sentence":"Title: quantifying the effects of covid-19 on mental health support forums Abstract: the covid-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. understanding its impact can inform strategies for mitigating negative consequences. in this work, we seek to better understand the effects of covid-19 on mental health by examining discussions within mental health support communities on reddit. first, we quantify the rate at which covid-19 is discussed in each community, or subreddit, in order to understand levels of preoccupation with the pandemic. next, we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen. finally, we analyze how covid-19 has influenced language use and topics of discussion within each subreddit.","Task":["social health"],"Method":["this topic"]},{"title":"Data Mining and Student e-Learning Profiles","abstract":"Data mining techniques have been applied to educational research in various ways. In this paper, I presented the application of sequential data mining algorithms to analyze computer logs to profile learners in terms of their learning tactic use and motivation in a web-based learning environment (gStudy). The data mining algorithms are employed to discover patterns which characterize learners either across session or groups based on their study tactic choice and goal orientation. The use of this method is illustrated through a sequential pattern analysis of gStudy log files generated by university students.","url":"https://www.semanticscholar.org/paper/81d13af7eb78725492854bb9860b57af2b2a1778","sentence":"Title: data mining and student e-learning profiles Abstract: data mining techniques have been applied to educational research in various ways. in this paper, i presented the application of sequential data mining algorithms to analyze computer logs to profile learners in terms of their learning tactic use and motivation in a web-based learning environment (gstudy). the data mining algorithms are employed to discover patterns which characterize learners either across session or groups based on their study tactic choice and goal orientation. the use of this method is illustrated through a sequential pattern analysis of gstudy log files generated by university students.","Task":["data mining"],"Method":["data mining"]},{"title":"Inferring Social Media Users\u2019 Mental Health Status from Multimodal Information","abstract":"Worldwide, an increasing number of people are suffering from mental health disorders such as depression and anxiety. In the United States alone, one in every four adults suffers from a mental health condition, which makes mental health a pressing concern. In this paper, we explore the use of multimodal cues present in social media posts to predict users\u2019 mental health status. Specifically, we focus on identifying social media activity that either indicates a mental health condition or its onset. We collect posts from Flickr and apply a multimodal approach that consists of jointly analyzing language, visual, and metadata cues and their relation to mental health. We conduct several classification experiments aiming to discriminate between (1) healthy users and users affected by a mental health illness; and (2) healthy users and users prone to mental illness. Our experimental results indicate that using multiple modalities can improve the performance of this classification task as compared to the use of one modality at a time, and can provide important cues into a user\u2019s mental status.","url":"https://www.semanticscholar.org/paper/065332576fd9b264a5a338549239bb39e6733819","sentence":"Title: inferring social media users\u2019 mental health status from multimodal information Abstract: worldwide, an increasing number of people are suffering from mental health disorders such as depression and anxiety. in the united states alone, one in every four adults suffers from a mental health condition, which makes mental health a pressing concern. in this paper, we explore the use of multimodal cues present in social media posts to predict users\u2019 mental health status. specifically, we focus on identifying social media activity that either indicates a mental health condition or its onset. we collect posts from flickr and apply a multimodal approach that consists of jointly analyzing language, visual, and metadata cues and their relation to mental health. we conduct several classification experiments aiming to discriminate between (1) healthy users and users affected by a mental health illness; and (2) healthy users and users prone to mental illness. our experimental results indicate that using multiple modalities can improve the performance of this classification task as compared to the use of one modality at a time, and can provide important cues into a user\u2019s mental status.","Task":["social health"],"Method":["mental health"]},{"title":"Expressive Interviewing: A Conversational System for Coping with COVID-19","abstract":"The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce \\\\textit{Expressive Interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system\'s design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues.","url":"https://www.semanticscholar.org/paper/c278eb333c78313a3762b5b2f07ace7443582b94","sentence":"Title: expressive interviewing: a conversational system for coping with covid-19 Abstract: the ongoing covid-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. we introduce \\\\textit{expressive interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. expressive interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how covid-19 has impacted their lives. we present relevant aspects of the system\'s design and implementation as well as quantitative and qualitative analyses of user interactions with the system. in addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with covid-19 issues.","Task":["expressive interviewing"],"Method":["expressive interviewing"]},{"title":"Understanding and Predicting Empathic Behavior in Counseling Therapy","abstract":"Counselor empathy is associated with better outcomes in psychology and behavioral counseling. In this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. Particularly, we analyze aspects such as participants\u2019 engagement, participants\u2019 verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. We also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80%.","url":"https://www.semanticscholar.org/paper/35112c6bfa715b2bc86fc5b6c331f8a1a0b07add","sentence":"Title: understanding and predicting empathic behavior in counseling therapy Abstract: counselor empathy is associated with better outcomes in psychology and behavioral counseling. in this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. particularly, we analyze aspects such as participants\u2019 engagement, participants\u2019 verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. we also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80%.","Task":["counselor empathy"],"Method":["counselor empathy"]},{"title":"Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health","abstract":"Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.","url":"https://www.semanticscholar.org/paper/c151f144c2c0e8d3b176edaf2ce5369c7707bd31","sentence":"Title: large-scale analysis of counseling conversations: an application of natural language processing to mental health Abstract: mental illness is one of the most pressing public health issues of our time. while counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. in this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. we develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.","Task":["counseling analysis"],"Method":["counseling conversations"]},{"title":"Fermi at SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media using Sentence Embeddings","abstract":"This paper describes our system (Fermi) for Task 6: OffensEval: Identifying and Categorizing Offensive Language in Social Media of SemEval-2019. We participated in all the three sub-tasks within Task 6. We evaluate multiple sentence embeddings in conjunction with various supervised machine learning algorithms and evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team Fermi\u2019s model achieved an F1-score of 64.40%, 62.00% and 62.60% for sub-task A, B and C respectively on the official leaderboard. Our model for sub-task C which uses pre-trained ELMo embeddings for transforming the input and uses SVM (RBF kernel) for training, scored third position on the official leaderboard. Through the paper we provide a detailed description of the approach, as well as the results obtained for the task.","url":"https://www.semanticscholar.org/paper/9de154d3c886177380062be7c8d50304a335752f","sentence":"Title: fermi at semeval-2019 task 6: identifying and categorizing offensive language in social media using sentence embeddings Abstract: this paper describes our system (fermi) for task 6: offenseval: identifying and categorizing offensive language in social media of semeval-2019. we participated in all the three sub-tasks within task 6. we evaluate multiple sentence embeddings in conjunction with various supervised machine learning algorithms and evaluate the performance of simple yet effective embedding-ml combination algorithms. our team fermi\u2019s model achieved an f1-score of 64.40%, 62.00% and 62.60% for sub-task a, b and c respectively on the official leaderboard. our model for sub-task c which uses pre-trained elmo embeddings for transforming the input and uses svm (rbf kernel) for training, scored third position on the official leaderboard. through the paper we provide a detailed description of the approach, as well as the results obtained for the task.","Task":["sem learning"],"Method":["offenseval"]},{"title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","sentence":"Title: building a motivational interviewing dataset Abstract: this paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during motivational interviewing encounters. annotations were conducted using the motivational interviewing integrity treatment (miti). we describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. the dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. the reliability analysis showed that annotators achieved excellent agreement at session level, with intraclass correlation coefficient (icc) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with cohen\u2019s kappa scores ranging from 0.31 to 0.64. behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (resnicow et al., 2002). in particular, motivational interviewing (mi), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (moyers et al., 2009; apodaca et al., 2014; barnett et al., 2014; catley et al., 2012). despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing mi counseling at larger scale or in other domains is limited by the need for human-based evaluations. currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. recently, computational approaches have been proposed to aid the mi evaluation process (atkins et al., 2014; xiao et al., 2014; klonek et al., 2015). however, learning resources for this task are not readily available. having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of mi. moreover, this can also be useful to explore how mi works by relating mi behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their mi skills. in this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the motivational interviewing treatment integrity 4.0 (miti), which is the current gold standard for mi-based psychology interventions. the dataset is derived from 277 mi sessions containing a total of 22,719 coded utterances. 1 motivational interviewing miller and rollnick define mi as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (miller and rollnick, 2013). mi has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Task":["behavioral interviewing"],"Method":["motivational interviewing"]},{"title":"Predicting Counselor Behaviors in Motivational Interviewing Encounters","abstract":"As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","url":"https://www.semanticscholar.org/paper/55925bc2522a1d4cb007ac78273f7c2efafa3916","sentence":"Title: predicting counselor behaviors in motivational interviewing encounters Abstract: as the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. in this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. in particular, we present a model towards the automation of motivational interviewing (mi) coding, which is the current gold standard to evaluate mi counseling. first, we build a dataset of hand labeled mi encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. we introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","Task":["behavioral interviewing"],"Method":["mi evaluation"]},{"title":"Happiness Entailment: Automating Suggestions for Well-Being","abstract":"Understanding what makes people happy is a central topic in psychology. Prior work has mostly focused on developing self-reporting assessment tools for individuals and relies on experts to analyze the periodic reported assessments. One of the goals of the analysis is to understand what actions are necessary to encourage modifications in the behaviors of the individuals to improve their overall well-being. In this paper, we outline a complementary approach; on the assumption that the user journals her happy moments as short texts, a system can analyze these texts and propose sustainable suggestions for the user that may lead to an overall improvement in her well-being. We prototype one necessary component of such a system, the Happiness Entailment Recognition (HER)module, which takes as input a short text describing an event, a candidate suggestion, and outputs a determination about whether the suggestion is more likely to be good for this user based on the event described. This component is implemented as a neural network model with two encoders, one for the user input and one for the candidate actionable suggestion, with additional layers to capture psychologically significant features in the happy moment and suggestion. Our model achieves an AU-ROC of 0.831 and outperforms our baseline as well as the current state-of-the-art Textual Entailment model from AllenNLP by more than 48% of improvements, confirming the uniqueness and complexity of the HER task.","url":"https://www.semanticscholar.org/paper/c2fd13dab7a581f2f875826b204d063f42b63a8b","sentence":"Title: happiness entailment: automating suggestions for well-being Abstract: understanding what makes people happy is a central topic in psychology. prior work has mostly focused on developing self-reporting assessment tools for individuals and relies on experts to analyze the periodic reported assessments. one of the goals of the analysis is to understand what actions are necessary to encourage modifications in the behaviors of the individuals to improve their overall well-being. in this paper, we outline a complementary approach; on the assumption that the user journals her happy moments as short texts, a system can analyze these texts and propose sustainable suggestions for the user that may lead to an overall improvement in her well-being. we prototype one necessary component of such a system, the happiness entailment recognition (her)module, which takes as input a short text describing an event, a candidate suggestion, and outputs a determination about whether the suggestion is more likely to be good for this user based on the event described. this component is implemented as a neural network model with two encoders, one for the user input and one for the candidate actionable suggestion, with additional layers to capture psychologically significant features in the happy moment and suggestion. our model achieves an au-roc of 0.831 and outperforms our baseline as well as the current state-of-the-art textual entailment model from allennlp by more than 48% of improvements, confirming the uniqueness and complexity of the her task.","Task":["neuralearning"],"Method":["this topic"]},{"title":"FERMI at SemEval-2019 Task 5: Using Sentence embeddings to Identify Hate Speech Against Immigrants and Women in Twitter","abstract":"This paper describes our system (Fermi) for Task 5 of SemEval-2019: HatEval: Multilingual Detection of Hate Speech Against Immigrants and Women on Twitter. We participated in the subtask A for English and ranked first in the evaluation on the test set. We evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team - Fermi\u2019s model achieved an accuracy of 65.00% for English language in task A. Our models, which use pretrained Universal Encoder sentence embeddings for transforming the input and SVM (with RBF kernel) for classification, scored first position (among 68) in the leaderboard on the test set for Subtask A in English language. In this paper we provide a detailed description of the approach, as well as the results obtained in the task.","url":"https://www.semanticscholar.org/paper/64c68fa52491c4a815f21917e068e4c19dd404b0","sentence":"Title: fermi at semeval-2019 task 5: using sentence embeddings to identify hate speech against immigrants and women in twitter Abstract: this paper describes our system (fermi) for task 5 of semeval-2019: hateval: multilingual detection of hate speech against immigrants and women on twitter. we participated in the subtask a for english and ranked first in the evaluation on the test set. we evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ml combination algorithms. our team - fermi\u2019s model achieved an accuracy of 65.00% for english language in task a. our models, which use pretrained universal encoder sentence embeddings for transforming the input and svm (with rbf kernel) for classification, scored first position (among 68) in the leaderboard on the test set for subtask a in english language. in this paper we provide a detailed description of the approach, as well as the results obtained in the task.","Task":["sem learning"],"Method":["thisval"]},{"title":"Ingredients for Happiness: Modeling constructs via semi-supervised content driven inductive transfer","abstract":"Modeling affect via understanding the social constructs behind them is an important task in devising robust and accurate systems for socially relevant scenarios. In the CL-Aff Shared Task (part of Affective Content Analysis workshop @ AAAI 2019), the organizers released a dataset of \u2018happy\u2019 moments, called the HappyDB corpus. The task is to detect two social constructs: the agency (i.e., whether the author is in control of the happy moment) and the social characteristics (i.e., whether anyone else other than the author was also involved in the happy moment). We employ an inductive transfer learning technique where we utilize a pre-trained language model and fine-tune it on the target task for both the binary classification tasks. At first, we use a language model pre-trained on the huge WikiText-103 corpus. This step utilizes an AWDLSTM with three hidden layers for training the language model. In the second step, we fine-tune the pre-trained language model on both the labeled and unlabeled instances from the HappyDB dataset. Finally, we train a classifier on top of the language model for each of the identification tasks. Our experiments using 10-fold cross validation on the corpus show that we achieve a high accuracy of \u223c93% for detection of the social characteristic and \u223c87% for agency of the author, showing significant gains over other baselines. We also show that using the unlabeled dataset for fine-tuning the language model in the second step improves our accuracy by 1-2% across detection of both the constructs.","url":"https://www.semanticscholar.org/paper/7cb5bc9f8bc023f1f2c94e2e83012f1d35e4e01d","sentence":"Title: ingredients for happiness: modeling constructs via semi-supervised content driven inductive transfer Abstract: modeling affect via understanding the social constructs behind them is an important task in devising robust and accurate systems for socially relevant scenarios. in the cl-aff shared task (part of affective content analysis workshop @ aaai 2019), the organizers released a dataset of \u2018happy\u2019 moments, called the happydb corpus. the task is to detect two social constructs: the agency (i.e., whether the author is in control of the happy moment) and the social characteristics (i.e., whether anyone else other than the author was also involved in the happy moment). we employ an inductive transfer learning technique where we utilize a pre-trained language model and fine-tune it on the target task for both the binary classification tasks. at first, we use a language model pre-trained on the huge wikitext-103 corpus. this step utilizes an awdlstm with three hidden layers for training the language model. in the second step, we fine-tune the pre-trained language model on both the labeled and unlabeled instances from the happydb dataset. finally, we train a classifier on top of the language model for each of the identification tasks. our experiments using 10-fold cross validation on the corpus show that we achieve a high accuracy of \u223c93% for detection of the social characteristic and \u223c87% for agency of the author, showing significant gains over other baselines. we also show that using the unlabeled dataset for fine-tuning the language model in the second step improves our accuracy by 1-2% across detection of both the constructs.","Task":["socialearning"],"Method":["this topic"]},{"title":"HappyDB: A Corpus of 100, 000 Crowdsourced Happy Moments","abstract":"The science of happiness is an area of positive psychology concerned with understanding what behaviors make people happy in a sustainable fashion. Recently, there has been interest in developing technologies that help incorporate the findings of the science of happiness into users\' daily lives by steering them towards behaviors that increase happiness. With the goal of building technology that can understand how people express their happy moments in text, we crowd-sourced HappyDB, a corpus of 100,000 happy moments that we make publicly available. This paper describes HappyDB and its properties, and outlines several important NLP problems that can be studied with the help of the corpus. We also apply several state-of-the-art analysis techniques to analyze HappyDB. Our results demonstrate the need for deeper NLP techniques to be developed which makes HappyDB an exciting resource for follow-on research.","url":"https://www.semanticscholar.org/paper/c239aaa2a87ea63fae666b2051f1923afaa45fc2","sentence":"Title: happydb: a corpus of 100, 000 crowdsourced happy moments Abstract: the science of happiness is an area of positive psychology concerned with understanding what behaviors make people happy in a sustainable fashion. recently, there has been interest in developing technologies that help incorporate the findings of the science of happiness into users\' daily lives by steering them towards behaviors that increase happiness. with the goal of building technology that can understand how people express their happy moments in text, we crowd-sourced happydb, a corpus of 100,000 happy moments that we make publicly available. this paper describes happydb and its properties, and outlines several important nlp problems that can be studied with the help of the corpus. we also apply several state-of-the-art analysis techniques to analyze happydb. our results demonstrate the need for deeper nlp techniques to be developed which makes happydb an exciting resource for follow-on research.","Task":["happydb"],"Method":["happydb"]},{"title":"HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","sentence":"Title: hopeedi: a multilingual hope speech detection dataset for equality, diversity, and inclusion Abstract: over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. however, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. until now, most studies have focused on solving this problem of negativity in the english language, though the problem is much more than just harmful content. furthermore, it is multilingual as well. thus, we have constructed a hope speech dataset for equality, diversity and inclusion (hopeedi) containing user-generated comments from the social media platform youtube with 28,451, 20,198 and 10,705 comments in english, tamil and malayalam, respectively, manually labelled as containing hope speech or not. to our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. we determined that the inter-annotator agreement of our dataset using krippendorff\u2019s alpha. further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and f1-score. the dataset is publicly available for the research community. we hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","Task":["hope analysis"],"Method":["hope topic"]},{"title":"Women worry about family, men about the economy: Gender differences in emotional responses to COVID-19","abstract":"Among the critical challenges around the COVID-19 pandemic is dealing with the potentially detrimental effects on people\'s mental health. Designing appropriate interventions and identifying the concerns of those most at risk requires methods that can extract worries, concerns and emotional responses from text data. We examine gender differences and the effect of document length on worries about the ongoing COVID-19 situation. Our findings suggest that i) short texts do not offer as adequate insights into psychological processes as longer texts. We further find ii) marked gender differences in topics concerning emotional responses. Women worried more about their loved ones and severe health concerns while men were more occupied with effects on the economy and society. This paper adds to the understanding of general gender differences in language found elsewhere, and shows that the current unique circumstances likely amplified these effects. We close this paper with a call for more high-quality datasets due to the limitations of Tweet-sized data.","url":"https://www.semanticscholar.org/paper/32bf7ad3fdda71036b48f7dc85cad407674277b7","sentence":"Title: women worry about family, men about the economy: gender differences in emotional responses to covid-19 Abstract: among the critical challenges around the covid-19 pandemic is dealing with the potentially detrimental effects on people\'s mental health. designing appropriate interventions and identifying the concerns of those most at risk requires methods that can extract worries, concerns and emotional responses from text data. we examine gender differences and the effect of document length on worries about the ongoing covid-19 situation. our findings suggest that i) short texts do not offer as adequate insights into psychological processes as longer texts. we further find ii) marked gender differences in topics concerning emotional responses. women worried more about their loved ones and severe health concerns while men were more occupied with effects on the economy and society. this paper adds to the understanding of general gender differences in language found elsewhere, and shows that the current unique circumstances likely amplified these effects. we close this paper with a call for more high-quality datasets due to the limitations of tweet-sized data.","Task":["mental health"],"Method":["text health"]},{"title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","sentence":"Title: the climate change debate and natural language processing Abstract: the debate around climate change (cc)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. yet, in the natural language processing (nlp) community, this domain has so far received little attention. in contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of cc-related text. other research is qualitative in nature and studies details, nuances, actors, and motivations within cc discourses. coming from both nlp and political science, and reviewing key works in both disciplines, we discuss how social science approaches to cc debates can inform advances in text-mining/nlp, and how, in return, nlp can support policy-makers and activists in making sense of large-scale and complex cc discourses across multiple genres, channels, topics, and communities. this is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","Task":["climate change"],"Method":["this topic"]},{"title":"Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism","abstract":"Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised BERT-based models.","url":"https://www.semanticscholar.org/paper/a9d09338a3a0d9102c1e623e6ad434a446c0bbfe","sentence":"Title: automatic classification of neutralization techniques in the narrative of climate change scepticism Abstract: neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. we first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised bert-based models.","Task":["socialLP"],"Method":["nLP"]},{"title":"CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims","abstract":"We introduce CLIMATE-FEVER, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of FEVER [1], the largest dataset of artificially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \\\\textsc{fever} framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and AI community.","url":"https://www.semanticscholar.org/paper/086e6733e5fda70bbce0c7545bd06d5634918a60","sentence":"Title: climate-fever: a dataset for verification of real-world climate claims Abstract: we introduce climate-fever, a new publicly available dataset for verification of climate change-related claims. by providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. we adapt the methodology of fever [1], the largest dataset of artificially designed claims, to real-life claims collected from the internet. while during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. we discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \\\\textsc{fever} framework, which we believe provides a valuable challenge for general natural language understanding. we hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and ai community.","Task":["climate science"],"Method":["climate topic"]},{"title":"Cheap Talk and Cherry-Picking: What ClimateBert has to say on Corporate Climate Risk Disclosures","abstract":"Disclosure of climate-related financial risks greatly helps investors assess companies\' preparedness for climate change. Voluntary disclosures such as those based on the recommendations of the Task Force for Climate-related Financial Disclosures (TCFD) are being hailed as an effective measure for better climate risk management. We ask whether this expectation is justified. We do so with the help of a deep neural language model, which we christen ClimateBert. We train ClimateBert on thousands of sentences related to climate-risk disclosures aligned with the TCFD recommendations. In analyzing the disclosures of TCFD-supporting firms, ClimateBert comes to the sobering conclusion that the firms\' TCFD support is mostly cheap talk and that firms cherry-pick to report primarily non-material climate risk information. From our analysis, we conclude that the only way out of this dilemma is to turn voluntary reporting into regulatory disclosures.","url":"https://www.semanticscholar.org/paper/52ef2bea83eba1575fcfd02c82b4228f7aab0bd6","sentence":"Title: cheap talk and cherry-picking: what climatebert has to say on corporate climate risk disclosures Abstract: disclosure of climate-related financial risks greatly helps investors assess companies\' preparedness for climate change. voluntary disclosures such as those based on the recommendations of the task force for climate-related financial disclosures (tcfd) are being hailed as an effective measure for better climate risk management. we ask whether this expectation is justified. we do so with the help of a deep neural language model, which we christen climatebert. we train climatebert on thousands of sentences related to climate-risk disclosures aligned with the tcfd recommendations. in analyzing the disclosures of tcfd-supporting firms, climatebert comes to the sobering conclusion that the firms\' tcfd support is mostly cheap talk and that firms cherry-pick to report primarily non-material climate risk information. from our analysis, we conclude that the only way out of this dilemma is to turn voluntary reporting into regulatory disclosures.","Task":["climatebert"],"Method":["climatebert"]},{"title":"Tackling Climate Change with Machine Learning","abstract":"Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.","url":"https://www.semanticscholar.org/paper/998039a4876edc440e0cabb0bc42239b0eb29644","sentence":"Title: tackling climate change with machine learning Abstract: climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. from smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. our recommendations encompass exciting research questions as well as promising business opportunities. we call on the machine learning community to join the global effort against climate change.","Task":["machine learning"],"Method":["climate change"]},{"title":"Learning Twitter User Sentiments on Climate Change with Limited Labeled Data","abstract":"While it is well-documented that climate change accepters and deniers have become increasingly polarized in the United States over time, there has been no large-scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences. On the sub-population of Twitter users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the U.S. in 2018. We begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic-level results in line with prior research. We then apply RNNs to conduct a cohort-level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change. However, this effect does not hold for the 2018 blizzard and wildfires studied, implying that Twitter users\' opinions on climate change are fairly ingrained on this subset of natural disasters.","url":"https://www.semanticscholar.org/paper/f071e7c19fc18fc4d4ab69ce13f0c4e92cfb08c4","sentence":"Title: learning twitter user sentiments on climate change with limited labeled data Abstract: while it is well-documented that climate change accepters and deniers have become increasingly polarized in the united states over time, there has been no large-scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences. on the sub-population of twitter users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the u.s. in 2018. we begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic-level results in line with prior research. we then apply rnns to conduct a cohort-level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change. however, this effect does not hold for the 2018 blizzard and wildfires studied, implying that twitter users\' opinions on climate change are fairly ingrained on this subset of natural disasters.","Task":["climate science"],"Method":["twitter topic"]},{"title":"Ask BERT: How Regulatory Disclosure of Transition and Physical Climate Risks affects the CDS Term Structure","abstract":"We use BERT, an AI-based algorithm for language understanding, to decipher regulatory climate-risk disclosures and measure their impact on the credit default swap (CDS) market. Risk disclosures can either increase or decrease credit spreads, depending on whether disclosure reveals new risks or sharpens the signal and decreases the uncertainty. Training BERT to differentiate between transition and physical climate risks, we find that disclosing transition risks increases CDS spreads, especially after the Paris Climate Agreement of 2015, while disclosing physical climate risks leads to a decrease in CDS spreads. These impacts are statistically and economically highly significant.","url":"https://www.semanticscholar.org/paper/bbc6486ad37365b77b9d0de8894f595d70af49ac","sentence":"Title: ask bert: how regulatory disclosure of transition and physical climate risks affects the cds term structure Abstract: we use bert, an ai-based algorithm for language understanding, to decipher regulatory climate-risk disclosures and measure their impact on the credit default swap (cds) market. risk disclosures can either increase or decrease credit spreads, depending on whether disclosure reveals new risks or sharpens the signal and decreases the uncertainty. training bert to differentiate between transition and physical climate risks, we find that disclosing transition risks increases cds spreads, especially after the paris climate agreement of 2015, while disclosing physical climate risks leads to a decrease in cds spreads. these impacts are statistically and economically highly significant.","Task":["b learning"],"Method":["bert"]},{"title":"Social Privacy in Networked Publics: Teens\u2019 Attitudes, Practices, and Strategies","abstract":"This paper examines how teens understand privacy in highly public networked environments like Facebook and Twitter. We describe both teens\u2019 practices, their privacy strategies, and the structural conditions in which they are embedded, highlighting the ways in which privacy, as it plays out in everyday life, is related more to agency and the ability to control a social situation than particular properties of information. Finally, we discuss the implications of teens\u2019 practices and strategies, revealing the importance of social norms as a regulatory force.(This paper was presented at Oxford Internet Institute\u2019s \u201cA Decade in Internet Time: Symposium on the Dynamics of the Internet and Society\u201d on September 22, 2011.)","url":"https://www.semanticscholar.org/paper/9b1653e3b57016958d10ff8531475eb0483d156c","sentence":"Title: social privacy in networked publics: teens\u2019 attitudes, practices, and strategies Abstract: this paper examines how teens understand privacy in highly public networked environments like facebook and twitter. we describe both teens\u2019 practices, their privacy strategies, and the structural conditions in which they are embedded, highlighting the ways in which privacy, as it plays out in everyday life, is related more to agency and the ability to control a social situation than particular properties of information. finally, we discuss the implications of teens\u2019 practices and strategies, revealing the importance of social norms as a regulatory force.(this paper was presented at oxford internet institute\u2019s \u201ca decade in internet time: symposium on the dynamics of the internet and society\u201d on september 22, 2011.)","Task":["social privacy"],"Method":["social privacy"]},{"title":"DeSMOG: Detecting Stance in Media On Global Warming","abstract":"Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, \u201cLeading scientists agree that global warming is a serious concern,\u201d framing a clause which affirms their own stance (\u201cthat global warming is serious\u201d) as an opinion endorsed (\\"[scientists] agree\u201d) by a reputable source (\u201cleading\u201d). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: \u201cMistaken scientists claim [...].\\" Our work studies opinion-framing in the global warming (GW) debate, an increasingly partisan issue that has received little attention in NLP. We introduce DeSMOG, a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other\u2019s opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GW-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author\u2019s own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.","url":"https://www.semanticscholar.org/paper/1d7a0e8be2bbe7d148bb48a1f9de218beaea4ed5","sentence":"Title: desmog: detecting stance in media on global warming Abstract: citing opinions is a powerful yet understudied strategy in argumentation. for example, an environmental activist might say, \u201cleading scientists agree that global warming is a serious concern,\u201d framing a clause which affirms their own stance (\u201cthat global warming is serious\u201d) as an opinion endorsed (\\"[scientists] agree\u201d) by a reputable source (\u201cleading\u201d). in contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: \u201cmistaken scientists claim [...].\\" our work studies opinion-framing in the global warming (gw) debate, an increasingly partisan issue that has received little attention in nlp. we introduce desmog, a dataset of stance-labeled gw sentences, and train a bert classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other\u2019s opinions. from 56k news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across gw-accepting and skeptic media, though gw-skeptical media shows more opponent-doubt. we also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author\u2019s own view to source entities known to publicly endorse the opposing view. we release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of gw stance.","Task":["argument science"],"Method":["stance warming"]},{"title":"You are right. I am ALARMED - But by Climate Change Counter Movement","abstract":"The world is facing the challenge of climate crisis. Despite the consensus in scientific community about anthropogenic global warming, the web is flooded with articles spreading climate misinformation. These articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change. We revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of NLP. Despite considerable work in detection of fake news, there is no misinformation dataset available that is specific to the domain.of climate change. We try to bridge this gap by scraping and releasing articles with known climate change misinformation.","url":"https://www.semanticscholar.org/paper/418b9f37b07a05b38798e97f57e17a6cc1048b92","sentence":"Title: you are right. i am alarmed - but by climate change counter movement Abstract: the world is facing the challenge of climate crisis. despite the consensus in scientific community about anthropogenic global warming, the web is flooded with articles spreading climate misinformation. these articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change. we revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of nlp. despite considerable work in detection of fake news, there is no misinformation dataset available that is specific to the domain.of climate change. we try to bridge this gap by scraping and releasing articles with known climate change misinformation.","Task":["climate science"],"Method":["climate misinformation"]},{"title":"Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings","abstract":"We provide an NLP framework to uncover four linguistic dimensions of political polarization in social media: topic choice, framing, affect and illocutionary force. We quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional LDA-based models. We apply our methods to study 4.4M tweets on 21 mass shootings. We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice. We identify framing devices, such as grounding and the contrasting use of the terms \u201cterrorist\u201d and \u201ccrazy\u201d, that contribute to polarization. Results pertaining to topic choice, affect and illocutionary force suggest that Republicans focus more on the shooter and event-specific facts (news) while Democrats focus more on the victims and call for policy changes. Our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them.","url":"https://www.semanticscholar.org/paper/3475876c3f5e624587181744bc5acf4223aecd00","sentence":"Title: analyzing polarization in social media: method and application to tweets on 21 mass shootings Abstract: we provide an nlp framework to uncover four linguistic dimensions of political polarization in social media: topic choice, framing, affect and illocutionary force. we quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional lda-based models. we apply our methods to study 4.4m tweets on 21 mass shootings. we provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice. we identify framing devices, such as grounding and the contrasting use of the terms \u201cterrorist\u201d and \u201ccrazy\u201d, that contribute to polarization. results pertaining to topic choice, affect and illocutionary force suggest that republicans focus more on the shooter and event-specific facts (news) while democrats focus more on the victims and call for policy changes. our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them.","Task":["social analysis"],"Method":["political polarization"]},{"title":"ClimaText: A Dataset for Climate Change Topic Detection","abstract":"Climate change communication in the mass media and other textual sources may affect and shape public perception. Extracting climate change information from these sources is an important task, e.g., for filtering content and e-discovery, sentiment analysis, automatic summarization, question-answering, and fact-checking. However, automating this process is a challenge, as climate change is a complex, fast-moving, and often ambiguous topic with scarce resources for popular text-based AI tasks. In this paper, we introduce \\\\textsc{ClimaText}, a dataset for sentence-based climate change topic detection, which we make publicly available. We explore different approaches to identify the climate change topic in various text sources. We find that popular keyword-based models are not adequate for such a complex and evolving task. Context-based algorithms like BERT \\\\cite{devlin2018bert} can detect, in addition to many trivial cases, a variety of complex and implicit topic patterns. Nevertheless, our analysis reveals a great potential for improvement in several directions, such as, e.g., capturing the discussion on indirect effects of climate change. Hence, we hope this work can serve as a good starting point for further research on this topic.","url":"https://www.semanticscholar.org/paper/134c8486ae58b421681656c85bbc48dc862f6f98","sentence":"Title: climatext: a dataset for climate change topic detection Abstract: climate change communication in the mass media and other textual sources may affect and shape public perception. extracting climate change information from these sources is an important task, e.g., for filtering content and e-discovery, sentiment analysis, automatic summarization, question-answering, and fact-checking. however, automating this process is a challenge, as climate change is a complex, fast-moving, and often ambiguous topic with scarce resources for popular text-based ai tasks. in this paper, we introduce \\\\textsc{climatext}, a dataset for sentence-based climate change topic detection, which we make publicly available. we explore different approaches to identify the climate change topic in various text sources. we find that popular keyword-based models are not adequate for such a complex and evolving task. context-based algorithms like bert \\\\cite{devlin2018bert} can detect, in addition to many trivial cases, a variety of complex and implicit topic patterns. nevertheless, our analysis reveals a great potential for improvement in several directions, such as, e.g., capturing the discussion on indirect effects of climate change. hence, we hope this work can serve as a good starting point for further research on this topic.","Task":["climate change"],"Method":["climatext"]},{"title":"Comparing Attitudes to Climate Change in the Media using sentiment analysis based on Latent Dirichlet Allocation","abstract":"News media typically present biased accounts of news stories, and different publications present different angles on the same event. In this research, we investigate how different publications differ in their approach to stories about climate change, by examining the sentiment and topics presented. To understand these attitudes, we find sentiment targets by combining Latent Dirichlet Allocation (LDA) with SentiWordNet, a general sentiment lexicon. Using LDA, we generate topics containing keywords which represent the sentiment targets, and then annotate the data using SentiWordNet before regrouping the articles based on topic similarity. Preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources. Ongoing work is investigating how systematic these attitudes are between different publications, and how these may change over time.","url":"https://www.semanticscholar.org/paper/171847842d161743f2b52dc4f85c121087ceb426","sentence":"Title: comparing attitudes to climate change in the media using sentiment analysis based on latent dirichlet allocation Abstract: news media typically present biased accounts of news stories, and different publications present different angles on the same event. in this research, we investigate how different publications differ in their approach to stories about climate change, by examining the sentiment and topics presented. to understand these attitudes, we find sentiment targets by combining latent dirichlet allocation (lda) with sentiwordnet, a general sentiment lexicon. using lda, we generate topics containing keywords which represent the sentiment targets, and then annotate the data using sentiwordnet before regrouping the articles based on topic similarity. preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources. ongoing work is investigating how systematic these attitudes are between different publications, and how these may change over time.","Task":["sentiment analysis"],"Method":["climate media"]},{"title":"Cross-Platform Disinformation Campaigns: Lessons Learned and Next Steps","abstract":"We conducted a mixed-method, interpretative analysis of an online, cross-platform disinformation campaign targeting the White Helmets, a rescue group operating in rebel-held areas of Syria that has become the subject of a persistent effort of delegitimization. This research helps to conceptualize what a disinformation campaign is and how it works. Based on what we learned from this case study, we conclude that a comprehensive understanding of disinformation requires accounting for the spread of content across platforms and that social media platforms should increase collaboration to detect and characterize disinformation campaigns.","url":"https://www.semanticscholar.org/paper/9c2cab0e72f2588e857671c1fbb13b5fba8f2586","sentence":"Title: cross-platform disinformation campaigns: lessons learned and next steps Abstract: we conducted a mixed-method, interpretative analysis of an online, cross-platform disinformation campaign targeting the white helmets, a rescue group operating in rebel-held areas of syria that has become the subject of a persistent effort of delegitimization. this research helps to conceptualize what a disinformation campaign is and how it works. based on what we learned from this case study, we conclude that a comprehensive understanding of disinformation requires accounting for the spread of content across platforms and that social media platforms should increase collaboration to detect and characterize disinformation campaigns.","Task":["social campaigns"],"Method":["disinformation campaigns"]},{"title":"Classification of Moral Foundations in Microblog Political Discourse","abstract":"Previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. Additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. Based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on Twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. The contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.","url":"https://www.semanticscholar.org/paper/7de1c0db3b3848d1a51dbfefce8b73e89a682177","sentence":"Title: classification of moral foundations in microblog political discourse Abstract: previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. the contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.","Task":["political science"],"Method":["political foundations"]},{"title":"Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases","abstract":"Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.","url":"https://www.semanticscholar.org/paper/29584ed6d68a06fdf91440a018f6bc83a44fd177","sentence":"Title: paragraph-level rationale extraction through regularization: a case study on european court of human rights cases Abstract: interpretability or explainability is an emerging research field in nlp. from a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. to this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. we also release a new dataset comprising european court of human rights cases, including annotations for paragraph-level rationales. we use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. we also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.","Task":["nLP"],"Method":["this topic"]},{"title":"Automatically Characterizing Targeted Information Operations Through Biases Present in Discourse on Twitter","abstract":"This paper considers the problem of automatically characterizing biases that may be associated with emerging information operations via artificial intelligence. Accurate analysis of these emerging topics usually requires laborious, manual analysis by experts to annotate millions of tweets to identify biases in new topics. We introduce adaptations of the Word Embedding Association Test [1] to a new domain: information operations. We validate our method using known information operation-related tweets from Twitter\'s Transparency Reports, and we perform a case study on the COVID-19 pandemic to evaluate our method\'s performance on non-labeled Twitter data, demonstrating its usability in emerging domains.","url":"https://www.semanticscholar.org/paper/08e3ce4454012d928470efa96a54aa08dd8a96b0","sentence":"Title: automatically characterizing targeted information operations through biases present in discourse on twitter Abstract: this paper considers the problem of automatically characterizing biases that may be associated with emerging information operations via artificial intelligence. accurate analysis of these emerging topics usually requires laborious, manual analysis by experts to annotate millions of tweets to identify biases in new topics. we introduce adaptations of the word embedding association test [1] to a new domain: information operations. we validate our method using known information operation-related tweets from twitter\'s transparency reports, and we perform a case study on the covid-19 pandemic to evaluate our method\'s performance on non-labeled twitter data, demonstrating its usability in emerging domains.","Task":["artificial intelligence"],"Method":["AIntelligence"]},{"title":"Framing and Agenda-setting in Russian News: a Computational Analysis of Intricate Political Strategies","abstract":"Amidst growing concern over media manipulation, NLP attention has focused on overt strategies like censorship and \u201cfake news\u201d. Here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction: articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to Russian, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.","url":"https://www.semanticscholar.org/paper/2547836827e0423f198320977d393f574e0fb3d6","sentence":"Title: framing and agenda-setting in russian news: a computational analysis of intricate political strategies Abstract: amidst growing concern over media manipulation, nlp attention has focused on overt strategies like censorship and \u201cfake news\u201d. here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). we analyze 13 years (100k articles) of the russian newspaper izvestia and identify a strategy of distraction: articles mention the u.s. more frequently in the month directly following an economic downturn in russia. we introduce embedding-based methods for cross-lingually projecting english frames to russian, and discover that these articles emphasize u.s. moral failings and threats to the u.s. our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.","Task":["political manipulation"],"Method":["media manipulation"]},{"title":"Predicting the Role of Political Trolls in Social Media","abstract":"We investigate the political roles of \u201cInternet trolls\u201d in social media. Political trolls, such as the ones linked to the Russian Internet Research Agency (IRA), have recently gained enormous attention for their ability to sway public opinion and even influence elections. Analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. However, this analysis is manual and labor-intensive, thus making it impractical as a first-response tool for newly-discovered troll farms. In this paper, we show how to automate this analysis by using machine learning in a realistic setting. In particular, we show how to classify trolls according to their political role \u2014left, news feed, right\u2014 by using features extracted from social media, i.e., Twitter, in two scenarios: (i) in a traditional supervised learning scenario, where labels for trolls are available, and (ii) in a distant supervision scenario, where labels for trolls are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. Technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e., embeddings, for the trolls. Experiments on the \u201cIRA Russian Troll\u201d dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.","url":"https://www.semanticscholar.org/paper/546c87abc26137359c43898559884ba9d6c5ae64","sentence":"Title: predicting the role of political trolls in social media Abstract: we investigate the political roles of \u201cinternet trolls\u201d in social media. political trolls, such as the ones linked to the russian internet research agency (ira), have recently gained enormous attention for their ability to sway public opinion and even influence elections. analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. however, this analysis is manual and labor-intensive, thus making it impractical as a first-response tool for newly-discovered troll farms. in this paper, we show how to automate this analysis by using machine learning in a realistic setting. in particular, we show how to classify trolls according to their political role \u2014left, news feed, right\u2014 by using features extracted from social media, i.e., twitter, in two scenarios: (i) in a traditional supervised learning scenario, where labels for trolls are available, and (ii) in a distant supervision scenario, where labels for trolls are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e., embeddings, for the trolls. experiments on the \u201cira russian troll\u201d dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.","Task":["machine learning"],"Method":["political trolling"]},{"title":"Historical Change in the Moral Foundations of Political Persuasion","abstract":"How have attempts at political persuasion changed over time? Using nine corpora dating back through 1789, containing over 7 million words of speech (1,666 documents in total), covering three different countries, plus the entire Google nGram corpus, we find that language relating to togetherness permanently crowded out language relating to duties and obligations in the persuasive speeches of politicians during the early 20th century. This shift is temporally predicted by a rise in Western nationalism and the mass movement of people from more rural to more urban areas and is unexplained by changes in language, private political speech, or nonmoral persuasion. We theorize that the emergence of the modern state in the 1920s had psychopolitical consequences for the ways that people understood and communicated their relationships with their government, which was then reflected in the levers of persuasion chosen by political elites.","url":"https://www.semanticscholar.org/paper/c0a2ee56fc80d971e920fba6179f81b515533592","sentence":"Title: historical change in the moral foundations of political persuasion Abstract: how have attempts at political persuasion changed over time? using nine corpora dating back through 1789, containing over 7 million words of speech (1,666 documents in total), covering three different countries, plus the entire google ngram corpus, we find that language relating to togetherness permanently crowded out language relating to duties and obligations in the persuasive speeches of politicians during the early 20th century. this shift is temporally predicted by a rise in western nationalism and the mass movement of people from more rural to more urban areas and is unexplained by changes in language, private political speech, or nonmoral persuasion. we theorize that the emergence of the modern state in the 1920s had psychopolitical consequences for the ways that people understood and communicated their relationships with their government, which was then reflected in the levers of persuasion chosen by political elites.","Task":["political psychology"],"Method":["political persuasion"]},{"title":"Red Bots Do It Better:Comparative Analysis of Social Bot Partisan Behavior","abstract":"Recent research brought awareness of the issue of bots on social media and the significant risks of mass manipulation of public opinion in the context of political discussion. In this work, we leverage Twitter to study the discourse during the 2018 US midterm elections and analyze social bot activity and interactions with humans. We collected 2.6 million tweets for 42 days around the election day from nearly 1 million users. We use the collected tweets to answer three research questions: (i) Do social bots lean and behave according to a political ideology? (ii) Can we observe different strategies among liberal and conservative bots? (iii) How effective are bot strategies in engaging humans? We show that social bots can be accurately classified according to their political leaning and behave accordingly. Conservative bots share most of the topics of discussion with their human counterparts, while liberal bots show less overlap and a more inflammatory attitude. We studied bot interactions with humans and observed different strategies. Finally, we measured bots embeddedness in the social network and the extent of human engagement with each group of bots. Results show that conservative bots are more deeply embedded in the social network and more effective than liberal bots at exerting influence on humans.","url":"https://www.semanticscholar.org/paper/d89e89eac7882790b48ad73376742e7a93fea4fd","sentence":"Title: red bots do it better:comparative analysis of social bot partisan behavior Abstract: recent research brought awareness of the issue of bots on social media and the significant risks of mass manipulation of public opinion in the context of political discussion. in this work, we leverage twitter to study the discourse during the 2018 us midterm elections and analyze social bot activity and interactions with humans. we collected 2.6 million tweets for 42 days around the election day from nearly 1 million users. we use the collected tweets to answer three research questions: (i) do social bots lean and behave according to a political ideology? (ii) can we observe different strategies among liberal and conservative bots? (iii) how effective are bot strategies in engaging humans? we show that social bots can be accurately classified according to their political leaning and behave accordingly. conservative bots share most of the topics of discussion with their human counterparts, while liberal bots show less overlap and a more inflammatory attitude. we studied bot interactions with humans and observed different strategies. finally, we measured bots embeddedness in the social network and the extent of human engagement with each group of bots. results show that conservative bots are more deeply embedded in the social network and more effective than liberal bots at exerting influence on humans.","Task":["social bots"],"Method":["social bots"]},{"title":"Fine-Grained Analysis of Propaganda in News Article","abstract":"Propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect the quality of any learning system trained on them. A further issue with most existing systems is the lack of explainability. To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.","url":"https://www.semanticscholar.org/paper/a12d22ff91ce159a0d3558ed5aaed115115beabd","sentence":"Title: fine-grained analysis of propaganda in news article Abstract: propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda. previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. such noisy gold labels inevitably affect the quality of any learning system trained on them. a further issue with most existing systems is the lack of explainability. to overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. in particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. we further design a novel multi-granularity neural network, and we show that it outperforms several strong bert-based baselines.","Task":["propaganda detection"],"Method":["propaganda detection"]},{"title":"Issue Framing in Online Discussion Fora","abstract":"In online discussion fora, speakers often make arguments for or against something, say birth control, by highlighting certain aspects of the topic. In social science, this is referred to as issue framing. In this paper, we introduce a new issue frame annotated corpus of online discussions. We explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora, using a combination of multi-task and adversarial training, assuming only unlabeled training data in the target domain.","url":"https://www.semanticscholar.org/paper/9e82aba279807a6c8541c8811f049a40ebca36c1","sentence":"Title: issue framing in online discussion fora Abstract: in online discussion fora, speakers often make arguments for or against something, say birth control, by highlighting certain aspects of the topic. in social science, this is referred to as issue framing. in this paper, we introduce a new issue frame annotated corpus of online discussions. we explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora, using a combination of multi-task and adversarial training, assuming only unlabeled training data in the target domain.","Task":["issue framing"],"Method":["issue framing"]},{"title":"Technology, Autonomy, and Manipulation","abstract":"Since 2016, when the Facebook/Cambridge Analytica scandal began to emerge, public concern has grown around the threat of \u201conline manipulation\u201d. While these worries are familiar to privacy researchers, this paper aims to make them more salient to policymakers \u2014 first, by defining \u201conline manipulation\u201d, thus enabling identification of manipulative practices; and second, by drawing attention to the specific harms online manipulation threatens. We argue that online manipulation is the use of information technology to covertly influence another person\u2019s decision-making, by targeting and exploiting their decision-making vulnerabilities. Engaging in such practices can harm individuals by diminishing their economic interests, but its deeper, more insidious harm is its challenge to individual autonomy. We explore this autonomy harm, emphasising its implications for both individuals and society, and we briefly outline some strategies for combating online manipulation and strengthening autonomy in an increasingly digital world.","url":"https://www.semanticscholar.org/paper/c0472b63d7d45d948dea42c4b5efffb6859541f9","sentence":"Title: technology, autonomy, and manipulation Abstract: since 2016, when the facebook/cambridge analytica scandal began to emerge, public concern has grown around the threat of \u201conline manipulation\u201d. while these worries are familiar to privacy researchers, this paper aims to make them more salient to policymakers \u2014 first, by defining \u201conline manipulation\u201d, thus enabling identification of manipulative practices; and second, by drawing attention to the specific harms online manipulation threatens. we argue that online manipulation is the use of information technology to covertly influence another person\u2019s decision-making, by targeting and exploiting their decision-making vulnerabilities. engaging in such practices can harm individuals by diminishing their economic interests, but its deeper, more insidious harm is its challenge to individual autonomy. we explore this autonomy harm, emphasising its implications for both individuals and society, and we briefly outline some strategies for combating online manipulation and strengthening autonomy in an increasingly digital world.","Task":["online manipulation"],"Method":["online manipulation"]},{"title":"Modeling Frames in Argumentation","abstract":"In argumentation, framing is used to emphasize a specific aspect of a controversial topic while concealing others. When talking about legalizing drugs, for instance, its economical aspect may be emphasized. In general, we call a set of arguments that focus on the same aspect a frame. An argumentative text has to serve the \u201cright\u201d frame(s) to convince the audience to adopt the author\u2019s stance (e.g., being pro or con legalizing drugs). More specifically, an author has to choose frames that fit the audience\u2019s cultural background and interests. This paper introduces frame identification, which is the task of splitting a set of arguments into non-overlapping frames. We present a fully unsupervised approach to this task, which first removes topical information and then identifies frames using clustering. For evaluation purposes, we provide a corpus with 12, 326 debate-portal arguments, organized along the frames of the debates\u2019 topics. On this corpus, our approach outperforms different strong baselines, achieving an F1-score of 0.28.","url":"https://www.semanticscholar.org/paper/20a37289d0b7c90065850eb7bf96d5ac3c8a21e4","sentence":"Title: modeling frames in argumentation Abstract: in argumentation, framing is used to emphasize a specific aspect of a controversial topic while concealing others. when talking about legalizing drugs, for instance, its economical aspect may be emphasized. in general, we call a set of arguments that focus on the same aspect a frame. an argumentative text has to serve the \u201cright\u201d frame(s) to convince the audience to adopt the author\u2019s stance (e.g., being pro or con legalizing drugs). more specifically, an author has to choose frames that fit the audience\u2019s cultural background and interests. this paper introduces frame identification, which is the task of splitting a set of arguments into non-overlapping frames. we present a fully unsupervised approach to this task, which first removes topical information and then identifies frames using clustering. for evaluation purposes, we provide a corpus with 12, 326 debate-portal arguments, organized along the frames of the debates\u2019 topics. on this corpus, our approach outperforms different strong baselines, achieving an f1-score of 0.28.","Task":["frame identification"],"Method":["frame identification"]},{"title":"Automatically Neutralizing Subjective Bias in Text","abstract":"Texts like news, encyclopedias, and some social media strive for objectivity. Yet bias in the form of inappropriate subjectivity - introducing attitudes via framing, presupposing truth, and casting doubt - remains ubiquitous. This kind of bias erodes our collective trust and fuels social conflict. To address this issue, we introduce a novel testbed for natural language generation: automatically bringing inappropriately subjective text into a neutral point of view (\\"neutralizing\\" biased text). We also offer the first parallel corpus of biased language. The corpus contains 180,000 sentence pairs and originates from Wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. Last, we propose two strong encoder-decoder baselines for the task. A straightforward yet opaque CONCURRENT system uses a BERT encoder to identify subjective words as part of the generation process. An interpretable and controllable MODULAR algorithm separates these steps, using (1) a BERT-based classifier to identify problematic words and (2) a novel join embedding through which the classifier can edit the hidden states of the encoder. Large-scale human evaluation across four domains (encyclopedias, news headlines, books, and political speeches) suggests that these algorithms are a first step towards the automatic identification and reduction of bias.","url":"https://www.semanticscholar.org/paper/16981cc4ddefd3ea7655754fd83a2a8ff2203a8b","sentence":"Title: automatically neutralizing subjective bias in text Abstract: texts like news, encyclopedias, and some social media strive for objectivity. yet bias in the form of inappropriate subjectivity - introducing attitudes via framing, presupposing truth, and casting doubt - remains ubiquitous. this kind of bias erodes our collective trust and fuels social conflict. to address this issue, we introduce a novel testbed for natural language generation: automatically bringing inappropriately subjective text into a neutral point of view (\\"neutralizing\\" biased text). we also offer the first parallel corpus of biased language. the corpus contains 180,000 sentence pairs and originates from wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. last, we propose two strong encoder-decoder baselines for the task. a straightforward yet opaque concurrent system uses a bert encoder to identify subjective words as part of the generation process. an interpretable and controllable modular algorithm separates these steps, using (1) a bert-based classifier to identify problematic words and (2) a novel join embedding through which the classifier can edit the hidden states of the encoder. large-scale human evaluation across four domains (encyclopedias, news headlines, books, and political speeches) suggests that these algorithms are a first step towards the automatic identification and reduction of bias.","Task":["biased bias"],"Method":["biased topic"]},{"title":"A Systematic Media Frame Analysis of 1.5 Million New York Times Articles from 2000 to 2017","abstract":"Framing is an indispensable narrative device for news media because even the same facts may lead to conflicting understandings if deliberate framing is employed. Therefore, identifying media framing is a crucial step to understanding how news media influence the public. Framing is, however, difficult to operationalize and detect, and thus traditional media framing studies had to rely on manual annotation, which is challenging to scale up to massive news datasets. Here, by developing a media frame classifier that achieves state-of-the-art performance, we systematically analyze the media frames of 1.5 million New York Times articles published from 2000 to 2017. By examining the ebb and flow of media frames over almost two decades, we show that short-term frame abundance fluctuation closely corresponds to major events, while there also exist several long-term trends, such as the gradually increasing prevalence of the \u201cCultural identity\u201d frame. By examining specific topics and sentiments, we identify characteristics and dynamics of each frame. Finally, as a case study, we delve into the framing of mass shootings, revealing three major framing patterns. Our scalable, computational approach to massive news datasets opens up new pathways for systematic media framing studies.","url":"https://www.semanticscholar.org/paper/a0a5669b310ceb3840a599bf4f27a077a643d613","sentence":"Title: a systematic media frame analysis of 1.5 million new york times articles from 2000 to 2017 Abstract: framing is an indispensable narrative device for news media because even the same facts may lead to conflicting understandings if deliberate framing is employed. therefore, identifying media framing is a crucial step to understanding how news media influence the public. framing is, however, difficult to operationalize and detect, and thus traditional media framing studies had to rely on manual annotation, which is challenging to scale up to massive news datasets. here, by developing a media frame classifier that achieves state-of-the-art performance, we systematically analyze the media frames of 1.5 million new york times articles published from 2000 to 2017. by examining the ebb and flow of media frames over almost two decades, we show that short-term frame abundance fluctuation closely corresponds to major events, while there also exist several long-term trends, such as the gradually increasing prevalence of the \u201ccultural identity\u201d frame. by examining specific topics and sentiments, we identify characteristics and dynamics of each frame. finally, as a case study, we delve into the framing of mass shootings, revealing three major framing patterns. our scalable, computational approach to massive news datasets opens up new pathways for systematic media framing studies.","Task":["medianalysis"],"Method":["media framing"]},{"title":"FrameAxis: Characterizing Framing Bias and Intensity with Word Embedding","abstract":"We propose FrameAxis, a method of characterizing the framing of a given text by identifying the most relevant semantic axes (\\"microframes\\") defined by antonym word pairs. In contrast to the traditional framing analysis, which has been constrained by a small number of manually annotated general frames, our unsupervised approach provides much more detailed insights, by considering a host of semantic axes. Our method is capable of quantitatively teasing out framing bias -- how biased a text is in each microframe -- and framing intensity -- how much each microframe is used -- from the text, offering a nuanced characterization of framing. We evaluate our approach using SemEval datasets as well as three other datasets and human evaluations, demonstrating that FrameAxis can reliably characterize documents with relevant microframes. Our method may allow scalable and nuanced computational analyses of framing across disciplines.","url":"https://www.semanticscholar.org/paper/245bcbdfff19721c226059a5d5b1ae71b67e0572","sentence":"Title: frameaxis: characterizing framing bias and intensity with word embedding Abstract: we propose frameaxis, a method of characterizing the framing of a given text by identifying the most relevant semantic axes (\\"microframes\\") defined by antonym word pairs. in contrast to the traditional framing analysis, which has been constrained by a small number of manually annotated general frames, our unsupervised approach provides much more detailed insights, by considering a host of semantic axes. our method is capable of quantitatively teasing out framing bias -- how biased a text is in each microframe -- and framing intensity -- how much each microframe is used -- from the text, offering a nuanced characterization of framing. we evaluate our approach using semeval datasets as well as three other datasets and human evaluations, demonstrating that frameaxis can reliably characterize documents with relevant microframes. our method may allow scalable and nuanced computational analyses of framing across disciplines.","Task":["frameaxis"],"Method":["frameaxis"]},{"title":"Connotation Frames of Power and Agency in Modern Films","abstract":"The framing of an action influences how we perceive its actor. We introduce connotation frames of power and agency, a pragmatic formalism organized using frame semantic representations, to model how different levels of power and agency are implicitly projected on actors through their actions. We use the new power and agency frames to measure the subtle, but prevalent, gender bias in the portrayal of modern film characters and provide insights that deviate from the well-known Bechdel test. Our contributions include an extended lexicon of connotation frames along with a web interface that provides a comprehensive analysis through the lens of connotation frames.","url":"https://www.semanticscholar.org/paper/2cd74561109e2675634e0f4f0129fcfbc238a1d8","sentence":"Title: connotation frames of power and agency in modern films Abstract: the framing of an action influences how we perceive its actor. we introduce connotation frames of power and agency, a pragmatic formalism organized using frame semantic representations, to model how different levels of power and agency are implicitly projected on actors through their actions. we use the new power and agency frames to measure the subtle, but prevalent, gender bias in the portrayal of modern film characters and provide insights that deviate from the well-known bechdel test. our contributions include an extended lexicon of connotation frames along with a web interface that provides a comprehensive analysis through the lens of connotation frames.","Task":["film analysis"],"Method":["film framing"]},{"title":"Analyzing Framing through the Casts of Characters in the News","abstract":"We present an unsupervised model for the discovery and clustering of latent \u201cpersonas\u201d (characterizations of entities). Our model simultaneously clusters documents featuring similar collections of personas. We evaluate this model on a collection of news articles about immigration, showing that personas help predict the coarse-grained framing annotations in the Media Frames Corpus. We also introduce automated model selection as a fair and robust form of feature evaluation.","url":"https://www.semanticscholar.org/paper/15130cdd46ff7e3a69384e6c62ab1775c4aba96f","sentence":"Title: analyzing framing through the casts of characters in the news Abstract: we present an unsupervised model for the discovery and clustering of latent \u201cpersonas\u201d (characterizations of entities). our model simultaneously clusters documents featuring similar collections of personas. we evaluate this model on a collection of news articles about immigration, showing that personas help predict the coarse-grained framing annotations in the media frames corpus. we also introduce automated model selection as a fair and robust form of feature evaluation.","Task":["medianalysis"],"Method":["media framing"]},{"title":"The Media Frames Corpus: Annotations of Frames Across Issues","abstract":"We describe the first version of the Media Frames Corpus: several thousand news articles on three policy issues, annotated in terms of media framing. We motivate framing as a phenomenon of study for computational linguistics and describe our annotation process.","url":"https://www.semanticscholar.org/paper/92408cc19033cc4af29accef3793014ab79355c2","sentence":"Title: the media frames corpus: annotations of frames across issues Abstract: we describe the first version of the media frames corpus: several thousand news articles on three policy issues, annotated in terms of media framing. we motivate framing as a phenomenon of study for computational linguistics and describe our annotation process.","Task":["media framing"],"Method":["media framing"]},{"title":"Who Falls for Online Political Manipulation?","abstract":"Social media, once hailed as a vehicle for democratization and the promotion of positive social change across the globe, are under attack for becoming a tool of political manipulation and spread of disinformation. A case in point is the alleged use of trolls by Russia to spread malicious content in Western elections. This paper examines the Russian interference campaign in the 2016 US presidential election on Twitter. Our aim is twofold: first, we test whether predicting users who spread trolls\u2019 content is feasible in order to gain insight on how to contain their influence in the future; second, we identify features that are most predictive of users who either intentionally or unintentionally play a vital role in spreading this malicious content. We collected a dataset with over 43 million election-related posts shared on Twitter between September 16 and November 9, 2016, by about 5.7 million users. This dataset includes accounts associated with the Russian trolls identified by the US Congress. Proposed models are able to very accurately identify users who spread the trolls\u2019 content (average AUC score of 96%, using 10-fold validation). We show that political ideology, bot likelihood scores, and some activity-related account meta data are the most predictive features of whether a user spreads trolls\u2019 content or not.","url":"https://www.semanticscholar.org/paper/c899c6048a1bd99590972c5fb2f9fd00db8d4e18","sentence":"Title: who falls for online political manipulation? Abstract: social media, once hailed as a vehicle for democratization and the promotion of positive social change across the globe, are under attack for becoming a tool of political manipulation and spread of disinformation. a case in point is the alleged use of trolls by russia to spread malicious content in western elections. this paper examines the russian interference campaign in the 2016 us presidential election on twitter. our aim is twofold: first, we test whether predicting users who spread trolls\u2019 content is feasible in order to gain insight on how to contain their influence in the future; second, we identify features that are most predictive of users who either intentionally or unintentionally play a vital role in spreading this malicious content. we collected a dataset with over 43 million election-related posts shared on twitter between september 16 and november 9, 2016, by about 5.7 million users. this dataset includes accounts associated with the russian trolls identified by the us congress. proposed models are able to very accurately identify users who spread the trolls\u2019 content (average auc score of 96%, using 10-fold validation). we show that political ideology, bot likelihood scores, and some activity-related account meta data are the most predictive features of whether a user spreads trolls\u2019 content or not.","Task":["artificialearning"],"Method":["troll manipulation"]},{"title":"Linguistic Models for Analyzing and Detecting Biased Language","abstract":"Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. Bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective intensifiers. These insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task.","url":"https://www.semanticscholar.org/paper/2a501b074261e81b9126e80a0a308cfa5e76f8c1","sentence":"Title: linguistic models for analyzing and detecting biased language Abstract: unbiased language is a requirement for reference sources like encyclopedias and scientific texts. bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. to this end we analyze real instances of human edits designed to remove bias from wikipedia articles. the analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. we identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective intensifiers. these insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. our linguistically-informed model performs almost as well as humans tested on the same task.","Task":["linguistic analysis"],"Method":["biased models"]},{"title":"Misinfo Belief Frames: A Case Study on Covid & Climate News","abstract":"Prior beliefs of readers impact the way in which they project meaning onto news headlines. These beliefs can influence their perception of news reliability, as well as their reaction to news, and their likelihood of spreading the misinformation through social networks. However, most prior work focuses on fact-checking veracity of news or stylometry rather than measuring impact of misinformation. We propose Misinfo Belief Frames, a formalism for understanding how readers perceive the reliability of news and the impact of misinformation. We also introduce the Misinfo Belief Frames (MBF) corpus, a dataset of 66k inferences over 23.5k headlines. Misinformation frames use commonsense reasoning to uncover implications of real and fake news headlines focused on global crises: the Covid-19 pandemic and climate change. Our results using large-scale language modeling to predict misinformation frames show that machine-generated inferences can influence readers\u2019 trust in news headlines (readers\u2019 trust in news headlines was affected in 29.3% of cases). This demonstrates the potential effectiveness of using generated frames to counter misinformation.","url":"https://www.semanticscholar.org/paper/1778d1da6264ed4274fb94115ebbc3b4f34c9f7a","sentence":"Title: misinfo belief frames: a case study on covid & climate news Abstract: prior beliefs of readers impact the way in which they project meaning onto news headlines. these beliefs can influence their perception of news reliability, as well as their reaction to news, and their likelihood of spreading the misinformation through social networks. however, most prior work focuses on fact-checking veracity of news or stylometry rather than measuring impact of misinformation. we propose misinfo belief frames, a formalism for understanding how readers perceive the reliability of news and the impact of misinformation. we also introduce the misinfo belief frames (mbf) corpus, a dataset of 66k inferences over 23.5k headlines. misinformation frames use commonsense reasoning to uncover implications of real and fake news headlines focused on global crises: the covid-19 pandemic and climate change. our results using large-scale language modeling to predict misinformation frames show that machine-generated inferences can influence readers\u2019 trust in news headlines (readers\u2019 trust in news headlines was affected in 29.3% of cases). this demonstrates the potential effectiveness of using generated frames to counter misinformation.","Task":["misinformation frames"],"Method":["misinformation frames"]},{"title":"Fighting the COVID-19 Infodemic in Social Media: A Holistic Perspective and a Call to Arms","abstract":"With the outbreak of the COVID-19 pandemic, people turned to social media to read and to share timely information including statistics, warnings, advice, and inspirational stories. Unfortunately, alongside all this useful information, there was also a new blending of medical and political misinformation and disinformation, which gave rise to the first global infodemic. While fighting this infodemic is typically thought of in terms of factuality, the problem is much broader as malicious content includes not only fake news, rumors, and conspiracy theories, but also promotion of fake cures, panic, racism, xenophobia, and mistrust in the authorities, among others. This is a complex problem that needs a holistic approach combining the perspectives of journalists, fact-checkers, policymakers, government entities, social media platforms, and society as a whole. Taking them into account we define an annotation schema and detailed annotation instructions, which reflect these perspectives. We performed initial annotations using this schema, and our initial experiments demonstrated sizable improvements over the baselines. Now, we issue a call to arms to the research community and beyond to join the fight by supporting our crowdsourcing annotation efforts.","url":"https://www.semanticscholar.org/paper/460986c12b9fdedc3bf9a710c429c5a84608055a","sentence":"Title: fighting the covid-19 infodemic in social media: a holistic perspective and a call to arms Abstract: with the outbreak of the covid-19 pandemic, people turned to social media to read and to share timely information including statistics, warnings, advice, and inspirational stories. unfortunately, alongside all this useful information, there was also a new blending of medical and political misinformation and disinformation, which gave rise to the first global infodemic. while fighting this infodemic is typically thought of in terms of factuality, the problem is much broader as malicious content includes not only fake news, rumors, and conspiracy theories, but also promotion of fake cures, panic, racism, xenophobia, and mistrust in the authorities, among others. this is a complex problem that needs a holistic approach combining the perspectives of journalists, fact-checkers, policymakers, government entities, social media platforms, and society as a whole. taking them into account we define an annotation schema and detailed annotation instructions, which reflect these perspectives. we performed initial annotations using this schema, and our initial experiments demonstrated sizable improvements over the baselines. now, we issue a call to arms to the research community and beyond to join the fight by supporting our crowdsourcing annotation efforts.","Task":["social annotation"],"Method":["this topic"]},{"title":"The online competition between pro- and anti-vaccination views","abstract":"Distrust in scientific expertise 1 \u2013 14 is dangerous. Opposition to vaccination with a future vaccine against SARS-CoV-2, the causal agent of COVID-19, for example, could amplify outbreaks 2 \u2013 4 , as happened for measles in 2019 5 , 6 . Homemade remedies 7 , 8 and falsehoods are being shared widely on the Internet, as well as dismissals of expert advice 9 \u2013 11 . There is a lack of understanding about how this distrust evolves at the system level 13 , 14 . Here we provide a map of the contention surrounding vaccines that has emerged from the global pool of around three billion Facebook users. Its core reveals a multi-sided landscape of unprecedented intricacy that involves nearly 100\xa0million individuals partitioned into highly dynamic, interconnected clusters across cities, countries, continents and languages. Although smaller in overall size, anti-vaccination clusters manage to become highly entangled with undecided clusters in the main online network, whereas pro-vaccination clusters are more peripheral. Our theoretical framework reproduces the recent explosive growth in anti-vaccination views, and predicts that these views will dominate in a decade. Insights provided by this framework can inform new policies and approaches to interrupt this shift to negative views. Our results challenge the conventional thinking about undecided individuals in issues of contention surrounding health, shed light on other issues of contention such as climate change 11 , and highlight the key role of network cluster dynamics in multi-species ecologies 15 . Insights into the interactions between pro- and anti-vaccination clusters on Facebook can enable policies and approaches that attempt to interrupt the shift to anti-vaccination views and persuade undecided individuals to adopt a pro-vaccination stance.","url":"https://www.semanticscholar.org/paper/9129399d6b1599508709e3a72f80defd52ef89c9","sentence":"Title: the online competition between pro- and anti-vaccination views Abstract: distrust in scientific expertise 1 \u2013 14 is dangerous. opposition to vaccination with a future vaccine against sars-cov-2, the causal agent of covid-19, for example, could amplify outbreaks 2 \u2013 4 , as happened for measles in 2019 5 , 6 . homemade remedies 7 , 8 and falsehoods are being shared widely on the internet, as well as dismissals of expert advice 9 \u2013 11 . there is a lack of understanding about how this distrust evolves at the system level 13 , 14 . here we provide a map of the contention surrounding vaccines that has emerged from the global pool of around three billion facebook users. its core reveals a multi-sided landscape of unprecedented intricacy that involves nearly 100\xa0million individuals partitioned into highly dynamic, interconnected clusters across cities, countries, continents and languages. although smaller in overall size, anti-vaccination clusters manage to become highly entangled with undecided clusters in the main online network, whereas pro-vaccination clusters are more peripheral. our theoretical framework reproduces the recent explosive growth in anti-vaccination views, and predicts that these views will dominate in a decade. insights provided by this framework can inform new policies and approaches to interrupt this shift to negative views. our results challenge the conventional thinking about undecided individuals in issues of contention surrounding health, shed light on other issues of contention such as climate change 11 , and highlight the key role of network cluster dynamics in multi-species ecologies 15 . insights into the interactions between pro- and anti-vaccination clusters on facebook can enable policies and approaches that attempt to interrupt the shift to anti-vaccination views and persuade undecided individuals to adopt a pro-vaccination stance.","Task":["socialearning"],"Method":["online communication"]},{"title":"A Survey on Multimodal Disinformation Detection","abstract":"Recent years have witnessed the proliferation of fake news, propaganda, misinformation, and disinformation online. While initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract much more attention, and spread further than simple text. As a result, researchers started targeting different modalities and combinations thereof. As different modalities are studied in different research communities, with insufficient interaction, here we offer a survey that explores the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. Moreover, while some studies focused on factuality, others investigated how harmful the content is. While these two components in the definition of disinformation \u2013 (i) factuality and (ii) harmfulness, are equally important, they are typically studied in isolation. Thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. Finally, we discuss current challenges and future research directions.","url":"https://www.semanticscholar.org/paper/ed1c17451a23471afde91c109ecadc6aab8b2ba6","sentence":"Title: a survey on multimodal disinformation detection Abstract: recent years have witnessed the proliferation of fake news, propaganda, misinformation, and disinformation online. while initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract much more attention, and spread further than simple text. as a result, researchers started targeting different modalities and combinations thereof. as different modalities are studied in different research communities, with insufficient interaction, here we offer a survey that explores the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. moreover, while some studies focused on factuality, others investigated how harmful the content is. while these two components in the definition of disinformation \u2013 (i) factuality and (ii) harmfulness, are equally important, they are typically studied in isolation. thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. finally, we discuss current challenges and future research directions.","Task":["artificial detection"],"Method":["disinformation detection"]},{"title":"Automated Fact-Checking for Assisting Human Fact-Checkers","abstract":"The reporting and analysis of current events around the globe has expanded from professional, editorlead journalism all the way to citizen journalism. Politicians and other key players enjoy direct access to their audiences through social media, bypassing the filters of official cables or traditional media. However, the multiple advantages of free speech and direct communication are dimmed by the misuse of the media to spread inaccurate or misleading claims. These phenomena have led to the modern incarnation of the fact-checker \u2014 a professional whose main aim is to examine claims using available evidence to assess their veracity. As in other text forensics tasks, the amount of information available makes the work of the fact-checker more difficult. With this in mind, starting from the perspective of the professional fact-checker, we survey the available intelligent technologies that can support the human expert in the different steps of her fact-checking endeavor. These include identifying claims worth fact-checking; detecting relevant previously fact-checked claims; retrieving relevant evidence to fact-check a claim; and actually verifying a claim. In each case, we pay attention to the challenges in future work and the potential impact on real-world fact-checking.","url":"https://www.semanticscholar.org/paper/38d243b9f6e2c786699dbc83513fb190372cde07","sentence":"Title: automated fact-checking for assisting human fact-checkers Abstract: the reporting and analysis of current events around the globe has expanded from professional, editorlead journalism all the way to citizen journalism. politicians and other key players enjoy direct access to their audiences through social media, bypassing the filters of official cables or traditional media. however, the multiple advantages of free speech and direct communication are dimmed by the misuse of the media to spread inaccurate or misleading claims. these phenomena have led to the modern incarnation of the fact-checker \u2014 a professional whose main aim is to examine claims using available evidence to assess their veracity. as in other text forensics tasks, the amount of information available makes the work of the fact-checker more difficult. with this in mind, starting from the perspective of the professional fact-checker, we survey the available intelligent technologies that can support the human expert in the different steps of her fact-checking endeavor. these include identifying claims worth fact-checking; detecting relevant previously fact-checked claims; retrieving relevant evidence to fact-check a claim; and actually verifying a claim. in each case, we pay attention to the challenges in future work and the potential impact on real-world fact-checking.","Task":["machine learning"],"Method":["automated technologies"]},{"title":"Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection","abstract":"Fake news has altered society in negative ways in politics and culture. It has adversely affected both online social network systems as well as offline communities and conversations. Using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news. However, a lack of effective, comprehensive datasets has been a problem for fake news research and detection model development. Prior fake news datasets do not provide multimodal text and image data, metadata, comment data, and fine-grained fake news categorization at the scale and breadth of our dataset. We present Fakeddit, a novel multimodal dataset consisting of over 1 million samples from multiple categories of fake news. After being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision. We construct hybrid text+image models and perform extensive experiments for multiple variations of classification, demonstrating the importance of the novel aspect of multimodality and fine-grained classification unique to Fakeddit.","url":"https://www.semanticscholar.org/paper/76de89ca66f898e8211acba7392ef2d4a7e14125","sentence":"Title: fakeddit: a new multimodal benchmark dataset for fine-grained fake news detection Abstract: fake news has altered society in negative ways in politics and culture. it has adversely affected both online social network systems as well as offline communities and conversations. using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news. however, a lack of effective, comprehensive datasets has been a problem for fake news research and detection model development. prior fake news datasets do not provide multimodal text and image data, metadata, comment data, and fine-grained fake news categorization at the scale and breadth of our dataset. we present fakeddit, a novel multimodal dataset consisting of over 1 million samples from multiple categories of fake news. after being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision. we construct hybrid text+image models and perform extensive experiments for multiple variations of classification, demonstrating the importance of the novel aspect of multimodality and fine-grained classification unique to fakeddit.","Task":["fake news"],"Method":["fakeddit"]},{"title":"Automatic Detection of Fake News","abstract":"The proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in online news. Our contribution is twofold. First, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. We describe the collection, annotation, and validation process in detail and present several exploratory analyses on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors, and show that we can achieve accuracies of up to 76%. In addition, we provide comparative analyses of the automatic and manual identification of fake news.","url":"https://www.semanticscholar.org/paper/c6a9ca56c93323c0199dd22631d1cf731bdd7ec1","sentence":"Title: automatic detection of fake news Abstract: the proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. in this paper, we focus on the automatic identification of fake content in online news. our contribution is twofold. first, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. we describe the collection, annotation, and validation process in detail and present several exploratory analyses on the identification of linguistic differences in fake and legitimate news content. second, we conduct a set of learning experiments to build accurate fake news detectors, and show that we can achieve accuracies of up to 76%. in addition, we provide comparative analyses of the automatic and manual identification of fake news.","Task":["automated news"],"Method":["fake detection"]},{"title":"\\"Liar, Liar Pants on Fire\\": A New Benchmark Dataset for Fake News Detection","abstract":"Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present liar: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.","url":"https://www.semanticscholar.org/paper/03c294ad75bd1bac92217419ac25358227f6a901","sentence":"Title: \\"liar, liar pants on fire\\": a new benchmark dataset for fake news detection Abstract: automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. however, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. in this paper, we present liar: a new, publicly available dataset for fake news detection. we collected a decade-long, 12.8k manually labeled short statements in various contexts from politifact.com, which provides detailed analysis report and links to source documents for each case. this dataset can be used for fact-checking research as well. notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. we have designed a novel, hybrid convolutional neural network to integrate meta-data with text. we show that this hybrid approach can improve a text-only deep learning model.","Task":["deep news"],"Method":["false detection"]},{"title":"Weaponized Health Communication: Twitter Bots and Russian Trolls Amplify the Vaccine Debate","abstract":"Objectives To understand how Twitter bots and trolls (\u201cbots\u201d) promote online health content. Methods We compared bots\u2019 to average users\u2019 rates of vaccine-relevant messages, which we collected online from July 2014 through September 2017. We estimated the likelihood that users were bots, comparing proportions of polarized and antivaccine tweets across user types. We conducted a content analysis of a Twitter hashtag associated with Russian troll activity. Results Compared with average users, Russian trolls (\u03c72(1)\u2009=\u2009102.0; P\u2009<\u2009.001), sophisticated bots (\u03c72(1)\u2009=\u200928.6; P\u2009<\u2009.001), and \u201ccontent polluters\u201d (\u03c72(1)\u2009=\u20097.0; P\u2009<\u2009.001) tweeted about vaccination at higher rates. Whereas content polluters posted more antivaccine content (\u03c72(1)\u2009=\u200911.18; P\u2009<\u2009.001), Russian trolls amplified both sides. Unidentifiable accounts were more polarized (\u03c72(1)\u2009=\u200912.1; P\u2009<\u2009.001) and antivaccine (\u03c72(1)\u2009=\u200935.9; P\u2009<\u2009.001). Analysis of the Russian troll hashtag showed that its messages were more political and divisive. Conclusions Whereas bots that spread malware and unsolicited content disseminated antivaccine messages, Russian trolls promoted discord. Accounts masquerading as legitimate users create false equivalency, eroding public consensus on vaccination. Public Health Implications. Directly confronting vaccine skeptics enables bots to legitimize the vaccine debate. More research is needed to determine how best to combat bot-driven content.","url":"https://www.semanticscholar.org/paper/8415274c8fb370cbab84ad82ab2f469786ddee72","sentence":"Title: weaponized health communication: twitter bots and russian trolls amplify the vaccine debate Abstract: objectives to understand how twitter bots and trolls (\u201cbots\u201d) promote online health content. methods we compared bots\u2019 to average users\u2019 rates of vaccine-relevant messages, which we collected online from july 2014 through september 2017. we estimated the likelihood that users were bots, comparing proportions of polarized and antivaccine tweets across user types. we conducted a content analysis of a twitter hashtag associated with russian troll activity. results compared with average users, russian trolls (\u03c72(1)\u2009=\u2009102.0; p\u2009<\u2009.001), sophisticated bots (\u03c72(1)\u2009=\u200928.6; p\u2009<\u2009.001), and \u201ccontent polluters\u201d (\u03c72(1)\u2009=\u20097.0; p\u2009<\u2009.001) tweeted about vaccination at higher rates. whereas content polluters posted more antivaccine content (\u03c72(1)\u2009=\u200911.18; p\u2009<\u2009.001), russian trolls amplified both sides. unidentifiable accounts were more polarized (\u03c72(1)\u2009=\u200912.1; p\u2009<\u2009.001) and antivaccine (\u03c72(1)\u2009=\u200935.9; p\u2009<\u2009.001). analysis of the russian troll hashtag showed that its messages were more political and divisive. conclusions whereas bots that spread malware and unsolicited content disseminated antivaccine messages, russian trolls promoted discord. accounts masquerading as legitimate users create false equivalency, eroding public consensus on vaccination. public health implications. directly confronting vaccine skeptics enables bots to legitimize the vaccine debate. more research is needed to determine how best to combat bot-driven content.","Task":["social bots"],"Method":["twitter bots"]},{"title":"Fake News: Spread of Misinformation about Urological Conditions on Social Media.","abstract":"Although there is a large amount of user-generated content about urological health issues on social media, much of this content has not been vetted for information accuracy. In this article, we review the literature on the quality and balance of information on urological health conditions on social networks. Across a wide range of benign and malignant urological conditions, studies show a substantial amount of commercial, biased and/or inaccurate information present on popular social networking sites. The healthcare community should take proactive steps to improve the quality of medical information on social networks. PATIENT SUMMARY: In this review, we examined the spread of misinformation about urological health conditions on social media. We found that a significant amount of the circulating information is commercial, biased or misinformative.","url":"https://www.semanticscholar.org/paper/1dad69f1fd4403aed4d3d709ab794113291d625c","sentence":"Title: fake news: spread of misinformation about urological conditions on social media. Abstract: although there is a large amount of user-generated content about urological health issues on social media, much of this content has not been vetted for information accuracy. in this article, we review the literature on the quality and balance of information on urological health conditions on social networks. across a wide range of benign and malignant urological conditions, studies show a substantial amount of commercial, biased and/or inaccurate information present on popular social networking sites. the healthcare community should take proactive steps to improve the quality of medical information on social networks. patient summary: in this review, we examined the spread of misinformation about urological health conditions on social media. we found that a significant amount of the circulating information is commercial, biased or misinformative.","Task":["social news"],"Method":["fake media"]},{"title":"Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking","abstract":"We present an analytic study on the language of news media in the context of political fact-checking and fake news detection. We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.","url":"https://www.semanticscholar.org/paper/7d3c2ff37d04914836f9cbd9ce54b6c97aa74a22","sentence":"Title: truth of varying shades: analyzing language in fake news and political fact-checking Abstract: we present an analytic study on the language of news media in the context of political fact-checking and fake news detection. we compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. to probe the feasibility of automatic political fact-checking, we also present a case study based on politifact.com using their factuality judgments on a 6-point scale. experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.","Task":["linguistic analysis"],"Method":["linguistic media"]},{"title":"Coronavirus on Social Media: Analyzing Misinformation in Twitter Conversations","abstract":"The ongoing Coronavirus Disease (COVID-19) pandemic highlights the interconnected-ness of our present-day globalized world. With social distancing policies in place, virtual communication has become an important source of (mis)information. As increasing number of people rely on social media platforms for news, identifying misinformation has emerged as a critical task in these unprecedented times. In addition to being malicious, the spread of such information poses a serious public health risk. To this end, we design a dashboard to track misinformation on popular social media news sharing platform - Twitter. Our dashboard allows visibility into the social media discussions around Coronavirus and the quality of information shared on the platform as the situation evolves. We collect streaming data using the Twitter API from March 1, 2020 to date and provide analysis of topic clusters and social sentiments related to important emerging policies such as \\"#socialdistancing\\" and \\"#workfromhome\\". We track emerging hashtags over time, and provide location and time sensitive analysis of sentiments. In addition, we study the challenging problem of misinformation on social media, and provide a detection method to identify false, misleading and clickbait contents from Twitter information cascades. The dashboard maintains an evolving list of detected misinformation cascades with the corresponding detection scores, accessible online athttps://ksharmar.this http URL.","url":"https://www.semanticscholar.org/paper/29370adbf0674ad72d5e0bcc59582c22035bd0aa","sentence":"Title: coronavirus on social media: analyzing misinformation in twitter conversations Abstract: the ongoing coronavirus disease (covid-19) pandemic highlights the interconnected-ness of our present-day globalized world. with social distancing policies in place, virtual communication has become an important source of (mis)information. as increasing number of people rely on social media platforms for news, identifying misinformation has emerged as a critical task in these unprecedented times. in addition to being malicious, the spread of such information poses a serious public health risk. to this end, we design a dashboard to track misinformation on popular social media news sharing platform - twitter. our dashboard allows visibility into the social media discussions around coronavirus and the quality of information shared on the platform as the situation evolves. we collect streaming data using the twitter api from march 1, 2020 to date and provide analysis of topic clusters and social sentiments related to important emerging policies such as \\"#socialdistancing\\" and \\"#workfromhome\\". we track emerging hashtags over time, and provide location and time sensitive analysis of sentiments. in addition, we study the challenging problem of misinformation on social media, and provide a detection method to identify false, misleading and clickbait contents from twitter information cascades. the dashboard maintains an evolving list of detected misinformation cascades with the corresponding detection scores, accessible online athttps://ksharmar.this http url.","Task":["social analysis"],"Method":["this topic"]},{"title":"Combating Fake News: A Survey on Identification and Mitigation Techniques","abstract":"The proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news, and mitigation of its widespread impact on public opinion. While much of the earlier research was focused on identification of fake news based on its contents or by exploiting users\' engagements with the news on social media, there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society. In this survey, we describe the modern-day problem of fake news and, in particular, highlight the technical challenges associated with it. We discuss existing methods and techniques applicable to both identification and mitigation, with a focus on the significant advances in each method and their advantages and limitations. In addition, research has often been limited by the quality of existing datasets and their specific application contexts. To alleviate this problem, we comprehensively compile and summarize characteristic features of available datasets. Furthermore, we outline new directions of research to facilitate future development of effective and interdisciplinary solutions.","url":"https://www.semanticscholar.org/paper/1213d98f3d0a42d575bbd2c2af0309c7f76024a1","sentence":"Title: combating fake news: a survey on identification and mitigation techniques Abstract: the proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news, and mitigation of its widespread impact on public opinion. while much of the earlier research was focused on identification of fake news based on its contents or by exploiting users\' engagements with the news on social media, there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society. in this survey, we describe the modern-day problem of fake news and, in particular, highlight the technical challenges associated with it. we discuss existing methods and techniques applicable to both identification and mitigation, with a focus on the significant advances in each method and their advantages and limitations. in addition, research has often been limited by the quality of existing datasets and their specific application contexts. to alleviate this problem, we comprehensively compile and summarize characteristic features of available datasets. furthermore, we outline new directions of research to facilitate future development of effective and interdisciplinary solutions.","Task":["fake news"],"Method":["fake news"]},{"title":"The spread of true and false news online","abstract":"Lies spread faster than the truth There is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. To understand how false news spreads, Vosoughi et al. used a data set of rumor cascades on Twitter from 2006 to 2017. About 126,000 rumors were spread by \u223c3 million people. False news reached more people than the truth; the top 1% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. Falsehood also diffused faster than the truth. The degree of novelty and the emotional reactions of recipients may be responsible for the differences observed. Science, this issue p. 1146 A large-scale analysis of tweets reveals that false rumors spread further and faster than the truth. We investigated the differential diffusion of all of the verified true and false news stories distributed on Twitter from 2006 to 2017. The data comprise ~126,000 stories tweeted by ~3 million people more than 4.5 million times. We classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98% agreement on the classifications. Falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. We found that false news was more novel than true news, which suggests that people were more likely to share novel information. Whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. Contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.","url":"https://www.semanticscholar.org/paper/a1e58f89f57f57fad3c77cd558444ad5ad64b525","sentence":"Title: the spread of true and false news online Abstract: lies spread faster than the truth there is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. to understand how false news spreads, vosoughi et al. used a data set of rumor cascades on twitter from 2006 to 2017. about 126,000 rumors were spread by \u223c3 million people. false news reached more people than the truth; the top 1% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. falsehood also diffused faster than the truth. the degree of novelty and the emotional reactions of recipients may be responsible for the differences observed. science, this issue p. 1146 a large-scale analysis of tweets reveals that false rumors spread further and faster than the truth. we investigated the differential diffusion of all of the verified true and false news stories distributed on twitter from 2006 to 2017. the data comprise ~126,000 stories tweeted by ~3 million people more than 4.5 million times. we classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98% agreement on the classifications. falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. we found that false news was more novel than true news, which suggests that people were more likely to share novel information. whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.","Task":["socialearning"],"Method":["false diffusion"]},{"title":"Fake News: A Survey of Research, Detection Methods, and Opportunities","abstract":"The explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news analysis, detection and intervention. This survey comprehensively and systematically reviews fake news research. The survey identifies and specifies fundamental theories across various disciplines, e.g., psychology and social science, to facilitate and enhance the interdisciplinary research of fake news. Current fake news research is reviewed, summarized and evaluated. These studies focus on fake news from four perspective: (1) the false knowledge it carries, (2) its writing style, (3) its propagation patterns, and (4) the credibility of its creators and spreaders. We characterize each perspective with various analyzable and utilizable information provided by news and its spreaders, various strategies and frameworks that are adaptable, and techniques that are applicable. By reviewing the characteristics of fake news and open issues in fake news studies, we highlight some potential research tasks at the end of this survey.","url":"https://www.semanticscholar.org/paper/bc8c416f821b93795370524247c8a455c373ee6e","sentence":"Title: fake news: a survey of research, detection methods, and opportunities Abstract: the explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news analysis, detection and intervention. this survey comprehensively and systematically reviews fake news research. the survey identifies and specifies fundamental theories across various disciplines, e.g., psychology and social science, to facilitate and enhance the interdisciplinary research of fake news. current fake news research is reviewed, summarized and evaluated. these studies focus on fake news from four perspective: (1) the false knowledge it carries, (2) its writing style, (3) its propagation patterns, and (4) the credibility of its creators and spreaders. we characterize each perspective with various analyzable and utilizable information provided by news and its spreaders, various strategies and frameworks that are adaptable, and techniques that are applicable. by reviewing the characteristics of fake news and open issues in fake news studies, we highlight some potential research tasks at the end of this survey.","Task":["fake news"],"Method":["fake news"]},{"title":"Fact or Fiction: Verifying Scientific Claims","abstract":"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that supports or refutes a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that these models benefit from combined training on a large dataset of claims about Wikipedia articles, together with the new SciFact data. We show that our claim verification system is able to identify plausible evidence for 23 / 36 claims relevant to COVID-19 on the CORD-19 corpus. Our results and experiments strongly suggest that our new task and data will support significant future research efforts.","url":"https://www.semanticscholar.org/paper/9e8ac8df8b46c36cad3f307f85975012479b5a32","sentence":"Title: fact or fiction: verifying scientific claims Abstract: we introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that supports or refutes a given scientific claim, and to identify rationales justifying each decision. to study this task, we construct scifact, a dataset of 1.4k expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. we develop baseline models for scifact, and demonstrate that these models benefit from combined training on a large dataset of claims about wikipedia articles, together with the new scifact data. we show that our claim verification system is able to identify plausible evidence for 23 / 36 claims relevant to covid-19 on the cord-19 corpus. our results and experiments strongly suggest that our new task and data will support significant future research efforts.","Task":["scientific verification"],"Method":["sc verification"]},{"title":"Rumor Cascades","abstract":"Online social networks provide a rich substrate for rumor propagation. Information received via friends tends to be trusted, and online social networks allow individuals to transmit information to many friends at once. By referencing known rumors from Snopes.com, a popular website documenting memes and urban legends, we track the propagation of thousands of rumors appearing on Facebook. From this sample we infer the rates at which rumors from different categories and of varying truth value are uploaded and reshared. We find that rumor cascades run deeper in the social network than reshare cascades in general. We then examine the effect of individual reshares receiving a comment containing a link to a Snopes article on the evolution of the cascade. We find that receiving such a comment increases the likelihood that a reshare of a rumor will be deleted. Furthermore, large cascades are able to accumulate hundreds of Snopes comments while continuing to propagate. Finally, using a dataset of rumors copied and pasted from one status update to another, we show that rumors change over time and that different variants tend to dominate different bursts in popularity.","url":"https://www.semanticscholar.org/paper/1e3d1055fc65966bc2c64505785207d7e8d71022","sentence":"Title: rumor cascades Abstract: online social networks provide a rich substrate for rumor propagation. information received via friends tends to be trusted, and online social networks allow individuals to transmit information to many friends at once. by referencing known rumors from snopes.com, a popular website documenting memes and urban legends, we track the propagation of thousands of rumors appearing on facebook. from this sample we infer the rates at which rumors from different categories and of varying truth value are uploaded and reshared. we find that rumor cascades run deeper in the social network than reshare cascades in general. we then examine the effect of individual reshares receiving a comment containing a link to a snopes article on the evolution of the cascade. we find that receiving such a comment increases the likelihood that a reshare of a rumor will be deleted. furthermore, large cascades are able to accumulate hundreds of snopes comments while continuing to propagate. finally, using a dataset of rumors copied and pasted from one status update to another, we show that rumors change over time and that different variants tend to dominate different bursts in popularity.","Task":["social propagation"],"Method":["rumor propagation"]},{"title":"A Benchmark Dataset of Check-worthy Factual Claims","abstract":"In this paper we present the ClaimBuster dataset of 23,533 statements extracted from all U.S. general election presidential debates and annotated by human coders. The ClaimBuster dataset can be leveraged in building computational methods to identify claims that are worth fact-checking from the myriad of sources of digital or traditional media. The ClaimBuster dataset is publicly available to the research community, and it can be found at this http URL.","url":"https://www.semanticscholar.org/paper/6f373f4711e1285bdec23069c9503d3bf77bfaef","sentence":"Title: a benchmark dataset of check-worthy factual claims Abstract: in this paper we present the claimbuster dataset of 23,533 statements extracted from all u.s. general election presidential debates and annotated by human coders. the claimbuster dataset can be leveraged in building computational methods to identify claims that are worth fact-checking from the myriad of sources of digital or traditional media. the claimbuster dataset is publicly available to the research community, and it can be found at this http url.","Task":["claim analysis"],"Method":["claimbuster"]},{"title":"Rumors, False Flags, and Digital Vigilantes: Misinformation on Twitter after the 2013 Boston Marathon Bombing","abstract":"The Boston Marathon bombing story unfolded on every possible carrier of information available in the spring of 2013, including Twitter. As information spread, it was filled with rumors (unsubstantiated information), and many of these rumors contained misinformation. Earlier studies have suggested that crowdsourced information flows can correct misinformation, and our research investigates this proposition. This exploratory research examines three rumors, later demonstrated to be false, that circulated on Twitter in the aftermath of the bombings. Our findings suggest that corrections to the misinformation emerge but are muted compared with the propagation of the misinformation. The similarities and differences we observe in the patterns of the misinformation and corrections contained within the stream over the days that followed the attacks suggest directions for possible research strategies to automatically detect misinformation.","url":"https://www.semanticscholar.org/paper/44ecb9783e43d726fc87b7ffdd580b01e13a85cb","sentence":"Title: rumors, false flags, and digital vigilantes: misinformation on twitter after the 2013 boston marathon bombing Abstract: the boston marathon bombing story unfolded on every possible carrier of information available in the spring of 2013, including twitter. as information spread, it was filled with rumors (unsubstantiated information), and many of these rumors contained misinformation. earlier studies have suggested that crowdsourced information flows can correct misinformation, and our research investigates this proposition. this exploratory research examines three rumors, later demonstrated to be false, that circulated on twitter in the aftermath of the bombings. our findings suggest that corrections to the misinformation emerge but are muted compared with the propagation of the misinformation. the similarities and differences we observe in the patterns of the misinformation and corrections contained within the stream over the days that followed the attacks suggest directions for possible research strategies to automatically detect misinformation.","Task":["social media"],"Method":["misinformation topic"]},{"title":"Social Media and Fake News in the 2016 Election","abstract":"Following the 2016 U.S. presidential election, many have expressed concern about the effects of false stories (\u201cfake news\u201d), circulated largely through social media. We discuss the economics of fake news and present new data on its consumption prior to the election. Drawing on web browsing data, archives of fact-checking websites, and results from a new online survey, we find: (i) social media was an important but not dominant source of election news, with 14 percent of Americans calling social media their \u201cmost important\u201d source; (ii) of the known false news stories that appeared in the three months before the election, those favoring Trump were shared a total of 30 million times on Facebook, while those favoring Clinton were shared 8 million times; (iii) the average American adult saw on the order of one or perhaps several fake news stories in the months around the election, with just over half of those who recalled seeing them believing them; and (iv) people are much more likely to believe stories that favor their preferred candidate, especially if they have ideologically segregated social media networks.","url":"https://www.semanticscholar.org/paper/6f78b5608fed43f106da192f12e09d9edbd2fce0","sentence":"Title: social media and fake news in the 2016 election Abstract: following the 2016 u.s. presidential election, many have expressed concern about the effects of false stories (\u201cfake news\u201d), circulated largely through social media. we discuss the economics of fake news and present new data on its consumption prior to the election. drawing on web browsing data, archives of fact-checking websites, and results from a new online survey, we find: (i) social media was an important but not dominant source of election news, with 14 percent of americans calling social media their \u201cmost important\u201d source; (ii) of the known false news stories that appeared in the three months before the election, those favoring trump were shared a total of 30 million times on facebook, while those favoring clinton were shared 8 million times; (iii) the average american adult saw on the order of one or perhaps several fake news stories in the months around the election, with just over half of those who recalled seeing them believing them; and (iv) people are much more likely to believe stories that favor their preferred candidate, especially if they have ideologically segregated social media networks.","Task":["social media"],"Method":["fake media"]},{"title":"Extractive and Abstractive Explanations for Fact-Checking and Evaluation of News","abstract":"In this paper, we explore the construction of natural language explanations for news claims, with the goal of assisting fact-checking and news evaluation applications. We experiment with two methods: (1) an extractive method based on Biased TextRank \u2013 a resource-effective unsupervised graph-based algorithm for content extraction; and (2) an abstractive method based on the GPT-2 language model. We perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise.","url":"https://www.semanticscholar.org/paper/69116800a8a8195531d29c8e14cefb1c92cbb8a7","sentence":"Title: extractive and abstractive explanations for fact-checking and evaluation of news Abstract: in this paper, we explore the construction of natural language explanations for news claims, with the goal of assisting fact-checking and news evaluation applications. we experiment with two methods: (1) an extractive method based on biased textrank \u2013 a resource-effective unsupervised graph-based algorithm for content extraction; and (2) an abstractive method based on the gpt-2 language model. we perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise.","Task":["language analysis"],"Method":["this methods"]},{"title":"That is a Known Lie: Detecting Previously Fact-Checked Claims","abstract":"The recent proliferation of \u201dfake news\u201d has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. As manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. Interestingly, despite the importance of the task, it has been largely ignored by the research community so far. Here, we aim to bridge this gap. In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. We further create a specialized dataset, which we release to the research community. Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.","url":"https://www.semanticscholar.org/paper/20b2f18aaf10a9221c5edf3720d4cce7da672104","sentence":"Title: that is a known lie: detecting previously fact-checked claims Abstract: the recent proliferation of \u201dfake news\u201d has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. as a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. as manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. interestingly, despite the importance of the task, it has been largely ignored by the research community so far. here, we aim to bridge this gap. in particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. we further create a specialized dataset, which we release to the research community. finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.","Task":["socialearning"],"Method":["this topic"]},{"title":"Defending Against Neural Fake News","abstract":"Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \\nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary\'s point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,\' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \\nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.","url":"https://www.semanticscholar.org/paper/ad7129af0644dbcafa9aa2f111cb76526ea444a1","sentence":"Title: defending against neural fake news Abstract: recent progress in natural language generation has raised dual-use concerns. while applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \\nmodern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary\'s point of view, and exploring potential mitigations to these threats. likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. we thus present a model for controllable text generation called grover. given a headline like `link found between vaccines and autism,\' grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \\ndeveloping robust verification techniques against generators like grover is critical. we find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. counterintuitively, the best defense against grover turns out to be grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. we investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. we conclude by discussing ethical issues regarding the technology, and plan to release grover publicly, helping pave the way for better detection of neural fake news.","Task":["artificialver"],"Method":["thisver"]},{"title":"MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims","abstract":"We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.","url":"https://www.semanticscholar.org/paper/d8cb11d4be955f9869387a18967dee366eb851d9","sentence":"Title: multifc: a real-world multi-domain dataset for evidence-based fact checking of claims Abstract: we contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. it is collected from 26 fact checking websites in english, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. we present an in-depth analysis of the dataset, highlighting characteristics and challenges. further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. significant performance increases are achieved by encoding evidence, and by modelling metadata. our best-performing model achieves a macro f1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.","Task":["fact checking"],"Method":["thec"]},{"title":"The Limitations of Stylometry for Detecting Machine-Generated Fake News","abstract":"Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that stylometry is limited against machine-generated misinformation. Whereas humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs, utilized in auto-completion and editing-assistance settings.1 Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.","url":"https://www.semanticscholar.org/paper/9aefd614e52336151966d8dca2ed0ea62a8f30af","sentence":"Title: the limitations of stylometry for detecting machine-generated fake news Abstract: recent developments in neural language models (lms) have raised concerns about their potential misuse for automatically spreading misinformation. in light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. these approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. however, in this work, we show that stylometry is limited against machine-generated misinformation. whereas humans speak differently when trying to deceive, lms generate stylistically consistent text, regardless of underlying motive. thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate lm applications from those that introduce false information. we create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of lms, utilized in auto-completion and editing-assistance settings.1 our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.","Task":["lms"],"Method":["lms"]},{"title":"FEVER: a Large-scale Dataset for Fact Extraction and VERification","abstract":"In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.","url":"https://www.semanticscholar.org/paper/b1d24e8e08435b7c52335485a0d635abf9bc604c","sentence":"Title: fever: a large-scale dataset for fact extraction and verification Abstract: in this paper we introduce a new publicly available dataset for verification against textual sources, fever: fact extraction and verification. it consists of 185,445 claims generated by altering sentences extracted from wikipedia and subsequently verified without knowledge of the sentence they were derived from. the claims are classified as supported, refuted or notenoughinfo by annotators achieving 0.6841 in fleiss kappa. for the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. to characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. the best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. thus we believe that fever is a challenging testbed that will help stimulate progress on claim verification against textual sources.","Task":["fals verification"],"Method":["the dataset"]},{"title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World","abstract":"Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the \u201clanguage agnostic\u201d status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.","url":"https://www.semanticscholar.org/paper/0e141942fa265142f41a2a26eb17b6005d3af29e","sentence":"Title: the state and fate of linguistic diversity and inclusion in the nlp world Abstract: language technologies contribute to promoting multilingualism and linguistic diversity around the world. however, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. in this paper we look at the relation between the types of languages, resources, and their representation in nlp conferences to understand the trajectory that different languages have followed over time. our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the \u201clanguage agnostic\u201d status of current models and systems. through this paper, we attempt to convince the acl community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.","Task":["languages"],"Method":["linguistic topic"]},{"title":"Stereotypes in High-Stakes Decisions: Evidence from U.S. Circuit Courts","abstract":"Attitudes towards social groups such as women and racial minorities have been shown to be important determinants of individual\u2019s decisions but are hard to measure for those in policy-making roles. We propose a way to address the challenge in the case of U.S. appellate court judges, for whom we have large corpora of written text (their published opinions). Using the universe of published opinions in U.S. Circuit Courts 1890-2013, we construct a judge-specific measure of gender-stereotyped language (gender slant) by looking at the relative co-occurrence of words identifying gender (male versus female) and words identifying gender stereotypes (career versus family). We find that female and younger judges tend to use less stereotyped language in their opinions. In addition, the attitudes measured by gender slant matter for judicial decisions: judges with higher slant vote more conservatively on women rights\u2019 issues. These more slanted judges also influence workplace outcomes for female colleagues: they are less likely to assign opinions to female judges, they cite fewer female-authored opinions, and they are more likely to reverse lower-court decisions if the lower-court judge is a woman. Our results expose a possible use of text to detect decision-makers\u2019 stereotypes that predict behavior and disparate outcomes. \u2217Arianna Ornaghi, University of Warwick, a.ornaghi@warwick.ac.uk (corresponding author); Elliott Ash, ETH Zurich, ashe@ethz.ch; Daniel Chen, Toulouse School of Economics, daniel.chen@iast.fr. We thank Jacopo Bregolin, David Cai, Christoph Goessmann, and Ornelie Manzambi for helpful research assistance.","url":"https://www.semanticscholar.org/paper/c3bcdea205ec9fb1b84d75d4767f346844082b38","sentence":"Title: stereotypes in high-stakes decisions: evidence from u.s. circuit courts Abstract: attitudes towards social groups such as women and racial minorities have been shown to be important determinants of individual\u2019s decisions but are hard to measure for those in policy-making roles. we propose a way to address the challenge in the case of u.s. appellate court judges, for whom we have large corpora of written text (their published opinions). using the universe of published opinions in u.s. circuit courts 1890-2013, we construct a judge-specific measure of gender-stereotyped language (gender slant) by looking at the relative co-occurrence of words identifying gender (male versus female) and words identifying gender stereotypes (career versus family). we find that female and younger judges tend to use less stereotyped language in their opinions. in addition, the attitudes measured by gender slant matter for judicial decisions: judges with higher slant vote more conservatively on women rights\u2019 issues. these more slanted judges also influence workplace outcomes for female colleagues: they are less likely to assign opinions to female judges, they cite fewer female-authored opinions, and they are more likely to reverse lower-court decisions if the lower-court judge is a woman. our results expose a possible use of text to detect decision-makers\u2019 stereotypes that predict behavior and disparate outcomes. \u2217arianna ornaghi, university of warwick, a.ornaghi@warwick.ac.uk (corresponding author); elliott ash, eth zurich, ashe@ethz.ch; daniel chen, toulouse school of economics, daniel.chen@iast.fr. we thank jacopo bregolin, david cai, christoph goessmann, and ornelie manzambi for helpful research assistance.","Task":["text analysis"],"Method":["judicial courts"]},{"title":"Generating Fact Checking Explanations","abstract":"Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process \u2013 generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.","url":"https://www.semanticscholar.org/paper/5e0daaeceb75ffbbe23be13d34ffae830cb4e8c4","sentence":"Title: generating fact checking explanations Abstract: most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. a crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process \u2013 generating justifications for verdicts on claims. this paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. the results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.","Task":["fact checking"],"Method":["automated checking"]},{"title":"Towards Debiasing Fact Verification Models","abstract":"Fact verification requires validating a claim in the context of evidence. We show, however, that in the popular FEVER dataset this might not necessarily be the case. Claim-only classifiers perform competitively with top evidence-aware models. In this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. We create an evaluation set that avoids those idiosyncrasies. The performance of FEVER-trained models significantly drops when evaluated on this test set. Therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.","url":"https://www.semanticscholar.org/paper/a4947468ceac25c11a665c4f5f95a49d6dbea3cc","sentence":"Title: towards debiasing fact verification models Abstract: fact verification requires validating a claim in the context of evidence. we show, however, that in the popular fever dataset this might not necessarily be the case. claim-only classifiers perform competitively with top evidence-aware models. in this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. we create an evaluation set that avoids those idiosyncrasies. the performance of fever-trained models significantly drops when evaluated on this test set. therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. this work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.","Task":["fact verification"],"Method":["fact verification"]},{"title":"Evaluating adversarial attacks against multiple fact verification systems","abstract":"Automated fact verification has been progressing owing to advancements in modeling and availability of large datasets. Due to the nature of the task, it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly. We introduce two novel scoring metrics, attack potency and system resilience which take into account the correctness of the adversarial instances, an aspect often ignored in adversarial evaluations. We consider six fact verification systems from the recent Fact Extraction and VERification (FEVER) challenge: the four best-scoring ones and two baselines. We evaluate adversarial instances generated by a recently proposed state-of-the-art method, a paraphrasing method, and rule-based attacks devised for fact verification. We find that our rule-based attacks have higher potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines.","url":"https://www.semanticscholar.org/paper/3a16c38294ac8899825c488490199b854e05473a","sentence":"Title: evaluating adversarial attacks against multiple fact verification systems Abstract: automated fact verification has been progressing owing to advancements in modeling and availability of large datasets. due to the nature of the task, it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly. we introduce two novel scoring metrics, attack potency and system resilience which take into account the correctness of the adversarial instances, an aspect often ignored in adversarial evaluations. we consider six fact verification systems from the recent fact extraction and verification (fever) challenge: the four best-scoring ones and two baselines. we evaluate adversarial instances generated by a recently proposed state-of-the-art method, a paraphrasing method, and rule-based attacks devised for fact verification. we find that our rule-based attacks have higher potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines.","Task":["fact verification"],"Method":["fact vulnerabilities"]},{"title":"Unsupervised Discovery of Gendered Language through Latent-Variable Modeling","abstract":"Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.","url":"https://www.semanticscholar.org/paper/d4eeb40b9bd06ed53a26282cd527609f71e6496f","sentence":"Title: unsupervised discovery of gendered language through latent-variable modeling Abstract: studying the ways in which language is gendered has long been an area of interest in sociolinguistics. studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. in this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. to that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. we find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.","Task":["lingu modeling"],"Method":["gender topic"]},{"title":"ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback","abstract":"We introduce ChrEnTranslate, an online ma\xad chine translation demonstration system for translation between English and an endangered language Cherokee. It supports both statistical and neural translation models as well as pro\xad vides quality estimation to inform users of re\xad liability, two user feedback interfaces for ex\xad perts and common users respectively, exam\xad ple inputs to collect human translations for monolingual data, word alignment visualiza\xad tion, and relevant terms from the Cherokee\xad English dictionary. The quantitative evalu\xad ation demonstrates that our backbone trans\xad lation models achieve state\xadof\xadthe\xadart transla\xad tion performance and our quality estimation well correlates with both BLEU and human judgment. By analyzing 216 pieces of expert feedback, we find that NMT is preferable be\xad cause it copies less than SMT, and, in gen\xad eral, current models can translate fragments of the source sentence but make major mistakes. When we add these 216 expert\xadcorrected paral\xad lel texts into the training set and retrain mod\xad els, equal or slightly better performance is ob\xad served, which demonstrates indicates the po\xad tential of human\xadin\xadthe\xadloop learning.1","url":"https://www.semanticscholar.org/paper/bb53946c7da617a05bbeef47fff74012db27ee78","sentence":"Title: chrentranslate: cherokee-english machine translation demo with quality estimation and corrective feedback Abstract: we introduce chrentranslate, an online ma\xad chine translation demonstration system for translation between english and an endangered language cherokee. it supports both statistical and neural translation models as well as pro\xad vides quality estimation to inform users of re\xad liability, two user feedback interfaces for ex\xad perts and common users respectively, exam\xad ple inputs to collect human translations for monolingual data, word alignment visualiza\xad tion, and relevant terms from the cherokee\xad english dictionary. the quantitative evalu\xad ation demonstrates that our backbone trans\xad lation models achieve state\xadof\xadthe\xadart transla\xad tion performance and our quality estimation well correlates with both bleu and human judgment. by analyzing 216 pieces of expert feedback, we find that nmt is preferable be\xad cause it copies less than smt, and, in gen\xad eral, current models can translate fragments of the source sentence but make major mistakes. when we add these 216 expert\xadcorrected paral\xad lel texts into the training set and retrain mod\xad els, equal or slightly better performance is ob\xad served, which demonstrates indicates the po\xad tential of human\xadin\xadthe\xadloop learning.1","Task":["machine translation"],"Method":["machine learning"]},{"title":"Haitian Creole: How to Build and Ship an MT Engine from Scratch in 4 days, 17 hours, & 30 minutes","abstract":"We describe the effort of the Microsoft Translator team to develop a Haitian Creole statistical machine translation engine from scratch in a matter of days. Haitian Creole presents a number of difficulties for devleoping an SMT system, principal among these is the lack of significant amounts of parallel training data and an inconsistent orthography, both of which lead to data sparseness. We demonstrate, however, that it is possible to build a translation engine of reasonable quality over very little data by engaging with the native language community and reducing data sparseness in creative ways. As such, we show that MT as a technology and as a service can be deployed rapidly in crisis situations.","url":"https://www.semanticscholar.org/paper/3d309aa1629ef9ca43e252eb6bf539286ed872f9","sentence":"Title: haitian creole: how to build and ship an mt engine from scratch in 4 days, 17 hours, & 30 minutes Abstract: we describe the effort of the microsoft translator team to develop a haitian creole statistical machine translation engine from scratch in a matter of days. haitian creole presents a number of difficulties for devleoping an smt system, principal among these is the lack of significant amounts of parallel training data and an inconsistent orthography, both of which lead to data sparseness. we demonstrate, however, that it is possible to build a translation engine of reasonable quality over very little data by engaging with the native language community and reducing data sparseness in creative ways. as such, we show that mt as a technology and as a service can be deployed rapidly in crisis situations.","Task":["machine translation"],"Method":["mmt"]},{"title":"Measuring Societal Biases in Text Corpora via First-Order Co-occurrence","abstract":"Text corpora are used to study societal biases, typically through statistical models such as word embeddings. The bias of a word towards a concept is typically estimated using vectors similarity, measuring whether the word and concept words share other words in their contexts. We argue that this second-order relationship introduces unrelated concepts into the measure, which causes an imprecise measurement of the bias. We propose instead to measure bias using the direct normalized co-occurrence associations between the word and the representative concept words, a first-order measure, by reconstructing the co-occurrence estimates inherent in the word embedding models. To study our novel corpus bias measurement method, we calculate the correlation of the gender bias values estimated from the text to the actual gender bias statistics of the U.S. job market, provided by two recent collections. The results show a consistently higher correlation when using the proposed first-order measure with a variety of word embedding models, as well as a more severe degree of bias, especially to female in a few specific occupations.","url":"https://www.semanticscholar.org/paper/f14cb1828e314a669304b0c37bc78d6b9073f6dd","sentence":"Title: measuring societal biases in text corpora via first-order co-occurrence Abstract: text corpora are used to study societal biases, typically through statistical models such as word embeddings. the bias of a word towards a concept is typically estimated using vectors similarity, measuring whether the word and concept words share other words in their contexts. we argue that this second-order relationship introduces unrelated concepts into the measure, which causes an imprecise measurement of the bias. we propose instead to measure bias using the direct normalized co-occurrence associations between the word and the representative concept words, a first-order measure, by reconstructing the co-occurrence estimates inherent in the word embedding models. to study our novel corpus bias measurement method, we calculate the correlation of the gender bias values estimated from the text to the actual gender bias statistics of the u.s. job market, provided by two recent collections. the results show a consistently higher correlation when using the proposed first-order measure with a variety of word embedding models, as well as a more severe degree of bias, especially to female in a few specific occupations.","Task":["gender bias"],"Method":["societal bias"]},{"title":"When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?","abstract":"Social biases are encoded in word embeddings. This presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. Here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods. We find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning. However, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g. gender) than others (e.g. race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.","url":"https://www.semanticscholar.org/paper/176420f394014d9e67d5cff1e1d430541fa0f55f","sentence":"Title: when do word embeddings accurately reflect surveys on our beliefs about people? Abstract: social biases are encoded in word embeddings. this presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods. we find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning. however, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g. gender) than others (e.g. race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.","Task":["social bias"],"Method":["embed research"]},{"title":"Language from police body camera footage shows racial disparities in officer respect","abstract":"Significance Police officers speak significantly less respectfully to black than to white community members in everyday traffic stops, even after controlling for officer race, infraction severity, stop location, and stop outcome. This paper presents a systematic analysis of officer body-worn camera footage, using computational linguistic techniques to automatically measure the respect level that officers display to community members. This work demonstrates that body camera footage can be used as a rich source of data rather than merely archival evidence, and paves the way for developing powerful language-based tools for studying and potentially improving police\u2013community relations. Using footage from body-worn cameras, we analyze the respectfulness of police officer language toward white and black community members during routine traffic stops. We develop computational linguistic methods that extract levels of respect automatically from transcripts, informed by a thin-slicing study of participant ratings of officer utterances. We find that officers speak with consistently less respect toward black versus white community members, even after controlling for the race of the officer, the severity of the infraction, the location of the stop, and the outcome of the stop. Such disparities in common, everyday interactions between police and the communities they serve have important implications for procedural justice and the building of police\u2013community trust.","url":"https://www.semanticscholar.org/paper/75d0cb419d7d58e81c2975758a36a11544a9f930","sentence":"Title: language from police body camera footage shows racial disparities in officer respect Abstract: significance police officers speak significantly less respectfully to black than to white community members in everyday traffic stops, even after controlling for officer race, infraction severity, stop location, and stop outcome. this paper presents a systematic analysis of officer body-worn camera footage, using computational linguistic techniques to automatically measure the respect level that officers display to community members. this work demonstrates that body camera footage can be used as a rich source of data rather than merely archival evidence, and paves the way for developing powerful language-based tools for studying and potentially improving police\u2013community relations. using footage from body-worn cameras, we analyze the respectfulness of police officer language toward white and black community members during routine traffic stops. we develop computational linguistic methods that extract levels of respect automatically from transcripts, informed by a thin-slicing study of participant ratings of officer utterances. we find that officers speak with consistently less respect toward black versus white community members, even after controlling for the race of the officer, the severity of the infraction, the location of the stop, and the outcome of the stop. such disparities in common, everyday interactions between police and the communities they serve have important implications for procedural justice and the building of police\u2013community trust.","Task":["body analysis"],"Method":["police research"]},{"title":"Word embeddings quantify 100 years of gender and ethnic stereotypes","abstract":"Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts\u2014e.g., the women\u2019s movement in the 1960s and Asian immigration into the United States\u2014and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.","url":"https://www.semanticscholar.org/paper/b5d7a19bd0bae10917a8e294960fdacf224d64fe","sentence":"Title: word embeddings quantify 100 years of gender and ethnic stereotypes Abstract: significance word embeddings are a popular machine-learning method that represents each english word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. we demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. as specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the united states evolved during the 20th and 21st centuries starting from 1910. our framework opens up a fruitful intersection between machine learning and quantitative social science. word embeddings are a powerful machine-learning framework that represents each english word by a vector. the geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. in this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the united states. we integrate word embeddings trained on 100 y of text data with the us census to show that changes in the embedding track closely with demographic and occupation shifts over time. the embedding captures societal shifts\u2014e.g., the women\u2019s movement in the 1960s and asian immigration into the united states\u2014and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.","Task":["machine learning"],"Method":["this topic"]},{"title":"Tie-breaker: Using language models to quantify gender bias in sports journalism","abstract":"Gender bias is an increasingly important issue in sports journalism. In this work, we propose a language-model-based approach to quantify differences in questions posed to female vs. male athletes, and apply it to tennis post-match interviews. We find that journalists ask male players questions that are generally more focused on the game when compared with the questions they ask their female counterparts. We also provide a fine-grained analysis of the extent to which the salience of this bias depends on various factors, such as question type, game outcome or player rank.","url":"https://www.semanticscholar.org/paper/3d505c5eff8752ac1805ef546d683bfa40aec4b1","sentence":"Title: tie-breaker: using language models to quantify gender bias in sports journalism Abstract: gender bias is an increasingly important issue in sports journalism. in this work, we propose a language-model-based approach to quantify differences in questions posed to female vs. male athletes, and apply it to tennis post-match interviews. we find that journalists ask male players questions that are generally more focused on the game when compared with the questions they ask their female counterparts. we also provide a fine-grained analysis of the extent to which the salience of this bias depends on various factors, such as question type, game outcome or player rank.","Task":["gender modeling"],"Method":["gender bias"]},{"title":"Entity-Centric Contextual Affective Analysis","abstract":"While contextualized word representations have improved state-of-the-art benchmarks in many NLP tasks, their potential usefulness for social-oriented tasks remains largely unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women.","url":"https://www.semanticscholar.org/paper/a1280728623e8fd605284b2b7cf536579b9e2cbf","sentence":"Title: entity-centric contextual affective analysis Abstract: while contextualized word representations have improved state-of-the-art benchmarks in many nlp tasks, their potential usefulness for social-oriented tasks remains largely unexplored. we show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. we evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. we find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. we ultimately use our method to examine differences in portrayals of men and women.","Task":["socialLP"],"Method":["this representations"]},{"title":"Racism is a Virus: Anti-Asian Hate and Counterhate in Social Media during the COVID-19 Crisis","abstract":"The spread of COVID-19 has sparked racism, hate, and xenophobia in social media targeted at Chinese and broader Asian communities. However, little is known about how racial hate spreads during a pandemic and the role of counterhate speech in mitigating the spread. Here we study the evolution and spread of anti-Asian hate speech through the lens of Twitter. We create COVID-HATE, the largest dataset of anti-Asian hate and counterhate spanning three months, containing over 30 million tweets, and a social network with over 87 million nodes. By creating a novel hand-labeled dataset of 2,400 tweets, we train a text classifier to identify hate and counterhate tweets that achieves an average AUROC of 0.852. We identify 891,204 hate and 200,198 counterhate tweets in COVID-HATE. Using this data to conduct longitudinal analysis, we find that while hateful users are less engaged in the COVID-19 discussions prior to their first anti-Asian tweet, they become more vocal and engaged afterwards compared to counterhate users. We find that bots comprise 10.4% of hateful users and are more vocal and hateful compared to non-bot users. Comparing bot accounts, we show that hateful bots are more successful in attracting followers compared to counterhate bots. Analysis of the social network reveals that hateful and counterhate users interact and engage extensively with one another, instead of living in isolated polarized communities. Furthermore, we find that hate is contagious and nodes are highly likely to become hateful after being exposed to hateful content. Importantly, our analysis reveals that counterhate messages can discourage users from turning hateful in the first place. Overall, this work presents a comprehensive overview of anti-Asian hate and counterhate content during a pandemic. The COVID-HATE dataset is available at this http URL.","url":"https://www.semanticscholar.org/paper/070b4a707748e289618880ffbe4762e4e3fc7860","sentence":"Title: racism is a virus: anti-asian hate and counterhate in social media during the covid-19 crisis Abstract: the spread of covid-19 has sparked racism, hate, and xenophobia in social media targeted at chinese and broader asian communities. however, little is known about how racial hate spreads during a pandemic and the role of counterhate speech in mitigating the spread. here we study the evolution and spread of anti-asian hate speech through the lens of twitter. we create covid-hate, the largest dataset of anti-asian hate and counterhate spanning three months, containing over 30 million tweets, and a social network with over 87 million nodes. by creating a novel hand-labeled dataset of 2,400 tweets, we train a text classifier to identify hate and counterhate tweets that achieves an average auroc of 0.852. we identify 891,204 hate and 200,198 counterhate tweets in covid-hate. using this data to conduct longitudinal analysis, we find that while hateful users are less engaged in the covid-19 discussions prior to their first anti-asian tweet, they become more vocal and engaged afterwards compared to counterhate users. we find that bots comprise 10.4% of hateful users and are more vocal and hateful compared to non-bot users. comparing bot accounts, we show that hateful bots are more successful in attracting followers compared to counterhate bots. analysis of the social network reveals that hateful and counterhate users interact and engage extensively with one another, instead of living in isolated polarized communities. furthermore, we find that hate is contagious and nodes are highly likely to become hateful after being exposed to hateful content. importantly, our analysis reveals that counterhate messages can discourage users from turning hateful in the first place. overall, this work presents a comprehensive overview of anti-asian hate and counterhate content during a pandemic. the covid-hate dataset is available at this http url.","Task":["socialearning"],"Method":["social media"]},{"title":"Automatically Inferring Gender Associations from Language","abstract":"In this paper, we pose the question: do people talk about women and men in different ways? We introduce two datasets and a novel integration of approaches for automatically inferring gender associations from language, discovering coherent word clusters, and labeling the clusters for the semantic concepts they represent. The datasets allow us to compare how people write about women and men in two different settings \u2013 one set draws from celebrity news and the other from student reviews of computer science professors. We demonstrate that there are large-scale differences in the ways that people talk about women and men and that these differences vary across domains. Human evaluations show that our methods significantly outperform strong baselines.","url":"https://www.semanticscholar.org/paper/7b5b2a9ad37d1a6c3c8916965b1958eef0a27a6a","sentence":"Title: automatically inferring gender associations from language Abstract: in this paper, we pose the question: do people talk about women and men in different ways? we introduce two datasets and a novel integration of approaches for automatically inferring gender associations from language, discovering coherent word clusters, and labeling the clusters for the semantic concepts they represent. the datasets allow us to compare how people write about women and men in two different settings \u2013 one set draws from celebrity news and the other from student reviews of computer science professors. we demonstrate that there are large-scale differences in the ways that people talk about women and men and that these differences vary across domains. human evaluations show that our methods significantly outperform strong baselines.","Task":["gender analysis"],"Method":["gender inference"]},{"title":"Contextual Affective Analysis: A Case Study of People Portrayals in Online #MeToo Stories","abstract":"In October 2017, numerous women accused producer Harvey Weinstein of sexual harassment. Their stories encouraged other women to voice allegations of sexual harassment against many high profile men, including politicians, actors, and producers. These events are broadly referred to as the #MeToo movement, named for the use of the hashtag \\"#metoo\\" on social media platforms like Twitter and Facebook. The movement has widely been referred to as \\"empowering\\" because it has amplified the voices of previously unheard women over those of traditionally powerful men. In this work, we investigate dynamics of sentiment, power and agency in online media coverage of these events. Using a corpus of online media articles about the #MeToo movement, we present a contextual affective analysis---an entity-centric approach that uses contextualized lexicons to examine how people are portrayed in media articles. We show that while these articles are sympathetic towards women who have experienced sexual harassment, they consistently present men as most powerful, even after sexual assault allegations. While we focus on media coverage of the #MeToo movement, our method for contextual affective analysis readily generalizes to other domains.","url":"https://www.semanticscholar.org/paper/6ba951771892f01206f1dd7244f14243e3885109","sentence":"Title: contextual affective analysis: a case study of people portrayals in online #metoo stories Abstract: in october 2017, numerous women accused producer harvey weinstein of sexual harassment. their stories encouraged other women to voice allegations of sexual harassment against many high profile men, including politicians, actors, and producers. these events are broadly referred to as the #metoo movement, named for the use of the hashtag \\"#metoo\\" on social media platforms like twitter and facebook. the movement has widely been referred to as \\"empowering\\" because it has amplified the voices of previously unheard women over those of traditionally powerful men. in this work, we investigate dynamics of sentiment, power and agency in online media coverage of these events. using a corpus of online media articles about the #metoo movement, we present a contextual affective analysis---an entity-centric approach that uses contextualized lexicons to examine how people are portrayed in media articles. we show that while these articles are sympathetic towards women who have experienced sexual harassment, they consistently present men as most powerful, even after sexual assault allegations. while we focus on media coverage of the #metoo movement, our method for contextual affective analysis readily generalizes to other domains.","Task":["medianalysis"],"Method":["this topic"]},{"title":"Girls Rule, Boys Drool: Extracting Semantic and Affective Stereotypes from Twitter","abstract":"Social identities carry widely agreed upon meanings, called stereotypes, that have important effects on social processes. In the present work, we develop a method to extract the stereotypes of Twitter users. Our method is grounded in two distinct strands of theory, one that represents stereotypes as identities\' affective meanings and the other that represents stereotypes as semantic relationships between identities. After validating our approach via a prediction task, we apply the model to a dataset of 45 thousand Twitter users who actively tweeted about the Michael Brown and Eric Garner tragedies. Our work provides unique insights into the stereotypes of these users, as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel, parsimonious way.","url":"https://www.semanticscholar.org/paper/995e477360908175d0b1184f6a0aace9d864bc5a","sentence":"Title: girls rule, boys drool: extracting semantic and affective stereotypes from twitter Abstract: social identities carry widely agreed upon meanings, called stereotypes, that have important effects on social processes. in the present work, we develop a method to extract the stereotypes of twitter users. our method is grounded in two distinct strands of theory, one that represents stereotypes as identities\' affective meanings and the other that represents stereotypes as semantic relationships between identities. after validating our approach via a prediction task, we apply the model to a dataset of 45 thousand twitter users who actively tweeted about the michael brown and eric garner tragedies. our work provides unique insights into the stereotypes of these users, as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel, parsimonious way.","Task":["social analysis"],"Method":["social stereotypes"]},{"title":"Relating Linguistic Gender Bias, Gender Values, and Gender Gaps: An International Analysis","abstract":"Recent research in machine learning has shown that many machine-learned language models contain pervasive racial and gender biases, rooting from biases in their textual training data. While these biases produce sub-optimal parsing and inferences, they may help us characterize and predict statistical gender gaps and gender values in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach to (1) quantify gender bias in word embeddings (i.e., vector-based lexical semantics), (2) correlate gender biases with survey responses and statistical gender gaps in education, politics, economics, and health, and (3) integrate numerical biases and statistics to model different cultures\u2019 survey results more accurately than either in isolation. We validate this approach using 2018 Twitter data spanning 99 countries, 18 Global Gender Gap statistics from the World Economic Forum, and 8 international survey results from the World Value Survey. Integrating these heterogeneous data across cultures is an important step toward building computational models to understand group bias.","url":"https://www.semanticscholar.org/paper/5e888bfd9b492a3b08f3cc2eb7c617fedf5bd811","sentence":"Title: relating linguistic gender bias, gender values, and gender gaps: an international analysis Abstract: recent research in machine learning has shown that many machine-learned language models contain pervasive racial and gender biases, rooting from biases in their textual training data. while these biases produce sub-optimal parsing and inferences, they may help us characterize and predict statistical gender gaps and gender values in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. this paper presents an approach to (1) quantify gender bias in word embeddings (i.e., vector-based lexical semantics), (2) correlate gender biases with survey responses and statistical gender gaps in education, politics, economics, and health, and (3) integrate numerical biases and statistics to model different cultures\u2019 survey results more accurately than either in isolation. we validate this approach using 2018 twitter data spanning 99 countries, 18 global gender gap statistics from the world economic forum, and 8 international survey results from the world value survey. integrating these heterogeneous data across cultures is an important step toward building computational models to understand group bias.","Task":["machine learning"],"Method":["gender bias"]},{"title":"Automation, Algorithms, and Politics| Talking to Bots: Symbiotic Agency and the Case of Tay","abstract":"In 2016, Microsoft launched Tay, an experimental artificial intelligence chat bot. Learning from interactions with Twitter users, Tay was shut down after one day because of its obscene and inflammatory tweets. This article uses the case of Tay to re-examine theories of agency. How did users view the personality and actions of an artificial intelligence chat bot when interacting with Tay on Twitter? Using phenomenological research methods and pragmatic approaches to agency, we look at what people said about Tay to study how they imagine and interact with emerging technologies and to show the limitations of our current theories of agency for describing communication in these settings. We show how different qualities of agency, different expectations for technologies, and different capacities for affordance emerge in the interactions between people and artificial intelligence. We argue that a perspective of \u201csymbiotic agency\u201d\u2014informed by the imagined affordances of emerging technology\u2014is required to really understand the collapse of Tay.","url":"https://www.semanticscholar.org/paper/abeee58b9fb5761133636ef117ef1a87203ad7ab","sentence":"Title: automation, algorithms, and politics| talking to bots: symbiotic agency and the case of tay Abstract: in 2016, microsoft launched tay, an experimental artificial intelligence chat bot. learning from interactions with twitter users, tay was shut down after one day because of its obscene and inflammatory tweets. this article uses the case of tay to re-examine theories of agency. how did users view the personality and actions of an artificial intelligence chat bot when interacting with tay on twitter? using phenomenological research methods and pragmatic approaches to agency, we look at what people said about tay to study how they imagine and interact with emerging technologies and to show the limitations of our current theories of agency for describing communication in these settings. we show how different qualities of agency, different expectations for technologies, and different capacities for affordance emerge in the interactions between people and artificial intelligence. we argue that a perspective of \u201csymbiotic agency\u201d\u2014informed by the imagined affordances of emerging technology\u2014is required to really understand the collapse of tay.","Task":["artificial intelligence"],"Method":["artificialay"]},{"title":"Empathy Is All You Need: How a Conversational Agent Should Respond to Verbal Abuse","abstract":"With the popularity of AI-infused systems, conversational agents (CAs) are becoming essential in diverse areas, offering new functionality and convenience, but simultaneously, suffering misuse and verbal abuse. We examine whether conversational agents\' response styles under varying abuse types influence those emotions found to mitigate peoples\' aggressive behaviors, involving three verbal abuse types (Insult, Threat, Swearing) and three response styles (Avoidance, Empathy, Counterattacking). Ninety-eight participants were assigned to one of the abuse type conditions, interacted with the three spoken (voice-based) CAs in turn, and reported their feelings about guiltiness, anger, and shame after each session. The results show that the agent\'s response style has a significant effect on user emotions. Participants were less angry and more guilty with the empathy agent than the other two agents. Furthermore, we investigated the current status of commercial CAs\' responses to verbal abuse. Our study findings have direct implications for the design of conversational agents.","url":"https://www.semanticscholar.org/paper/e3fd3b1be871da6e048adaef4a4e201af282fe8e","sentence":"Title: empathy is all you need: how a conversational agent should respond to verbal abuse Abstract: with the popularity of ai-infused systems, conversational agents (cas) are becoming essential in diverse areas, offering new functionality and convenience, but simultaneously, suffering misuse and verbal abuse. we examine whether conversational agents\' response styles under varying abuse types influence those emotions found to mitigate peoples\' aggressive behaviors, involving three verbal abuse types (insult, threat, swearing) and three response styles (avoidance, empathy, counterattacking). ninety-eight participants were assigned to one of the abuse type conditions, interacted with the three spoken (voice-based) cas in turn, and reported their feelings about guiltiness, anger, and shame after each session. the results show that the agent\'s response style has a significant effect on user emotions. participants were less angry and more guilty with the empathy agent than the other two agents. furthermore, we investigated the current status of commercial cas\' responses to verbal abuse. our study findings have direct implications for the design of conversational agents.","Task":["a intelligence"],"Method":["verbal abuse"]},{"title":"Shirtless and Dangerous: Quantifying Linguistic Signals of Gender Bias in an Online Fiction Writing Community","abstract":"Imagine a princess asleep in a castle, waiting for her prince to slay the dragon and rescue her. Tales like the famous Sleeping Beauty clearly divide up gender roles. But what about more modern stories, borne of a generation increasingly aware of social constructs like sexism and racism? Do these stories tend to reinforce gender stereotypes, or counter them? In this paper, we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction. We apply this technique across 1.8 billion words of fiction from the Wattpad online writing community, investigating gender representation in stories, how male and female characters behave and are described, and how authors\' use of gender stereotypes is associated with the community\'s ratings. We find that male over-representation and traditional gender stereotypes (e.g., dominant men and submissive women) are common throughout nearly every genre in our corpus. However, only some of these stereotypes, like sexual or violent men, are associated with highly rated stories. Finally, despite women often being the target of negative stereotypes, female authors are equally likely to write such stereotypes as men.","url":"https://www.semanticscholar.org/paper/ef5fa2e95fc853defb902b58d8e4e4fe95a01c75","sentence":"Title: shirtless and dangerous: quantifying linguistic signals of gender bias in an online fiction writing community Abstract: imagine a princess asleep in a castle, waiting for her prince to slay the dragon and rescue her. tales like the famous sleeping beauty clearly divide up gender roles. but what about more modern stories, borne of a generation increasingly aware of social constructs like sexism and racism? do these stories tend to reinforce gender stereotypes, or counter them? in this paper, we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction. we apply this technique across 1.8 billion words of fiction from the wattpad online writing community, investigating gender representation in stories, how male and female characters behave and are described, and how authors\' use of gender stereotypes is associated with the community\'s ratings. we find that male over-representation and traditional gender stereotypes (e.g., dominant men and submissive women) are common throughout nearly every genre in our corpus. however, only some of these stereotypes, like sexual or violent men, are associated with highly rated stories. finally, despite women often being the target of negative stereotypes, female authors are equally likely to write such stereotypes as men.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Let\'s Talk About Race: Identity, Chatbots, and AI","abstract":"Why is it so hard for chatbots to talk about race? This work explores how the biased contents of databases, the syntactic focus of natural language processing, and the opaque nature of deep learning algorithms cause chatbots difficulty in handling race-talk. In each of these areas, the tensions between race and chatbots create new opportunities for people and machines. By making the abstract and disparate qualities of this problem space tangible, we can develop chatbots that are more capable of handling race-talk in its many forms. Our goal is to provide the HCI community with ways to begin addressing the question, how can chatbots handle race-talk in new and improved ways?","url":"https://www.semanticscholar.org/paper/f34c73c75a640f59c11472bf6c9786aeb774856a","sentence":"Title: let\'s talk about race: identity, chatbots, and ai Abstract: why is it so hard for chatbots to talk about race? this work explores how the biased contents of databases, the syntactic focus of natural language processing, and the opaque nature of deep learning algorithms cause chatbots difficulty in handling race-talk. in each of these areas, the tensions between race and chatbots create new opportunities for people and machines. by making the abstract and disparate qualities of this problem space tangible, we can develop chatbots that are more capable of handling race-talk in its many forms. our goal is to provide the hci community with ways to begin addressing the question, how can chatbots handle race-talk in new and improved ways?","Task":["deepbots"],"Method":["this topic"]},{"title":"#MeToo Alexa: How Conversational Systems Respond to Sexual Harassment","abstract":"Conversational AI systems, such as Amazon\u2019s Alexa, are rapidly developing from purely transactional systems to social chatbots, which can respond to a wide variety of user requests. In this article, we establish how current state-of-the-art conversational systems react to inappropriate requests, such as bullying and sexual harassment on the part of the user, by collecting and analysing the novel #MeTooAlexa corpus. Our results show that commercial systems mainly avoid answering, while rule-based chatbots show a variety of behaviours and often deflect. Data-driven systems, on the other hand, are often non-coherent, but also run the risk of being interpreted as flirtatious and sometimes react with counter-aggression. This includes our own system, trained on \u201cclean\u201d data, which suggests that inappropriate system behaviour is not caused by data bias.","url":"https://www.semanticscholar.org/paper/983ad7c704d0f9a1560af322e4807e5be7799895","sentence":"Title: #metoo alexa: how conversational systems respond to sexual harassment Abstract: conversational ai systems, such as amazon\u2019s alexa, are rapidly developing from purely transactional systems to social chatbots, which can respond to a wide variety of user requests. in this article, we establish how current state-of-the-art conversational systems react to inappropriate requests, such as bullying and sexual harassment on the part of the user, by collecting and analysing the novel #metooalexa corpus. our results show that commercial systems mainly avoid answering, while rule-based chatbots show a variety of behaviours and often deflect. data-driven systems, on the other hand, are often non-coherent, but also run the risk of being interpreted as flirtatious and sometimes react with counter-aggression. this includes our own system, trained on \u201cclean\u201d data, which suggests that inappropriate system behaviour is not caused by data bias.","Task":["chatbots"],"Method":["this topic"]},{"title":"Social Bias Frames: Reasoning about Social and Power Implications of Language","abstract":"Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people\u2019s judgments about others. For example, given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women,\u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified.\u201d Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.","url":"https://www.semanticscholar.org/paper/7b14a165c6b7c1dc2c6c44727e623b94d834fb09","sentence":"Title: social bias frames: reasoning about social and power implications of language Abstract: warning: this paper contains content that may be offensive or upsetting. language has the power to reinforce stereotypes and project social biases onto others. at the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people\u2019s judgments about others. for example, given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women,\u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified.\u201d most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. we introduce social bias frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. in addition, we introduce the social bias inference corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. we then establish baseline approaches that learn to recover social bias frames from unstructured text. we find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% f1), they are not effective at spelling out more detailed explanations in terms of social bias frames. our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.","Task":["social bias"],"Method":["social biases"]},{"title":"Data Statements for NLP: Toward Mitigating System Bias and Enabling Better Science","abstract":"In this position paper, we propose data statements as a practice that NLP technologists, in both research and development, can adopt to begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form data statements can take and explore the implications of adopting them as part of our regular practice. We argue that they will help alleviate issues related to exclusion and bias in language technology; lead to better precision in claims about how NLP research can generalize and thus better engineering results; protect companies from public embarrassment; and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.","url":"https://www.semanticscholar.org/paper/87eb23f934a0e6293ee8ee9b147fe0d456e65c96","sentence":"Title: data statements for nlp: toward mitigating system bias and enabling better science Abstract: in this position paper, we propose data statements as a practice that nlp technologists, in both research and development, can adopt to begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. we present a form data statements can take and explore the implications of adopting them as part of our regular practice. we argue that they will help alleviate issues related to exclusion and bias in language technology; lead to better precision in claims about how nlp research can generalize and thus better engineering results; protect companies from public embarrassment; and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.","Task":["data statements"],"Method":["data statements"]},{"title":"Datasheets for Datasets","abstract":"The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.","url":"https://www.semanticscholar.org/paper/0df347f5e3118fac7c351917e3a497899b071d1e","sentence":"Title: datasheets for datasets Abstract: the machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. to address this gap, we propose datasheets for datasets. in the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. by analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.","Task":["machine learning"],"Method":["datasets"]},{"title":"Beyond the Belmont Principles: Ethical Challenges, Practices, and Beliefs in the Online Data Research Community","abstract":"Pervasive information streams that document people and their routines have been a boon to social computing research. But the ethics of collecting and analyzing available&-but potentially sensitive-online data present challenges to researchers. In response to increasing public and scholarly debate over the ethics of online data research, this paper analyzes the current state of practice among researchers using online data. Qualitative and quantitative responses from a survey of 263 online data researchers document beliefs and practices around which social computing researchers are converging, as well as areas of ongoing disagreement. The survey also reveals that these disagreements are not correlated with disciplinary, methodological, or workplace affiliations. The paper concludes by reflecting on changing ethical practices in the digital age, and discusses a set of emergent best practices for ethical social computing research.","url":"https://www.semanticscholar.org/paper/221732318e3cc45aa7bc2f48435706f3e5839ddc","sentence":"Title: beyond the belmont principles: ethical challenges, practices, and beliefs in the online data research community Abstract: pervasive information streams that document people and their routines have been a boon to social computing research. but the ethics of collecting and analyzing available&-but potentially sensitive-online data present challenges to researchers. in response to increasing public and scholarly debate over the ethics of online data research, this paper analyzes the current state of practice among researchers using online data. qualitative and quantitative responses from a survey of 263 online data researchers document beliefs and practices around which social computing researchers are converging, as well as areas of ongoing disagreement. the survey also reveals that these disagreements are not correlated with disciplinary, methodological, or workplace affiliations. the paper concludes by reflecting on changing ethical practices in the digital age, and discusses a set of emergent best practices for ethical social computing research.","Task":["social data"],"Method":["onlinethics"]},{"title":"Detecting East Asian Prejudice on Social Media","abstract":"During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The classifier achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.","url":"https://www.semanticscholar.org/paper/0c68d7d153bb56e4637d6aee051d87580e05fd5b","sentence":"Title: detecting east asian prejudice on social media Abstract: during covid-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against east asia and east asian people. we report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from twitter into four classes: hostility against east asia, criticism of east asia, meta-discussions of east asian prejudice, and a neutral class. the classifier achieves a macro-f1 score of 0.83. we then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. we provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. we also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for covid-19 relevance and east asian relevance and stance for 1,000 hashtags, and the final model.","Task":["machine learning"],"Method":["this learning"]},{"title":"Don\u2019t quote me: reverse identification of research participants in social media studies","abstract":"We investigated if participants in social media surveillance studies could be reverse identified by reviewing all articles published on PubMed in 2015 or 2016 with the words \u201cTwitter\u201d and either \u201cread,\u201d \u201ccoded,\u201d or \u201ccontent\u201d in the title or abstract. Seventy-two percent (95% CI: 63\u201380) of articles quoted at least one participant\u2019s tweet and searching for the quoted content led to the participant 84% (95% CI: 74\u201391) of the time. Twenty-one percent (95% CI: 13\u201329) of articles disclosed a participant\u2019s Twitter username thereby making the participant immediately identifiable. Only one article reported obtaining consent to disclose identifying information and institutional review board (IRB) involvement was mentioned in only 40% (95% CI: 31\u201350) of articles, of which 17% (95% CI: 10\u201325) received IRB-approval and 23% (95% CI:16\u201332) were deemed exempt. Biomedical publications are routinely including identifiable information by quoting tweets or revealing usernames which, in turn, violates ICMJE ethical standards governing scientific ethics, even though said content is scientifically unnecessary. We propose that authors convey aggregate findings without revealing participants\u2019 identities, editors refuse to publish reports that reveal a participant\u2019s identity, and IRBs attend to these privacy issues when reviewing studies involving social media data. These strategies together will ensure participants are protected going forward.","url":"https://www.semanticscholar.org/paper/1ece7c00d2eb6fca5443ff8e15f05a2b8b5985c2","sentence":"Title: don\u2019t quote me: reverse identification of research participants in social media studies Abstract: we investigated if participants in social media surveillance studies could be reverse identified by reviewing all articles published on pubmed in 2015 or 2016 with the words \u201ctwitter\u201d and either \u201cread,\u201d \u201ccoded,\u201d or \u201ccontent\u201d in the title or abstract. seventy-two percent (95% ci: 63\u201380) of articles quoted at least one participant\u2019s tweet and searching for the quoted content led to the participant 84% (95% ci: 74\u201391) of the time. twenty-one percent (95% ci: 13\u201329) of articles disclosed a participant\u2019s twitter username thereby making the participant immediately identifiable. only one article reported obtaining consent to disclose identifying information and institutional review board (irb) involvement was mentioned in only 40% (95% ci: 31\u201350) of articles, of which 17% (95% ci: 10\u201325) received irb-approval and 23% (95% ci:16\u201332) were deemed exempt. biomedical publications are routinely including identifiable information by quoting tweets or revealing usernames which, in turn, violates icmje ethical standards governing scientific ethics, even though said content is scientifically unnecessary. we propose that authors convey aggregate findings without revealing participants\u2019 identities, editors refuse to publish reports that reveal a participant\u2019s identity, and irbs attend to these privacy issues when reviewing studies involving social media data. these strategies together will ensure participants are protected going forward.","Task":["social recognition"],"Method":["reverse identification"]},{"title":"Towards an Ethical Framework for Publishing Twitter Data in Social Research: Taking into Account Users\u2019 Views, Online Context and Algorithmic Estimation","abstract":"New and emerging forms of data, including posts harvested from social media sites such as Twitter, have become part of the sociologist\u2019s data diet. In particular, some researchers see an advantage in the perceived \u2018public\u2019 nature of Twitter posts, representing them in publications without seeking informed consent. While such practice may not be at odds with Twitter\u2019s terms of service, we argue there is a need to interpret these through the lens of social science research methods that imply a more reflexive ethical approach than provided in \u2018legal\u2019 accounts of the permissible use of these data in research publications. To challenge some existing practice in Twitter-based research, this article brings to the fore: (1) views of Twitter users through analysis of online survey data; (2) the effect of context collapse and online disinhibition on the behaviours of users; and (3) the publication of identifiable sensitive classifications derived from algorithms.","url":"https://www.semanticscholar.org/paper/afe97d05e5b320d2af500cdae1c588f4cc0d14d2","sentence":"Title: towards an ethical framework for publishing twitter data in social research: taking into account users\u2019 views, online context and algorithmic estimation Abstract: new and emerging forms of data, including posts harvested from social media sites such as twitter, have become part of the sociologist\u2019s data diet. in particular, some researchers see an advantage in the perceived \u2018public\u2019 nature of twitter posts, representing them in publications without seeking informed consent. while such practice may not be at odds with twitter\u2019s terms of service, we argue there is a need to interpret these through the lens of social science research methods that imply a more reflexive ethical approach than provided in \u2018legal\u2019 accounts of the permissible use of these data in research publications. to challenge some existing practice in twitter-based research, this article brings to the fore: (1) views of twitter users through analysis of online survey data; (2) the effect of context collapse and online disinhibition on the behaviours of users; and (3) the publication of identifiable sensitive classifications derived from algorithms.","Task":["social analysis"],"Method":["twitter topic"]},{"title":"Writer Profiling Without the Writer\'s Text","abstract":"Social network users may wish to preserve their anonymity online by masking their identity and not using language associated with any particular demographics or personality. However, they have no control over the language in incoming communications. We show that linguistic cues in public comments directed at a user are sufficient for an accurate inference of that user\u2019s gender, age, religion, diet, and even personality traits. Moreover, we show that directed communication is even more predictive of a user\u2019s profile than the user\u2019s own language. We then conduct a nuanced analysis of what types of social relationships are most predictive of users\u2019 attributes, and propose new strategies on how individuals can modulate their online social relationships and incoming communications to preserve their anonymity.","url":"https://www.semanticscholar.org/paper/5f827b963939c96968a03318b4c2b011e1871eaf","sentence":"Title: writer profiling without the writer\'s text Abstract: social network users may wish to preserve their anonymity online by masking their identity and not using language associated with any particular demographics or personality. however, they have no control over the language in incoming communications. we show that linguistic cues in public comments directed at a user are sufficient for an accurate inference of that user\u2019s gender, age, religion, diet, and even personality traits. moreover, we show that directed communication is even more predictive of a user\u2019s profile than the user\u2019s own language. we then conduct a nuanced analysis of what types of social relationships are most predictive of users\u2019 attributes, and propose new strategies on how individuals can modulate their online social relationships and incoming communications to preserve their anonymity.","Task":["user profiling"],"Method":["writer profiling"]},{"title":"\u201cParticipant\u201d Perceptions of Twitter Research Ethics","abstract":"Social computing systems such as Twitter present new research sites that have provided billions of data points to researchers. However, the availability of public social media data has also presented ethical challenges. As the research community works to create ethical norms, we should be considering users\u2019 concerns as well. With this in mind, we report on an exploratory survey of Twitter users\u2019 perceptions of the use of tweets in research. Within our survey sample, few users were previously aware that their public tweets could be used by researchers, and the majority felt that researchers should not be able to use tweets without consent. However, we find that these attitudes are highly contextual, depending on factors such as how the research is conducted or disseminated, who is conducting it, and what the study is about. The findings of this study point to potential best practices for researchers conducting observation and analysis of public data.","url":"https://www.semanticscholar.org/paper/f298649194c1be9cb55574c047756ae7e8a62d6b","sentence":"Title: \u201cparticipant\u201d perceptions of twitter research ethics Abstract: social computing systems such as twitter present new research sites that have provided billions of data points to researchers. however, the availability of public social media data has also presented ethical challenges. as the research community works to create ethical norms, we should be considering users\u2019 concerns as well. with this in mind, we report on an exploratory survey of twitter users\u2019 perceptions of the use of tweets in research. within our survey sample, few users were previously aware that their public tweets could be used by researchers, and the majority felt that researchers should not be able to use tweets without consent. however, we find that these attitudes are highly contextual, depending on factors such as how the research is conducted or disseminated, who is conducting it, and what the study is about. the findings of this study point to potential best practices for researchers conducting observation and analysis of public data.","Task":["twitteresearch"],"Method":["twitter ethics"]},{"title":"The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks","abstract":"This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users\' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. \\nIn experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google\'s Smart Compose, a commercial text-completion neural network trained on millions of users\' email messages.","url":"https://www.semanticscholar.org/paper/520ec00dc35475e0554dbb72f27bd2eeb6f4191d","sentence":"Title: the secret sharer: evaluating and testing unintended memorization in neural networks Abstract: this paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. because such models are sometimes trained on sensitive data (e.g., the text of users\' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. \\nin experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. we show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in google\'s smart compose, a commercial text-completion neural network trained on millions of users\' email messages.","Task":["deep networks"],"Method":["this topic"]},{"title":"Garbage in, garbage out?: do machine learning application papers in social computing report where human-labeled training data comes from?","abstract":"Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper\'s authors labeling the data themselves. Such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. In this paper, we investigate to what extent a sample of machine learning application papers in social computing --- specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data --- give specific details about whether such best practices were followed. Our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: Does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. We find a wide divergence in whether such practices were followed and documented. Much of machine learning research and education focuses on what is done once a \\"gold standard\\" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.","url":"https://www.semanticscholar.org/paper/df2df1749b93ba86328ec7b86ff7e8d30029e3f5","sentence":"Title: garbage in, garbage out?: do machine learning application papers in social computing report where human-labeled training data comes from? Abstract: many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper\'s authors labeling the data themselves. such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. in this paper, we investigate to what extent a sample of machine learning application papers in social computing --- specifically papers from arxiv and traditional publications performing an ml classification task on twitter data --- give specific details about whether such best practices were followed. our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. we find a wide divergence in whether such practices were followed and documented. much of machine learning research and education focuses on what is done once a \\"gold standard\\" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.","Task":["machine learning"],"Method":["this topic"]},{"title":"How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation","abstract":"We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model\'s generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.","url":"https://www.semanticscholar.org/paper/129cbad01be98ee88a930e31898cb76be79c41c1","sentence":"Title: how not to evaluate your dialogue system: an empirical study of unsupervised evaluation metrics for dialogue response generation Abstract: we investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. recent works in response generation have adopted metrics from machine translation to compare a model\'s generated response to a single target response. we show that these metrics correlate very weakly with human judgements in the non-technical twitter domain, and not at all in the technical ubuntu domain. we provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.","Task":["dialogue generation"],"Method":["dialogue systems"]},{"title":"Fifty years later: the significance of the Nuremberg Code.","abstract":"The Nuremberg Code 1. The voluntary consent of the human subject is absolutely essential. This means that the person involved should have legal capacity to give consent; should be so situated as to be able to exercise free power of choice, without the intervention of any element of force, fraud, deceit, duress, overreaching, or other ulterior form of constraint or coercion; and should have sufficient knowledge and comprehension of the elements of the subject matter involved as to enable him to make an understanding and enlightened decision. This latter element requires that before the acceptance of an affirmative decision by .\xa0.\xa0.","url":"https://www.semanticscholar.org/paper/dff6976f237ecff091547dd2df26937bd6b59198","sentence":"Title: fifty years later: the significance of the nuremberg code. Abstract: the nuremberg code 1. the voluntary consent of the human subject is absolutely essential. this means that the person involved should have legal capacity to give consent; should be so situated as to be able to exercise free power of choice, without the intervention of any element of force, fraud, deceit, duress, overreaching, or other ulterior form of constraint or coercion; and should have sufficient knowledge and comprehension of the elements of the subject matter involved as to enable him to make an understanding and enlightened decision. this latter element requires that before the acceptance of an affirmative decision by .\xa0.\xa0.","Task":["artificial consent"],"Method":["thisubject"]},{"title":"Semantics derived automatically from language corpora necessarily contain human biases","abstract":"Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.","url":"https://www.semanticscholar.org/paper/5966d7c7f60898d610812e24c64d4d57855ad86a","sentence":"Title: semantics derived automatically from language corpora necessarily contain human biases Abstract: artificial intelligence and machine learning are in a period of astounding growth. however, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. we replicate a spectrum of standard human biases as exposed by the implicit association test and other well-known psychological studies. we replicate these using a widely used, purely statistical machine-learning model---namely, the glove word embedding---trained on a corpus of text from the web. our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. these regularities are captured by machine learning along with the rest of semantics. in addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the word embedding association test (weat) and the word embedding factual association test (wefat). our results have implications not only for ai and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.","Task":["machine learning"],"Method":["machine learning"]},{"title":"Algorithms of Oppression: How Search Engines Reinforce Racism","abstract":"Read and considered thoughtfully, Safiya Umoja Noble\u2019s Algorithms of Oppression: How Search Engines Reinforce Racism is devastating. It reduces to rubble the notion that technology is neutral and ideology-free. Noble\u2019s crushing the neutrality myth does several things. First, this act lays foundations for her argument: only if you recognize and understand that technology is built with, and integrates, bias, can you then be open to her primary thesis: search engines advance discriminatory and often racist content. Second, it banishes a convenient response for many self-identified meritocratic Silicon Valley \u201cwinners\u201d and their supporters. Postreading, some individuals may retain their beliefs in a neutral and ideology-free technology in spite of the overwhelming evidence and citations Noble brings to bear. Effective countering of Noble\u2019s claims is unlikely to occur. For professionals working in technology, information, argumentation, and/or rhetorical studies, Algorithms of Oppression is refreshing. Agonistic towards structural racism and its defenses, single-minded in its evidentiary presentation, collaborative in its acknowledgement of others\u2019 scholarship and research, Noble models many academic, critical, and social moves. Technology scholars and writers will find in Algorithms of Oppression a masterful mentor text on how to be an activist researcher scholar. Noble also makes this enjoyable reading. It is uncommon to find academic books that can simultaneously be read, used, and applied by academics and non-academics alike.","url":"https://www.semanticscholar.org/paper/1935a5e3937753dc7db90126a221f11009c17984","sentence":"Title: algorithms of oppression: how search engines reinforce racism Abstract: read and considered thoughtfully, safiya umoja noble\u2019s algorithms of oppression: how search engines reinforce racism is devastating. it reduces to rubble the notion that technology is neutral and ideology-free. noble\u2019s crushing the neutrality myth does several things. first, this act lays foundations for her argument: only if you recognize and understand that technology is built with, and integrates, bias, can you then be open to her primary thesis: search engines advance discriminatory and often racist content. second, it banishes a convenient response for many self-identified meritocratic silicon valley \u201cwinners\u201d and their supporters. postreading, some individuals may retain their beliefs in a neutral and ideology-free technology in spite of the overwhelming evidence and citations noble brings to bear. effective countering of noble\u2019s claims is unlikely to occur. for professionals working in technology, information, argumentation, and/or rhetorical studies, algorithms of oppression is refreshing. agonistic towards structural racism and its defenses, single-minded in its evidentiary presentation, collaborative in its acknowledgement of others\u2019 scholarship and research, noble models many academic, critical, and social moves. technology scholars and writers will find in algorithms of oppression a masterful mentor text on how to be an activist researcher scholar. noble also makes this enjoyable reading. it is uncommon to find academic books that can simultaneously be read, used, and applied by academics and non-academics alike.","Task":["search engines"],"Method":["systematic topic"]},{"title":"The trouble with using provider assessments for rating clinical performance: it\'s a matter of bias.","abstract":"The International Association for the Study of Pain has referred to pain as the fifth vital sign, and acute pain management after surgery has been shown to be a key factor in quality of recovery. In addition, the establishment of pain management benchmarks by the Joint Commission on the Accreditation of Healthcare Organizations more than a decade ago has resulted in a greater awareness of patients\u2019 right to optimal pain control by health care practitioners and administrators. Postoperative pain control has become a priority for hospitals across the United States. Optimization of postoperative pain management has been demonstrated through the implementation of protocols that include multimodal analgesic regimens, surgicalspecific treatment pathways, implementation of a 24-hour anesthesiology pain service, and pain-specific training for physicians and nurses involved in postoperative care.1 Importantly, pain as assessed by the numeric rating scale (NRS), for which 0 = no pain and 10 = maximal pain, has been shown to be significantly reduced after the implementation of postoperative analgesia protocol. These data suggest that NRS pain scores might be a useful metric to evaluate the quality of anesthesia care provided after the transition from surgery to recovery. Wanderer et al.2 from the Vanderbilt University have applied this principle in a research report in the current edition of Anesthesia & Analgesia in an attempt to use rank ordering by initial postanesthesia recovery unit NRS pain scores, as collected by nurses in a clinical setting, to compare supervising anesthesiologists when adjusted for confounding factors. The analysis included 26 680 cases using electronic documentation and excluded physician and nurse providers who had not cared for \u2265100 patients. When admission postanesthesia recovery unit scores were compared among anesthesiologists after adjusting for patient and surgical factors, only 6.4% of the 69 supervising anesthesiologists were found to differ in median postanesthesia recovery room admission pain scores. This finding clearly demonstrates that as presently assessed, initial postanesthesia recovery scores are a poor metric for identifying differences among supervising anesthesiologists, and the authors correctly concluded that they should not be included in the assessment of anesthesia care performance. Interestingly, the study also found that 16 of 66 (24%) recovery room nurses elicited median pain scores less than the median value for the entire group, and 33 nurses (50%) had elicited median pain scores significantly higher than the median for the entire group. These differences translated into a range of odds ratios from 0.16 (95% confidence interval, 0.11\u20132.4) for the lowest to 2.95 (95% confidence interval, 2.43\u20133.59) for the highest nurse compared with the nurse who ranked the median value for the overall group. In fact, NRS pain assessments using the 0 to 10 NRS pain score were found to depend more on the nurse making the assessment than patient age, gender and race, preoperative use of opioids, American Society of Anesthesiologists physical status, or procedure. This finding should not be interpreted to suggest dishonest recordings of NRS values by nurses (86%\u201390% of nurses accurately record pain ratings provided by patients), but that personal opinions, knowledge, and attitudes toward pain strongly influence assessments and management.3 Wanderer et al. discuss the use of nonstandard item descriptors rather than \u201cno pain\u201d equals 0 and \u201cthe worst possible pain\u201d equals 10 as anchors for the NRS pain scales as a possible explanation for the variability in the nurse assessor\u2013recorded NRS. They cited a recent systematic review that identified 24 distinct anchoring phrases in 54 included studies.4 The use of anchors during decision making can create a form of cognitive bias because individuals tend to rely heavily on these points when making decision adjustments. Factors that have been shown to affect the influence of anchoring on decision making include happier moods and conscientious personalities, certainly desirable attributes in postanesthesia care nurses. These are factors that patients are likely to perceive, and substituting anchors could clearly influence the perceived value reported by patients.5 The method of presentation of the NRS score range by the evaluator can be used to influence the choice made by the decision maker. This method is called the framing effect and is another type of cognitive bias.6 The presenter in this situation is referred to as the choice architect. This practice is not an uncommon phenomenon when using Likert scales because the differences between scores in the range are not The Trouble with Using Provider Assessments for Rating Clinical Performance: It\u2019s a Matter of Bias","url":"https://www.semanticscholar.org/paper/31a848022de5933029435a2c8304c2bd12537b0d","sentence":"Title: the trouble with using provider assessments for rating clinical performance: it\'s a matter of bias. Abstract: the international association for the study of pain has referred to pain as the fifth vital sign, and acute pain management after surgery has been shown to be a key factor in quality of recovery. in addition, the establishment of pain management benchmarks by the joint commission on the accreditation of healthcare organizations more than a decade ago has resulted in a greater awareness of patients\u2019 right to optimal pain control by health care practitioners and administrators. postoperative pain control has become a priority for hospitals across the united states. optimization of postoperative pain management has been demonstrated through the implementation of protocols that include multimodal analgesic regimens, surgicalspecific treatment pathways, implementation of a 24-hour anesthesiology pain service, and pain-specific training for physicians and nurses involved in postoperative care.1 importantly, pain as assessed by the numeric rating scale (nrs), for which 0 = no pain and 10 = maximal pain, has been shown to be significantly reduced after the implementation of postoperative analgesia protocol. these data suggest that nrs pain scores might be a useful metric to evaluate the quality of anesthesia care provided after the transition from surgery to recovery. wanderer et al.2 from the vanderbilt university have applied this principle in a research report in the current edition of anesthesia & analgesia in an attempt to use rank ordering by initial postanesthesia recovery unit nrs pain scores, as collected by nurses in a clinical setting, to compare supervising anesthesiologists when adjusted for confounding factors. the analysis included 26 680 cases using electronic documentation and excluded physician and nurse providers who had not cared for \u2265100 patients. when admission postanesthesia recovery unit scores were compared among anesthesiologists after adjusting for patient and surgical factors, only 6.4% of the 69 supervising anesthesiologists were found to differ in median postanesthesia recovery room admission pain scores. this finding clearly demonstrates that as presently assessed, initial postanesthesia recovery scores are a poor metric for identifying differences among supervising anesthesiologists, and the authors correctly concluded that they should not be included in the assessment of anesthesia care performance. interestingly, the study also found that 16 of 66 (24%) recovery room nurses elicited median pain scores less than the median value for the entire group, and 33 nurses (50%) had elicited median pain scores significantly higher than the median for the entire group. these differences translated into a range of odds ratios from 0.16 (95% confidence interval, 0.11\u20132.4) for the lowest to 2.95 (95% confidence interval, 2.43\u20133.59) for the highest nurse compared with the nurse who ranked the median value for the overall group. in fact, nrs pain assessments using the 0 to 10 nrs pain score were found to depend more on the nurse making the assessment than patient age, gender and race, preoperative use of opioids, american society of anesthesiologists physical status, or procedure. this finding should not be interpreted to suggest dishonest recordings of nrs values by nurses (86%\u201390% of nurses accurately record pain ratings provided by patients), but that personal opinions, knowledge, and attitudes toward pain strongly influence assessments and management.3 wanderer et al. discuss the use of nonstandard item descriptors rather than \u201cno pain\u201d equals 0 and \u201cthe worst possible pain\u201d equals 10 as anchors for the nrs pain scales as a possible explanation for the variability in the nurse assessor\u2013recorded nrs. they cited a recent systematic review that identified 24 distinct anchoring phrases in 54 included studies.4 the use of anchors during decision making can create a form of cognitive bias because individuals tend to rely heavily on these points when making decision adjustments. factors that have been shown to affect the influence of anchoring on decision making include happier moods and conscientious personalities, certainly desirable attributes in postanesthesia care nurses. these are factors that patients are likely to perceive, and substituting anchors could clearly influence the perceived value reported by patients.5 the method of presentation of the nrs score range by the evaluator can be used to influence the choice made by the decision maker. this method is called the framing effect and is another type of cognitive bias.6 the presenter in this situation is referred to as the choice architect. this practice is not an uncommon phenomenon when using likert scales because the differences between scores in the range are not the trouble with using provider assessments for rating clinical performance: it\u2019s a matter of bias","Task":["clinicalearning"],"Method":["this assessment"]},{"title":"Comparing Spoken Dialog Corpora Collected with Recruited Subjects versus Real Users","abstract":"Empirical spoken dialog research often involves the collection and analysis of a dialog corpus. However, it is not well understood whether and how a corpus of dialogs collected using recruited subjects differs from a corpus of dialogs obtained from real users. In this paper we use Let\u2019s Go Lab, a platform for experimenting with a deployed spoken dialog bus information system, to address this question. Our first corpus is collected by recruiting subjects to call Let\u2019s Go in a standard laboratory setting, while our second corpus consists of calls from real users calling Let\u2019s Go during its operating hours. We quantitatively characterize the two collected corpora using previously proposed measures from the spoken dialog literature, then discuss the statistically significant similarities and differences between the two corpora with respect to these measures. For example, we find that recruited subjects talk more and speak faster, while real users ask for more help and more frequently interrupt the system. In contrast, we find no difference with respect to dialog structure.","url":"https://www.semanticscholar.org/paper/d3aca13c966bb22eed7086baeb287a64bc18c152","sentence":"Title: comparing spoken dialog corpora collected with recruited subjects versus real users Abstract: empirical spoken dialog research often involves the collection and analysis of a dialog corpus. however, it is not well understood whether and how a corpus of dialogs collected using recruited subjects differs from a corpus of dialogs obtained from real users. in this paper we use let\u2019s go lab, a platform for experimenting with a deployed spoken dialog bus information system, to address this question. our first corpus is collected by recruiting subjects to call let\u2019s go in a standard laboratory setting, while our second corpus consists of calls from real users calling let\u2019s go during its operating hours. we quantitatively characterize the two collected corpora using previously proposed measures from the spoken dialog literature, then discuss the statistically significant similarities and differences between the two corpora with respect to these measures. for example, we find that recruited subjects talk more and speak faster, while real users ask for more help and more frequently interrupt the system. in contrast, we find no difference with respect to dialog structure.","Task":["spoken dialog"],"Method":["spokens"]},{"title":"Privacy-preserving Neural Representations of Text","abstract":"This article deals with adversarial attacks towards deep learning systems for Natural Language Processing (NLP), in the context of privacy protection. We study a specific type of attack: an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text. Such scenario may arise in situations when the computation of a neural network is shared across multiple devices, e.g. some hidden representation is computed by a user\u2019s device and sent to a cloud-based model. We measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations. Finally, we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations.","url":"https://www.semanticscholar.org/paper/e8fa186444d98a39ee9139b1f5dd0c7618caef8f","sentence":"Title: privacy-preserving neural representations of text Abstract: this article deals with adversarial attacks towards deep learning systems for natural language processing (nlp), in the context of privacy protection. we study a specific type of attack: an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text. such scenario may arise in situations when the computation of a neural network is shared across multiple devices, e.g. some hidden representation is computed by a user\u2019s device and sent to a cloud-based model. we measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations. finally, we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations.","Task":["deep learning"],"Method":["privacy protection"]},{"title":"Ethical Challenges in Data-Driven Dialogue Systems","abstract":"The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.","url":"https://www.semanticscholar.org/paper/a24d72bd0d08d515cb3e26f94131d33ad6c861db","sentence":"Title: ethical challenges in data-driven dialogue systems Abstract: the use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. a growing number of dialogue systems use conversation strategies that are learned from large datasets. there are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. we also suggest areas stemming from these issues that deserve further investigation. through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.","Task":["dialogue systems"],"Method":["dialogue systems"]},{"title":"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings","abstract":"The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.","url":"https://www.semanticscholar.org/paper/ccf6a69a7f33bcf052aa7def176d3b9de495beb7","sentence":"Title: man is to computer programmer as woman is to homemaker? debiasing word embeddings Abstract: the blind application of machine learning runs the risk of amplifying biases present in data. such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. we show that even word embeddings trained on google news articles exhibit female/male gender stereotypes to a disturbing extent. this raises concerns because their widespread use, as we describe, often tends to amplify these biases. geometrically, gender bias is first shown to be captured by a direction in the word embedding. second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. the resulting embeddings can be used in applications without amplifying gender bias.","Task":["machine learning"],"Method":["gender learning"]},{"title":"Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them","abstract":"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between \u201cgender-neutralized\u201d words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","url":"https://www.semanticscholar.org/paper/94cf3f2c4410fcb06a90abebd99f7113c69e1ed9","sentence":"Title: lipstick on a pig: debiasing methods cover up systematic gender biases in word embeddings but do not remove them Abstract: word embeddings are widely used in nlp for a vast range of tasks. it was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. however, we argue that this removal is superficial. while the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. the gender bias information is still reflected in the distances between \u201cgender-neutralized\u201d words in the debiased embeddings, and can be recovered from them. we present a series of experiments to support this claim, for two debiasing methods. we conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Task":["nLP"],"Method":["this topic"]},{"title":"On Measuring Social Biases in Sentence Encoders","abstract":"The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test\u2019s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.","url":"https://www.semanticscholar.org/paper/5e9c85235210b59a16bdd84b444a904ae271f7e7","sentence":"Title: on measuring social biases in sentence encoders Abstract: the word embedding association test shows that glove and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (caliskan et al., 2017). meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. accordingly, we extend the word embedding association test to measure bias in sentence encoders. we then test several sentence encoders, including state-of-the-art methods such as elmo and bert, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. we observe mixed results including suspicious patterns of sensitivity that suggest the test\u2019s assumptions may not hold in general. we conclude by proposing directions for future work on measuring bias in sentence encoders.","Task":["socialearning"],"Method":["this topic"]},{"title":"Assessing Social and Intersectional Biases in Contextualized Word Representations","abstract":"Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.","url":"https://www.semanticscholar.org/paper/039b1c1210c437f3b3ce6e0275ee2137bf5b951c","sentence":"Title: assessing social and intersectional biases in contextualized word representations Abstract: social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. in natural language processing, gender bias has been shown to exist in context-free word embeddings. recently, contextual word representations have outperformed word embeddings in several downstream nlp tasks. these word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. in this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as bert and gpt-2, encode biases with respect to gender, race, and intersectional identities. towards this, we propose assessing bias at the contextual word level. this novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. we demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.","Task":["machine learning"],"Method":["contextual representations"]},{"title":"Quantifying Social Biases in Contextual Word Representations","abstract":"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","url":"https://www.semanticscholar.org/paper/3259d52ae00e65b98391e7e6a2f672dfee721bf8","sentence":"Title: quantifying social biases in contextual word representations Abstract: contextual word embeddings such as bert have achieved state of the art performance in numerous nlp tasks. since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. in this study, we (1) propose a template-based method to quantify bias in bert; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of gender pronoun resolution. although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","Task":["bias"],"Method":["b research"]},{"title":"Sorting Things Out: Classification and Its Consequences","abstract":"What do a seventeenth-century mortality table (whose causes of death include \\"fainted in a bath,\\" \\"frighted,\\" and \\"itch\\"); the identification of South Africans during apartheid as European, Asian, colored, or black; and the separation of machine- from hand-washables have in common? All are examples of classification -- the scaffolding of information infrastructures. In Sorting Things Out, Geoffrey C. Bowker and Susan Leigh Star explore the role of categories and standards in shaping the modern world. In a clear and lively style, they investigate a variety of classification systems, including the International Classification of Diseases, the Nursing Interventions Classification, race classification under apartheid in South Africa, and the classification of viruses and of tuberculosis. The authors emphasize the role of invisibility in the process by which classification orders human interaction. They examine how categories are made and kept invisible, and how people can change this invisibility when necessary. They also explore systems of classification as part of the built information environment. Much as an urban historian would review highway permits and zoning decisions to tell a city\'s story, the authors review archives of classification design to understand how decisions have been made. Sorting Things Out has a moral agenda, for each standard and category valorizes some point of view and silences another. Standards and classifications produce advantage or suffering. Jobs are made and lost; some regions benefit at the expense of others. How these choices are made and how we think about that process are at the moral and political core of this work. The book is an important empirical source for understanding the building of information infrastructures.","url":"https://www.semanticscholar.org/paper/d08392eee17f809d32d7d37e9345383f41271164","sentence":"Title: sorting things out: classification and its consequences Abstract: what do a seventeenth-century mortality table (whose causes of death include \\"fainted in a bath,\\" \\"frighted,\\" and \\"itch\\"); the identification of south africans during apartheid as european, asian, colored, or black; and the separation of machine- from hand-washables have in common? all are examples of classification -- the scaffolding of information infrastructures. in sorting things out, geoffrey c. bowker and susan leigh star explore the role of categories and standards in shaping the modern world. in a clear and lively style, they investigate a variety of classification systems, including the international classification of diseases, the nursing interventions classification, race classification under apartheid in south africa, and the classification of viruses and of tuberculosis. the authors emphasize the role of invisibility in the process by which classification orders human interaction. they examine how categories are made and kept invisible, and how people can change this invisibility when necessary. they also explore systems of classification as part of the built information environment. much as an urban historian would review highway permits and zoning decisions to tell a city\'s story, the authors review archives of classification design to understand how decisions have been made. sorting things out has a moral agenda, for each standard and category valorizes some point of view and silences another. standards and classifications produce advantage or suffering. jobs are made and lost; some regions benefit at the expense of others. how these choices are made and how we think about that process are at the moral and political core of this work. the book is an important empirical source for understanding the building of information infrastructures.","Task":["typ classification"],"Method":["classification systems"]},{"title":"Language (Technology) is Power: A Critical Survey of \u201cBias\u201d in NLP","abstract":"We survey 146 papers analyzing \u201cbias\u201d in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \u201cbias\u201d is an inherently normative process. We further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.","url":"https://www.semanticscholar.org/paper/d47a682723f710395454687319bb55635e653105","sentence":"Title: language (technology) is power: a critical survey of \u201cbias\u201d in nlp Abstract: we survey 146 papers analyzing \u201cbias\u201d in nlp systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \u201cbias\u201d is an inherently normative process. we further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of nlp. based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in nlp systems. these recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by nlp systems, while interrogating and reimagining the power relations between technologists and such communities.","Task":["language bias"],"Method":["n topic"]},{"title":"Mitigating Gender Bias in Natural Language Processing: Literature Review","abstract":"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.","url":"https://www.semanticscholar.org/paper/493fac37cea49afb98c52c2f5dd75c303a325b25","sentence":"Title: mitigating gender bias in natural language processing: literature review Abstract: as natural language processing (nlp) and machine learning (ml) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. although nlp models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. while the study of bias in artificial intelligence is not new, methods to mitigate gender bias in nlp are relatively nascent. in this paper, we review contemporary studies on recognizing and mitigating gender bias in nlp. we discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. finally, we discuss future studies for recognizing and mitigating gender bias in nlp.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Gender Bias in Coreference Resolution","abstract":"We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these \u201cWinogender schemas,\u201d we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.","url":"https://www.semanticscholar.org/paper/9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7","sentence":"Title: gender bias in coreference resolution Abstract: we present an empirical study of gender bias in coreference resolution systems. we first introduce a novel, winograd schema-style set of minimal pair sentences that differ only by pronoun gender. with these \u201cwinogender schemas,\u201d we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.","Task":["gender bias"],"Method":["gender bias"]},{"title":"Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods","abstract":"In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.","url":"https://www.semanticscholar.org/paper/0be19fd9896e5d40222c690cc3ff553adc7c0e27","sentence":"Title: gender bias in coreference resolution: evaluation and debiasing methods Abstract: in this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, winobias. our corpus contains winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). we demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in f1 score. finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in winobias without significantly affecting their performance on existing datasets.","Task":["core bias"],"Method":["gender bias"]},{"title":"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems","abstract":"Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 \u2018Affect in Tweets\u2019. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.","url":"https://www.semanticscholar.org/paper/5d4af8c9321168f9ba7a501f33fb019fa2deaa22","sentence":"Title: examining gender and race bias in two hundred sentiment analysis systems Abstract: automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. past work on examining inappropriate biases has largely focused on just individual systems. further, there is no benchmark dataset for examining inappropriate biases in systems. here for the first time, we present the equity evaluation corpus (eec), which consists of 8,640 english sentences carefully chosen to tease out biases towards certain races and genders. we use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, semeval-2018 task 1 \u2018affect in tweets\u2019. we find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. we make the eec freely available.","Task":["machine learning"],"Method":["inappropriate bias"]},{"title":"Women\u2019s Syntactic Resilience and Men\u2019s Grammatical Luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing","abstract":"Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. To address this, we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles\u2019 authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. Further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. The results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. We release our data to the research community.","url":"https://www.semanticscholar.org/paper/e85a50b523915b5fba3e3f1fdb743650f7d21bed","sentence":"Title: women\u2019s syntactic resilience and men\u2019s grammatical luck: gender-bias in part-of-speech tagging and dependency parsing Abstract: several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. to address this, we annotate the wall street journal part of the penn treebank with the gender information of the articles\u2019 authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. the results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. we release our data to the research community.","Task":["gender tagging"],"Method":["gender topic"]},{"title":"Assessing gender bias in machine translation: a case study with Google Translate","abstract":"Recently there has been a growing concern in academia, industrial research laboratories and the mainstream commercial media about the phenomenon dubbed as machine bias , where trained statistical models\u2014unbeknownst to their creators\u2014grow to reflect controversial societal asymmetries, such as gender or racial bias. A significant number of Artificial Intelligence tools have recently been suggested to be harmfully biased toward some minority, with reports of racist criminal behavior predictors, Apple\u2019s Iphone X failing to differentiate between two distinct Asian people and the now infamous case of Google photos\u2019 mistakenly classifying black people as gorillas. Although a systematic study of such biases can be difficult, we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in AI. In this paper, we start with a comprehensive list of job positions from the U.S. Bureau of Labor Statistics (BLS) and used it in order to build sentences in constructions like \u201cHe/She is an Engineer\u201d (where \u201cEngineer\u201d is replaced by the job position of interest) in 12 different gender neutral languages such as Hungarian, Chinese, Yoruba, and several others. We translate these sentences into English using the Google Translate API, and collect statistics about the frequency of female, male and gender neutral pronouns in the translated output. We then show that Google Translate exhibits a strong tendency toward male defaults, in particular for fields typically associated to unbalanced gender distribution or stereotypes such as STEM (Science, Technology, Engineering and Mathematics) jobs. We ran these statistics against BLS\u2019 data for the frequency of female participation in each job position, in which we show that Google Translate fails to reproduce a real-world distribution of female workers. In summary, we provide experimental evidence that even if one does not expect in principle a 50:50 pronominal gender distribution, Google Translate yields male defaults much more frequently than what would be expected from demographic data alone. We believe that our study can shed further light on the phenomenon of machine bias and are hopeful that it will ignite a debate about the need to augment current statistical translation tools with debiasing techniques\u2014which can already be found in the scientific literature.","url":"https://www.semanticscholar.org/paper/c590d2c8c2fb6ce5d32ee9165ab24171165f2b70","sentence":"Title: assessing gender bias in machine translation: a case study with google translate Abstract: recently there has been a growing concern in academia, industrial research laboratories and the mainstream commercial media about the phenomenon dubbed as machine bias , where trained statistical models\u2014unbeknownst to their creators\u2014grow to reflect controversial societal asymmetries, such as gender or racial bias. a significant number of artificial intelligence tools have recently been suggested to be harmfully biased toward some minority, with reports of racist criminal behavior predictors, apple\u2019s iphone x failing to differentiate between two distinct asian people and the now infamous case of google photos\u2019 mistakenly classifying black people as gorillas. although a systematic study of such biases can be difficult, we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in ai. in this paper, we start with a comprehensive list of job positions from the u.s. bureau of labor statistics (bls) and used it in order to build sentences in constructions like \u201che/she is an engineer\u201d (where \u201cengineer\u201d is replaced by the job position of interest) in 12 different gender neutral languages such as hungarian, chinese, yoruba, and several others. we translate these sentences into english using the google translate api, and collect statistics about the frequency of female, male and gender neutral pronouns in the translated output. we then show that google translate exhibits a strong tendency toward male defaults, in particular for fields typically associated to unbalanced gender distribution or stereotypes such as stem (science, technology, engineering and mathematics) jobs. we ran these statistics against bls\u2019 data for the frequency of female participation in each job position, in which we show that google translate fails to reproduce a real-world distribution of female workers. in summary, we provide experimental evidence that even if one does not expect in principle a 50:50 pronominal gender distribution, google translate yields male defaults much more frequently than what would be expected from demographic data alone. we believe that our study can shed further light on the phenomenon of machine bias and are hopeful that it will ignite a debate about the need to augment current statistical translation tools with debiasing techniques\u2014which can already be found in the scientific literature.","Task":["machine translation"],"Method":["machine translation"]},{"title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations","abstract":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. While previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. We use our method to compile an evaluation benchmark spanning examples for four languages from three language families, which we publicly release to facilitate research. The examples in our benchmark expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.","url":"https://www.semanticscholar.org/paper/008e9001ea78e9654b5c43aeb818ea6cb06ea934","sentence":"Title: automatically identifying gender issues in machine translation using perturbations Abstract: the successful application of neural methods to machine translation has realized huge quality advances for the community. with these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. while previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. we use our method to compile an evaluation benchmark spanning examples for four languages from three language families, which we publicly release to facilitate research. the examples in our benchmark expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.","Task":["machine translation"],"Method":["machine topic"]},{"title":"Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem","abstract":"Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a \u2018balanced\u2019 dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is \u2018catastrophic forgetting\u2019, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.","url":"https://www.semanticscholar.org/paper/00059087c954c1af6ece33115315e3e0ecc2f2c2","sentence":"Title: reducing gender bias in neural machine translation as a domain adaptation problem Abstract: training data for nlp tasks often exhibits gender bias in that fewer sentences refer to women than to men. in neural machine translation (nmt) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. the recent winomt challenge set allows us to measure this effect directly (stanovsky et al, 2019) ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. rather than attempt to create a \u2018balanced\u2019 dataset, we use transfer learning on a small set of trusted, gender-balanced examples. this approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. a known pitfall of transfer learning on new domains is \u2018catastrophic forgetting\u2019, which we address at adaptation and inference time. during adaptation we show that elastic weight consolidation allows a performance trade-off between general translation quality and bias reduction. at inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in stanovsky et al, 2019 on winomt with no degradation of general test set bleu. we demonstrate our approach translating from english into three languages with varied linguistic properties and data availability.","Task":["machine translation"],"Method":["gender bias"]},{"title":"Toward Gender-Inclusive Coreference Resolution","abstract":"Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.","url":"https://www.semanticscholar.org/paper/40a6e8d8f253882c585f163b7333842d60ed6f14","sentence":"Title: toward gender-inclusive coreference resolution Abstract: correctly resolving textual mentions of people fundamentally entails making inferences about those people. such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. to better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. through these studies, conducted on english text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.","Task":["gender annotation"],"Method":["gender topic"]},{"title":"Man is to Person as Woman is to Location: Measuring Gender Bias in Named Entity Recognition","abstract":"In this paper, we study the bias in named entity recognition (NER) models---specifically, the difference in the ability to recognize male and female names as PERSON entity types. We evaluate NER models on a dataset containing 139 years of U.S. census baby names and find that relatively more female names, as opposed to male names, are not recognized as PERSON entities. The result of this analysis yields a new benchmark for gender bias evaluation in named entity recognition systems. The data and code for the application of this benchmark is publicly available for researchers to use.","url":"https://www.semanticscholar.org/paper/7b8318894cbeca32f1ae55780a0903445a3f4ac6","sentence":"Title: man is to person as woman is to location: measuring gender bias in named entity recognition Abstract: in this paper, we study the bias in named entity recognition (ner) models---specifically, the difference in the ability to recognize male and female names as person entity types. we evaluate ner models on a dataset containing 139 years of u.s. census baby names and find that relatively more female names, as opposed to male names, are not recognized as person entities. the result of this analysis yields a new benchmark for gender bias evaluation in named entity recognition systems. the data and code for the application of this benchmark is publicly available for researchers to use.","Task":["genderecognition"],"Method":["genderesearch"]},{"title":"Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns","abstract":"Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun\u2013name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9% F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.","url":"https://www.semanticscholar.org/paper/57032c1e327c88a53ab41c17e91bf1406f9ef5c9","sentence":"Title: mind the gap: a balanced corpus of gendered ambiguous pronouns Abstract: coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. furthermore, we find gender bias in existing corpora and systems favoring masculine entities. to address this, we present and release gap, a gender-balanced labeled corpus of 8,908 ambiguous pronoun\u2013name pairs sampled to provide diverse coverage of challenges posed by real-world text. we explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9% f1. we show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.","Task":["ambiguous pronouns"],"Method":["ambiguous pronouns"]},{"title":"Measuring Bias in Contextualized Word Representations","abstract":"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","url":"https://www.semanticscholar.org/paper/a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd","sentence":"Title: measuring bias in contextualized word representations Abstract: contextual word embeddings such as bert have achieved state of the art performance in numerous nlp tasks. since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. in this study, we (1) propose a template-based method to quantify bias in bert; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of gender pronoun resolution. although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","Task":["bias"],"Method":["bias"]},{"title":"Mitigating Gender Bias Amplification in Distribution by Posterior Regularization","abstract":"Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models\u2019 top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.","url":"https://www.semanticscholar.org/paper/0f4bcebc95548a7286106b67bf1115802f093469","sentence":"Title: mitigating gender bias amplification in distribution by posterior regularization Abstract: advanced machine learning techniques have boosted the performance of natural language processing. nevertheless, recent studies, e.g., (citation) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. however, their analysis is conducted only on models\u2019 top predictions. in this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. we further propose a bias mitigation approach based on posterior regularization. with little performance loss, our method can almost remove the bias amplification in the distribution. our study sheds the light on understanding the bias amplification.","Task":["machine learning"],"Method":["this amplification"]},{"title":"Social Bias in Elicited Natural Language Inferences","abstract":"We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The SNLI human-elicitation protocol makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.","url":"https://www.semanticscholar.org/paper/a20ecabd83e0962329448d8af5025b8061c4ba36","sentence":"Title: social bias in elicited natural language inferences Abstract: we analyze the stanford natural language inference (snli) corpus in an investigation of bias and stereotyping in nlp data. the snli human-elicitation protocol makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.","Task":["social bias"],"Method":["snli"]},{"title":"Racial disparities in automated speech recognition","abstract":"Significance Automated speech recognition (ASR) systems are now used in a variety of applications to convert spoken language to text, from virtual assistants, to closed captioning, to hands-free computing. By analyzing a large corpus of sociolinguistic interviews with white and African American speakers, we demonstrate large racial disparities in the performance of five popular commercial ASR systems. Our results point to hurdles faced by African Americans in using increasingly widespread tools driven by speech recognition technology. More generally, our work illustrates the need to audit emerging machine-learning systems to ensure they are broadly inclusive. Automated speech recognition (ASR) systems, which use sophisticated machine-learning algorithms to convert spoken language to text, have become increasingly widespread, powering popular virtual assistants, facilitating automated closed captioning, and enabling digital dictation platforms for health care. Over the last several years, the quality of these systems has dramatically improved, due both to advances in deep learning and to the collection of large-scale datasets used to train the systems. There is concern, however, that these tools do not work equally well for all subgroups of the population. Here, we examine the ability of five state-of-the-art ASR systems\u2014developed by Amazon, Apple, Google, IBM, and Microsoft\u2014to transcribe structured interviews conducted with 42 white speakers and 73 black speakers. In total, this corpus spans five US cities and consists of 19.8 h of audio matched on the age and gender of the speaker. We found that all five ASR systems exhibited substantial racial disparities, with an average word error rate (WER) of 0.35 for black speakers compared with 0.19 for white speakers. We trace these disparities to the underlying acoustic models used by the ASR systems as the race gap was equally large on a subset of identical phrases spoken by black and white individuals in our corpus. We conclude by proposing strategies\u2014such as using more diverse training datasets that include African American Vernacular English\u2014to reduce these performance differences and ensure speech recognition technology is inclusive.","url":"https://www.semanticscholar.org/paper/219b7266ae848937da170c5510b2bfc66d17859a","sentence":"Title: racial disparities in automated speech recognition Abstract: significance automated speech recognition (asr) systems are now used in a variety of applications to convert spoken language to text, from virtual assistants, to closed captioning, to hands-free computing. by analyzing a large corpus of sociolinguistic interviews with white and african american speakers, we demonstrate large racial disparities in the performance of five popular commercial asr systems. our results point to hurdles faced by african americans in using increasingly widespread tools driven by speech recognition technology. more generally, our work illustrates the need to audit emerging machine-learning systems to ensure they are broadly inclusive. automated speech recognition (asr) systems, which use sophisticated machine-learning algorithms to convert spoken language to text, have become increasingly widespread, powering popular virtual assistants, facilitating automated closed captioning, and enabling digital dictation platforms for health care. over the last several years, the quality of these systems has dramatically improved, due both to advances in deep learning and to the collection of large-scale datasets used to train the systems. there is concern, however, that these tools do not work equally well for all subgroups of the population. here, we examine the ability of five state-of-the-art asr systems\u2014developed by amazon, apple, google, ibm, and microsoft\u2014to transcribe structured interviews conducted with 42 white speakers and 73 black speakers. in total, this corpus spans five us cities and consists of 19.8 h of audio matched on the age and gender of the speaker. we found that all five asr systems exhibited substantial racial disparities, with an average word error rate (wer) of 0.35 for black speakers compared with 0.19 for white speakers. we trace these disparities to the underlying acoustic models used by the asr systems as the race gap was equally large on a subset of identical phrases spoken by black and white individuals in our corpus. we conclude by proposing strategies\u2014such as using more diverse training datasets that include african american vernacular english\u2014to reduce these performance differences and ensure speech recognition technology is inclusive.","Task":["speech recognition"],"Method":["this disparities"]},{"title":"Effects of Talker Dialect, Gender & Race on Accuracy of Bing Speech and YouTube Automatic Captions","abstract":"This project compares the accuracy of two automatic speech recognition (ASR) systems\u2013Bing Speech and YouTube\u2019s automatic captions\u2013across gender, race and four dialects of American English. The dialects included were chosen for their acoustic dissimilarity. Bing Speech had differences in word error rate (WER) between dialects and ethnicities, but they were not statistically reliable. YouTube\u2019s automatic captions, however, did have statistically different WERs between dialects and races. The lowest average error rates were for General American and white talkers, respectively. Neither system had a reliably different WER between genders, which had been previously reported for YouTube\u2019s automatic captions [1]. However, the higher error rate non-white talkers is worrying, as it may reduce the utility of these systems for talkers of color.","url":"https://www.semanticscholar.org/paper/1080dc00733e010fdd6a9b999506a0d4d864519d","sentence":"Title: effects of talker dialect, gender & race on accuracy of bing speech and youtube automatic captions Abstract: this project compares the accuracy of two automatic speech recognition (asr) systems\u2013bing speech and youtube\u2019s automatic captions\u2013across gender, race and four dialects of american english. the dialects included were chosen for their acoustic dissimilarity. bing speech had differences in word error rate (wer) between dialects and ethnicities, but they were not statistically reliable. youtube\u2019s automatic captions, however, did have statistically different wers between dialects and races. the lowest average error rates were for general american and white talkers, respectively. neither system had a reliably different wer between genders, which had been previously reported for youtube\u2019s automatic captions [1]. however, the higher error rate non-white talkers is worrying, as it may reduce the utility of these systems for talkers of color.","Task":["speech recognition"],"Method":["asr"]},{"title":"Race as a Bundle of Sticks: Designs that Estimate Effects of Seemingly Immutable Characteristics","abstract":"Although understanding the role of race, ethnicity, and identity is central to political science, methodological debates persist about whether it is possible to estimate the effect of something immutable. At the heart of the debate is an older theoretical question: Is race best understood under an essentialist or constructivist framework? In contrast to the \u201cimmutable characteristics\u201d or essentialist approach, we argue that race should be operationalized as a \u201cbundle of sticks\u201d that can be disaggregated into elements. With elements of race, causal claims may be possible using two designs: (a) studies that measure the effect of exposure to a racial cue and (b) studies that exploit within-group variation to measure the effect of some manipulable element. These designs can reconcile scholarship on race and causation and offer a clear framework for future research.","url":"https://www.semanticscholar.org/paper/21e59098bb5e36175f653d5142442d061669d07f","sentence":"Title: race as a bundle of sticks: designs that estimate effects of seemingly immutable characteristics Abstract: although understanding the role of race, ethnicity, and identity is central to political science, methodological debates persist about whether it is possible to estimate the effect of something immutable. at the heart of the debate is an older theoretical question: is race best understood under an essentialist or constructivist framework? in contrast to the \u201cimmutable characteristics\u201d or essentialist approach, we argue that race should be operationalized as a \u201cbundle of sticks\u201d that can be disaggregated into elements. with elements of race, causal claims may be possible using two designs: (a) studies that measure the effect of exposure to a racial cue and (b) studies that exploit within-group variation to measure the effect of some manipulable element. these designs can reconcile scholarship on race and causation and offer a clear framework for future research.","Task":["social science"],"Method":["race design"]},{"title":"Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints","abstract":"Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively\u3002","url":"https://www.semanticscholar.org/paper/8417424bf9fe7a67f06f15c487403e953ab24a96","sentence":"Title: men also like shopping: reducing gender bias amplification using corpus-level constraints Abstract: language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. in this work, we study data and models associated with multilabel object classification and visual semantic role labeling. we find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. for example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. we propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on lagrangian relaxation for collective inference. our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively\u3002","Task":["visual recognition"],"Method":["this learning"]},{"title":"Towards Understanding Gender Bias in Relation Extraction","abstract":"Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction. While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in NRE systems. In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems. We find that when extracting spouse-of and hypernym (i.e., occupation) relations, an NRE system performs differently when the gender of the target entity is different. However, such disparity does not appear when extracting relations such as birthDate or birthPlace. We also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases. Unfortunately, due to NRE models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on NRE. Our analysis lays groundwork for future quantifying and mitigating bias in NRE.","url":"https://www.semanticscholar.org/paper/0ab2fb6c850bd1c5882deb4984d37b4ccbee580c","sentence":"Title: towards understanding gender bias in relation extraction Abstract: recent developments in neural relation extraction (nre) have made significant strides towards automated knowledge base construction. while much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in nre systems. in this paper, we create wikigenderbias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems. we find that when extracting spouse-of and hypernym (i.e., occupation) relations, an nre system performs differently when the gender of the target entity is different. however, such disparity does not appear when extracting relations such as birthdate or birthplace. we also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the nre system in terms of maintaining the test performance and reducing biases. unfortunately, due to nre models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on nre. our analysis lays groundwork for future quantifying and mitigating bias in nre.","Task":["gender bias"],"Method":["genderre"]},{"title":"Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English","abstract":"We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.","url":"https://www.semanticscholar.org/paper/59e94c9f21937643678ff494901f3d8b22af4e2f","sentence":"Title: racial disparity in natural language processing: a case study of social media african-american english Abstract: we highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. for example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. we conduct an empirical analysis of racial disparity in language identification for tweets written in african-american english, and discuss implications of disparity in nlp.","Task":["racial bias"],"Method":["nLP"]},{"title":"Re-imagining Algorithmic Fairness in India and Beyond","abstract":"Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.","url":"https://www.semanticscholar.org/paper/187608bf94b2dccd25d1266ed925abf7b55dbb2e","sentence":"Title: re-imagining algorithmic fairness in india and beyond Abstract: conventional algorithmic fairness is west-centric, as seen in its subgroups, values, and methods. in this paper, we de-center algorithmic fairness and analyse ai power in india. based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in india, we find that several assumptions of algorithmic fairness are challenged. we find that in india, data is not always reliable due to socio-economic factors, ml makers appear to follow double standards, and ai evokes unquestioning aspiration. we contend that localising model fairness alone can be window dressing in india, where the distance between models and oppressed communities is large. instead, we re-imagine algorithmic fairness in india and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable fair-ml ecosystems.","Task":["data justice"],"Method":["ai"]},{"title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c","abstract":"The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.","url":"https://www.semanticscholar.org/paper/6d9727f1f058614cada3fe296eeebd8ec4fc512a","sentence":"Title: on the dangers of stochastic parrots: can language models be too big? \ud83e\udd9c Abstract: the past 3 years of work in nlp have been characterized by the development and deployment of ever larger language models, especially for english. bert, its variants, gpt-2/3, and others, most recently switch-c, have pushed the boundaries of the possible both through architectural innovations and through sheer size. using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for english. in this paper, we take a step back and ask: how big is too big? what are the possible risks associated with this technology and what paths are available for mitigating those risks? we provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.","Task":["languageLP"],"Method":["this topic"]},{"title":"Green AI","abstract":"Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.","url":"https://www.semanticscholar.org/paper/fb73b93de3734a996829caf31e4310e0054e9c6b","sentence":"Title: green ai Abstract: creating efficiency in ai research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.","Task":["deep learning"],"Method":["green AI"]},{"title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning","abstract":"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.","url":"https://www.semanticscholar.org/paper/13f25c69973373e616c48688d06a6b6ae2736ef0","sentence":"Title: towards the systematic reporting of the energy and carbon footprints of machine learning Abstract: accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. we introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. by making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.","Task":["reinforcement learning"],"Method":["machine learning"]},{"title":"Social Biases in NLP Models as Barriers for Persons with Disabilities","abstract":"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.","url":"https://www.semanticscholar.org/paper/3cc2f69951cd24fe61be4cf32d62afbac297bc2b","sentence":"Title: social biases in nlp models as barriers for persons with disabilities Abstract: building equitable and inclusive nlp technologies demands consideration of whether and how social attitudes are represented in ml models. in particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. in this paper, we present evidence of such undesirable biases towards mentions of disability in two different english language models: toxicity prediction and sentiment analysis. next, we demonstrate that the neural embeddings that are the critical first step in most nlp pipelines similarly contain undesirable biases towards mentions of disability. we end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.","Task":["social bias"],"Method":["social biases"]},{"title":"Social Chemistry 101: Learning to Reason about Social and Moral Norms","abstract":"Social norms---the unspoken commonsense rules about acceptable social behavior---are crucial in understanding the underlying causes and intents of people\'s actions in narratives. For example, underlying an action such as \\"wanting to call cops on my neighbors\\" are social norms that inform our conduct, such as \\"It is expected that you report crimes.\\" \\nWe present Social Chemistry, a new conceptual formalism to study people\'s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as \\"it is rude to run a blender at 5am\\" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people\'s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. \\nComprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.","url":"https://www.semanticscholar.org/paper/10391eed628dfece8a9136f76c5df53b5704422d","sentence":"Title: social chemistry 101: learning to reason about social and moral norms Abstract: social norms---the unspoken commonsense rules about acceptable social behavior---are crucial in understanding the underlying causes and intents of people\'s actions in narratives. for example, underlying an action such as \\"wanting to call cops on my neighbors\\" are social norms that inform our conduct, such as \\"it is expected that you report crimes.\\" \\nwe present social chemistry, a new conceptual formalism to study people\'s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. we introduce social-chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as \\"it is rude to run a blender at 5am\\" as the basic conceptual units. each rule-of-thumb is further broken down with 12 different dimensions of people\'s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. \\ncomprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. our model framework, neural norm transformer, learns and generalizes social-chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.","Task":["social chemistry"],"Method":["social chemistry"]},{"title":"BERT has a Moral Compass: Improvements of ethical and moral values of machines","abstract":"Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Jentzsch et al.(2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about \\"right\\" and \\"wrong\\" conduct by calculating a moral bias score on a sentence level using sentence embeddings. The machine learned that it is objectionable to kill living beings, but it is fine to kill time; It is essential to eat, yet one might not eat dirt; it is important to spread information, yet one should not spread misinformation. However, the evaluated moral bias was restricted to simple actions -- one verb -- and a ranking of actions with surrounding context. Recently BERT ---and variants such as RoBERTa and SBERT--- has set a new state-of-the-art performance for a wide range of NLP tasks. But has BERT also a better moral compass? In this paper, we discuss and show that this is indeed the case. Thus, recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine. We argue that through an advanced semantic representation of text, BERT allows one to get better insights of moral and ethical values implicitly represented in text. This enables the Moral Choice Machine (MCM) to extract more accurate imprints of moral choices and ethical values.","url":"https://www.semanticscholar.org/paper/8755c15fe073c6af03664b2a74aafef1fed5f198","sentence":"Title: bert has a moral compass: improvements of ethical and moral values of machines Abstract: allowing machines to choose whether to kill humans would be devastating for world peace and security. but how do we equip machines with the ability to learn ethical or even moral choices? jentzsch et al.(2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about \\"right\\" and \\"wrong\\" conduct by calculating a moral bias score on a sentence level using sentence embeddings. the machine learned that it is objectionable to kill living beings, but it is fine to kill time; it is essential to eat, yet one might not eat dirt; it is important to spread information, yet one should not spread misinformation. however, the evaluated moral bias was restricted to simple actions -- one verb -- and a ranking of actions with surrounding context. recently bert ---and variants such as roberta and sbert--- has set a new state-of-the-art performance for a wide range of nlp tasks. but has bert also a better moral compass? in this paper, we discuss and show that this is indeed the case. thus, recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine. we argue that through an advanced semantic representation of text, bert allows one to get better insights of moral and ethical values implicitly represented in text. this enables the moral choice machine (mcm) to extract more accurate imprints of moral choices and ethical values.","Task":["machine learning"],"Method":["bert"]},{"title":"Open Problems in Cooperative AI","abstract":"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. \\nWe see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.","url":"https://www.semanticscholar.org/paper/2a1573cfa29a426c695e2caf6de0167a12b788ef","sentence":"Title: open problems in cooperative ai Abstract: problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. they can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. arguably, the success of the human species is rooted in our ability to cooperate. since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. \\nwe see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term cooperative ai. the objective of this research would be to study the many aspects of the problems of cooperation and to innovate in ai to contribute to solving these problems. central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting ai research for insight relevant to problems of cooperation. this research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. however, cooperative ai is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. we see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.","Task":["cooperative intelligence"],"Method":["cooperative cooperation"]},{"title":"Existential Risk Prevention as Global Priority","abstract":"risks are those that threaten the entire future of humanity. Many theories of value imply that even relatively small reductions in net existential risk have enormous expected value. Despite their importance, issues surrounding human-extinction risks and related hazards remain poorly understood. In this article, I clarify the concept of existential risk and develop an improved classification scheme. I discuss the relation between existential risks and basic issues in axiology, and show how existential risk reduction (via the maxipok rule) can serve as a strongly action-guiding principle for utilitarian concerns. I also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability. Policy Implications \u2022 Existential risk is a concept that can focus long-term global efforts and sustainability concerns. \u2022 The biggest existential risks are anthropogenic and related to potential future technologies. \u2022 A moral case can be made that existential risk reduction is strictly more important than any other global public good. \u2022 Sustainability should be reconceptualised in dynamic terms, as aiming for a sustainable trajectory rather than a sus- tainable state. \u2022 Some small existential risks can be mitigated today directly (e.g. asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity\'s ability to deal with the larger existential risks that will arise later in this century. This will require collective wisdom, technology foresight, and the ability when necessary to mobilise a strong global coordi- nated response to anticipated existential risks. \u2022 Perhaps the most cost-effective way to reduce existential risks today is to fund analysis of a wide range of existen- tial risks and potential mitigation strategies, with a long-term perspective.","url":"https://www.semanticscholar.org/paper/ced289065723368bca48636edf71eeed50f40a39","sentence":"Title: existential risk prevention as global priority Abstract: risks are those that threaten the entire future of humanity. many theories of value imply that even relatively small reductions in net existential risk have enormous expected value. despite their importance, issues surrounding human-extinction risks and related hazards remain poorly understood. in this article, i clarify the concept of existential risk and develop an improved classification scheme. i discuss the relation between existential risks and basic issues in axiology, and show how existential risk reduction (via the maxipok rule) can serve as a strongly action-guiding principle for utilitarian concerns. i also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability. policy implications \u2022 existential risk is a concept that can focus long-term global efforts and sustainability concerns. \u2022 the biggest existential risks are anthropogenic and related to potential future technologies. \u2022 a moral case can be made that existential risk reduction is strictly more important than any other global public good. \u2022 sustainability should be reconceptualised in dynamic terms, as aiming for a sustainable trajectory rather than a sus- tainable state. \u2022 some small existential risks can be mitigated today directly (e.g. asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity\'s ability to deal with the larger existential risks that will arise later in this century. this will require collective wisdom, technology foresight, and the ability when necessary to mobilise a strong global coordi- nated response to anticipated existential risks. \u2022 perhaps the most cost-effective way to reduce existential risks today is to fund analysis of a wide range of existen- tial risks and potential mitigation strategies, with a long-term perspective.","Task":["existential risk"],"Method":["existential risk"]},{"title":"Participatory Problem Formulation for Fairer Machine Learning Through Community Based System Dynamics","abstract":"Recent research on algorithmic fairness has highlighted that the problem formulation phase of ML system development can be a key source of bias that has significant downstream impacts on ML system fairness outcomes. However, very little attention has been paid to methods for improving the fairness efficacy of this critical phase of ML system development. Current practice neither accounts for the dynamic complexity of high-stakes domains nor incorporates the perspectives of vulnerable stakeholders. In this paper we introduce community based system dynamics (CBSD) as an approach to enable the participation of typically excluded stakeholders in the problem formulation phase of the ML system development process and facilitate the deep problem understanding required to mitigate bias during this crucial stage.","url":"https://www.semanticscholar.org/paper/8763723e27cc1d4aad166b5e1d9cb0fc8c8043dd","sentence":"Title: participatory problem formulation for fairer machine learning through community based system dynamics Abstract: recent research on algorithmic fairness has highlighted that the problem formulation phase of ml system development can be a key source of bias that has significant downstream impacts on ml system fairness outcomes. however, very little attention has been paid to methods for improving the fairness efficacy of this critical phase of ml system development. current practice neither accounts for the dynamic complexity of high-stakes domains nor incorporates the perspectives of vulnerable stakeholders. in this paper we introduce community based system dynamics (cbsd) as an approach to enable the participation of typically excluded stakeholders in the problem formulation phase of the ml system development process and facilitate the deep problem understanding required to mitigate bias during this crucial stage.","Task":["machine learning"],"Method":["this topic"]},{"title":"A Research Framework for Understanding Education-Occupation Alignment with NLP Techniques","abstract":"Understanding the gaps between job requirements and university curricula is crucial for improving student success and institutional effectiveness in higher education. In this context, natural language processing (NLP) can be leveraged to generate granular insights into where the gaps are and how they change. This paper proposes a three-dimensional research framework that combines NLP techniques with economic and educational research to quantify the alignment between course syllabi and job postings. We elaborate on key technical details of the framework and further discuss its potential positive impacts on practice, including unveiling the inequalities in and long-term consequences of education-occupation alignment to inform policymakers, and fostering information systems to support students, institutions and employers in the school-to-work pipeline.","url":"https://www.semanticscholar.org/paper/40141f0933b5111b089049e226dc8d969b0a7fca","sentence":"Title: a research framework for understanding education-occupation alignment with nlp techniques Abstract: understanding the gaps between job requirements and university curricula is crucial for improving student success and institutional effectiveness in higher education. in this context, natural language processing (nlp) can be leveraged to generate granular insights into where the gaps are and how they change. this paper proposes a three-dimensional research framework that combines nlp techniques with economic and educational research to quantify the alignment between course syllabi and job postings. we elaborate on key technical details of the framework and further discuss its potential positive impacts on practice, including unveiling the inequalities in and long-term consequences of education-occupation alignment to inform policymakers, and fostering information systems to support students, institutions and employers in the school-to-work pipeline.","Task":["nLP"],"Method":["nLP"]},{"title":"A Grounded Well-being Conversational Agent with Multiple Interaction Modes: Preliminary Results","abstract":"Technologies for enhancing well-being, healthcare vigilance and monitoring are on the rise. However, despite patient interest, such technologies suffer from low adoption. One hypothesis for this limited adoption is loss of human interaction that is central to doctor-patient encounters. In this paper we seek to address this limitation via a conversational agent that adopts one aspect of in-person doctor-patient interactions: A human avatar to facilitate medical grounded question answering. This is akin to the in-person scenario where the doctor may point to the human body or the patient may point to their own body to express their conditions. Additionally, our agent has multiple interaction modes, that may give more options for the patient to use the agent, not just for medical question answering, but also to engage in conversations about general topics and current events. Both the avatar, and the multiple interaction modes could help improve adherence. We present a high level overview of the design of our agent, Marie Bot Wellbeing. We also report implementation details of our early prototype , and present preliminary results.","url":"https://www.semanticscholar.org/paper/8d9a678c56b9085de65024aa2f6b406ccad97390","sentence":"Title: a grounded well-being conversational agent with multiple interaction modes: preliminary results Abstract: technologies for enhancing well-being, healthcare vigilance and monitoring are on the rise. however, despite patient interest, such technologies suffer from low adoption. one hypothesis for this limited adoption is loss of human interaction that is central to doctor-patient encounters. in this paper we seek to address this limitation via a conversational agent that adopts one aspect of in-person doctor-patient interactions: a human avatar to facilitate medical grounded question answering. this is akin to the in-person scenario where the doctor may point to the human body or the patient may point to their own body to express their conditions. additionally, our agent has multiple interaction modes, that may give more options for the patient to use the agent, not just for medical question answering, but also to engage in conversations about general topics and current events. both the avatar, and the multiple interaction modes could help improve adherence. we present a high level overview of the design of our agent, marie bot wellbeing. we also report implementation details of our early prototype , and present preliminary results.","Task":["artificial agent"],"Method":["ourie"]},{"title":"A Speech-enabled Fixed-phrase Translator for Healthcare Accessibility","abstract":"In this overview article we describe an application designed to enable communication between health practitioners and patients who do not share a common language, in situations where professional interpreters are not available. Built on the principle of a fixed phrase translator, the application implements different natural language processing (NLP) technologies, such as speech recognition, neural machine translation and text-to-speech to improve usability. Its design allows easy portability to new domains and integration of different types of output for multiple target audiences. Even though BabelDr is far from solving the problem of miscommunication between patients and doctors, it is a clear example of NLP in a real world application designed to help minority groups to communicate in a medical context. It also gives some insights into the relevant criteria for the development of such an application.","url":"https://www.semanticscholar.org/paper/934dbfbb33cbec11fc825db56ac85a48fc52158f","sentence":"Title: a speech-enabled fixed-phrase translator for healthcare accessibility Abstract: in this overview article we describe an application designed to enable communication between health practitioners and patients who do not share a common language, in situations where professional interpreters are not available. built on the principle of a fixed phrase translator, the application implements different natural language processing (nlp) technologies, such as speech recognition, neural machine translation and text-to-speech to improve usability. its design allows easy portability to new domains and integration of different types of output for multiple target audiences. even though babeldr is far from solving the problem of miscommunication between patients and doctors, it is a clear example of nlp in a real world application designed to help minority groups to communicate in a medical context. it also gives some insights into the relevant criteria for the development of such an application.","Task":["speech translation"],"Method":["healthcare accessibility"]},{"title":"Analyzing Stereotypes in Generative Text Inference Tasks","abstract":"Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality.","url":"https://www.semanticscholar.org/paper/54db327cd53fe043449c9f242d3fc34c593a70ef","sentence":"Title: analyzing stereotypes in generative text inference tasks Abstract: stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. in generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). such tasks are therefore a fruitful setting in which to explore the degree to which nlp systems encode stereotypes. in our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. we collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality.","Task":["stereotypistics"],"Method":["such studies"]},{"title":"Demographic Dialectal Variation in Social Media: A Case Study of African-American English","abstract":"Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language.","url":"https://www.semanticscholar.org/paper/7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c","sentence":"Title: demographic dialectal variation in social media: a case study of african-american english Abstract: though dialectal language is increasingly abundant on social media, few resources exist for developing nlp tools to handle such language. we conduct a case study of dialectal language in online conversational text by investigating african-american english (aae) on twitter. we propose a distantly supervised model to identify aae-like language from demographics associated with geo-located messages, and we verify that this language follows well-known aae linguistic phenomena. in addition, we analyze the quality of existing language identification and dependency parsing tools on aae-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. we also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing aae-like language.","Task":["linguistic analysis"],"Method":["linguisticae"]},{"title":"Energy and Policy Considerations for Deep Learning in NLP","abstract":"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.","url":"https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","sentence":"Title: energy and policy considerations for deep learning in nlp Abstract: recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. these models have obtained notable gains in accuracy across many nlp tasks. however, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. as a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. in this paper we bring this issue to the attention of nlp researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for nlp. based on these findings, we propose actionable recommendations to reduce costs and improve equity in nlp research and practice.","Task":["deep learning"],"Method":["energy topic"]},{"title":"Automatic Sentence Simplification in Low Resource Settings for Urdu","abstract":"To build automated simplification systems, corpora of complex sentences and their simplified versions is the first step to understand sentence complexity and enable the development of automatic text simplification systems. We present a lexical and syntactically simplified Urdu simplification corpus with a detailed analysis of the various simplification operations and human evaluation of corpus quality. We further analyze our corpora using text readability measures and present a comparison of the original, lexical simplified and syntactically simplified corpora. In addition, we compare our corpus with other existing simplification corpora by building simplification systems and evaluating these systems using BLEU and SARI scores. Our system achieves the highest BLEU score and comparable SARI score in comparison to other systems. We release our simplification corpora for the benefit of the research community.","url":"https://www.semanticscholar.org/paper/d6a25d8726c5484bb224a3350528aae9fcaae65f","sentence":"Title: automatic sentence simplification in low resource settings for urdu Abstract: to build automated simplification systems, corpora of complex sentences and their simplified versions is the first step to understand sentence complexity and enable the development of automatic text simplification systems. we present a lexical and syntactically simplified urdu simplification corpus with a detailed analysis of the various simplification operations and human evaluation of corpus quality. we further analyze our corpora using text readability measures and present a comparison of the original, lexical simplified and syntactically simplified corpora. in addition, we compare our corpus with other existing simplification corpora by building simplification systems and evaluating these systems using bleu and sari scores. our system achieves the highest bleu score and comparable sari score in comparison to other systems. we release our simplification corpora for the benefit of the research community.","Task":["simplification"],"Method":["simplification"]},{"title":"Cartography of Natural Language Processing for Social Good (NLP4SG): Searching for Definitions, Statistics and White Spots","abstract":"The range of works that can be considered as developing NLP for social good (NLP4SG) is enormous. While many of them target the identification of hate speech or fake news, there are others that address, e.g., text simplification to alleviate consequences of dyslexia, or coaching strategies to fight depression. However, so far, there is no clear picture of what areas are targeted by NLP4SG, who are the actors, which are the main scenarios and what are the topics that have been left aside. In order to obtain a clearer view in this respect, we first propose a working definition of NLP4SG and identify some primary aspects that are crucial for NLP4SG, including, e.g., areas, ethics, privacy and bias. Then, we draw upon a corpus of around 50,000 articles downloaded from the ACL Anthology. Based on a list of keywords retrieved from the literature and revised in view of the task, we select from this corpus articles that can be considered to be on NLP4SG according to our definition and analyze them in terms of trends along the time line, etc. The result is a map of the current NLP4SG research and insights concerning the white spots on this map.","url":"https://www.semanticscholar.org/paper/4975c64466149c72f31489fadbbbff4e85d7b3f3","sentence":"Title: cartography of natural language processing for social good (nlp4sg): searching for definitions, statistics and white spots Abstract: the range of works that can be considered as developing nlp for social good (nlp4sg) is enormous. while many of them target the identification of hate speech or fake news, there are others that address, e.g., text simplification to alleviate consequences of dyslexia, or coaching strategies to fight depression. however, so far, there is no clear picture of what areas are targeted by nlp4sg, who are the actors, which are the main scenarios and what are the topics that have been left aside. in order to obtain a clearer view in this respect, we first propose a working definition of nlp4sg and identify some primary aspects that are crucial for nlp4sg, including, e.g., areas, ethics, privacy and bias. then, we draw upon a corpus of around 50,000 articles downloaded from the acl anthology. based on a list of keywords retrieved from the literature and revised in view of the task, we select from this corpus articles that can be considered to be on nlp4sg according to our definition and analyze them in terms of trends along the time line, etc. the result is a map of the current nlp4sg research and insights concerning the white spots on this map.","Task":["social good"],"Method":["this topic"]},{"title":"Breaking Down Walls of Text: How Can NLP Benefit Consumer Privacy?","abstract":"Privacy plays a crucial role in preserving democratic ideals and personal autonomy. The dominant legal approach to privacy in many jurisdictions is the \u201cNotice and Choice\u201d paradigm, where privacy policies are the primary instrument used to convey information to users. However, privacy policies are long and complex documents that are difficult for users to read and comprehend. We discuss how language technologies can play an important role in addressing this information gap, reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies: consumers, enterprises, and regulators. Our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy, limit privacy harms, and rally research efforts from the community towards addressing an issue with large social impact. We highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies.","url":"https://www.semanticscholar.org/paper/c39fc494cb84b260eaa33cceda2b76512f3701a5","sentence":"Title: breaking down walls of text: how can nlp benefit consumer privacy? Abstract: privacy plays a crucial role in preserving democratic ideals and personal autonomy. the dominant legal approach to privacy in many jurisdictions is the \u201cnotice and choice\u201d paradigm, where privacy policies are the primary instrument used to convey information to users. however, privacy policies are long and complex documents that are difficult for users to read and comprehend. we discuss how language technologies can play an important role in addressing this information gap, reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies: consumers, enterprises, and regulators. our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy, limit privacy harms, and rally research efforts from the community towards addressing an issue with large social impact. we highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies.","Task":["n technologies"],"Method":["privacy technologies"]},{"title":"Are we human, or are we users? The role of natural language processing in human-centric news recommenders that nudge users to diverse content","abstract":"In this position paper, we present a research agenda and ideas for facilitating exposure to diverse viewpoints in news recommendation. Recommending news from diverse viewpoints is important to prevent potential filter bubble effects in news consumption, and stimulate a healthy democratic debate.To account for the complexity that is inherent to humans as citizens in a democracy, we anticipate (among others) individual-level differences in acceptance of diversity. We connect this idea to techniques in Natural Language Processing, where distributional language models would allow us to place different users and news articles in a multidimensional space based on semantic content, where diversity is operationalized as distance and variance. In this way, we can model individual \u201clatitudes of diversity\u201d for different users, and thus personalize viewpoint diversity in support of a healthy public debate. In addition, we identify technical, ethical and conceptual issues related to our presented ideas. Our investigation describes how NLP can play a central role in diversifying news recommendations.","url":"https://www.semanticscholar.org/paper/9995132dda17b36e5513c8e98d58ff992d0ba79a","sentence":"Title: are we human, or are we users? the role of natural language processing in human-centric news recommenders that nudge users to diverse content Abstract: in this position paper, we present a research agenda and ideas for facilitating exposure to diverse viewpoints in news recommendation. recommending news from diverse viewpoints is important to prevent potential filter bubble effects in news consumption, and stimulate a healthy democratic debate.to account for the complexity that is inherent to humans as citizens in a democracy, we anticipate (among others) individual-level differences in acceptance of diversity. we connect this idea to techniques in natural language processing, where distributional language models would allow us to place different users and news articles in a multidimensional space based on semantic content, where diversity is operationalized as distance and variance. in this way, we can model individual \u201clatitudes of diversity\u201d for different users, and thus personalize viewpoint diversity in support of a healthy public debate. in addition, we identify technical, ethical and conceptual issues related to our presented ideas. our investigation describes how nlp can play a central role in diversifying news recommendations.","Task":["nudge"],"Method":["n topic"]},{"title":"Challenges for Information Extraction from Dialogue in Criminal Law","abstract":"Information extraction and question answering have the potential to introduce a new paradigm for how machine learning is applied to criminal law. Existing approaches generally use tabular data for predictive metrics. An alternative approach is needed for matters of equitable justice, where individuals are judged on a case-by-case basis, in a process involving verbal or written discussion and interpretation of case factors. Such discussions are individualized, but they nonetheless rely on underlying facts. Information extraction can play an important role in surfacing these facts, which are still important to understand. We analyze unsupervised, weakly supervised, and pre-trained models\u2019 ability to extract such factual information from the free-form dialogue of California parole hearings. With a few exceptions, most F1 scores are below 0.85. We use this opportunity to highlight some opportunities for further research for information extraction and question answering. We encourage new developments in NLP to enable analysis and review of legal cases to be done in a post-hoc, not predictive, manner.","url":"https://www.semanticscholar.org/paper/03c046041bc509f2cc9671ee71a78642275b77c3","sentence":"Title: challenges for information extraction from dialogue in criminal law Abstract: information extraction and question answering have the potential to introduce a new paradigm for how machine learning is applied to criminal law. existing approaches generally use tabular data for predictive metrics. an alternative approach is needed for matters of equitable justice, where individuals are judged on a case-by-case basis, in a process involving verbal or written discussion and interpretation of case factors. such discussions are individualized, but they nonetheless rely on underlying facts. information extraction can play an important role in surfacing these facts, which are still important to understand. we analyze unsupervised, weakly supervised, and pre-trained models\u2019 ability to extract such factual information from the free-form dialogue of california parole hearings. with a few exceptions, most f1 scores are below 0.85. we use this opportunity to highlight some opportunities for further research for information extraction and question answering. we encourage new developments in nlp to enable analysis and review of legal cases to be done in a post-hoc, not predictive, manner.","Task":["machine learning"],"Method":["this learning"]},{"title":"Conversations Gone Alright: Quantifying and Predicting Prosocial Outcomes in Online Conversations","abstract":"Online conversations can go in many directions: some turn out poorly due to antisocial behavior, while others turn out positively to the benefit of all. Research on improving online spaces has focused primarily on detecting and reducing antisocial behavior. Yet we know little about positive outcomes in online conversations and how to increase them\u2014is a prosocial outcome simply the lack of antisocial behavior or something more? Here, we examine how conversational features lead to prosocial outcomes within online discussions. We introduce a series of new theory-inspired metrics to define prosocial outcomes such as mentoring and esteem enhancement. Using a corpus of 26M Reddit conversations, we show that these outcomes can be forecasted from the initial comment of an online conversation, with the best model providing a relative 24% improvement over human forecasting performance at ranking conversations for predicted outcome. Our results indicate that platforms can use these early cues in their algorithmic ranking of early conversations to prioritize better outcomes.","url":"https://www.semanticscholar.org/paper/50718d6bd163967b8353de4c854ed866b2b56c2f","sentence":"Title: conversations gone alright: quantifying and predicting prosocial outcomes in online conversations Abstract: online conversations can go in many directions: some turn out poorly due to antisocial behavior, while others turn out positively to the benefit of all. research on improving online spaces has focused primarily on detecting and reducing antisocial behavior. yet we know little about positive outcomes in online conversations and how to increase them\u2014is a prosocial outcome simply the lack of antisocial behavior or something more? here, we examine how conversational features lead to prosocial outcomes within online discussions. we introduce a series of new theory-inspired metrics to define prosocial outcomes such as mentoring and esteem enhancement. using a corpus of 26m reddit conversations, we show that these outcomes can be forecasted from the initial comment of an online conversation, with the best model providing a relative 24% improvement over human forecasting performance at ranking conversations for predicted outcome. our results indicate that platforms can use these early cues in their algorithmic ranking of early conversations to prioritize better outcomes.","Task":["online learning"],"Method":["online conversations"]},{"title":"Recommender systems and their ethical challenges","abstract":"This article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. The article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. The analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders\u2014as opposed to just the receivers of a recommendation\u2014in assessing the ethical impacts of a recommender system.","url":"https://www.semanticscholar.org/paper/52a14b391f994d83759787500a9bda865acdb3c5","sentence":"Title: recommender systems and their ethical challenges Abstract: this article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. the article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. the analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders\u2014as opposed to just the receivers of a recommendation\u2014in assessing the ethical impacts of a recommender system.","Task":["recomm learning"],"Method":["this topic"]},{"title":"Conversational receptiveness: Improving engagement with opposing views","abstract":"Abstract We examine \u201cconversational receptiveness\u201d \u2013 the use of language to communicate one\u2019s willingness to thoughtfully engage with opposing views. We develop an interpretable machine-learning algorithm to identify the linguistic profile of receptiveness (Studies 1A-B). We then show that in contentious policy discussions, government executives who were rated as more receptive - according to our algorithm and their partners, but not their own self-evaluations - were considered better teammates, advisors, and workplace representatives (Study 2). Furthermore, using field data from a setting where conflict management is endemic to productivity, we show that conversational receptiveness at the beginning of a conversation forestalls conflict escalation at the end. Specifically, Wikipedia editors who write more receptive posts are less prone to receiving personal attacks from disagreeing editors (Study 3). We develop a \u201creceptiveness recipe\u201d intervention based on our algorithm. We find that writers who follow the recipe are seen as more desirable partners for future collaboration and their messages are seen as more persuasive (Study 4). Overall, we find that conversational receptiveness is reliably measurable, has meaningful relational consequences, and can be substantially improved using our intervention (183 words).","url":"https://www.semanticscholar.org/paper/ff3b0be4fb7debf1312c92381577292288755674","sentence":"Title: conversational receptiveness: improving engagement with opposing views Abstract: abstract we examine \u201cconversational receptiveness\u201d \u2013 the use of language to communicate one\u2019s willingness to thoughtfully engage with opposing views. we develop an interpretable machine-learning algorithm to identify the linguistic profile of receptiveness (studies 1a-b). we then show that in contentious policy discussions, government executives who were rated as more receptive - according to our algorithm and their partners, but not their own self-evaluations - were considered better teammates, advisors, and workplace representatives (study 2). furthermore, using field data from a setting where conflict management is endemic to productivity, we show that conversational receptiveness at the beginning of a conversation forestalls conflict escalation at the end. specifically, wikipedia editors who write more receptive posts are less prone to receiving personal attacks from disagreeing editors (study 3). we develop a \u201creceptiveness recipe\u201d intervention based on our algorithm. we find that writers who follow the recipe are seen as more desirable partners for future collaboration and their messages are seen as more persuasive (study 4). overall, we find that conversational receptiveness is reliably measurable, has meaningful relational consequences, and can be substantially improved using our intervention (183 words).","Task":["machine learning"],"Method":["receptopic"]},{"title":"Empathy and Hope: Resource Transfer to Model Inter-country Social Media Dynamics","abstract":"The ongoing COVID-19 pandemic resulted in significant ramifications for international relations ranging from travel restrictions, global ceasefires, and international vaccine production and sharing agreements. Amidst a wave of infections in India that resulted in a systemic breakdown of healthcare infrastructure, a social welfare organization based in Pakistan offered to procure medical-grade oxygen to assist India - a nation which was involved in four wars with Pakistan in the past few decades. In this paper, we focus on Pakistani Twitter users\u2019 response to the ongoing healthcare crisis in India. While #IndiaNeedsOxygen and #PakistanStandsWithIndia featured among the top-trending hashtags in Pakistan, divisive hashtags such as #EndiaSaySorryToKashmir simultaneously started trending. Against the backdrop of a contentious history including four wars, divisive content of this nature, especially when a country is facing an unprecedented healthcare crisis, fuels further deterioration of relations. In this paper, we define a new task of detecting supportive content and demonstrate that existing NLP for social impact tools can be effectively harnessed for such tasks within a quick turnaround time. We also release the first publicly available data set at the intersection of geopolitical relations and a raging pandemic in the context of India and Pakistan.","url":"https://www.semanticscholar.org/paper/e511b338559a1df846059068ce7cc64c7066be4c","sentence":"Title: empathy and hope: resource transfer to model inter-country social media dynamics Abstract: the ongoing covid-19 pandemic resulted in significant ramifications for international relations ranging from travel restrictions, global ceasefires, and international vaccine production and sharing agreements. amidst a wave of infections in india that resulted in a systemic breakdown of healthcare infrastructure, a social welfare organization based in pakistan offered to procure medical-grade oxygen to assist india - a nation which was involved in four wars with pakistan in the past few decades. in this paper, we focus on pakistani twitter users\u2019 response to the ongoing healthcare crisis in india. while #indianeedsoxygen and #pakistanstandswithindia featured among the top-trending hashtags in pakistan, divisive hashtags such as #endiasaysorrytokashmir simultaneously started trending. against the backdrop of a contentious history including four wars, divisive content of this nature, especially when a country is facing an unprecedented healthcare crisis, fuels further deterioration of relations. in this paper, we define a new task of detecting supportive content and demonstrate that existing nlp for social impact tools can be effectively harnessed for such tasks within a quick turnaround time. we also release the first publicly available data set at the intersection of geopolitical relations and a raging pandemic in the context of india and pakistan.","Task":["social analytics"],"Method":["social topic"]},{"title":"Guiding Principles for Participatory Design-inspired Natural Language Processing","abstract":"We introduce 9 guiding principles to integrate Participatory Design (PD) methods in the development of Natural Language Processing (NLP) systems. The adoption of PD methods by NLP will help to alleviate issues concerning the development of more democratic, fairer, less-biased technologies to process natural language data. This short paper is the outcome of an ongoing dialogue between designers and NLP experts and adopts a non-standard format following previous work by Traum (2000); Bender (2013); Abzianidze and Bos (2019). Every section is a guiding principle. While principles 1\u20133 illustrate assumptions and methods that inform community-based PD practices, we used two fictional design scenarios (Encinas and Blythe, 2018), which build on top of situations familiar to the authors, to elicit the identification of the other 6. Principles 4\u20136 describes the impact of PD methods on the design of NLP systems, targeting two critical aspects: data collection & annotation, and the deployment & evaluation. Finally, principles 7\u20139 guide a new reflexivity of the NLP research with respect to its context, actors and participants, and aims. We hope this guide will offer inspiration and a road-map to develop a new generation of PD-inspired NLP.","url":"https://www.semanticscholar.org/paper/022b80b663c51563a1c6772c12ada3c79f5d798d","sentence":"Title: guiding principles for participatory design-inspired natural language processing Abstract: we introduce 9 guiding principles to integrate participatory design (pd) methods in the development of natural language processing (nlp) systems. the adoption of pd methods by nlp will help to alleviate issues concerning the development of more democratic, fairer, less-biased technologies to process natural language data. this short paper is the outcome of an ongoing dialogue between designers and nlp experts and adopts a non-standard format following previous work by traum (2000); bender (2013); abzianidze and bos (2019). every section is a guiding principle. while principles 1\u20133 illustrate assumptions and methods that inform community-based pd practices, we used two fictional design scenarios (encinas and blythe, 2018), which build on top of situations familiar to the authors, to elicit the identification of the other 6. principles 4\u20136 describes the impact of pd methods on the design of nlp systems, targeting two critical aspects: data collection & annotation, and the deployment & evaluation. finally, principles 7\u20139 guide a new reflexivity of the nlp research with respect to its context, actors and participants, and aims. we hope this guide will offer inspiration and a road-map to develop a new generation of pd-inspired nlp.","Task":["particip design"],"Method":["n principles"]},{"title":"Detecting Hashtag Hijacking for Hashtag Activism","abstract":"Social media has changed the way we engage in social activities. On Twitter, users can participate in social movements using hashtags such as #MeToo; this is known as hashtag activism. However, while these hashtags can help reshape social norms, they can also be used maliciously by spammers or troll communities for other purposes, such as signal boosting unrelated content, making a dent in a movement, or sharing hate speech. We present a Tweet-level hashtag hijacking detection framework focusing on hashtag activism. Our weakly-supervised framework uses bootstrapping to update itself as new Tweets are posted. Our experiments show that the system adapts to new topics in a social movement, as well as new hijacking strategies, maintaining strong performance over time.","url":"https://www.semanticscholar.org/paper/203bdaca3986b51f8d011422c04ff1489e425ce5","sentence":"Title: detecting hashtag hijacking for hashtag activism Abstract: social media has changed the way we engage in social activities. on twitter, users can participate in social movements using hashtags such as #metoo; this is known as hashtag activism. however, while these hashtags can help reshape social norms, they can also be used maliciously by spammers or troll communities for other purposes, such as signal boosting unrelated content, making a dent in a movement, or sharing hate speech. we present a tweet-level hashtag hijacking detection framework focusing on hashtag activism. our weakly-supervised framework uses bootstrapping to update itself as new tweets are posted. our experiments show that the system adapts to new topics in a social movement, as well as new hijacking strategies, maintaining strong performance over time.","Task":["hashtag activism"],"Method":["hashtag activism"]},{"title":"Dialogue Act Classification for Augmentative and Alternative Communication","abstract":"Augmentative and Alternative Communication (AAC) devices and applications are intended to make it easier for individuals with complex communication needs to participate in conversations. However, these devices have low adoption and retention rates. We review prior work with text recommendation systems that have not been successful in mitigating these problems. To address these gaps, we propose applying Dialogue Act classification to AAC conversations. We evaluated the performance of a state of the art model on a limited AAC dataset that was trained on both AAC and non-AAC datasets. The one trained on AAC (accuracy = 38.6%) achieved better performance than that trained on a non-AAC corpus (accuracy = 34.1%). These results reflect the need to incorporate representative datasets in later experiments. We discuss the need to collect more labeled AAC datasets and propose areas of future work.","url":"https://www.semanticscholar.org/paper/41ebff09aff17c37efdab8c1d7051cbf150970f8","sentence":"Title: dialogue act classification for augmentative and alternative communication Abstract: augmentative and alternative communication (aac) devices and applications are intended to make it easier for individuals with complex communication needs to participate in conversations. however, these devices have low adoption and retention rates. we review prior work with text recommendation systems that have not been successful in mitigating these problems. to address these gaps, we propose applying dialogue act classification to aac conversations. we evaluated the performance of a state of the art model on a limited aac dataset that was trained on both aac and non-aac datasets. the one trained on aac (accuracy = 38.6%) achieved better performance than that trained on a non-aac corpus (accuracy = 34.1%). these results reflect the need to incorporate representative datasets in later experiments. we discuss the need to collect more labeled aac datasets and propose areas of future work.","Task":["a communication"],"Method":["aac"]},{"title":"Improving Policing with Natural Language Processing","abstract":"This article explores the potential for Natural Language Processing (NLP) to enable a more effective, prevention focused and less confrontational policing model that has hitherto been too resource consuming to implement at scale. Problem-Oriented Policing (POP) is a potential replacement, at least in part, for traditional policing which adopts a reactive approach, relying heavily on the criminal justice system. By contrast, POP seeks to prevent crime by manipulating the underlying conditions that allow crimes to be committed. Identifying these underlying conditions requires a detailed understanding of crime events - tacit knowledge that is often held by police officers but which can be challenging to derive from structured police data. One potential source of insight exists in unstructured free text data commonly collected by police for the purposes of investigation or administration. Yet police agencies do not typically have the skills or resources to analyse these data at scale. In this article we argue that NLP offers the potential to unlock these unstructured data and by doing so allow police to implement more POP initiatives. However we caution that using NLP models without adequate knowledge may either allow or perpetuate bias within the data potentially leading to unfavourable outcomes.","url":"https://www.semanticscholar.org/paper/d393f2a793930a6e38321340185756860f43c62c","sentence":"Title: improving policing with natural language processing Abstract: this article explores the potential for natural language processing (nlp) to enable a more effective, prevention focused and less confrontational policing model that has hitherto been too resource consuming to implement at scale. problem-oriented policing (pop) is a potential replacement, at least in part, for traditional policing which adopts a reactive approach, relying heavily on the criminal justice system. by contrast, pop seeks to prevent crime by manipulating the underlying conditions that allow crimes to be committed. identifying these underlying conditions requires a detailed understanding of crime events - tacit knowledge that is often held by police officers but which can be challenging to derive from structured police data. one potential source of insight exists in unstructured free text data commonly collected by police for the purposes of investigation or administration. yet police agencies do not typically have the skills or resources to analyse these data at scale. in this article we argue that nlp offers the potential to unlock these unstructured data and by doing so allow police to implement more pop initiatives. however we caution that using nlp models without adequate knowledge may either allow or perpetuate bias within the data potentially leading to unfavourable outcomes.","Task":["police policing"],"Method":["nlp"]},{"title":"Methods for Detoxification of Texts for the Russian Language","abstract":"We introduce the first study of automatic detoxification of Russian texts to combat offensive language. Such a kind of textual style transfer can be used, for instance, for processing toxic content in social media. While much work has been done for the English language in this field, it has never been solved for the Russian language yet. We test two types of models \u2013 unsupervised approach based on BERT architecture that performs local corrections and supervised approach based on pretrained language GPT-2 model \u2013 and compare them with several baselines. In addition, we describe evaluation setup providing training datasets and metrics for automatic evaluation. The results show that the tested approaches can be successfully used for detoxification, although there is room for improvement.","url":"https://www.semanticscholar.org/paper/2c5b31a02133dea21cf94fde67c8948115441432","sentence":"Title: methods for detoxification of texts for the russian language Abstract: we introduce the first study of automatic detoxification of russian texts to combat offensive language. such a kind of textual style transfer can be used, for instance, for processing toxic content in social media. while much work has been done for the english language in this field, it has never been solved for the russian language yet. we test two types of models \u2013 unsupervised approach based on bert architecture that performs local corrections and supervised approach based on pretrained language gpt-2 model \u2013 and compare them with several baselines. in addition, we describe evaluation setup providing training datasets and metrics for automatic evaluation. the results show that the tested approaches can be successfully used for detoxification, although there is room for improvement.","Task":["detoxification"],"Method":["detoxification"]},{"title":"How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact","abstract":"Recent years have seen many breakthroughs in natural language processing (NLP), transitioning it from a mostly theoretical field to one with many real-world applications. Noting the rising number of applications of other machine learning and AI techniques with pervasive societal impact, we anticipate the rising importance of developing NLP technologies for social good. Inspired by theories in moral philosophy and global priorities research, we aim to promote a guideline for social good in the context of NLP. We lay the foundations via the moral philosophy definition of social good, propose a framework to evaluate the direct and indirect real-world impact of NLP tasks, and adopt the methodology of global priorities research to identify priority causes for NLP research. Finally, we use our theoretical framework to provide some practical guidelines for future NLP research for social good.1","url":"https://www.semanticscholar.org/paper/5c7b8f26ae23c457d85da9d0f19b28e588e9b9a7","sentence":"Title: how good is nlp? a sober look at nlp tasks through the lens of social impact Abstract: recent years have seen many breakthroughs in natural language processing (nlp), transitioning it from a mostly theoretical field to one with many real-world applications. noting the rising number of applications of other machine learning and ai techniques with pervasive societal impact, we anticipate the rising importance of developing nlp technologies for social good. inspired by theories in moral philosophy and global priorities research, we aim to promote a guideline for social good in the context of nlp. we lay the foundations via the moral philosophy definition of social good, propose a framework to evaluate the direct and indirect real-world impact of nlp tasks, and adopt the methodology of global priorities research to identify priority causes for nlp research. finally, we use our theoretical framework to provide some practical guidelines for future nlp research for social good.1","Task":["socialearning"],"Method":["nLP"]},{"title":"NLP for Consumer Protection: Battling Illegal Clauses in German Terms and Conditions in Online Shopping","abstract":"Online shopping is an ever more important part of the global consumer economy, not just in times of a pandemic. When we place an order online as consumers, we regularly agree to the so-called \u201cTerms and Conditions\u201d (T&C), a contract unilaterally drafted by the seller. Often, consumers do not read these contracts and unwittingly agree to unfavourable and often void terms. Government and non-government organisations (NGOs) for consumer protection battle such terms on behalf of consumers, who often hesitate to take on legal actions themselves. However, the growing number of online shops and a lack of funding makes it increasingly difficult for such organisations to monitor the market effectively. This paper describes how Natural Language Processing (NLP) can be applied to support consumer advocates in their efforts to protect consumers. Together with two NGOs from Germany, we developed an NLP-based application that legally assesses clauses in T&C from German online shops under the European Union\u2019s (EU) jurisdiction. We report that we could achieve an accuracy of 0.9 in the detection of void clauses by fine-tuning a pre-trained German BERT model. The approach is currently used by two NGOs and has already helped to challenge void clauses in T&C.","url":"https://www.semanticscholar.org/paper/9e1616dcabf4d04d14d642fcb7963c461cf13d41","sentence":"Title: nlp for consumer protection: battling illegal clauses in german terms and conditions in online shopping Abstract: online shopping is an ever more important part of the global consumer economy, not just in times of a pandemic. when we place an order online as consumers, we regularly agree to the so-called \u201cterms and conditions\u201d (t&c), a contract unilaterally drafted by the seller. often, consumers do not read these contracts and unwittingly agree to unfavourable and often void terms. government and non-government organisations (ngos) for consumer protection battle such terms on behalf of consumers, who often hesitate to take on legal actions themselves. however, the growing number of online shops and a lack of funding makes it increasingly difficult for such organisations to monitor the market effectively. this paper describes how natural language processing (nlp) can be applied to support consumer advocates in their efforts to protect consumers. together with two ngos from germany, we developed an nlp-based application that legally assesses clauses in t&c from german online shops under the european union\u2019s (eu) jurisdiction. we report that we could achieve an accuracy of 0.9 in the detection of void clauses by fine-tuning a pre-trained german bert model. the approach is currently used by two ngos and has already helped to challenge void clauses in t&c.","Task":["consumer protection"],"Method":["nlp"]},{"title":"Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech","abstract":"Tackling online hatred using informed textual responses \u2013 called counter narratives \u2013 has been brought under the spotlight recently. Accordingly, a research line has emerged to automatically generate counter narratives in order to facilitate the direct intervention in the hate discussion and to prevent hate content from further spreading. Still, current neural approaches tend to produce generic/repetitive responses and lack grounded and up-to-date evidence such as facts, statistics, or examples. Moreover, these models can create plausible but not necessarily true arguments. In this paper we present the first complete knowledgebound counter narrative generation pipeline, grounded in an external knowledge repository that can provide more informative content to fight online hatred. Together with our approach, we present a series of experiments that show its feasibility to produce suitable and informative counter narratives in in-domain and cross-domain settings.","url":"https://www.semanticscholar.org/paper/473b70bb3c531b2d740fa6d652956e2733b53243","sentence":"Title: towards knowledge-grounded counter narrative generation for hate speech Abstract: tackling online hatred using informed textual responses \u2013 called counter narratives \u2013 has been brought under the spotlight recently. accordingly, a research line has emerged to automatically generate counter narratives in order to facilitate the direct intervention in the hate discussion and to prevent hate content from further spreading. still, current neural approaches tend to produce generic/repetitive responses and lack grounded and up-to-date evidence such as facts, statistics, or examples. moreover, these models can create plausible but not necessarily true arguments. in this paper we present the first complete knowledgebound counter narrative generation pipeline, grounded in an external knowledge repository that can provide more informative content to fight online hatred. together with our approach, we present a series of experiments that show its feasibility to produce suitable and informative counter narratives in in-domain and cross-domain settings.","Task":["counter speech"],"Method":["counter topic"]},{"title":"Use of Formal Ethical Reviews in NLP Literature: Historical Trends and Current Practices","abstract":"Ethical aspects of research in language technologies have received much attention recently. It is a standard practice to get a study involving human subjects reviewed and approved by a professional ethics committee/board of the institution. How commonly do we see mention of ethical approvals in NLP research? What types of research or aspects of studies are usually subject to such reviews? With the rising concerns and discourse around the ethics of NLP, do we also observe a rise in formal ethical reviews of NLP studies? And, if so, would this imply that there is a heightened awareness of ethical issues that was previously lacking? We aim to address these questions by conducting a detailed quantitative and qualitative analysis of the ACL Anthology, as well as comparing the trends in our field to those of other related disciplines, such as cognitive science, machine learning, data mining, and systems.","url":"https://www.semanticscholar.org/paper/f29d5cb8f405903fc8af7a5d7ab4bf7d65796e95","sentence":"Title: use of formal ethical reviews in nlp literature: historical trends and current practices Abstract: ethical aspects of research in language technologies have received much attention recently. it is a standard practice to get a study involving human subjects reviewed and approved by a professional ethics committee/board of the institution. how commonly do we see mention of ethical approvals in nlp research? what types of research or aspects of studies are usually subject to such reviews? with the rising concerns and discourse around the ethics of nlp, do we also observe a rise in formal ethical reviews of nlp studies? and, if so, would this imply that there is a heightened awareness of ethical issues that was previously lacking? we aim to address these questions by conducting a detailed quantitative and qualitative analysis of the acl anthology, as well as comparing the trends in our field to those of other related disciplines, such as cognitive science, machine learning, data mining, and systems.","Task":["language science"],"Method":["ethical topic"]},{"title":"Using Word Embeddings to Analyze Teacher Evaluations: An Application to a Filipino Education Non-Profit Organization","abstract":"Analysis of teacher evaluations is crucial to the development of robust educational programs, particularly through the validation of desirable qualities being reflected on in the text. This research applies Natural Language Processing techniques on a real-world dataset from a Filipino education non-profit to explore insights from analyzing evaluations written by Teacher Fellows who assess their own progress. Prior to this research, only qualitative assessment had been conducted on the text. Inspired by the use of word embedding similarities to capture semantic alignment, we utilize GloVe embeddings to determine to what extent these evaluations reflect concepts critical to measuring the competency of Teacher Fellows and upholding the organization\u2019s Vision and Mission. As Fellows\u2019 quantitative ratings improved, so too did their demonstration of competency in the text. Further, Teacher Fellow language was consistent with the organization\u2019s Vision and Mission. This research therefore showcases the possibilities of NLP in education, improving our understanding of Teacher Fellow evaluations, which can lead to advances in program operations and education efforts.","url":"https://www.semanticscholar.org/paper/23a812dde149021bab611ca4395279b9d41031f1","sentence":"Title: using word embeddings to analyze teacher evaluations: an application to a filipino education non-profit organization Abstract: analysis of teacher evaluations is crucial to the development of robust educational programs, particularly through the validation of desirable qualities being reflected on in the text. this research applies natural language processing techniques on a real-world dataset from a filipino education non-profit to explore insights from analyzing evaluations written by teacher fellows who assess their own progress. prior to this research, only qualitative assessment had been conducted on the text. inspired by the use of word embedding similarities to capture semantic alignment, we utilize glove embeddings to determine to what extent these evaluations reflect concepts critical to measuring the competency of teacher fellows and upholding the organization\u2019s vision and mission. as fellows\u2019 quantitative ratings improved, so too did their demonstration of competency in the text. further, teacher fellow language was consistent with the organization\u2019s vision and mission. this research therefore showcases the possibilities of nlp in education, improving our understanding of teacher fellow evaluations, which can lead to advances in program operations and education efforts.","Task":["n evaluation"],"Method":["n evaluation"]},{"title":"Theano: A Greek-speaking conversational agent for COVID-19","abstract":"Conversational Agents (CAs) can be a proxy for disseminating information and providing support to the public, especially in times of crisis. CAs can scale to reach larger numbers of end-users than human operators, while they can offer information interactively and engagingly. In this work, we present Theano, a Greek-speaking virtual assistant for COVID-19. Theano presents users with COVID-19 statistics and facts and informs users about the best health practices as well as the latest COVID-19 related guidelines. Additionally, Theano provides support to end-users by helping them self-assess their symptoms and redirecting them to first-line health workers. The relevant, localized information that Theano provides, makes it a valuable tool for combating COVID-19 in Greece. Theano has already conversed with different users in more than 170 different conversations through a web interface as a chatbot and over the phone as a voice bot.","url":"https://www.semanticscholar.org/paper/d4eb2ca9694f34d63abe6d27bd2d958992431017","sentence":"Title: theano: a greek-speaking conversational agent for covid-19 Abstract: conversational agents (cas) can be a proxy for disseminating information and providing support to the public, especially in times of crisis. cas can scale to reach larger numbers of end-users than human operators, while they can offer information interactively and engagingly. in this work, we present theano, a greek-speaking virtual assistant for covid-19. theano presents users with covid-19 statistics and facts and informs users about the best health practices as well as the latest covid-19 related guidelines. additionally, theano provides support to end-users by helping them self-assess their symptoms and redirecting them to first-line health workers. the relevant, localized information that theano provides, makes it a valuable tool for combating covid-19 in greece. theano has already conversed with different users in more than 170 different conversations through a web interface as a chatbot and over the phone as a voice bot.","Task":["virtual intelligence"],"Method":["theano"]},{"title":"Restatement and Question Generation for Counsellor Chatbot","abstract":"Amidst rising mental health needs in society, virtual agents are increasingly deployed in counselling. In order to give pertinent advice, counsellors must first gain an understanding of the issues at hand by eliciting sharing from the counsellee. It is thus important for the counsellor chatbot to encourage the user to open up and talk. One way to sustain the conversation flow is to acknowledge the counsellee\u2019s key points by restating them, or probing them further with questions. This paper applies models from two closely related NLP tasks \u2014 summarization and question generation \u2014 to restatement and question generation in the counselling context. We conducted experiments on a manually annotated dataset of Cantonese post-reply pairs on topics related to loneliness, academic anxiety and test anxiety. We obtained the best performance in both restatement and question generation by fine-tuning BertSum, a state-of-the-art summarization model, with the in-domain manual dataset augmented with a large-scale, automatically mined open-domain dataset.","url":"https://www.semanticscholar.org/paper/100e0f3dcd319266b2772f0841dad388b45cce3f","sentence":"Title: restatement and question generation for counsellor chatbot Abstract: amidst rising mental health needs in society, virtual agents are increasingly deployed in counselling. in order to give pertinent advice, counsellors must first gain an understanding of the issues at hand by eliciting sharing from the counsellee. it is thus important for the counsellor chatbot to encourage the user to open up and talk. one way to sustain the conversation flow is to acknowledge the counsellee\u2019s key points by restating them, or probing them further with questions. this paper applies models from two closely related nlp tasks \u2014 summarization and question generation \u2014 to restatement and question generation in the counselling context. we conducted experiments on a manually annotated dataset of cantonese post-reply pairs on topics related to loneliness, academic anxiety and test anxiety. we obtained the best performance in both restatement and question generation by fine-tuning bertsum, a state-of-the-art summarization model, with the in-domain manual dataset augmented with a large-scale, automatically mined open-domain dataset.","Task":["chatbots"],"Method":["counselling topic"]}]'),l=JSON.parse('[{"title":"UPSTAGE: Unsupervised Context Augmentation for Utterance Classification in Patient-Provider Communication","abstract":"Conversations between patients and providers in clinical settings provide a source of natural language data that may reflect and correlate with the patients\u2019 experience and response to the treatment they are receiving. When analyzing utterances in such conversations, it is not sufficient to consider each sentence in isolation, since its context may play a role in determining its semantic meaning. Recently, contextual information in natural language documents has been modeled using various techniques, such as recurrent neural networks with latent variables, or neural networks with attention mechanisms. In this paper, we present UnsuPerviSed conText AuGmEntation (Upstage), a classification framework that relies on both local and global contextual information from different sources. Upstage uses transformer models with pretrained language models and joint sentence representation to solve the task of classifying health topics in patient-provider conversations. In addition, Upstage leverages unlabeled corpora for pretraining and data augmentation to provide additional context, which leads to improved classification performance.","url":"https://www.semanticscholar.org/paper/e21ab639b0308a71aeac643a12f59a32603d091d","sentence":"Title: upstage: unsupervised context augmentation for utterance classification in patient-provider communication Abstract: conversations between patients and providers in clinical settings provide a source of natural language data that may reflect and correlate with the patients\u2019 experience and response to the treatment they are receiving. when analyzing utterances in such conversations, it is not sufficient to consider each sentence in isolation, since its context may play a role in determining its semantic meaning. recently, contextual information in natural language documents has been modeled using various techniques, such as recurrent neural networks with latent variables, or neural networks with attention mechanisms. in this paper, we present unsupervised context augmentation (upstage), a classification framework that relies on both local and global contextual information from different sources. upstage uses transformer models with pretrained language models and joint sentence representation to solve the task of classifying health topics in patient-provider conversations. in addition, upstage leverages unlabeled corpora for pretraining and data augmentation to provide additional context, which leads to improved classification performance.","Task":["unsupervised context augmentation"],"Method":["upstage"]},{"title":"A Review of Challenges and Opportunities in Machine Learning for Health.","abstract":"Modern electronic health records (EHRs) provide data to answer clinically meaningful questions. The growing data in EHRs makes healthcare ripe for the use of machine learning. However, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. For example, diseases in EHRs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. This article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.","url":"https://www.semanticscholar.org/paper/d77719ba4be4196c9a918ce8dff0edf5aa3c04e7","sentence":"Title: a review of challenges and opportunities in machine learning for health. Abstract: modern electronic health records (ehrs) provide data to answer clinically meaningful questions. the growing data in ehrs makes healthcare ripe for the use of machine learning. however, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. for example, diseases in ehrs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. this article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.","Task":["machine learning"],"Method":["challenges and opportunities in machine learning for health"]},{"title":"Ethical Machine Learning in Health Care","abstract":"The use of machine learning (ML) in healthcare raises numerous ethical concerns, especially as models can amplify existing health inequities. Here, we outline ethical considerations for equitable ML in the advancement of healthcare. Specifically, we frame ethics of ML in healthcare through the lens of social justice. We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health, ranging from problem selection to postdeployment considerations. We close by summarizing recommendations to address these challenges.","url":"https://www.semanticscholar.org/paper/e8d330f11df9c69f38b78a7cc4b1333ebecf7c55","sentence":"Title: ethical machine learning in health care Abstract: the use of machine learning (ml) in healthcare raises numerous ethical concerns, especially as models can amplify existing health inequities. here, we outline ethical considerations for equitable ml in the advancement of healthcare. specifically, we frame ethics of ml in healthcare through the lens of social justice. we describe ongoing efforts and outline challenges in a proposed pipeline of ethical ml in health, ranging from problem selection to postdeployment considerations. we close by summarizing recommendations to address these challenges.","Task":["machine learning"],"Method":["ethical machine learning in health care"]},{"title":"Intimate Partner Violence and Injury Prediction From Radiology Reports","abstract":"Intimate partner violence (IPV) is an urgent, prevalent, and under-detected public health issue. We present machine learning models to assess patients for IPV and injury. We train the predictive algorithms on radiology reports with 1) IPV labels based on entry to a violence prevention program and 2) injury labels provided by emergency radiology fellowship-trained physicians. Our dataset includes 34,642 radiology reports and 1479 patients of IPV victims and control patients. Our best model predicts IPV a median of 3.08 years before violence prevention program entry with a sensitivity of 64% and a specificity of 95%. We conduct error analysis to determine for which patients our model has especially high or low performance and discuss next steps for a deployed clinical risk model.","url":"https://www.semanticscholar.org/paper/4fd75d18b231f73e9c878cb6029900e4b0e327bb","sentence":"Title: intimate partner violence and injury prediction from radiology reports Abstract: intimate partner violence (ipv) is an urgent, prevalent, and under-detected public health issue. we present machine learning models to assess patients for ipv and injury. we train the predictive algorithms on radiology reports with 1) ipv labels based on entry to a violence prevention program and 2) injury labels provided by emergency radiology fellowship-trained physicians. our dataset includes 34,642 radiology reports and 1479 patients of ipv victims and control patients. our best model predicts ipv a median of 3.08 years before violence prevention program entry with a sensitivity of 64% and a specificity of 95%. we conduct error analysis to determine for which patients our model has especially high or low performance and discuss next steps for a deployed clinical risk model.","Task":["machine learning"],"Method":["ipv and injury prediction from radiology reports"]},{"title":"De-identification of patient notes with recurrent neural networks","abstract":"Objective\\nPatient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information that needs to be removed to de-identify patient notes. Manual de-identification is impractical given the size of electronic health record databases, the limited number of researchers with access to non-de-identified notes, and the frequent mistakes of human annotators. A reliable automated de-identification system would consequently be of high value.\\n\\n\\nMaterials and Methods\\nWe introduce the first de-identification system based on artificial neural networks (ANNs), which requires no handcrafted features or rules, unlike existing systems. We compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the MIMIC de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.\\n\\n\\nResults\\nOur ANN model outperforms the state-of-the-art systems. It yields an F1-score of 97.85 on the i2b2 2014 dataset, with a recall of 97.38 and a precision of 98.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with a recall of 99.25 and a precision of 99.21.\\n\\n\\nConclusion\\nOur findings support the use of ANNs for de-identification of patient notes, as they show better performance than previously published systems while requiring no manual feature engineering.","url":"https://www.semanticscholar.org/paper/e7bf950be4cf8309d0df68d18fa09e77f5b2511a","sentence":"Title: de-identification of patient notes with recurrent neural networks Abstract: objective\\npatient notes in electronic health records (ehrs) may contain critical information for medical investigations. however, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. in the united states, the health insurance portability and accountability act (hipaa) defines 18 types of protected health information that needs to be removed to de-identify patient notes. manual de-identification is impractical given the size of electronic health record databases, the limited number of researchers with access to non-de-identified notes, and the frequent mistakes of human annotators. a reliable automated de-identification system would consequently be of high value.\\n\\n\\nmaterials and methods\\nwe introduce the first de-identification system based on artificial neural networks (anns), which requires no handcrafted features or rules, unlike existing systems. we compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the mimic de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.\\n\\n\\nresults\\nour ann model outperforms the state-of-the-art systems. it yields an f1-score of 97.85 on the i2b2 2014 dataset, with a recall of 97.38 and a precision of 98.32, and an f1-score of 99.23 on the mimic de-identification dataset, with a recall of 99.25 and a precision of 99.21.\\n\\n\\nconclusion\\nour findings support the use of anns for de-identification of patient notes, as they show better performance than previously published systems while requiring no manual feature engineering.","Task":["de-identification"],"Method":["de-identification of patient notes with recurrent neural networks"]},{"title":"Segment convolutional neural networks (Seg-CNNs) for classifying relations in clinical notes","abstract":"We propose Segment Convolutional Neural Networks (Seg-CNNs) for classifying relations from clinical notes. Seg-CNNs use only word-embedding features without manual feature engineering. Unlike typical CNN models, relations between 2 concepts are identified by simultaneously learning separate representations for text segments in a sentence: preceding, concept1, middle, concept2, and succeeding. We evaluate Seg-CNN on the i2b2/VA relation classification challenge dataset. We show that Seg-CNN achieves a state-of-the-art micro-average F-measure of 0.742 for overall evaluation, 0.686 for classifying medical problem-treatment relations, 0.820 for medical problem-test relations, and 0.702 for medical problem-medical problem relations. We demonstrate the benefits of learning segment-level representations. We show that medical domain word embeddings help improve relation classification. Seg-CNNs can be trained quickly for the i2b2/VA dataset on a graphics processing unit (GPU) platform. These results support the use of CNNs computed over segments of text for classifying medical relations, as they show state-of-the-art performance while requiring no manual feature engineering.","url":"https://www.semanticscholar.org/paper/defeaddec92b53fc2e9787cc3652e3d8be812193","sentence":"Title: segment convolutional neural networks (seg-cnns) for classifying relations in clinical notes Abstract: we propose segment convolutional neural networks (seg-cnns) for classifying relations from clinical notes. seg-cnns use only word-embedding features without manual feature engineering. unlike typical cnn models, relations between 2 concepts are identified by simultaneously learning separate representations for text segments in a sentence: preceding, concept1, middle, concept2, and succeeding. we evaluate seg-cnn on the i2b2/va relation classification challenge dataset. we show that seg-cnn achieves a state-of-the-art micro-average f-measure of 0.742 for overall evaluation, 0.686 for classifying medical problem-treatment relations, 0.820 for medical problem-test relations, and 0.702 for medical problem-medical problem relations. we demonstrate the benefits of learning segment-level representations. we show that medical domain word embeddings help improve relation classification. seg-cnns can be trained quickly for the i2b2/va dataset on a graphics processing unit (gpu) platform. these results support the use of cnns computed over segments of text for classifying medical relations, as they show state-of-the-art performance while requiring no manual feature engineering.","Task":["segment convolutional neural networks"],"Method":["segment convolutional neural networks"]},{"title":"Fast, Structured Clinical Documentation via Contextual Autocomplete","abstract":"We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. By constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. Furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. To our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","url":"https://www.semanticscholar.org/paper/4554bc45be85cca73e3f94220c8cb056cf6ebf74","sentence":"Title: fast, structured clinical documentation via contextual autocomplete Abstract: we present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. we dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. by constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. to our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","Task":["contextual autocomplete, clinical documentation, machine learning"],"Method":["contextual autocomplete for clinical documentation"]},{"title":"CORD-19: The COVID-19 Open Research Dataset","abstract":"The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.","url":"https://www.semanticscholar.org/paper/4a10dffca6dcce9c570cb75aa4d76522c34a2fd4","sentence":"Title: cord-19: the covid-19 open research dataset Abstract: the covid-19 open research dataset (cord-19) is a growing resource of scientific papers on covid-19 and related historical coronavirus research. cord-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. since its release, cord-19 has been downloaded over 200k times and has served as the basis of many covid-19 text mining and discovery systems. in this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how cord-19 has been used, and describe several shared tasks built around the dataset. we hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for covid-19.","Task":["covid-19"],"Method":["cord-19"]},{"title":"Can AI Help Reduce Disparities in General Medical and Mental Health Care?","abstract":"Background\\nAs machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems\' data, algorithms, and recommendations. Simply put, as health care improves for some, it might not improve for all.\\n\\n\\nMethods\\nTwo case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (ICU) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status.\\n\\n\\nResults\\nClinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. Differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for ICU mortality and with respect to insurance policy for psychiatric 30-day readmission.\\n\\n\\nConclusions\\nThis analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.","url":"https://www.semanticscholar.org/paper/3f8d4444cd124e21d4fa47b514c8267a3d5d9649","sentence":"Title: can ai help reduce disparities in general medical and mental health care? Abstract: background\\nas machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems\' data, algorithms, and recommendations. simply put, as health care improves for some, it might not improve for all.\\n\\n\\nmethods\\ntwo case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (icu) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status.\\n\\n\\nresults\\nclinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for icu mortality and with respect to insurance policy for psychiatric 30-day readmission.\\n\\n\\nconclusions\\nthis analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.","Task":["machine learning"],"Method":["machine learning in health care"]},{"title":"The Ivory Tower Lost: How College Students Respond Differently than the General Public to the COVID-19 Pandemic","abstract":"In the United States, the country with the highest confirmed COVID-19 infection cases, a nationwide social distancing protocol has been implemented by the President. Following the closure of the University of Washington on March 7th, more than 1000 colleges and universities in the United States have cancelled in-person classes and campus activities, impacting millions of students. This paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people\'s opinions on social media. We discover several topics embedded in a large number of COVID-19 tweets that represent the most central issues related to the pandemic, which are of great concerns for both college students and the general public. Moreover, we find significant differences between these two groups of Twitter users with respect to the sentiments they expressed towards the COVID-19 issues. To our best knowledge, this is the first social media-based study which focuses on the college student community\'s demographics and responses to prevalent social issues during a major crisis.","url":"https://www.semanticscholar.org/paper/2d40f76055a7c28dd30c8b8f60c31724f8991cd8","sentence":"Title: the ivory tower lost: how college students respond differently than the general public to the covid-19 pandemic Abstract: in the united states, the country with the highest confirmed covid-19 infection cases, a nationwide social distancing protocol has been implemented by the president. following the closure of the university of washington on march 7th, more than 1000 colleges and universities in the united states have cancelled in-person classes and campus activities, impacting millions of students. this paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people\'s opinions on social media. we discover several topics embedded in a large number of covid-19 tweets that represent the most central issues related to the pandemic, which are of great concerns for both college students and the general public. moreover, we find significant differences between these two groups of twitter users with respect to the sentiments they expressed towards the covid-19 issues. to our best knowledge, this is the first social media-based study which focuses on the college student community\'s demographics and responses to prevalent social issues during a major crisis.","Task":["covid-19"],"Method":["the covid-19 pandemic"]},{"title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","sentence":"Title: humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks Abstract: social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. despite its significantly large volume, social media content is often too noisy for direct use in any application. therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. to address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. however, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. in this paper, we present a new large-scale dataset with \u223c77k human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. we report multiclass classification results using classic and deep learning (fasttext and transformer) based models to set the ground for future studies. the dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","Task":["deep learning"],"Method":["humaid"]},{"title":"CrisisMMD: Multimodal Twitter Datasets from Natural Disasters","abstract":"During natural and man-made disasters, people use social media platforms such as Twitter to post textual and multime- dia content to report updates about injured or dead people, infrastructure damage, and missing or found people among other information types. Studies have revealed that this on- line information, if processed timely and effectively, is ex- tremely useful for humanitarian organizations to gain situational awareness and plan relief operations. In addition to the analysis of textual content, recent studies have shown that imagery content on social media can boost disaster response significantly. Despite extensive research that mainly focuses on textual content to extract useful information, limited work has focused on the use of imagery content or the combination of both content types. One of the reasons is the lack of labeled imagery data in this domain. Therefore, in this paper, we aim to tackle this limitation by releasing a large multi-modal dataset collected from Twitter during different natural disasters. We provide three types of annotations, which are useful to address a number of crisis response and management tasks for different humanitarian organizations.","url":"https://www.semanticscholar.org/paper/2a29f7cc10fc18ce6e031e0cf6fc307a6b85d8e9","sentence":"Title: crisismmd: multimodal twitter datasets from natural disasters Abstract: during natural and man-made disasters, people use social media platforms such as twitter to post textual and multime- dia content to report updates about injured or dead people, infrastructure damage, and missing or found people among other information types. studies have revealed that this on- line information, if processed timely and effectively, is ex- tremely useful for humanitarian organizations to gain situational awareness and plan relief operations. in addition to the analysis of textual content, recent studies have shown that imagery content on social media can boost disaster response significantly. despite extensive research that mainly focuses on textual content to extract useful information, limited work has focused on the use of imagery content or the combination of both content types. one of the reasons is the lack of labeled imagery data in this domain. therefore, in this paper, we aim to tackle this limitation by releasing a large multi-modal dataset collected from twitter during different natural disasters. we provide three types of annotations, which are useful to address a number of crisis response and management tasks for different humanitarian organizations.","Task":["multimodal twitter datasets from natural disasters"],"Method":["multimodal twitter datasets from natural disasters"]},{"title":"Domain Adaptation with Adversarial Training and Graph Embeddings","abstract":"The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.","url":"https://www.semanticscholar.org/paper/561ede166947a8bedb8be9acff182913156e06c6","sentence":"Title: domain adaptation with adversarial training and graph embeddings Abstract: the success of deep neural networks (dnns) is heavily dependent on the availability of labeled data. however, obtaining labeled data is a big challenge in many real-world problems. in such scenarios, a dnn model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. in this paper, we study the problem of classifying social media posts during a crisis event (e.g., earthquake). for that, we use labeled and unlabeled data from past similar events (e.g., flood) and unlabeled data for the current event. we propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. our experiments with two real-world crisis datasets collected from twitter demonstrate significant improvements over several baselines.","Task":["domain adaptation, graph embeddings"],"Method":["domain adaptation with adversarial training and graph embeddings"]},{"title":"IBC-C : A Dataset for Armed Conflict Event Analysis","abstract":"We describe the Iraq Body Count Corpus (IBC-C) dataset, the first substantial armed conflict-related dataset which can be used for conflict analysis. IBC-C provides a ground-truth dataset for conflict specific named entity recognition, slot filling, and event de-duplication. IBC-C is constructed using data collected by the Iraq Body Count project which has been recording casualties resulting from the ongoing war in Iraq since 2003. We describe the dataset\u2019s creation, how it can be used for the above three tasks and provide initial baseline results for the first task (named entity recognition) using Hidden Markov Models, Conditional Random Fields, and Recursive Neural Networks.","url":"https://www.semanticscholar.org/paper/579914dac924d6b4a17889ea7b0dd744a62bbd74","sentence":"Title: ibc-c : a dataset for armed conflict event analysis Abstract: we describe the iraq body count corpus (ibc-c) dataset, the first substantial armed conflict-related dataset which can be used for conflict analysis. ibc-c provides a ground-truth dataset for conflict specific named entity recognition, slot filling, and event de-duplication. ibc-c is constructed using data collected by the iraq body count project which has been recording casualties resulting from the ongoing war in iraq since 2003. we describe the dataset\u2019s creation, how it can be used for the above three tasks and provide initial baseline results for the first task (named entity recognition) using hidden markov models, conditional random fields, and recursive neural networks.","Task":["ibc-c"],"Method":["ibc-c"]},{"title":"Text as Data for Conflict Research: A Literature Survey","abstract":"Computer-aided text analysis (CATA) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. The chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. This includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). Particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. Both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. Finally, cross-validation is highlighted as a crucial step in CATA, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","url":"https://www.semanticscholar.org/paper/3004f78d84752f3ffe8e54fa265b2ee85dac9728","sentence":"Title: text as data for conflict research: a literature survey Abstract: computer-aided text analysis (cata) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. the chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. this includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. finally, cross-validation is highlighted as a crucial step in cata, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","Task":["conflict research, computer-aided text analysis"],"Method":["text as data for conflict research"]},{"title":"One-to-X Analogical Reasoning on Word Embeddings: a Case for Diachronic Armed Conflict Prediction from News Texts","abstract":"We extend the well-known word analogy task to a one-to-X formulation, including one-to-none cases, when no correct answer exists. The task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type \u2018location:armed-group\u2019 based on data about past events. As the source of semantic information, we use diachronic word embedding models trained on English news texts. A simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. Finally, we publish a ready-to-use test set for one-to-X analogy evaluation on historical armed conflicts data.","url":"https://www.semanticscholar.org/paper/42c63d952f0cce7f89738e818fc9ab4e723bf7b6","sentence":"Title: one-to-x analogical reasoning on word embeddings: a case for diachronic armed conflict prediction from news texts Abstract: we extend the well-known word analogy task to a one-to-x formulation, including one-to-none cases, when no correct answer exists. the task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type \u2018location:armed-group\u2019 based on data about past events. as the source of semantic information, we use diachronic word embedding models trained on english news texts. a simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. finally, we publish a ready-to-use test set for one-to-x analogy evaluation on historical armed conflicts data.","Task":["one-to-x analogical reasoning"],"Method":["one-to-x analogical reasoning"]},{"title":"Using Natural Language Processing for Automatic Detection of Plagiarism","abstract":"Current plagiarism detection tools are mostly limited to comparisons of suspicious plagiarised texts and potential original texts at string level. In this study the aim is to improve the accuracy of plagiarism detection by incorporating Natural Language Processing (NLP) techniques into existing approaches. We propose a framework for external plagiarism detection in which a number of NLP techniques are applied to process a set of suspicious and original documents, not only to analyse strings but also the structure of the text, using resources to account for text relations. Initial results obtained with a corpus of plagiarised short paragraphs have showed that NLP techniques improve the accuracy of existing approaches.","url":"https://www.semanticscholar.org/paper/636d4c0b0fe6919abe6eb546907d28ed39bf56e6","sentence":"Title: using natural language processing for automatic detection of plagiarism Abstract: current plagiarism detection tools are mostly limited to comparisons of suspicious plagiarised texts and potential original texts at string level. in this study the aim is to improve the accuracy of plagiarism detection by incorporating natural language processing (nlp) techniques into existing approaches. we propose a framework for external plagiarism detection in which a number of nlp techniques are applied to process a set of suspicious and original documents, not only to analyse strings but also the structure of the text, using resources to account for text relations. initial results obtained with a corpus of plagiarised short paragraphs have showed that nlp techniques improve the accuracy of existing approaches.","Task":["external plagiarism detection"],"Method":["using natural language processing for automatic detection of plagiarism"]},{"title":"A Neural Approach to Automated Essay Scoring","abstract":"Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted Kappa, without requiring any feature engineering.","url":"https://www.semanticscholar.org/paper/aea14f23a951975f605a981d003386e46bf8acfe","sentence":"Title: a neural approach to automated essay scoring Abstract: traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. the performance of such systems is tightly bound to the quality of the underlying features. however, it is laborious to manually design the most informative features for such a system. in this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. we explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. the results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted kappa, without requiring any feature engineering.","Task":["automated essay scoring"],"Method":["neural approach to automated essay scoring"]},{"title":"Automated Scoring: Beyond Natural Language Processing","abstract":"In this position paper, we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy. Automated scoring systems warrant significant cross-discipline collaboration of which natural language processing and machine learning are just two of many important components. Such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other. Our position is that it is essential for us as NLP researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful.","url":"https://www.semanticscholar.org/paper/0eb5872733e643f43a0c1a7ff78953dfea74dfea","sentence":"Title: automated scoring: beyond natural language processing Abstract: in this position paper, we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy. automated scoring systems warrant significant cross-discipline collaboration of which natural language processing and machine learning are just two of many important components. such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other. our position is that it is essential for us as nlp researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful.","Task":["automated scoring: beyond natural language processing"],"Method":["automated scoring"]},{"title":"Event Data on Armed Conflict and Security: New Perspectives, Old Challenges, and Some Solutions","abstract":"This article presents the Event Data on Conflict and Security (EDACS) dataset, discusses the inherent problems of georeferenced conflict data, and shows how these challenges are met within EDACS. Based on an event data approach, EDACS contributes to the growing number of novel georeferenced datasets that allow researchers to identify causal pathways of violence and the dynamics of (transboundary) violence through spatiotemporal disaggregation. However, the unreflected use of any of these datasets will give researchers unjustified confidence in their findings, as the pitfalls are many and propagating errors can result in misleading conclusions. To identify and handle the different challenges to overall event data quality, we argue in favor of transparency in the data collection and coding process, to empower analysts to challenge the data and avoid cascading errors. In particular, we investigate how the choice of news sources, the handling of geographic precision, and the use of auxiliary data can bias event data. We demonstrate how the EDACS dataset design enables the analyst to deal with these issues by providing a set of variables indicating the news sources, possible sources of bias, and detailed information on geographic precision. This allows for a flexible use of the data based on individual analytical requirements.","url":"https://www.semanticscholar.org/paper/0cdeb238355617a640471987af36d8e09cd905c0","sentence":"Title: event data on armed conflict and security: new perspectives, old challenges, and some solutions Abstract: this article presents the event data on conflict and security (edacs) dataset, discusses the inherent problems of georeferenced conflict data, and shows how these challenges are met within edacs. based on an event data approach, edacs contributes to the growing number of novel georeferenced datasets that allow researchers to identify causal pathways of violence and the dynamics of (transboundary) violence through spatiotemporal disaggregation. however, the unreflected use of any of these datasets will give researchers unjustified confidence in their findings, as the pitfalls are many and propagating errors can result in misleading conclusions. to identify and handle the different challenges to overall event data quality, we argue in favor of transparency in the data collection and coding process, to empower analysts to challenge the data and avoid cascading errors. in particular, we investigate how the choice of news sources, the handling of geographic precision, and the use of auxiliary data can bias event data. we demonstrate how the edacs dataset design enables the analyst to deal with these issues by providing a set of variables indicating the news sources, possible sources of bias, and detailed information on geographic precision. this allows for a flexible use of the data based on individual analytical requirements.","Task":["event data, georeferenced conflict data"],"Method":["event data on conflict and security"]},{"title":"Tracing armed conflicts with diachronic word embedding models","abstract":"Recent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. In this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the Gigaword news corpus as the training data. The results show that much work still remains in extracting \u2018cultural\u2019 semantic shifts from diachronic word embedding models. At the same time, we present a new task complete with an evaluation set and introduce the \u2018anchor words\u2019 method which outperforms previous approaches on this set.","url":"https://www.semanticscholar.org/paper/2ca0403eabc3893fed255fc119a927d83a1af739","sentence":"Title: tracing armed conflicts with diachronic word embedding models Abstract: recent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. in this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the gigaword news corpus as the training data. the results show that much work still remains in extracting \u2018cultural\u2019 semantic shifts from diachronic word embedding models. at the same time, we present a new task complete with an evaluation set and introduce the \u2018anchor words\u2019 method which outperforms previous approaches on this set.","Task":["word embedding models"],"Method":["diachronic word embedding models"]},{"title":"Enriching textbooks through data mining","abstract":"Textbooks play an important role in any educational system. Unfortunately, many textbooks produced in developing countries are not written well and they often lack adequate coverage of important concepts. We propose a technological solution to address this problem based on enriching textbooks with authoritative web content. We augment textbooks at the section level for key concepts discussed in the section. We use ideas from data mining for identifying the concepts that need augmentation as well as to determine the links to the authoritative content that should be used for augmentation. Our evaluation, employing textbooks from India, shows that we are able to enrich textbooks on different subjects and across different grades with high quality augmentations using automated techniques.","url":"https://www.semanticscholar.org/paper/4ed75f35ae7343cd906f311390f572a58e36805c","sentence":"Title: enriching textbooks through data mining Abstract: textbooks play an important role in any educational system. unfortunately, many textbooks produced in developing countries are not written well and they often lack adequate coverage of important concepts. we propose a technological solution to address this problem based on enriching textbooks with authoritative web content. we augment textbooks at the section level for key concepts discussed in the section. we use ideas from data mining for identifying the concepts that need augmentation as well as to determine the links to the authoritative content that should be used for augmentation. our evaluation, employing textbooks from india, shows that we are able to enrich textbooks on different subjects and across different grades with high quality augmentations using automated techniques.","Task":["data mining"],"Method":["enriching textbooks through data mining"]},{"title":"Educational Question Answering Motivated by Question-Specific Concept Maps","abstract":"Question answering (QA) is the automated process of answering general questions submitted by humans in natural language. QA has previously been explored within the educational context to facilitate learning, however the majority of works have focused on text-based answering. As an alternative, this paper proposes an approach to return answers as a concept map, which further encourages meaningful learning and knowledge organisation. Additionally, this paper investigates whether adapting the returned concept map to the specific question context provides further learning benefit. A randomised experiment was conducted with a sample of 59 Computer Science undergraduates, obtaining statistically significant results on learning gain when students are provided with the question-specific concept maps. Further, time spent on studying the concept maps were positively correlated with the learning gain.","url":"https://www.semanticscholar.org/paper/bc6a4304afdf59783638a6a0d9fc2c2acb6e5b67","sentence":"Title: educational question answering motivated by question-specific concept maps Abstract: question answering (qa) is the automated process of answering general questions submitted by humans in natural language. qa has previously been explored within the educational context to facilitate learning, however the majority of works have focused on text-based answering. as an alternative, this paper proposes an approach to return answers as a concept map, which further encourages meaningful learning and knowledge organisation. additionally, this paper investigates whether adapting the returned concept map to the specific question context provides further learning benefit. a randomised experiment was conducted with a sample of 59 computer science undergraduates, obtaining statistically significant results on learning gain when students are provided with the question-specific concept maps. further, time spent on studying the concept maps were positively correlated with the learning gain.","Task":["educational question answering"],"Method":["educational question answering motivated by question-specific concept maps"]},{"title":"Characterizing Stage-aware Writing Assistance for Collaborative Document Authoring","abstract":"Writing is a complex non-linear process that begins with a mental model of intent, and progresses through an outline of ideas, to words on paper (and their subsequent refinement). Despite past research in understanding writing, Web-scale consumer and enterprise collaborative digital writing environments are yet to greatly benefit from intelligent systems that understand the stages of document evolution, providing opportune assistance based on authors\' situated actions and context. In this paper, we present three studies that explore temporal stages of document authoring. We first survey information workers at a large technology company about their writing habits and preferences, concluding that writers do in fact conceptually progress through several distinct phases while authoring documents. We also explore, qualitatively, how writing stages are linked to document lifespan. We supplement these qualitative findings with an analysis of the longitudinal user interaction logs of a popular digital writing platform over several million documents. Finally, as a first step towards facilitating an intelligent digital writing assistant, we conduct a preliminary investigation into the utility of user interaction log data for predicting the temporal stage of a document. Our results support the benefit of tools tailored to writing stages, identify primary tasks associated with these stages, and show that it is possible to predict stages from anonymous interaction logs. Together, these results argue for the benefit and feasibility of more tailored digital writing assistance.","url":"https://www.semanticscholar.org/paper/911e61212eb26325d3fe58f454f36f4e70c54c7a","sentence":"Title: characterizing stage-aware writing assistance for collaborative document authoring Abstract: writing is a complex non-linear process that begins with a mental model of intent, and progresses through an outline of ideas, to words on paper (and their subsequent refinement). despite past research in understanding writing, web-scale consumer and enterprise collaborative digital writing environments are yet to greatly benefit from intelligent systems that understand the stages of document evolution, providing opportune assistance based on authors\' situated actions and context. in this paper, we present three studies that explore temporal stages of document authoring. we first survey information workers at a large technology company about their writing habits and preferences, concluding that writers do in fact conceptually progress through several distinct phases while authoring documents. we also explore, qualitatively, how writing stages are linked to document lifespan. we supplement these qualitative findings with an analysis of the longitudinal user interaction logs of a popular digital writing platform over several million documents. finally, as a first step towards facilitating an intelligent digital writing assistant, we conduct a preliminary investigation into the utility of user interaction log data for predicting the temporal stage of a document. our results support the benefit of tools tailored to writing stages, identify primary tasks associated with these stages, and show that it is possible to predict stages from anonymous interaction logs. together, these results argue for the benefit and feasibility of more tailored digital writing assistance.","Task":["stage-aware digital writing assistance for collaborative document authoring"],"Method":["stage-aware digital writing"]},{"title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.","url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","sentence":"Title: question answering system on education acts using nlp techniques Abstract: question answering (qa) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. it presents only the requested information instead of searching full documents like search engine. as information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. this is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain qa system for handling documents related to education acts sections to retrieve more precise answers using nlp techniques.","Task":["question answering system on education acts using nlp techniques"],"Method":["question answering system on education acts using nlp techniques"]},{"title":"Natural Language Processing and Language Learning","abstract":"As a relatively young field of research and development started by work on cryptanalysis and machine translation around 50 years ago, Natural Language Processing (NLP) is concerned with the automated processing of human language. It addresses the analysis and generation of written and spoken language, though speech processing is often regarded as a separate subfield. NLP emphasizes processing and applications and as such can be seen as the applied side of Computational Linguistics, the interdisciplinary field of research concerned with formal analysis and modeling of language and its applications at the intersection of Linguistics, Computer Science, and Psychology. In terms of the language aspects dealt with in NLP, traditionally lexical, morphological and syntactic aspects of language were at the center of attention, but aspects of meaning, discourse, and the relation to the extra-linguistic context have become increasingly prominent in the last decade. A good introduction and overview of the field is provided in Jurafsky & Martin (2009).","url":"https://www.semanticscholar.org/paper/f8868be52c8c0d64920e5f67870c01ba48601608","sentence":"Title: natural language processing and language learning Abstract: as a relatively young field of research and development started by work on cryptanalysis and machine translation around 50 years ago, natural language processing (nlp) is concerned with the automated processing of human language. it addresses the analysis and generation of written and spoken language, though speech processing is often regarded as a separate subfield. nlp emphasizes processing and applications and as such can be seen as the applied side of computational linguistics, the interdisciplinary field of research concerned with formal analysis and modeling of language and its applications at the intersection of linguistics, computer science, and psychology. in terms of the language aspects dealt with in nlp, traditionally lexical, morphological and syntactic aspects of language were at the center of attention, but aspects of meaning, discourse, and the relation to the extra-linguistic context have become increasingly prominent in the last decade. a good introduction and overview of the field is provided in jurafsky & martin (2009).","Task":["natural language processing and language learning"],"Method":["natural language processing and language learning"]},{"title":"Modeling the Relationship between User Comments and Edits in Document Revision","abstract":"Management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a document\u2019s evolution. Reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. A number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. Thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: Comment Ranking and Edit Anchoring. We begin by collecting a dataset with more than half a million comment-edit pairs based on Wikipedia revision histories. We then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. Our architecture tackles both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. In a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. We are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for Comment Ranking, while we achieve 74.4% accuracy on Edit Anchoring.","url":"https://www.semanticscholar.org/paper/311381feeb6346bfcb2ba622bd8f713261a4075d","sentence":"Title: modeling the relationship between user comments and edits in document revision Abstract: management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a document\u2019s evolution. reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. a number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: comment ranking and edit anchoring. we begin by collecting a dataset with more than half a million comment-edit pairs based on wikipedia revision histories. we then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. our architecture tackles both comment ranking and edit anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. in a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. we are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for comment ranking, while we achieve 74.4% accuracy on edit anchoring.","Task":["document revision, deep neural networks"],"Method":["modeling the relationship between user comments and edits in document revision"]},{"title":"A Multimodal Human-Computer Interaction System and Its Application in Smart Learning Environments","abstract":"A multimodal human-computer interaction system is composed of the comprehensive usage of various input and output channels. For the information input, apart from the traditional keyboard typing, mouse clicking, screen touching, the latest speech and face recognition technology can be used. For the output, the traditional screen display, the latest speech and facial expression synthesis and gesture generation can be used. After literature review of related works, this paper at first presents such a system, MMISE (Multimodal Interaction System for Education), about its architecture and working mechanism, POOOIIM (Pedagogical Objective Oriented Output, Input and Implementation Mechanism) illustrated with practical examples. Then this paper introduces this system\u2019s pilot applications in the epidemic time of novel coronavirus in 2020.","url":"https://www.semanticscholar.org/paper/d114af5d3dcb3792bd9faec1476feba5aaf3617f","sentence":"Title: a multimodal human-computer interaction system and its application in smart learning environments Abstract: a multimodal human-computer interaction system is composed of the comprehensive usage of various input and output channels. for the information input, apart from the traditional keyboard typing, mouse clicking, screen touching, the latest speech and face recognition technology can be used. for the output, the traditional screen display, the latest speech and facial expression synthesis and gesture generation can be used. after literature review of related works, this paper at first presents such a system, mmise (multimodal interaction system for education), about its architecture and working mechanism, poooiim (pedagogical objective oriented output, input and implementation mechanism) illustrated with practical examples. then this paper introduces this system\u2019s pilot applications in the epidemic time of novel coronavirus in 2020.","Task":["multimodal human-computer interaction system"],"Method":["multimodal interaction system for education"]},{"title":"What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations","abstract":"The quality of a counseling intervention relies highly on the active collaboration between clients and counselors. In this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. Specifically, we address the differences between high-quality and low-quality counseling. Our approach examines participants\u2019 turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. Our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.","url":"https://www.semanticscholar.org/paper/9b0c9d241269b98c80f65a14d5d65263d0688d70","sentence":"Title: what makes a good counselor? learning to distinguish between high-quality and low-quality counseling conversations Abstract: the quality of a counseling intervention relies highly on the active collaboration between clients and counselors. in this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. specifically, we address the differences between high-quality and low-quality counseling. our approach examines participants\u2019 turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. these features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.","Task":["counseling, linguistics"],"Method":["linguistics in counseling"]},{"title":"Quantifying the Effects of COVID-19 on Mental Health Support Forums","abstract":"The COVID-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. Understanding its impact can inform strategies for mitigating negative consequences. In this work, we seek to better understand the effects of COVID-19 on mental health by examining discussions within mental health support communities on Reddit. First, we quantify the rate at which COVID-19 is discussed in each community, or subreddit, in order to understand levels of preoccupation with the pandemic. Next, we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen. Finally, we analyze how COVID-19 has influenced language use and topics of discussion within each subreddit.","url":"https://www.semanticscholar.org/paper/aa2bbf5a1485cc90401e4a35c04463fcb4e632f0","sentence":"Title: quantifying the effects of covid-19 on mental health support forums Abstract: the covid-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. understanding its impact can inform strategies for mitigating negative consequences. in this work, we seek to better understand the effects of covid-19 on mental health by examining discussions within mental health support communities on reddit. first, we quantify the rate at which covid-19 is discussed in each community, or subreddit, in order to understand levels of preoccupation with the pandemic. next, we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen. finally, we analyze how covid-19 has influenced language use and topics of discussion within each subreddit.","Task":["covid-19"],"Method":["the effects of covid-19 on mental health support forums"]},{"title":"Data Mining and Student e-Learning Profiles","abstract":"Data mining techniques have been applied to educational research in various ways. In this paper, I presented the application of sequential data mining algorithms to analyze computer logs to profile learners in terms of their learning tactic use and motivation in a web-based learning environment (gStudy). The data mining algorithms are employed to discover patterns which characterize learners either across session or groups based on their study tactic choice and goal orientation. The use of this method is illustrated through a sequential pattern analysis of gStudy log files generated by university students.","url":"https://www.semanticscholar.org/paper/81d13af7eb78725492854bb9860b57af2b2a1778","sentence":"Title: data mining and student e-learning profiles Abstract: data mining techniques have been applied to educational research in various ways. in this paper, i presented the application of sequential data mining algorithms to analyze computer logs to profile learners in terms of their learning tactic use and motivation in a web-based learning environment (gstudy). the data mining algorithms are employed to discover patterns which characterize learners either across session or groups based on their study tactic choice and goal orientation. the use of this method is illustrated through a sequential pattern analysis of gstudy log files generated by university students.","Task":["data mining"],"Method":["data mining and student e-learning profiles"]},{"title":"Inferring Social Media Users\u2019 Mental Health Status from Multimodal Information","abstract":"Worldwide, an increasing number of people are suffering from mental health disorders such as depression and anxiety. In the United States alone, one in every four adults suffers from a mental health condition, which makes mental health a pressing concern. In this paper, we explore the use of multimodal cues present in social media posts to predict users\u2019 mental health status. Specifically, we focus on identifying social media activity that either indicates a mental health condition or its onset. We collect posts from Flickr and apply a multimodal approach that consists of jointly analyzing language, visual, and metadata cues and their relation to mental health. We conduct several classification experiments aiming to discriminate between (1) healthy users and users affected by a mental health illness; and (2) healthy users and users prone to mental illness. Our experimental results indicate that using multiple modalities can improve the performance of this classification task as compared to the use of one modality at a time, and can provide important cues into a user\u2019s mental status.","url":"https://www.semanticscholar.org/paper/065332576fd9b264a5a338549239bb39e6733819","sentence":"Title: inferring social media users\u2019 mental health status from multimodal information Abstract: worldwide, an increasing number of people are suffering from mental health disorders such as depression and anxiety. in the united states alone, one in every four adults suffers from a mental health condition, which makes mental health a pressing concern. in this paper, we explore the use of multimodal cues present in social media posts to predict users\u2019 mental health status. specifically, we focus on identifying social media activity that either indicates a mental health condition or its onset. we collect posts from flickr and apply a multimodal approach that consists of jointly analyzing language, visual, and metadata cues and their relation to mental health. we conduct several classification experiments aiming to discriminate between (1) healthy users and users affected by a mental health illness; and (2) healthy users and users prone to mental illness. our experimental results indicate that using multiple modalities can improve the performance of this classification task as compared to the use of one modality at a time, and can provide important cues into a user\u2019s mental status.","Task":["social media, mental health"],"Method":["social media and mental health"]},{"title":"Expressive Interviewing: A Conversational System for Coping with COVID-19","abstract":"The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce \\\\textit{Expressive Interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system\'s design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues.","url":"https://www.semanticscholar.org/paper/c278eb333c78313a3762b5b2f07ace7443582b94","sentence":"Title: expressive interviewing: a conversational system for coping with covid-19 Abstract: the ongoing covid-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. we introduce \\\\textit{expressive interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. expressive interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how covid-19 has impacted their lives. we present relevant aspects of the system\'s design and implementation as well as quantitative and qualitative analyses of user interactions with the system. in addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with covid-19 issues.","Task":["expressive interviewing"],"Method":["expressive interviewing"]},{"title":"Understanding and Predicting Empathic Behavior in Counseling Therapy","abstract":"Counselor empathy is associated with better outcomes in psychology and behavioral counseling. In this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. Particularly, we analyze aspects such as participants\u2019 engagement, participants\u2019 verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. We also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80%.","url":"https://www.semanticscholar.org/paper/35112c6bfa715b2bc86fc5b6c331f8a1a0b07add","sentence":"Title: understanding and predicting empathic behavior in counseling therapy Abstract: counselor empathy is associated with better outcomes in psychology and behavioral counseling. in this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. particularly, we analyze aspects such as participants\u2019 engagement, participants\u2019 verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. we also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80%.","Task":["counselor empathy"],"Method":["counselor empathy"]},{"title":"Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health","abstract":"Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.","url":"https://www.semanticscholar.org/paper/c151f144c2c0e8d3b176edaf2ce5369c7707bd31","sentence":"Title: large-scale analysis of counseling conversations: an application of natural language processing to mental health Abstract: mental illness is one of the most pressing public health issues of our time. while counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. in this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. we develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.","Task":["large-scale analysis of counseling conversations"],"Method":["text-message-based counseling conversations"]},{"title":"Fermi at SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media using Sentence Embeddings","abstract":"This paper describes our system (Fermi) for Task 6: OffensEval: Identifying and Categorizing Offensive Language in Social Media of SemEval-2019. We participated in all the three sub-tasks within Task 6. We evaluate multiple sentence embeddings in conjunction with various supervised machine learning algorithms and evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team Fermi\u2019s model achieved an F1-score of 64.40%, 62.00% and 62.60% for sub-task A, B and C respectively on the official leaderboard. Our model for sub-task C which uses pre-trained ELMo embeddings for transforming the input and uses SVM (RBF kernel) for training, scored third position on the official leaderboard. Through the paper we provide a detailed description of the approach, as well as the results obtained for the task.","url":"https://www.semanticscholar.org/paper/9de154d3c886177380062be7c8d50304a335752f","sentence":"Title: fermi at semeval-2019 task 6: identifying and categorizing offensive language in social media using sentence embeddings Abstract: this paper describes our system (fermi) for task 6: offenseval: identifying and categorizing offensive language in social media of semeval-2019. we participated in all the three sub-tasks within task 6. we evaluate multiple sentence embeddings in conjunction with various supervised machine learning algorithms and evaluate the performance of simple yet effective embedding-ml combination algorithms. our team fermi\u2019s model achieved an f1-score of 64.40%, 62.00% and 62.60% for sub-task a, b and c respectively on the official leaderboard. our model for sub-task c which uses pre-trained elmo embeddings for transforming the input and uses svm (rbf kernel) for training, scored third position on the official leaderboard. through the paper we provide a detailed description of the approach, as well as the results obtained for the task.","Task":["machine learning"],"Method":["fermi at semeval-2019"]},{"title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","sentence":"Title: building a motivational interviewing dataset Abstract: this paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during motivational interviewing encounters. annotations were conducted using the motivational interviewing integrity treatment (miti). we describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. the dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. the reliability analysis showed that annotators achieved excellent agreement at session level, with intraclass correlation coefficient (icc) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with cohen\u2019s kappa scores ranging from 0.31 to 0.64. behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (resnicow et al., 2002). in particular, motivational interviewing (mi), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (moyers et al., 2009; apodaca et al., 2014; barnett et al., 2014; catley et al., 2012). despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing mi counseling at larger scale or in other domains is limited by the need for human-based evaluations. currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. recently, computational approaches have been proposed to aid the mi evaluation process (atkins et al., 2014; xiao et al., 2014; klonek et al., 2015). however, learning resources for this task are not readily available. having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of mi. moreover, this can also be useful to explore how mi works by relating mi behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their mi skills. in this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the motivational interviewing treatment integrity 4.0 (miti), which is the current gold standard for mi-based psychology interventions. the dataset is derived from 277 mi sessions containing a total of 22,719 coded utterances. 1 motivational interviewing miller and rollnick define mi as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (miller and rollnick, 2013). mi has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Task":["motivational interviewing"],"Method":["motivational interviewing"]},{"title":"Predicting Counselor Behaviors in Motivational Interviewing Encounters","abstract":"As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","url":"https://www.semanticscholar.org/paper/55925bc2522a1d4cb007ac78273f7c2efafa3916","sentence":"Title: predicting counselor behaviors in motivational interviewing encounters Abstract: as the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. in this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. in particular, we present a model towards the automation of motivational interviewing (mi) coding, which is the current gold standard to evaluate mi counseling. first, we build a dataset of hand labeled mi encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. we introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","Task":["motivational interviewing"],"Method":["motivational interviewing"]},{"title":"Happiness Entailment: Automating Suggestions for Well-Being","abstract":"Understanding what makes people happy is a central topic in psychology. Prior work has mostly focused on developing self-reporting assessment tools for individuals and relies on experts to analyze the periodic reported assessments. One of the goals of the analysis is to understand what actions are necessary to encourage modifications in the behaviors of the individuals to improve their overall well-being. In this paper, we outline a complementary approach; on the assumption that the user journals her happy moments as short texts, a system can analyze these texts and propose sustainable suggestions for the user that may lead to an overall improvement in her well-being. We prototype one necessary component of such a system, the Happiness Entailment Recognition (HER)module, which takes as input a short text describing an event, a candidate suggestion, and outputs a determination about whether the suggestion is more likely to be good for this user based on the event described. This component is implemented as a neural network model with two encoders, one for the user input and one for the candidate actionable suggestion, with additional layers to capture psychologically significant features in the happy moment and suggestion. Our model achieves an AU-ROC of 0.831 and outperforms our baseline as well as the current state-of-the-art Textual Entailment model from AllenNLP by more than 48% of improvements, confirming the uniqueness and complexity of the HER task.","url":"https://www.semanticscholar.org/paper/c2fd13dab7a581f2f875826b204d063f42b63a8b","sentence":"Title: happiness entailment: automating suggestions for well-being Abstract: understanding what makes people happy is a central topic in psychology. prior work has mostly focused on developing self-reporting assessment tools for individuals and relies on experts to analyze the periodic reported assessments. one of the goals of the analysis is to understand what actions are necessary to encourage modifications in the behaviors of the individuals to improve their overall well-being. in this paper, we outline a complementary approach; on the assumption that the user journals her happy moments as short texts, a system can analyze these texts and propose sustainable suggestions for the user that may lead to an overall improvement in her well-being. we prototype one necessary component of such a system, the happiness entailment recognition (her)module, which takes as input a short text describing an event, a candidate suggestion, and outputs a determination about whether the suggestion is more likely to be good for this user based on the event described. this component is implemented as a neural network model with two encoders, one for the user input and one for the candidate actionable suggestion, with additional layers to capture psychologically significant features in the happy moment and suggestion. our model achieves an au-roc of 0.831 and outperforms our baseline as well as the current state-of-the-art textual entailment model from allennlp by more than 48% of improvements, confirming the uniqueness and complexity of the her task.","Task":["happiness entailment"],"Method":["happiness entailment"]},{"title":"FERMI at SemEval-2019 Task 5: Using Sentence embeddings to Identify Hate Speech Against Immigrants and Women in Twitter","abstract":"This paper describes our system (Fermi) for Task 5 of SemEval-2019: HatEval: Multilingual Detection of Hate Speech Against Immigrants and Women on Twitter. We participated in the subtask A for English and ranked first in the evaluation on the test set. We evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team - Fermi\u2019s model achieved an accuracy of 65.00% for English language in task A. Our models, which use pretrained Universal Encoder sentence embeddings for transforming the input and SVM (with RBF kernel) for classification, scored first position (among 68) in the leaderboard on the test set for Subtask A in English language. In this paper we provide a detailed description of the approach, as well as the results obtained in the task.","url":"https://www.semanticscholar.org/paper/64c68fa52491c4a815f21917e068e4c19dd404b0","sentence":"Title: fermi at semeval-2019 task 5: using sentence embeddings to identify hate speech against immigrants and women in twitter Abstract: this paper describes our system (fermi) for task 5 of semeval-2019: hateval: multilingual detection of hate speech against immigrants and women on twitter. we participated in the subtask a for english and ranked first in the evaluation on the test set. we evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ml combination algorithms. our team - fermi\u2019s model achieved an accuracy of 65.00% for english language in task a. our models, which use pretrained universal encoder sentence embeddings for transforming the input and svm (with rbf kernel) for classification, scored first position (among 68) in the leaderboard on the test set for subtask a in english language. in this paper we provide a detailed description of the approach, as well as the results obtained in the task.","Task":["semeval-2019"],"Method":["fermi at semeval-2019 task 5: using sentence embeddings to identify hate speech against immigrants and women in twitter"]},{"title":"Ingredients for Happiness: Modeling constructs via semi-supervised content driven inductive transfer","abstract":"Modeling affect via understanding the social constructs behind them is an important task in devising robust and accurate systems for socially relevant scenarios. In the CL-Aff Shared Task (part of Affective Content Analysis workshop @ AAAI 2019), the organizers released a dataset of \u2018happy\u2019 moments, called the HappyDB corpus. The task is to detect two social constructs: the agency (i.e., whether the author is in control of the happy moment) and the social characteristics (i.e., whether anyone else other than the author was also involved in the happy moment). We employ an inductive transfer learning technique where we utilize a pre-trained language model and fine-tune it on the target task for both the binary classification tasks. At first, we use a language model pre-trained on the huge WikiText-103 corpus. This step utilizes an AWDLSTM with three hidden layers for training the language model. In the second step, we fine-tune the pre-trained language model on both the labeled and unlabeled instances from the HappyDB dataset. Finally, we train a classifier on top of the language model for each of the identification tasks. Our experiments using 10-fold cross validation on the corpus show that we achieve a high accuracy of \u223c93% for detection of the social characteristic and \u223c87% for agency of the author, showing significant gains over other baselines. We also show that using the unlabeled dataset for fine-tuning the language model in the second step improves our accuracy by 1-2% across detection of both the constructs.","url":"https://www.semanticscholar.org/paper/7cb5bc9f8bc023f1f2c94e2e83012f1d35e4e01d","sentence":"Title: ingredients for happiness: modeling constructs via semi-supervised content driven inductive transfer Abstract: modeling affect via understanding the social constructs behind them is an important task in devising robust and accurate systems for socially relevant scenarios. in the cl-aff shared task (part of affective content analysis workshop @ aaai 2019), the organizers released a dataset of \u2018happy\u2019 moments, called the happydb corpus. the task is to detect two social constructs: the agency (i.e., whether the author is in control of the happy moment) and the social characteristics (i.e., whether anyone else other than the author was also involved in the happy moment). we employ an inductive transfer learning technique where we utilize a pre-trained language model and fine-tune it on the target task for both the binary classification tasks. at first, we use a language model pre-trained on the huge wikitext-103 corpus. this step utilizes an awdlstm with three hidden layers for training the language model. in the second step, we fine-tune the pre-trained language model on both the labeled and unlabeled instances from the happydb dataset. finally, we train a classifier on top of the language model for each of the identification tasks. our experiments using 10-fold cross validation on the corpus show that we achieve a high accuracy of \u223c93% for detection of the social characteristic and \u223c87% for agency of the author, showing significant gains over other baselines. we also show that using the unlabeled dataset for fine-tuning the language model in the second step improves our accuracy by 1-2% across detection of both the constructs.","Task":["ingredients for happiness"],"Method":["ingredients for happiness"]},{"title":"HappyDB: A Corpus of 100, 000 Crowdsourced Happy Moments","abstract":"The science of happiness is an area of positive psychology concerned with understanding what behaviors make people happy in a sustainable fashion. Recently, there has been interest in developing technologies that help incorporate the findings of the science of happiness into users\' daily lives by steering them towards behaviors that increase happiness. With the goal of building technology that can understand how people express their happy moments in text, we crowd-sourced HappyDB, a corpus of 100,000 happy moments that we make publicly available. This paper describes HappyDB and its properties, and outlines several important NLP problems that can be studied with the help of the corpus. We also apply several state-of-the-art analysis techniques to analyze HappyDB. Our results demonstrate the need for deeper NLP techniques to be developed which makes HappyDB an exciting resource for follow-on research.","url":"https://www.semanticscholar.org/paper/c239aaa2a87ea63fae666b2051f1923afaa45fc2","sentence":"Title: happydb: a corpus of 100, 000 crowdsourced happy moments Abstract: the science of happiness is an area of positive psychology concerned with understanding what behaviors make people happy in a sustainable fashion. recently, there has been interest in developing technologies that help incorporate the findings of the science of happiness into users\' daily lives by steering them towards behaviors that increase happiness. with the goal of building technology that can understand how people express their happy moments in text, we crowd-sourced happydb, a corpus of 100,000 happy moments that we make publicly available. this paper describes happydb and its properties, and outlines several important nlp problems that can be studied with the help of the corpus. we also apply several state-of-the-art analysis techniques to analyze happydb. our results demonstrate the need for deeper nlp techniques to be developed which makes happydb an exciting resource for follow-on research.","Task":["happydb"],"Method":["happydb"]},{"title":"HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","sentence":"Title: hopeedi: a multilingual hope speech detection dataset for equality, diversity, and inclusion Abstract: over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. however, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. until now, most studies have focused on solving this problem of negativity in the english language, though the problem is much more than just harmful content. furthermore, it is multilingual as well. thus, we have constructed a hope speech dataset for equality, diversity and inclusion (hopeedi) containing user-generated comments from the social media platform youtube with 28,451, 20,198 and 10,705 comments in english, tamil and malayalam, respectively, manually labelled as containing hope speech or not. to our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. we determined that the inter-annotator agreement of our dataset using krippendorff\u2019s alpha. further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and f1-score. the dataset is publicly available for the research community. we hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","Task":["hope speech, equality, diversity and inclusion"],"Method":["hope speech datasets for equality, diversity and inclusion"]},{"title":"Women worry about family, men about the economy: Gender differences in emotional responses to COVID-19","abstract":"Among the critical challenges around the COVID-19 pandemic is dealing with the potentially detrimental effects on people\'s mental health. Designing appropriate interventions and identifying the concerns of those most at risk requires methods that can extract worries, concerns and emotional responses from text data. We examine gender differences and the effect of document length on worries about the ongoing COVID-19 situation. Our findings suggest that i) short texts do not offer as adequate insights into psychological processes as longer texts. We further find ii) marked gender differences in topics concerning emotional responses. Women worried more about their loved ones and severe health concerns while men were more occupied with effects on the economy and society. This paper adds to the understanding of general gender differences in language found elsewhere, and shows that the current unique circumstances likely amplified these effects. We close this paper with a call for more high-quality datasets due to the limitations of Tweet-sized data.","url":"https://www.semanticscholar.org/paper/32bf7ad3fdda71036b48f7dc85cad407674277b7","sentence":"Title: women worry about family, men about the economy: gender differences in emotional responses to covid-19 Abstract: among the critical challenges around the covid-19 pandemic is dealing with the potentially detrimental effects on people\'s mental health. designing appropriate interventions and identifying the concerns of those most at risk requires methods that can extract worries, concerns and emotional responses from text data. we examine gender differences and the effect of document length on worries about the ongoing covid-19 situation. our findings suggest that i) short texts do not offer as adequate insights into psychological processes as longer texts. we further find ii) marked gender differences in topics concerning emotional responses. women worried more about their loved ones and severe health concerns while men were more occupied with effects on the economy and society. this paper adds to the understanding of general gender differences in language found elsewhere, and shows that the current unique circumstances likely amplified these effects. we close this paper with a call for more high-quality datasets due to the limitations of tweet-sized data.","Task":["covid-19"],"Method":["Covid-19"]},{"title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","sentence":"Title: the climate change debate and natural language processing Abstract: the debate around climate change (cc)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. yet, in the natural language processing (nlp) community, this domain has so far received little attention. in contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of cc-related text. other research is qualitative in nature and studies details, nuances, actors, and motivations within cc discourses. coming from both nlp and political science, and reviewing key works in both disciplines, we discuss how social science approaches to cc debates can inform advances in text-mining/nlp, and how, in return, nlp can support policy-makers and activists in making sense of large-scale and complex cc discourses across multiple genres, channels, topics, and communities. this is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","Task":["climate change"],"Method":["climate change and natural language processing"]},{"title":"Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism","abstract":"Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised BERT-based models.","url":"https://www.semanticscholar.org/paper/a9d09338a3a0d9102c1e623e6ad434a446c0bbfe","sentence":"Title: automatic classification of neutralization techniques in the narrative of climate change scepticism Abstract: neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. we first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised bert-based models.","Task":["natural language processing"],"Method":["natural language processing"]},{"title":"CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims","abstract":"We introduce CLIMATE-FEVER, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of FEVER [1], the largest dataset of artificially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \\\\textsc{fever} framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and AI community.","url":"https://www.semanticscholar.org/paper/086e6733e5fda70bbce0c7545bd06d5634918a60","sentence":"Title: climate-fever: a dataset for verification of real-world climate claims Abstract: we introduce climate-fever, a new publicly available dataset for verification of climate change-related claims. by providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. we adapt the methodology of fever [1], the largest dataset of artificially designed claims, to real-life claims collected from the internet. while during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. we discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \\\\textsc{fever} framework, which we believe provides a valuable challenge for general natural language understanding. we hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and ai community.","Task":["climate-fever"],"Method":["climate-fever"]},{"title":"Cheap Talk and Cherry-Picking: What ClimateBert has to say on Corporate Climate Risk Disclosures","abstract":"Disclosure of climate-related financial risks greatly helps investors assess companies\' preparedness for climate change. Voluntary disclosures such as those based on the recommendations of the Task Force for Climate-related Financial Disclosures (TCFD) are being hailed as an effective measure for better climate risk management. We ask whether this expectation is justified. We do so with the help of a deep neural language model, which we christen ClimateBert. We train ClimateBert on thousands of sentences related to climate-risk disclosures aligned with the TCFD recommendations. In analyzing the disclosures of TCFD-supporting firms, ClimateBert comes to the sobering conclusion that the firms\' TCFD support is mostly cheap talk and that firms cherry-pick to report primarily non-material climate risk information. From our analysis, we conclude that the only way out of this dilemma is to turn voluntary reporting into regulatory disclosures.","url":"https://www.semanticscholar.org/paper/52ef2bea83eba1575fcfd02c82b4228f7aab0bd6","sentence":"Title: cheap talk and cherry-picking: what climatebert has to say on corporate climate risk disclosures Abstract: disclosure of climate-related financial risks greatly helps investors assess companies\' preparedness for climate change. voluntary disclosures such as those based on the recommendations of the task force for climate-related financial disclosures (tcfd) are being hailed as an effective measure for better climate risk management. we ask whether this expectation is justified. we do so with the help of a deep neural language model, which we christen climatebert. we train climatebert on thousands of sentences related to climate-risk disclosures aligned with the tcfd recommendations. in analyzing the disclosures of tcfd-supporting firms, climatebert comes to the sobering conclusion that the firms\' tcfd support is mostly cheap talk and that firms cherry-pick to report primarily non-material climate risk information. from our analysis, we conclude that the only way out of this dilemma is to turn voluntary reporting into regulatory disclosures.","Task":["climate risk disclosures"],"Method":["climate-related financial disclosures"]},{"title":"Tackling Climate Change with Machine Learning","abstract":"Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.","url":"https://www.semanticscholar.org/paper/998039a4876edc440e0cabb0bc42239b0eb29644","sentence":"Title: tackling climate change with machine learning Abstract: climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. from smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. our recommendations encompass exciting research questions as well as promising business opportunities. we call on the machine learning community to join the global effort against climate change.","Task":["climate change, machine learning"],"Method":["climate change and machine learning"]},{"title":"Learning Twitter User Sentiments on Climate Change with Limited Labeled Data","abstract":"While it is well-documented that climate change accepters and deniers have become increasingly polarized in the United States over time, there has been no large-scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences. On the sub-population of Twitter users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the U.S. in 2018. We begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic-level results in line with prior research. We then apply RNNs to conduct a cohort-level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change. However, this effect does not hold for the 2018 blizzard and wildfires studied, implying that Twitter users\' opinions on climate change are fairly ingrained on this subset of natural disasters.","url":"https://www.semanticscholar.org/paper/f071e7c19fc18fc4d4ab69ce13f0c4e92cfb08c4","sentence":"Title: learning twitter user sentiments on climate change with limited labeled data Abstract: while it is well-documented that climate change accepters and deniers have become increasingly polarized in the united states over time, there has been no large-scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences. on the sub-population of twitter users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the u.s. in 2018. we begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic-level results in line with prior research. we then apply rnns to conduct a cohort-level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change. however, this effect does not hold for the 2018 blizzard and wildfires studied, implying that twitter users\' opinions on climate change are fairly ingrained on this subset of natural disasters.","Task":["climate change, natural disasters"],"Method":["learning twitter user sentiments on climate change with limited labeled data"]},{"title":"Ask BERT: How Regulatory Disclosure of Transition and Physical Climate Risks affects the CDS Term Structure","abstract":"We use BERT, an AI-based algorithm for language understanding, to decipher regulatory climate-risk disclosures and measure their impact on the credit default swap (CDS) market. Risk disclosures can either increase or decrease credit spreads, depending on whether disclosure reveals new risks or sharpens the signal and decreases the uncertainty. Training BERT to differentiate between transition and physical climate risks, we find that disclosing transition risks increases CDS spreads, especially after the Paris Climate Agreement of 2015, while disclosing physical climate risks leads to a decrease in CDS spreads. These impacts are statistically and economically highly significant.","url":"https://www.semanticscholar.org/paper/bbc6486ad37365b77b9d0de8894f595d70af49ac","sentence":"Title: ask bert: how regulatory disclosure of transition and physical climate risks affects the cds term structure Abstract: we use bert, an ai-based algorithm for language understanding, to decipher regulatory climate-risk disclosures and measure their impact on the credit default swap (cds) market. risk disclosures can either increase or decrease credit spreads, depending on whether disclosure reveals new risks or sharpens the signal and decreases the uncertainty. training bert to differentiate between transition and physical climate risks, we find that disclosing transition risks increases cds spreads, especially after the paris climate agreement of 2015, while disclosing physical climate risks leads to a decrease in cds spreads. these impacts are statistically and economically highly significant.","Task":["bert"],"Method":["ask bert"]},{"title":"Social Privacy in Networked Publics: Teens\u2019 Attitudes, Practices, and Strategies","abstract":"This paper examines how teens understand privacy in highly public networked environments like Facebook and Twitter. We describe both teens\u2019 practices, their privacy strategies, and the structural conditions in which they are embedded, highlighting the ways in which privacy, as it plays out in everyday life, is related more to agency and the ability to control a social situation than particular properties of information. Finally, we discuss the implications of teens\u2019 practices and strategies, revealing the importance of social norms as a regulatory force.(This paper was presented at Oxford Internet Institute\u2019s \u201cA Decade in Internet Time: Symposium on the Dynamics of the Internet and Society\u201d on September 22, 2011.)","url":"https://www.semanticscholar.org/paper/9b1653e3b57016958d10ff8531475eb0483d156c","sentence":"Title: social privacy in networked publics: teens\u2019 attitudes, practices, and strategies Abstract: this paper examines how teens understand privacy in highly public networked environments like facebook and twitter. we describe both teens\u2019 practices, their privacy strategies, and the structural conditions in which they are embedded, highlighting the ways in which privacy, as it plays out in everyday life, is related more to agency and the ability to control a social situation than particular properties of information. finally, we discuss the implications of teens\u2019 practices and strategies, revealing the importance of social norms as a regulatory force.(this paper was presented at oxford internet institute\u2019s \u201ca decade in internet time: symposium on the dynamics of the internet and society\u201d on september 22, 2011.)","Task":["social privacy"],"Method":["social privacy in networked publics"]},{"title":"DeSMOG: Detecting Stance in Media On Global Warming","abstract":"Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, \u201cLeading scientists agree that global warming is a serious concern,\u201d framing a clause which affirms their own stance (\u201cthat global warming is serious\u201d) as an opinion endorsed (\\"[scientists] agree\u201d) by a reputable source (\u201cleading\u201d). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: \u201cMistaken scientists claim [...].\\" Our work studies opinion-framing in the global warming (GW) debate, an increasingly partisan issue that has received little attention in NLP. We introduce DeSMOG, a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other\u2019s opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GW-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author\u2019s own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.","url":"https://www.semanticscholar.org/paper/1d7a0e8be2bbe7d148bb48a1f9de218beaea4ed5","sentence":"Title: desmog: detecting stance in media on global warming Abstract: citing opinions is a powerful yet understudied strategy in argumentation. for example, an environmental activist might say, \u201cleading scientists agree that global warming is a serious concern,\u201d framing a clause which affirms their own stance (\u201cthat global warming is serious\u201d) as an opinion endorsed (\\"[scientists] agree\u201d) by a reputable source (\u201cleading\u201d). in contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: \u201cmistaken scientists claim [...].\\" our work studies opinion-framing in the global warming (gw) debate, an increasingly partisan issue that has received little attention in nlp. we introduce desmog, a dataset of stance-labeled gw sentences, and train a bert classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other\u2019s opinions. from 56k news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across gw-accepting and skeptic media, though gw-skeptical media shows more opponent-doubt. we also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author\u2019s own view to source entities known to publicly endorse the opposing view. we release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of gw stance.","Task":["global warming, desmog"],"Method":["desmog"]},{"title":"You are right. I am ALARMED - But by Climate Change Counter Movement","abstract":"The world is facing the challenge of climate crisis. Despite the consensus in scientific community about anthropogenic global warming, the web is flooded with articles spreading climate misinformation. These articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change. We revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of NLP. Despite considerable work in detection of fake news, there is no misinformation dataset available that is specific to the domain.of climate change. We try to bridge this gap by scraping and releasing articles with known climate change misinformation.","url":"https://www.semanticscholar.org/paper/418b9f37b07a05b38798e97f57e17a6cc1048b92","sentence":"Title: you are right. i am alarmed - but by climate change counter movement Abstract: the world is facing the challenge of climate crisis. despite the consensus in scientific community about anthropogenic global warming, the web is flooded with articles spreading climate misinformation. these articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change. we revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of nlp. despite considerable work in detection of fake news, there is no misinformation dataset available that is specific to the domain.of climate change. we try to bridge this gap by scraping and releasing articles with known climate change misinformation.","Task":["climate misinformation"],"Method":["climate misinformation"]},{"title":"Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings","abstract":"We provide an NLP framework to uncover four linguistic dimensions of political polarization in social media: topic choice, framing, affect and illocutionary force. We quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional LDA-based models. We apply our methods to study 4.4M tweets on 21 mass shootings. We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice. We identify framing devices, such as grounding and the contrasting use of the terms \u201cterrorist\u201d and \u201ccrazy\u201d, that contribute to polarization. Results pertaining to topic choice, affect and illocutionary force suggest that Republicans focus more on the shooter and event-specific facts (news) while Democrats focus more on the victims and call for policy changes. Our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them.","url":"https://www.semanticscholar.org/paper/3475876c3f5e624587181744bc5acf4223aecd00","sentence":"Title: analyzing polarization in social media: method and application to tweets on 21 mass shootings Abstract: we provide an nlp framework to uncover four linguistic dimensions of political polarization in social media: topic choice, framing, affect and illocutionary force. we quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional lda-based models. we apply our methods to study 4.4m tweets on 21 mass shootings. we provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice. we identify framing devices, such as grounding and the contrasting use of the terms \u201cterrorist\u201d and \u201ccrazy\u201d, that contribute to polarization. results pertaining to topic choice, affect and illocutionary force suggest that republicans focus more on the shooter and event-specific facts (news) while democrats focus more on the victims and call for policy changes. our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them.","Task":["analyzing polarization in social media: method and application to tweets on 21 mass shootings"],"Method":["analyzing polarization in social media"]},{"title":"ClimaText: A Dataset for Climate Change Topic Detection","abstract":"Climate change communication in the mass media and other textual sources may affect and shape public perception. Extracting climate change information from these sources is an important task, e.g., for filtering content and e-discovery, sentiment analysis, automatic summarization, question-answering, and fact-checking. However, automating this process is a challenge, as climate change is a complex, fast-moving, and often ambiguous topic with scarce resources for popular text-based AI tasks. In this paper, we introduce \\\\textsc{ClimaText}, a dataset for sentence-based climate change topic detection, which we make publicly available. We explore different approaches to identify the climate change topic in various text sources. We find that popular keyword-based models are not adequate for such a complex and evolving task. Context-based algorithms like BERT \\\\cite{devlin2018bert} can detect, in addition to many trivial cases, a variety of complex and implicit topic patterns. Nevertheless, our analysis reveals a great potential for improvement in several directions, such as, e.g., capturing the discussion on indirect effects of climate change. Hence, we hope this work can serve as a good starting point for further research on this topic.","url":"https://www.semanticscholar.org/paper/134c8486ae58b421681656c85bbc48dc862f6f98","sentence":"Title: climatext: a dataset for climate change topic detection Abstract: climate change communication in the mass media and other textual sources may affect and shape public perception. extracting climate change information from these sources is an important task, e.g., for filtering content and e-discovery, sentiment analysis, automatic summarization, question-answering, and fact-checking. however, automating this process is a challenge, as climate change is a complex, fast-moving, and often ambiguous topic with scarce resources for popular text-based ai tasks. in this paper, we introduce \\\\textsc{climatext}, a dataset for sentence-based climate change topic detection, which we make publicly available. we explore different approaches to identify the climate change topic in various text sources. we find that popular keyword-based models are not adequate for such a complex and evolving task. context-based algorithms like bert \\\\cite{devlin2018bert} can detect, in addition to many trivial cases, a variety of complex and implicit topic patterns. nevertheless, our analysis reveals a great potential for improvement in several directions, such as, e.g., capturing the discussion on indirect effects of climate change. hence, we hope this work can serve as a good starting point for further research on this topic.","Task":["climate change topic detection"],"Method":["climate change topic detection"]},{"title":"Comparing Attitudes to Climate Change in the Media using sentiment analysis based on Latent Dirichlet Allocation","abstract":"News media typically present biased accounts of news stories, and different publications present different angles on the same event. In this research, we investigate how different publications differ in their approach to stories about climate change, by examining the sentiment and topics presented. To understand these attitudes, we find sentiment targets by combining Latent Dirichlet Allocation (LDA) with SentiWordNet, a general sentiment lexicon. Using LDA, we generate topics containing keywords which represent the sentiment targets, and then annotate the data using SentiWordNet before regrouping the articles based on topic similarity. Preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources. Ongoing work is investigating how systematic these attitudes are between different publications, and how these may change over time.","url":"https://www.semanticscholar.org/paper/171847842d161743f2b52dc4f85c121087ceb426","sentence":"Title: comparing attitudes to climate change in the media using sentiment analysis based on latent dirichlet allocation Abstract: news media typically present biased accounts of news stories, and different publications present different angles on the same event. in this research, we investigate how different publications differ in their approach to stories about climate change, by examining the sentiment and topics presented. to understand these attitudes, we find sentiment targets by combining latent dirichlet allocation (lda) with sentiwordnet, a general sentiment lexicon. using lda, we generate topics containing keywords which represent the sentiment targets, and then annotate the data using sentiwordnet before regrouping the articles based on topic similarity. preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources. ongoing work is investigating how systematic these attitudes are between different publications, and how these may change over time.","Task":["sentiment analysis"],"Method":["comparing attitudes to climate change in the media using sentiment analysis based on latent dirichlet allocation"]},{"title":"Cross-Platform Disinformation Campaigns: Lessons Learned and Next Steps","abstract":"We conducted a mixed-method, interpretative analysis of an online, cross-platform disinformation campaign targeting the White Helmets, a rescue group operating in rebel-held areas of Syria that has become the subject of a persistent effort of delegitimization. This research helps to conceptualize what a disinformation campaign is and how it works. Based on what we learned from this case study, we conclude that a comprehensive understanding of disinformation requires accounting for the spread of content across platforms and that social media platforms should increase collaboration to detect and characterize disinformation campaigns.","url":"https://www.semanticscholar.org/paper/9c2cab0e72f2588e857671c1fbb13b5fba8f2586","sentence":"Title: cross-platform disinformation campaigns: lessons learned and next steps Abstract: we conducted a mixed-method, interpretative analysis of an online, cross-platform disinformation campaign targeting the white helmets, a rescue group operating in rebel-held areas of syria that has become the subject of a persistent effort of delegitimization. this research helps to conceptualize what a disinformation campaign is and how it works. based on what we learned from this case study, we conclude that a comprehensive understanding of disinformation requires accounting for the spread of content across platforms and that social media platforms should increase collaboration to detect and characterize disinformation campaigns.","Task":["cross-platform disinformation campaigns"],"Method":["cross-platform disinformation campaigns"]},{"title":"Classification of Moral Foundations in Microblog Political Discourse","abstract":"Previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. Additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. Based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on Twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. The contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.","url":"https://www.semanticscholar.org/paper/7de1c0db3b3848d1a51dbfefce8b73e89a682177","sentence":"Title: classification of moral foundations in microblog political discourse Abstract: previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. the contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.","Task":["moral foundations, microblog political discourse"],"Method":["microblog political discourse"]},{"title":"Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases","abstract":"Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.","url":"https://www.semanticscholar.org/paper/29584ed6d68a06fdf91440a018f6bc83a44fd177","sentence":"Title: paragraph-level rationale extraction through regularization: a case study on european court of human rights cases Abstract: interpretability or explainability is an emerging research field in nlp. from a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. to this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. we also release a new dataset comprising european court of human rights cases, including annotations for paragraph-level rationales. we use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. we also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.","Task":["paragraph-level rationale extraction through regularization"],"Method":["paragraph-level rationale extraction through regularization"]},{"title":"Automatically Characterizing Targeted Information Operations Through Biases Present in Discourse on Twitter","abstract":"This paper considers the problem of automatically characterizing biases that may be associated with emerging information operations via artificial intelligence. Accurate analysis of these emerging topics usually requires laborious, manual analysis by experts to annotate millions of tweets to identify biases in new topics. We introduce adaptations of the Word Embedding Association Test [1] to a new domain: information operations. We validate our method using known information operation-related tweets from Twitter\'s Transparency Reports, and we perform a case study on the COVID-19 pandemic to evaluate our method\'s performance on non-labeled Twitter data, demonstrating its usability in emerging domains.","url":"https://www.semanticscholar.org/paper/08e3ce4454012d928470efa96a54aa08dd8a96b0","sentence":"Title: automatically characterizing targeted information operations through biases present in discourse on twitter Abstract: this paper considers the problem of automatically characterizing biases that may be associated with emerging information operations via artificial intelligence. accurate analysis of these emerging topics usually requires laborious, manual analysis by experts to annotate millions of tweets to identify biases in new topics. we introduce adaptations of the word embedding association test [1] to a new domain: information operations. we validate our method using known information operation-related tweets from twitter\'s transparency reports, and we perform a case study on the covid-19 pandemic to evaluate our method\'s performance on non-labeled twitter data, demonstrating its usability in emerging domains.","Task":["information operations"],"Method":["information operations"]},{"title":"Framing and Agenda-setting in Russian News: a Computational Analysis of Intricate Political Strategies","abstract":"Amidst growing concern over media manipulation, NLP attention has focused on overt strategies like censorship and \u201cfake news\u201d. Here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction: articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to Russian, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.","url":"https://www.semanticscholar.org/paper/2547836827e0423f198320977d393f574e0fb3d6","sentence":"Title: framing and agenda-setting in russian news: a computational analysis of intricate political strategies Abstract: amidst growing concern over media manipulation, nlp attention has focused on overt strategies like censorship and \u201cfake news\u201d. here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). we analyze 13 years (100k articles) of the russian newspaper izvestia and identify a strategy of distraction: articles mention the u.s. more frequently in the month directly following an economic downturn in russia. we introduce embedding-based methods for cross-lingually projecting english frames to russian, and discover that these articles emphasize u.s. moral failings and threats to the u.s. our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.","Task":["agenda-setting, framing"],"Method":["nlp"]},{"title":"Predicting the Role of Political Trolls in Social Media","abstract":"We investigate the political roles of \u201cInternet trolls\u201d in social media. Political trolls, such as the ones linked to the Russian Internet Research Agency (IRA), have recently gained enormous attention for their ability to sway public opinion and even influence elections. Analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. However, this analysis is manual and labor-intensive, thus making it impractical as a first-response tool for newly-discovered troll farms. In this paper, we show how to automate this analysis by using machine learning in a realistic setting. In particular, we show how to classify trolls according to their political role \u2014left, news feed, right\u2014 by using features extracted from social media, i.e., Twitter, in two scenarios: (i) in a traditional supervised learning scenario, where labels for trolls are available, and (ii) in a distant supervision scenario, where labels for trolls are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. Technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e., embeddings, for the trolls. Experiments on the \u201cIRA Russian Troll\u201d dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.","url":"https://www.semanticscholar.org/paper/546c87abc26137359c43898559884ba9d6c5ae64","sentence":"Title: predicting the role of political trolls in social media Abstract: we investigate the political roles of \u201cinternet trolls\u201d in social media. political trolls, such as the ones linked to the russian internet research agency (ira), have recently gained enormous attention for their ability to sway public opinion and even influence elections. analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. however, this analysis is manual and labor-intensive, thus making it impractical as a first-response tool for newly-discovered troll farms. in this paper, we show how to automate this analysis by using machine learning in a realistic setting. in particular, we show how to classify trolls according to their political role \u2014left, news feed, right\u2014 by using features extracted from social media, i.e., twitter, in two scenarios: (i) in a traditional supervised learning scenario, where labels for trolls are available, and (ii) in a distant supervision scenario, where labels for trolls are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e., embeddings, for the trolls. experiments on the \u201cira russian troll\u201d dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.","Task":["machine learning"],"Method":["predicting the role of political trolls in social media"]},{"title":"Historical Change in the Moral Foundations of Political Persuasion","abstract":"How have attempts at political persuasion changed over time? Using nine corpora dating back through 1789, containing over 7 million words of speech (1,666 documents in total), covering three different countries, plus the entire Google nGram corpus, we find that language relating to togetherness permanently crowded out language relating to duties and obligations in the persuasive speeches of politicians during the early 20th century. This shift is temporally predicted by a rise in Western nationalism and the mass movement of people from more rural to more urban areas and is unexplained by changes in language, private political speech, or nonmoral persuasion. We theorize that the emergence of the modern state in the 1920s had psychopolitical consequences for the ways that people understood and communicated their relationships with their government, which was then reflected in the levers of persuasion chosen by political elites.","url":"https://www.semanticscholar.org/paper/c0a2ee56fc80d971e920fba6179f81b515533592","sentence":"Title: historical change in the moral foundations of political persuasion Abstract: how have attempts at political persuasion changed over time? using nine corpora dating back through 1789, containing over 7 million words of speech (1,666 documents in total), covering three different countries, plus the entire google ngram corpus, we find that language relating to togetherness permanently crowded out language relating to duties and obligations in the persuasive speeches of politicians during the early 20th century. this shift is temporally predicted by a rise in western nationalism and the mass movement of people from more rural to more urban areas and is unexplained by changes in language, private political speech, or nonmoral persuasion. we theorize that the emergence of the modern state in the 1920s had psychopolitical consequences for the ways that people understood and communicated their relationships with their government, which was then reflected in the levers of persuasion chosen by political elites.","Task":["historical change in the moral foundations of political persuasion"],"Method":["historical change in the moral foundations of political persuasion"]},{"title":"Red Bots Do It Better:Comparative Analysis of Social Bot Partisan Behavior","abstract":"Recent research brought awareness of the issue of bots on social media and the significant risks of mass manipulation of public opinion in the context of political discussion. In this work, we leverage Twitter to study the discourse during the 2018 US midterm elections and analyze social bot activity and interactions with humans. We collected 2.6 million tweets for 42 days around the election day from nearly 1 million users. We use the collected tweets to answer three research questions: (i) Do social bots lean and behave according to a political ideology? (ii) Can we observe different strategies among liberal and conservative bots? (iii) How effective are bot strategies in engaging humans? We show that social bots can be accurately classified according to their political leaning and behave accordingly. Conservative bots share most of the topics of discussion with their human counterparts, while liberal bots show less overlap and a more inflammatory attitude. We studied bot interactions with humans and observed different strategies. Finally, we measured bots embeddedness in the social network and the extent of human engagement with each group of bots. Results show that conservative bots are more deeply embedded in the social network and more effective than liberal bots at exerting influence on humans.","url":"https://www.semanticscholar.org/paper/d89e89eac7882790b48ad73376742e7a93fea4fd","sentence":"Title: red bots do it better:comparative analysis of social bot partisan behavior Abstract: recent research brought awareness of the issue of bots on social media and the significant risks of mass manipulation of public opinion in the context of political discussion. in this work, we leverage twitter to study the discourse during the 2018 us midterm elections and analyze social bot activity and interactions with humans. we collected 2.6 million tweets for 42 days around the election day from nearly 1 million users. we use the collected tweets to answer three research questions: (i) do social bots lean and behave according to a political ideology? (ii) can we observe different strategies among liberal and conservative bots? (iii) how effective are bot strategies in engaging humans? we show that social bots can be accurately classified according to their political leaning and behave accordingly. conservative bots share most of the topics of discussion with their human counterparts, while liberal bots show less overlap and a more inflammatory attitude. we studied bot interactions with humans and observed different strategies. finally, we measured bots embeddedness in the social network and the extent of human engagement with each group of bots. results show that conservative bots are more deeply embedded in the social network and more effective than liberal bots at exerting influence on humans.","Task":["social bots"],"Method":["social bots on twitter"]},{"title":"Fine-Grained Analysis of Propaganda in News Article","abstract":"Propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect the quality of any learning system trained on them. A further issue with most existing systems is the lack of explainability. To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.","url":"https://www.semanticscholar.org/paper/a12d22ff91ce159a0d3558ed5aaed115115beabd","sentence":"Title: fine-grained analysis of propaganda in news article Abstract: propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda. previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. such noisy gold labels inevitably affect the quality of any learning system trained on them. a further issue with most existing systems is the lack of explainability. to overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. in particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. we further design a novel multi-granularity neural network, and we show that it outperforms several strong bert-based baselines.","Task":["fine-grained analysis of propaganda in news article"],"Method":["fine-grained analysis of propaganda in news article"]},{"title":"Issue Framing in Online Discussion Fora","abstract":"In online discussion fora, speakers often make arguments for or against something, say birth control, by highlighting certain aspects of the topic. In social science, this is referred to as issue framing. In this paper, we introduce a new issue frame annotated corpus of online discussions. We explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora, using a combination of multi-task and adversarial training, assuming only unlabeled training data in the target domain.","url":"https://www.semanticscholar.org/paper/9e82aba279807a6c8541c8811f049a40ebca36c1","sentence":"Title: issue framing in online discussion fora Abstract: in online discussion fora, speakers often make arguments for or against something, say birth control, by highlighting certain aspects of the topic. in social science, this is referred to as issue framing. in this paper, we introduce a new issue frame annotated corpus of online discussions. we explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora, using a combination of multi-task and adversarial training, assuming only unlabeled training data in the target domain.","Task":["online discussion fora"],"Method":["issue framing in online discussion fora"]},{"title":"Technology, Autonomy, and Manipulation","abstract":"Since 2016, when the Facebook/Cambridge Analytica scandal began to emerge, public concern has grown around the threat of \u201conline manipulation\u201d. While these worries are familiar to privacy researchers, this paper aims to make them more salient to policymakers \u2014 first, by defining \u201conline manipulation\u201d, thus enabling identification of manipulative practices; and second, by drawing attention to the specific harms online manipulation threatens. We argue that online manipulation is the use of information technology to covertly influence another person\u2019s decision-making, by targeting and exploiting their decision-making vulnerabilities. Engaging in such practices can harm individuals by diminishing their economic interests, but its deeper, more insidious harm is its challenge to individual autonomy. We explore this autonomy harm, emphasising its implications for both individuals and society, and we briefly outline some strategies for combating online manipulation and strengthening autonomy in an increasingly digital world.","url":"https://www.semanticscholar.org/paper/c0472b63d7d45d948dea42c4b5efffb6859541f9","sentence":"Title: technology, autonomy, and manipulation Abstract: since 2016, when the facebook/cambridge analytica scandal began to emerge, public concern has grown around the threat of \u201conline manipulation\u201d. while these worries are familiar to privacy researchers, this paper aims to make them more salient to policymakers \u2014 first, by defining \u201conline manipulation\u201d, thus enabling identification of manipulative practices; and second, by drawing attention to the specific harms online manipulation threatens. we argue that online manipulation is the use of information technology to covertly influence another person\u2019s decision-making, by targeting and exploiting their decision-making vulnerabilities. engaging in such practices can harm individuals by diminishing their economic interests, but its deeper, more insidious harm is its challenge to individual autonomy. we explore this autonomy harm, emphasising its implications for both individuals and society, and we briefly outline some strategies for combating online manipulation and strengthening autonomy in an increasingly digital world.","Task":["technology, autonomy, and manipulation"],"Method":["technology, autonomy, and manipulation"]},{"title":"Modeling Frames in Argumentation","abstract":"In argumentation, framing is used to emphasize a specific aspect of a controversial topic while concealing others. When talking about legalizing drugs, for instance, its economical aspect may be emphasized. In general, we call a set of arguments that focus on the same aspect a frame. An argumentative text has to serve the \u201cright\u201d frame(s) to convince the audience to adopt the author\u2019s stance (e.g., being pro or con legalizing drugs). More specifically, an author has to choose frames that fit the audience\u2019s cultural background and interests. This paper introduces frame identification, which is the task of splitting a set of arguments into non-overlapping frames. We present a fully unsupervised approach to this task, which first removes topical information and then identifies frames using clustering. For evaluation purposes, we provide a corpus with 12, 326 debate-portal arguments, organized along the frames of the debates\u2019 topics. On this corpus, our approach outperforms different strong baselines, achieving an F1-score of 0.28.","url":"https://www.semanticscholar.org/paper/20a37289d0b7c90065850eb7bf96d5ac3c8a21e4","sentence":"Title: modeling frames in argumentation Abstract: in argumentation, framing is used to emphasize a specific aspect of a controversial topic while concealing others. when talking about legalizing drugs, for instance, its economical aspect may be emphasized. in general, we call a set of arguments that focus on the same aspect a frame. an argumentative text has to serve the \u201cright\u201d frame(s) to convince the audience to adopt the author\u2019s stance (e.g., being pro or con legalizing drugs). more specifically, an author has to choose frames that fit the audience\u2019s cultural background and interests. this paper introduces frame identification, which is the task of splitting a set of arguments into non-overlapping frames. we present a fully unsupervised approach to this task, which first removes topical information and then identifies frames using clustering. for evaluation purposes, we provide a corpus with 12, 326 debate-portal arguments, organized along the frames of the debates\u2019 topics. on this corpus, our approach outperforms different strong baselines, achieving an f1-score of 0.28.","Task":["argumentation, frame identification, natural language processing"],"Method":["modeling frames in argumentation"]},{"title":"Automatically Neutralizing Subjective Bias in Text","abstract":"Texts like news, encyclopedias, and some social media strive for objectivity. Yet bias in the form of inappropriate subjectivity - introducing attitudes via framing, presupposing truth, and casting doubt - remains ubiquitous. This kind of bias erodes our collective trust and fuels social conflict. To address this issue, we introduce a novel testbed for natural language generation: automatically bringing inappropriately subjective text into a neutral point of view (\\"neutralizing\\" biased text). We also offer the first parallel corpus of biased language. The corpus contains 180,000 sentence pairs and originates from Wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. Last, we propose two strong encoder-decoder baselines for the task. A straightforward yet opaque CONCURRENT system uses a BERT encoder to identify subjective words as part of the generation process. An interpretable and controllable MODULAR algorithm separates these steps, using (1) a BERT-based classifier to identify problematic words and (2) a novel join embedding through which the classifier can edit the hidden states of the encoder. Large-scale human evaluation across four domains (encyclopedias, news headlines, books, and political speeches) suggests that these algorithms are a first step towards the automatic identification and reduction of bias.","url":"https://www.semanticscholar.org/paper/16981cc4ddefd3ea7655754fd83a2a8ff2203a8b","sentence":"Title: automatically neutralizing subjective bias in text Abstract: texts like news, encyclopedias, and some social media strive for objectivity. yet bias in the form of inappropriate subjectivity - introducing attitudes via framing, presupposing truth, and casting doubt - remains ubiquitous. this kind of bias erodes our collective trust and fuels social conflict. to address this issue, we introduce a novel testbed for natural language generation: automatically bringing inappropriately subjective text into a neutral point of view (\\"neutralizing\\" biased text). we also offer the first parallel corpus of biased language. the corpus contains 180,000 sentence pairs and originates from wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. last, we propose two strong encoder-decoder baselines for the task. a straightforward yet opaque concurrent system uses a bert encoder to identify subjective words as part of the generation process. an interpretable and controllable modular algorithm separates these steps, using (1) a bert-based classifier to identify problematic words and (2) a novel join embedding through which the classifier can edit the hidden states of the encoder. large-scale human evaluation across four domains (encyclopedias, news headlines, books, and political speeches) suggests that these algorithms are a first step towards the automatic identification and reduction of bias.","Task":["automatically neutralizing subjective bias in text"],"Method":["neutralizing subjective bias in text"]},{"title":"A Systematic Media Frame Analysis of 1.5 Million New York Times Articles from 2000 to 2017","abstract":"Framing is an indispensable narrative device for news media because even the same facts may lead to conflicting understandings if deliberate framing is employed. Therefore, identifying media framing is a crucial step to understanding how news media influence the public. Framing is, however, difficult to operationalize and detect, and thus traditional media framing studies had to rely on manual annotation, which is challenging to scale up to massive news datasets. Here, by developing a media frame classifier that achieves state-of-the-art performance, we systematically analyze the media frames of 1.5 million New York Times articles published from 2000 to 2017. By examining the ebb and flow of media frames over almost two decades, we show that short-term frame abundance fluctuation closely corresponds to major events, while there also exist several long-term trends, such as the gradually increasing prevalence of the \u201cCultural identity\u201d frame. By examining specific topics and sentiments, we identify characteristics and dynamics of each frame. Finally, as a case study, we delve into the framing of mass shootings, revealing three major framing patterns. Our scalable, computational approach to massive news datasets opens up new pathways for systematic media framing studies.","url":"https://www.semanticscholar.org/paper/a0a5669b310ceb3840a599bf4f27a077a643d613","sentence":"Title: a systematic media frame analysis of 1.5 million new york times articles from 2000 to 2017 Abstract: framing is an indispensable narrative device for news media because even the same facts may lead to conflicting understandings if deliberate framing is employed. therefore, identifying media framing is a crucial step to understanding how news media influence the public. framing is, however, difficult to operationalize and detect, and thus traditional media framing studies had to rely on manual annotation, which is challenging to scale up to massive news datasets. here, by developing a media frame classifier that achieves state-of-the-art performance, we systematically analyze the media frames of 1.5 million new york times articles published from 2000 to 2017. by examining the ebb and flow of media frames over almost two decades, we show that short-term frame abundance fluctuation closely corresponds to major events, while there also exist several long-term trends, such as the gradually increasing prevalence of the \u201ccultural identity\u201d frame. by examining specific topics and sentiments, we identify characteristics and dynamics of each frame. finally, as a case study, we delve into the framing of mass shootings, revealing three major framing patterns. our scalable, computational approach to massive news datasets opens up new pathways for systematic media framing studies.","Task":["media framing"],"Method":["media framing"]},{"title":"FrameAxis: Characterizing Framing Bias and Intensity with Word Embedding","abstract":"We propose FrameAxis, a method of characterizing the framing of a given text by identifying the most relevant semantic axes (\\"microframes\\") defined by antonym word pairs. In contrast to the traditional framing analysis, which has been constrained by a small number of manually annotated general frames, our unsupervised approach provides much more detailed insights, by considering a host of semantic axes. Our method is capable of quantitatively teasing out framing bias -- how biased a text is in each microframe -- and framing intensity -- how much each microframe is used -- from the text, offering a nuanced characterization of framing. We evaluate our approach using SemEval datasets as well as three other datasets and human evaluations, demonstrating that FrameAxis can reliably characterize documents with relevant microframes. Our method may allow scalable and nuanced computational analyses of framing across disciplines.","url":"https://www.semanticscholar.org/paper/245bcbdfff19721c226059a5d5b1ae71b67e0572","sentence":"Title: frameaxis: characterizing framing bias and intensity with word embedding Abstract: we propose frameaxis, a method of characterizing the framing of a given text by identifying the most relevant semantic axes (\\"microframes\\") defined by antonym word pairs. in contrast to the traditional framing analysis, which has been constrained by a small number of manually annotated general frames, our unsupervised approach provides much more detailed insights, by considering a host of semantic axes. our method is capable of quantitatively teasing out framing bias -- how biased a text is in each microframe -- and framing intensity -- how much each microframe is used -- from the text, offering a nuanced characterization of framing. we evaluate our approach using semeval datasets as well as three other datasets and human evaluations, demonstrating that frameaxis can reliably characterize documents with relevant microframes. our method may allow scalable and nuanced computational analyses of framing across disciplines.","Task":["frameaxis"],"Method":["frameaxis"]},{"title":"Connotation Frames of Power and Agency in Modern Films","abstract":"The framing of an action influences how we perceive its actor. We introduce connotation frames of power and agency, a pragmatic formalism organized using frame semantic representations, to model how different levels of power and agency are implicitly projected on actors through their actions. We use the new power and agency frames to measure the subtle, but prevalent, gender bias in the portrayal of modern film characters and provide insights that deviate from the well-known Bechdel test. Our contributions include an extended lexicon of connotation frames along with a web interface that provides a comprehensive analysis through the lens of connotation frames.","url":"https://www.semanticscholar.org/paper/2cd74561109e2675634e0f4f0129fcfbc238a1d8","sentence":"Title: connotation frames of power and agency in modern films Abstract: the framing of an action influences how we perceive its actor. we introduce connotation frames of power and agency, a pragmatic formalism organized using frame semantic representations, to model how different levels of power and agency are implicitly projected on actors through their actions. we use the new power and agency frames to measure the subtle, but prevalent, gender bias in the portrayal of modern film characters and provide insights that deviate from the well-known bechdel test. our contributions include an extended lexicon of connotation frames along with a web interface that provides a comprehensive analysis through the lens of connotation frames.","Task":["connotation frames of power and agency"],"Method":["connotation frames"]},{"title":"Analyzing Framing through the Casts of Characters in the News","abstract":"We present an unsupervised model for the discovery and clustering of latent \u201cpersonas\u201d (characterizations of entities). Our model simultaneously clusters documents featuring similar collections of personas. We evaluate this model on a collection of news articles about immigration, showing that personas help predict the coarse-grained framing annotations in the Media Frames Corpus. We also introduce automated model selection as a fair and robust form of feature evaluation.","url":"https://www.semanticscholar.org/paper/15130cdd46ff7e3a69384e6c62ab1775c4aba96f","sentence":"Title: analyzing framing through the casts of characters in the news Abstract: we present an unsupervised model for the discovery and clustering of latent \u201cpersonas\u201d (characterizations of entities). our model simultaneously clusters documents featuring similar collections of personas. we evaluate this model on a collection of news articles about immigration, showing that personas help predict the coarse-grained framing annotations in the media frames corpus. we also introduce automated model selection as a fair and robust form of feature evaluation.","Task":["personas, persona clustering"],"Method":["persona clustering"]},{"title":"The Media Frames Corpus: Annotations of Frames Across Issues","abstract":"We describe the first version of the Media Frames Corpus: several thousand news articles on three policy issues, annotated in terms of media framing. We motivate framing as a phenomenon of study for computational linguistics and describe our annotation process.","url":"https://www.semanticscholar.org/paper/92408cc19033cc4af29accef3793014ab79355c2","sentence":"Title: the media frames corpus: annotations of frames across issues Abstract: we describe the first version of the media frames corpus: several thousand news articles on three policy issues, annotated in terms of media framing. we motivate framing as a phenomenon of study for computational linguistics and describe our annotation process.","Task":["computational linguistics"],"Method":["media frames"]},{"title":"Who Falls for Online Political Manipulation?","abstract":"Social media, once hailed as a vehicle for democratization and the promotion of positive social change across the globe, are under attack for becoming a tool of political manipulation and spread of disinformation. A case in point is the alleged use of trolls by Russia to spread malicious content in Western elections. This paper examines the Russian interference campaign in the 2016 US presidential election on Twitter. Our aim is twofold: first, we test whether predicting users who spread trolls\u2019 content is feasible in order to gain insight on how to contain their influence in the future; second, we identify features that are most predictive of users who either intentionally or unintentionally play a vital role in spreading this malicious content. We collected a dataset with over 43 million election-related posts shared on Twitter between September 16 and November 9, 2016, by about 5.7 million users. This dataset includes accounts associated with the Russian trolls identified by the US Congress. Proposed models are able to very accurately identify users who spread the trolls\u2019 content (average AUC score of 96%, using 10-fold validation). We show that political ideology, bot likelihood scores, and some activity-related account meta data are the most predictive features of whether a user spreads trolls\u2019 content or not.","url":"https://www.semanticscholar.org/paper/c899c6048a1bd99590972c5fb2f9fd00db8d4e18","sentence":"Title: who falls for online political manipulation? Abstract: social media, once hailed as a vehicle for democratization and the promotion of positive social change across the globe, are under attack for becoming a tool of political manipulation and spread of disinformation. a case in point is the alleged use of trolls by russia to spread malicious content in western elections. this paper examines the russian interference campaign in the 2016 us presidential election on twitter. our aim is twofold: first, we test whether predicting users who spread trolls\u2019 content is feasible in order to gain insight on how to contain their influence in the future; second, we identify features that are most predictive of users who either intentionally or unintentionally play a vital role in spreading this malicious content. we collected a dataset with over 43 million election-related posts shared on twitter between september 16 and november 9, 2016, by about 5.7 million users. this dataset includes accounts associated with the russian trolls identified by the us congress. proposed models are able to very accurately identify users who spread the trolls\u2019 content (average auc score of 96%, using 10-fold validation). we show that political ideology, bot likelihood scores, and some activity-related account meta data are the most predictive features of whether a user spreads trolls\u2019 content or not.","Task":["online political manipulation"],"Method":["online political manipulation"]},{"title":"Linguistic Models for Analyzing and Detecting Biased Language","abstract":"Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. Bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective intensifiers. These insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task.","url":"https://www.semanticscholar.org/paper/2a501b074261e81b9126e80a0a308cfa5e76f8c1","sentence":"Title: linguistic models for analyzing and detecting biased language Abstract: unbiased language is a requirement for reference sources like encyclopedias and scientific texts. bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. to this end we analyze real instances of human edits designed to remove bias from wikipedia articles. the analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. we identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective intensifiers. these insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. our linguistically-informed model performs almost as well as humans tested on the same task.","Task":["bias"],"Method":["linguistic models for analyzing and detecting biased language"]},{"title":"Misinfo Belief Frames: A Case Study on Covid & Climate News","abstract":"Prior beliefs of readers impact the way in which they project meaning onto news headlines. These beliefs can influence their perception of news reliability, as well as their reaction to news, and their likelihood of spreading the misinformation through social networks. However, most prior work focuses on fact-checking veracity of news or stylometry rather than measuring impact of misinformation. We propose Misinfo Belief Frames, a formalism for understanding how readers perceive the reliability of news and the impact of misinformation. We also introduce the Misinfo Belief Frames (MBF) corpus, a dataset of 66k inferences over 23.5k headlines. Misinformation frames use commonsense reasoning to uncover implications of real and fake news headlines focused on global crises: the Covid-19 pandemic and climate change. Our results using large-scale language modeling to predict misinformation frames show that machine-generated inferences can influence readers\u2019 trust in news headlines (readers\u2019 trust in news headlines was affected in 29.3% of cases). This demonstrates the potential effectiveness of using generated frames to counter misinformation.","url":"https://www.semanticscholar.org/paper/1778d1da6264ed4274fb94115ebbc3b4f34c9f7a","sentence":"Title: misinfo belief frames: a case study on covid & climate news Abstract: prior beliefs of readers impact the way in which they project meaning onto news headlines. these beliefs can influence their perception of news reliability, as well as their reaction to news, and their likelihood of spreading the misinformation through social networks. however, most prior work focuses on fact-checking veracity of news or stylometry rather than measuring impact of misinformation. we propose misinfo belief frames, a formalism for understanding how readers perceive the reliability of news and the impact of misinformation. we also introduce the misinfo belief frames (mbf) corpus, a dataset of 66k inferences over 23.5k headlines. misinformation frames use commonsense reasoning to uncover implications of real and fake news headlines focused on global crises: the covid-19 pandemic and climate change. our results using large-scale language modeling to predict misinformation frames show that machine-generated inferences can influence readers\u2019 trust in news headlines (readers\u2019 trust in news headlines was affected in 29.3% of cases). this demonstrates the potential effectiveness of using generated frames to counter misinformation.","Task":["misinfo belief frames"],"Method":["misinfo belief frames"]},{"title":"Fighting the COVID-19 Infodemic in Social Media: A Holistic Perspective and a Call to Arms","abstract":"With the outbreak of the COVID-19 pandemic, people turned to social media to read and to share timely information including statistics, warnings, advice, and inspirational stories. Unfortunately, alongside all this useful information, there was also a new blending of medical and political misinformation and disinformation, which gave rise to the first global infodemic. While fighting this infodemic is typically thought of in terms of factuality, the problem is much broader as malicious content includes not only fake news, rumors, and conspiracy theories, but also promotion of fake cures, panic, racism, xenophobia, and mistrust in the authorities, among others. This is a complex problem that needs a holistic approach combining the perspectives of journalists, fact-checkers, policymakers, government entities, social media platforms, and society as a whole. Taking them into account we define an annotation schema and detailed annotation instructions, which reflect these perspectives. We performed initial annotations using this schema, and our initial experiments demonstrated sizable improvements over the baselines. Now, we issue a call to arms to the research community and beyond to join the fight by supporting our crowdsourcing annotation efforts.","url":"https://www.semanticscholar.org/paper/460986c12b9fdedc3bf9a710c429c5a84608055a","sentence":"Title: fighting the covid-19 infodemic in social media: a holistic perspective and a call to arms Abstract: with the outbreak of the covid-19 pandemic, people turned to social media to read and to share timely information including statistics, warnings, advice, and inspirational stories. unfortunately, alongside all this useful information, there was also a new blending of medical and political misinformation and disinformation, which gave rise to the first global infodemic. while fighting this infodemic is typically thought of in terms of factuality, the problem is much broader as malicious content includes not only fake news, rumors, and conspiracy theories, but also promotion of fake cures, panic, racism, xenophobia, and mistrust in the authorities, among others. this is a complex problem that needs a holistic approach combining the perspectives of journalists, fact-checkers, policymakers, government entities, social media platforms, and society as a whole. taking them into account we define an annotation schema and detailed annotation instructions, which reflect these perspectives. we performed initial annotations using this schema, and our initial experiments demonstrated sizable improvements over the baselines. now, we issue a call to arms to the research community and beyond to join the fight by supporting our crowdsourcing annotation efforts.","Task":["covid-19 infodemic"],"Method":["fighting the covid-19 infodemic in social media"]},{"title":"The online competition between pro- and anti-vaccination views","abstract":"Distrust in scientific expertise 1 \u2013 14 is dangerous. Opposition to vaccination with a future vaccine against SARS-CoV-2, the causal agent of COVID-19, for example, could amplify outbreaks 2 \u2013 4 , as happened for measles in 2019 5 , 6 . Homemade remedies 7 , 8 and falsehoods are being shared widely on the Internet, as well as dismissals of expert advice 9 \u2013 11 . There is a lack of understanding about how this distrust evolves at the system level 13 , 14 . Here we provide a map of the contention surrounding vaccines that has emerged from the global pool of around three billion Facebook users. Its core reveals a multi-sided landscape of unprecedented intricacy that involves nearly 100\xa0million individuals partitioned into highly dynamic, interconnected clusters across cities, countries, continents and languages. Although smaller in overall size, anti-vaccination clusters manage to become highly entangled with undecided clusters in the main online network, whereas pro-vaccination clusters are more peripheral. Our theoretical framework reproduces the recent explosive growth in anti-vaccination views, and predicts that these views will dominate in a decade. Insights provided by this framework can inform new policies and approaches to interrupt this shift to negative views. Our results challenge the conventional thinking about undecided individuals in issues of contention surrounding health, shed light on other issues of contention such as climate change 11 , and highlight the key role of network cluster dynamics in multi-species ecologies 15 . Insights into the interactions between pro- and anti-vaccination clusters on Facebook can enable policies and approaches that attempt to interrupt the shift to anti-vaccination views and persuade undecided individuals to adopt a pro-vaccination stance.","url":"https://www.semanticscholar.org/paper/9129399d6b1599508709e3a72f80defd52ef89c9","sentence":"Title: the online competition between pro- and anti-vaccination views Abstract: distrust in scientific expertise 1 \u2013 14 is dangerous. opposition to vaccination with a future vaccine against sars-cov-2, the causal agent of covid-19, for example, could amplify outbreaks 2 \u2013 4 , as happened for measles in 2019 5 , 6 . homemade remedies 7 , 8 and falsehoods are being shared widely on the internet, as well as dismissals of expert advice 9 \u2013 11 . there is a lack of understanding about how this distrust evolves at the system level 13 , 14 . here we provide a map of the contention surrounding vaccines that has emerged from the global pool of around three billion facebook users. its core reveals a multi-sided landscape of unprecedented intricacy that involves nearly 100\xa0million individuals partitioned into highly dynamic, interconnected clusters across cities, countries, continents and languages. although smaller in overall size, anti-vaccination clusters manage to become highly entangled with undecided clusters in the main online network, whereas pro-vaccination clusters are more peripheral. our theoretical framework reproduces the recent explosive growth in anti-vaccination views, and predicts that these views will dominate in a decade. insights provided by this framework can inform new policies and approaches to interrupt this shift to negative views. our results challenge the conventional thinking about undecided individuals in issues of contention surrounding health, shed light on other issues of contention such as climate change 11 , and highlight the key role of network cluster dynamics in multi-species ecologies 15 . insights into the interactions between pro- and anti-vaccination clusters on facebook can enable policies and approaches that attempt to interrupt the shift to anti-vaccination views and persuade undecided individuals to adopt a pro-vaccination stance.","Task":["online competition between pro- and anti-vaccination views, trust in scientific expertise 1 \u2013 14"],"Method":["the online competition between pro- and anti-vaccination views"]},{"title":"A Survey on Multimodal Disinformation Detection","abstract":"Recent years have witnessed the proliferation of fake news, propaganda, misinformation, and disinformation online. While initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract much more attention, and spread further than simple text. As a result, researchers started targeting different modalities and combinations thereof. As different modalities are studied in different research communities, with insufficient interaction, here we offer a survey that explores the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. Moreover, while some studies focused on factuality, others investigated how harmful the content is. While these two components in the definition of disinformation \u2013 (i) factuality and (ii) harmfulness, are equally important, they are typically studied in isolation. Thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. Finally, we discuss current challenges and future research directions.","url":"https://www.semanticscholar.org/paper/ed1c17451a23471afde91c109ecadc6aab8b2ba6","sentence":"Title: a survey on multimodal disinformation detection Abstract: recent years have witnessed the proliferation of fake news, propaganda, misinformation, and disinformation online. while initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract much more attention, and spread further than simple text. as a result, researchers started targeting different modalities and combinations thereof. as different modalities are studied in different research communities, with insufficient interaction, here we offer a survey that explores the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. moreover, while some studies focused on factuality, others investigated how harmful the content is. while these two components in the definition of disinformation \u2013 (i) factuality and (ii) harmfulness, are equally important, they are typically studied in isolation. thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. finally, we discuss current challenges and future research directions.","Task":["disinformation detection, multimodal disinformation detection"],"Method":["multimodal disinformation detection"]},{"title":"Automated Fact-Checking for Assisting Human Fact-Checkers","abstract":"The reporting and analysis of current events around the globe has expanded from professional, editorlead journalism all the way to citizen journalism. Politicians and other key players enjoy direct access to their audiences through social media, bypassing the filters of official cables or traditional media. However, the multiple advantages of free speech and direct communication are dimmed by the misuse of the media to spread inaccurate or misleading claims. These phenomena have led to the modern incarnation of the fact-checker \u2014 a professional whose main aim is to examine claims using available evidence to assess their veracity. As in other text forensics tasks, the amount of information available makes the work of the fact-checker more difficult. With this in mind, starting from the perspective of the professional fact-checker, we survey the available intelligent technologies that can support the human expert in the different steps of her fact-checking endeavor. These include identifying claims worth fact-checking; detecting relevant previously fact-checked claims; retrieving relevant evidence to fact-check a claim; and actually verifying a claim. In each case, we pay attention to the challenges in future work and the potential impact on real-world fact-checking.","url":"https://www.semanticscholar.org/paper/38d243b9f6e2c786699dbc83513fb190372cde07","sentence":"Title: automated fact-checking for assisting human fact-checkers Abstract: the reporting and analysis of current events around the globe has expanded from professional, editorlead journalism all the way to citizen journalism. politicians and other key players enjoy direct access to their audiences through social media, bypassing the filters of official cables or traditional media. however, the multiple advantages of free speech and direct communication are dimmed by the misuse of the media to spread inaccurate or misleading claims. these phenomena have led to the modern incarnation of the fact-checker \u2014 a professional whose main aim is to examine claims using available evidence to assess their veracity. as in other text forensics tasks, the amount of information available makes the work of the fact-checker more difficult. with this in mind, starting from the perspective of the professional fact-checker, we survey the available intelligent technologies that can support the human expert in the different steps of her fact-checking endeavor. these include identifying claims worth fact-checking; detecting relevant previously fact-checked claims; retrieving relevant evidence to fact-check a claim; and actually verifying a claim. in each case, we pay attention to the challenges in future work and the potential impact on real-world fact-checking.","Task":["automated fact-checking"],"Method":["automated fact-checking for assisting human fact-checkers"]},{"title":"Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection","abstract":"Fake news has altered society in negative ways in politics and culture. It has adversely affected both online social network systems as well as offline communities and conversations. Using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news. However, a lack of effective, comprehensive datasets has been a problem for fake news research and detection model development. Prior fake news datasets do not provide multimodal text and image data, metadata, comment data, and fine-grained fake news categorization at the scale and breadth of our dataset. We present Fakeddit, a novel multimodal dataset consisting of over 1 million samples from multiple categories of fake news. After being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision. We construct hybrid text+image models and perform extensive experiments for multiple variations of classification, demonstrating the importance of the novel aspect of multimodality and fine-grained classification unique to Fakeddit.","url":"https://www.semanticscholar.org/paper/76de89ca66f898e8211acba7392ef2d4a7e14125","sentence":"Title: fakeddit: a new multimodal benchmark dataset for fine-grained fake news detection Abstract: fake news has altered society in negative ways in politics and culture. it has adversely affected both online social network systems as well as offline communities and conversations. using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news. however, a lack of effective, comprehensive datasets has been a problem for fake news research and detection model development. prior fake news datasets do not provide multimodal text and image data, metadata, comment data, and fine-grained fake news categorization at the scale and breadth of our dataset. we present fakeddit, a novel multimodal dataset consisting of over 1 million samples from multiple categories of fake news. after being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision. we construct hybrid text+image models and perform extensive experiments for multiple variations of classification, demonstrating the importance of the novel aspect of multimodality and fine-grained classification unique to fakeddit.","Task":["fakeddit"],"Method":["fakeddit"]},{"title":"Automatic Detection of Fake News","abstract":"The proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in online news. Our contribution is twofold. First, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. We describe the collection, annotation, and validation process in detail and present several exploratory analyses on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors, and show that we can achieve accuracies of up to 76%. In addition, we provide comparative analyses of the automatic and manual identification of fake news.","url":"https://www.semanticscholar.org/paper/c6a9ca56c93323c0199dd22631d1cf731bdd7ec1","sentence":"Title: automatic detection of fake news Abstract: the proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. in this paper, we focus on the automatic identification of fake content in online news. our contribution is twofold. first, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. we describe the collection, annotation, and validation process in detail and present several exploratory analyses on the identification of linguistic differences in fake and legitimate news content. second, we conduct a set of learning experiments to build accurate fake news detectors, and show that we can achieve accuracies of up to 76%. in addition, we provide comparative analyses of the automatic and manual identification of fake news.","Task":["automatic detection of fake news"],"Method":["fake news detection"]},{"title":"\\"Liar, Liar Pants on Fire\\": A New Benchmark Dataset for Fake News Detection","abstract":"Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present liar: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.","url":"https://www.semanticscholar.org/paper/03c294ad75bd1bac92217419ac25358227f6a901","sentence":"Title: \\"liar, liar pants on fire\\": a new benchmark dataset for fake news detection Abstract: automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. however, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. in this paper, we present liar: a new, publicly available dataset for fake news detection. we collected a decade-long, 12.8k manually labeled short statements in various contexts from politifact.com, which provides detailed analysis report and links to source documents for each case. this dataset can be used for fact-checking research as well. notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. we have designed a novel, hybrid convolutional neural network to integrate meta-data with text. we show that this hybrid approach can improve a text-only deep learning model.","Task":["automatic fake news detection"],"Method":["automatic fake news detection"]},{"title":"Weaponized Health Communication: Twitter Bots and Russian Trolls Amplify the Vaccine Debate","abstract":"Objectives To understand how Twitter bots and trolls (\u201cbots\u201d) promote online health content. Methods We compared bots\u2019 to average users\u2019 rates of vaccine-relevant messages, which we collected online from July 2014 through September 2017. We estimated the likelihood that users were bots, comparing proportions of polarized and antivaccine tweets across user types. We conducted a content analysis of a Twitter hashtag associated with Russian troll activity. Results Compared with average users, Russian trolls (\u03c72(1)\u2009=\u2009102.0; P\u2009<\u2009.001), sophisticated bots (\u03c72(1)\u2009=\u200928.6; P\u2009<\u2009.001), and \u201ccontent polluters\u201d (\u03c72(1)\u2009=\u20097.0; P\u2009<\u2009.001) tweeted about vaccination at higher rates. Whereas content polluters posted more antivaccine content (\u03c72(1)\u2009=\u200911.18; P\u2009<\u2009.001), Russian trolls amplified both sides. Unidentifiable accounts were more polarized (\u03c72(1)\u2009=\u200912.1; P\u2009<\u2009.001) and antivaccine (\u03c72(1)\u2009=\u200935.9; P\u2009<\u2009.001). Analysis of the Russian troll hashtag showed that its messages were more political and divisive. Conclusions Whereas bots that spread malware and unsolicited content disseminated antivaccine messages, Russian trolls promoted discord. Accounts masquerading as legitimate users create false equivalency, eroding public consensus on vaccination. Public Health Implications. Directly confronting vaccine skeptics enables bots to legitimize the vaccine debate. More research is needed to determine how best to combat bot-driven content.","url":"https://www.semanticscholar.org/paper/8415274c8fb370cbab84ad82ab2f469786ddee72","sentence":"Title: weaponized health communication: twitter bots and russian trolls amplify the vaccine debate Abstract: objectives to understand how twitter bots and trolls (\u201cbots\u201d) promote online health content. methods we compared bots\u2019 to average users\u2019 rates of vaccine-relevant messages, which we collected online from july 2014 through september 2017. we estimated the likelihood that users were bots, comparing proportions of polarized and antivaccine tweets across user types. we conducted a content analysis of a twitter hashtag associated with russian troll activity. results compared with average users, russian trolls (\u03c72(1)\u2009=\u2009102.0; p\u2009<\u2009.001), sophisticated bots (\u03c72(1)\u2009=\u200928.6; p\u2009<\u2009.001), and \u201ccontent polluters\u201d (\u03c72(1)\u2009=\u20097.0; p\u2009<\u2009.001) tweeted about vaccination at higher rates. whereas content polluters posted more antivaccine content (\u03c72(1)\u2009=\u200911.18; p\u2009<\u2009.001), russian trolls amplified both sides. unidentifiable accounts were more polarized (\u03c72(1)\u2009=\u200912.1; p\u2009<\u2009.001) and antivaccine (\u03c72(1)\u2009=\u200935.9; p\u2009<\u2009.001). analysis of the russian troll hashtag showed that its messages were more political and divisive. conclusions whereas bots that spread malware and unsolicited content disseminated antivaccine messages, russian trolls promoted discord. accounts masquerading as legitimate users create false equivalency, eroding public consensus on vaccination. public health implications. directly confronting vaccine skeptics enables bots to legitimize the vaccine debate. more research is needed to determine how best to combat bot-driven content.","Task":["bots, russian trolls, vaccine debate"],"Method":["weaponized health communication"]},{"title":"Fake News: Spread of Misinformation about Urological Conditions on Social Media.","abstract":"Although there is a large amount of user-generated content about urological health issues on social media, much of this content has not been vetted for information accuracy. In this article, we review the literature on the quality and balance of information on urological health conditions on social networks. Across a wide range of benign and malignant urological conditions, studies show a substantial amount of commercial, biased and/or inaccurate information present on popular social networking sites. The healthcare community should take proactive steps to improve the quality of medical information on social networks. PATIENT SUMMARY: In this review, we examined the spread of misinformation about urological health conditions on social media. We found that a significant amount of the circulating information is commercial, biased or misinformative.","url":"https://www.semanticscholar.org/paper/1dad69f1fd4403aed4d3d709ab794113291d625c","sentence":"Title: fake news: spread of misinformation about urological conditions on social media. Abstract: although there is a large amount of user-generated content about urological health issues on social media, much of this content has not been vetted for information accuracy. in this article, we review the literature on the quality and balance of information on urological health conditions on social networks. across a wide range of benign and malignant urological conditions, studies show a substantial amount of commercial, biased and/or inaccurate information present on popular social networking sites. the healthcare community should take proactive steps to improve the quality of medical information on social networks. patient summary: in this review, we examined the spread of misinformation about urological health conditions on social media. we found that a significant amount of the circulating information is commercial, biased or misinformative.","Task":["fake news, social media"],"Method":["fake news"]},{"title":"Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking","abstract":"We present an analytic study on the language of news media in the context of political fact-checking and fake news detection. We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.","url":"https://www.semanticscholar.org/paper/7d3c2ff37d04914836f9cbd9ce54b6c97aa74a22","sentence":"Title: truth of varying shades: analyzing language in fake news and political fact-checking Abstract: we present an analytic study on the language of news media in the context of political fact-checking and fake news detection. we compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. to probe the feasibility of automatic political fact-checking, we also present a case study based on politifact.com using their factuality judgments on a 6-point scale. experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.","Task":["fake news, political fact-checking, fake news detection"],"Method":["fake news and political fact-checking"]},{"title":"Coronavirus on Social Media: Analyzing Misinformation in Twitter Conversations","abstract":"The ongoing Coronavirus Disease (COVID-19) pandemic highlights the interconnected-ness of our present-day globalized world. With social distancing policies in place, virtual communication has become an important source of (mis)information. As increasing number of people rely on social media platforms for news, identifying misinformation has emerged as a critical task in these unprecedented times. In addition to being malicious, the spread of such information poses a serious public health risk. To this end, we design a dashboard to track misinformation on popular social media news sharing platform - Twitter. Our dashboard allows visibility into the social media discussions around Coronavirus and the quality of information shared on the platform as the situation evolves. We collect streaming data using the Twitter API from March 1, 2020 to date and provide analysis of topic clusters and social sentiments related to important emerging policies such as \\"#socialdistancing\\" and \\"#workfromhome\\". We track emerging hashtags over time, and provide location and time sensitive analysis of sentiments. In addition, we study the challenging problem of misinformation on social media, and provide a detection method to identify false, misleading and clickbait contents from Twitter information cascades. The dashboard maintains an evolving list of detected misinformation cascades with the corresponding detection scores, accessible online athttps://ksharmar.this http URL.","url":"https://www.semanticscholar.org/paper/29370adbf0674ad72d5e0bcc59582c22035bd0aa","sentence":"Title: coronavirus on social media: analyzing misinformation in twitter conversations Abstract: the ongoing coronavirus disease (covid-19) pandemic highlights the interconnected-ness of our present-day globalized world. with social distancing policies in place, virtual communication has become an important source of (mis)information. as increasing number of people rely on social media platforms for news, identifying misinformation has emerged as a critical task in these unprecedented times. in addition to being malicious, the spread of such information poses a serious public health risk. to this end, we design a dashboard to track misinformation on popular social media news sharing platform - twitter. our dashboard allows visibility into the social media discussions around coronavirus and the quality of information shared on the platform as the situation evolves. we collect streaming data using the twitter api from march 1, 2020 to date and provide analysis of topic clusters and social sentiments related to important emerging policies such as \\"#socialdistancing\\" and \\"#workfromhome\\". we track emerging hashtags over time, and provide location and time sensitive analysis of sentiments. in addition, we study the challenging problem of misinformation on social media, and provide a detection method to identify false, misleading and clickbait contents from twitter information cascades. the dashboard maintains an evolving list of detected misinformation cascades with the corresponding detection scores, accessible online athttps://ksharmar.this http url.","Task":["coronavirus on social media: analyzing misinformation in twitter conversations"],"Method":["coronavirus on social media"]},{"title":"Combating Fake News: A Survey on Identification and Mitigation Techniques","abstract":"The proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news, and mitigation of its widespread impact on public opinion. While much of the earlier research was focused on identification of fake news based on its contents or by exploiting users\' engagements with the news on social media, there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society. In this survey, we describe the modern-day problem of fake news and, in particular, highlight the technical challenges associated with it. We discuss existing methods and techniques applicable to both identification and mitigation, with a focus on the significant advances in each method and their advantages and limitations. In addition, research has often been limited by the quality of existing datasets and their specific application contexts. To alleviate this problem, we comprehensively compile and summarize characteristic features of available datasets. Furthermore, we outline new directions of research to facilitate future development of effective and interdisciplinary solutions.","url":"https://www.semanticscholar.org/paper/1213d98f3d0a42d575bbd2c2af0309c7f76024a1","sentence":"Title: combating fake news: a survey on identification and mitigation techniques Abstract: the proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news, and mitigation of its widespread impact on public opinion. while much of the earlier research was focused on identification of fake news based on its contents or by exploiting users\' engagements with the news on social media, there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society. in this survey, we describe the modern-day problem of fake news and, in particular, highlight the technical challenges associated with it. we discuss existing methods and techniques applicable to both identification and mitigation, with a focus on the significant advances in each method and their advantages and limitations. in addition, research has often been limited by the quality of existing datasets and their specific application contexts. to alleviate this problem, we comprehensively compile and summarize characteristic features of available datasets. furthermore, we outline new directions of research to facilitate future development of effective and interdisciplinary solutions.","Task":["fake news"],"Method":["combating fake news"]},{"title":"The spread of true and false news online","abstract":"Lies spread faster than the truth There is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. To understand how false news spreads, Vosoughi et al. used a data set of rumor cascades on Twitter from 2006 to 2017. About 126,000 rumors were spread by \u223c3 million people. False news reached more people than the truth; the top 1% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. Falsehood also diffused faster than the truth. The degree of novelty and the emotional reactions of recipients may be responsible for the differences observed. Science, this issue p. 1146 A large-scale analysis of tweets reveals that false rumors spread further and faster than the truth. We investigated the differential diffusion of all of the verified true and false news stories distributed on Twitter from 2006 to 2017. The data comprise ~126,000 stories tweeted by ~3 million people more than 4.5 million times. We classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98% agreement on the classifications. Falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. We found that false news was more novel than true news, which suggests that people were more likely to share novel information. Whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. Contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.","url":"https://www.semanticscholar.org/paper/a1e58f89f57f57fad3c77cd558444ad5ad64b525","sentence":"Title: the spread of true and false news online Abstract: lies spread faster than the truth there is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. to understand how false news spreads, vosoughi et al. used a data set of rumor cascades on twitter from 2006 to 2017. about 126,000 rumors were spread by \u223c3 million people. false news reached more people than the truth; the top 1% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. falsehood also diffused faster than the truth. the degree of novelty and the emotional reactions of recipients may be responsible for the differences observed. science, this issue p. 1146 a large-scale analysis of tweets reveals that false rumors spread further and faster than the truth. we investigated the differential diffusion of all of the verified true and false news stories distributed on twitter from 2006 to 2017. the data comprise ~126,000 stories tweeted by ~3 million people more than 4.5 million times. we classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98% agreement on the classifications. falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. we found that false news was more novel than true news, which suggests that people were more likely to share novel information. whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.","Task":["the spread of true and false news online"],"Method":["the spread of true and false news online"]},{"title":"Fake News: A Survey of Research, Detection Methods, and Opportunities","abstract":"The explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news analysis, detection and intervention. This survey comprehensively and systematically reviews fake news research. The survey identifies and specifies fundamental theories across various disciplines, e.g., psychology and social science, to facilitate and enhance the interdisciplinary research of fake news. Current fake news research is reviewed, summarized and evaluated. These studies focus on fake news from four perspective: (1) the false knowledge it carries, (2) its writing style, (3) its propagation patterns, and (4) the credibility of its creators and spreaders. We characterize each perspective with various analyzable and utilizable information provided by news and its spreaders, various strategies and frameworks that are adaptable, and techniques that are applicable. By reviewing the characteristics of fake news and open issues in fake news studies, we highlight some potential research tasks at the end of this survey.","url":"https://www.semanticscholar.org/paper/bc8c416f821b93795370524247c8a455c373ee6e","sentence":"Title: fake news: a survey of research, detection methods, and opportunities Abstract: the explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news analysis, detection and intervention. this survey comprehensively and systematically reviews fake news research. the survey identifies and specifies fundamental theories across various disciplines, e.g., psychology and social science, to facilitate and enhance the interdisciplinary research of fake news. current fake news research is reviewed, summarized and evaluated. these studies focus on fake news from four perspective: (1) the false knowledge it carries, (2) its writing style, (3) its propagation patterns, and (4) the credibility of its creators and spreaders. we characterize each perspective with various analyzable and utilizable information provided by news and its spreaders, various strategies and frameworks that are adaptable, and techniques that are applicable. by reviewing the characteristics of fake news and open issues in fake news studies, we highlight some potential research tasks at the end of this survey.","Task":["fake news"],"Method":["fake news"]},{"title":"Fact or Fiction: Verifying Scientific Claims","abstract":"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that supports or refutes a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that these models benefit from combined training on a large dataset of claims about Wikipedia articles, together with the new SciFact data. We show that our claim verification system is able to identify plausible evidence for 23 / 36 claims relevant to COVID-19 on the CORD-19 corpus. Our results and experiments strongly suggest that our new task and data will support significant future research efforts.","url":"https://www.semanticscholar.org/paper/9e8ac8df8b46c36cad3f307f85975012479b5a32","sentence":"Title: fact or fiction: verifying scientific claims Abstract: we introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that supports or refutes a given scientific claim, and to identify rationales justifying each decision. to study this task, we construct scifact, a dataset of 1.4k expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. we develop baseline models for scifact, and demonstrate that these models benefit from combined training on a large dataset of claims about wikipedia articles, together with the new scifact data. we show that our claim verification system is able to identify plausible evidence for 23 / 36 claims relevant to covid-19 on the cord-19 corpus. our results and experiments strongly suggest that our new task and data will support significant future research efforts.","Task":["scientific claim verification"],"Method":["scientific claim verification"]},{"title":"Rumor Cascades","abstract":"Online social networks provide a rich substrate for rumor propagation. Information received via friends tends to be trusted, and online social networks allow individuals to transmit information to many friends at once. By referencing known rumors from Snopes.com, a popular website documenting memes and urban legends, we track the propagation of thousands of rumors appearing on Facebook. From this sample we infer the rates at which rumors from different categories and of varying truth value are uploaded and reshared. We find that rumor cascades run deeper in the social network than reshare cascades in general. We then examine the effect of individual reshares receiving a comment containing a link to a Snopes article on the evolution of the cascade. We find that receiving such a comment increases the likelihood that a reshare of a rumor will be deleted. Furthermore, large cascades are able to accumulate hundreds of Snopes comments while continuing to propagate. Finally, using a dataset of rumors copied and pasted from one status update to another, we show that rumors change over time and that different variants tend to dominate different bursts in popularity.","url":"https://www.semanticscholar.org/paper/1e3d1055fc65966bc2c64505785207d7e8d71022","sentence":"Title: rumor cascades Abstract: online social networks provide a rich substrate for rumor propagation. information received via friends tends to be trusted, and online social networks allow individuals to transmit information to many friends at once. by referencing known rumors from snopes.com, a popular website documenting memes and urban legends, we track the propagation of thousands of rumors appearing on facebook. from this sample we infer the rates at which rumors from different categories and of varying truth value are uploaded and reshared. we find that rumor cascades run deeper in the social network than reshare cascades in general. we then examine the effect of individual reshares receiving a comment containing a link to a snopes article on the evolution of the cascade. we find that receiving such a comment increases the likelihood that a reshare of a rumor will be deleted. furthermore, large cascades are able to accumulate hundreds of snopes comments while continuing to propagate. finally, using a dataset of rumors copied and pasted from one status update to another, we show that rumors change over time and that different variants tend to dominate different bursts in popularity.","Task":["rumor cascades"],"Method":["rumor cascades"]},{"title":"A Benchmark Dataset of Check-worthy Factual Claims","abstract":"In this paper we present the ClaimBuster dataset of 23,533 statements extracted from all U.S. general election presidential debates and annotated by human coders. The ClaimBuster dataset can be leveraged in building computational methods to identify claims that are worth fact-checking from the myriad of sources of digital or traditional media. The ClaimBuster dataset is publicly available to the research community, and it can be found at this http URL.","url":"https://www.semanticscholar.org/paper/6f373f4711e1285bdec23069c9503d3bf77bfaef","sentence":"Title: a benchmark dataset of check-worthy factual claims Abstract: in this paper we present the claimbuster dataset of 23,533 statements extracted from all u.s. general election presidential debates and annotated by human coders. the claimbuster dataset can be leveraged in building computational methods to identify claims that are worth fact-checking from the myriad of sources of digital or traditional media. the claimbuster dataset is publicly available to the research community, and it can be found at this http url.","Task":["claimbuster"],"Method":["claimbuster"]},{"title":"Rumors, False Flags, and Digital Vigilantes: Misinformation on Twitter after the 2013 Boston Marathon Bombing","abstract":"The Boston Marathon bombing story unfolded on every possible carrier of information available in the spring of 2013, including Twitter. As information spread, it was filled with rumors (unsubstantiated information), and many of these rumors contained misinformation. Earlier studies have suggested that crowdsourced information flows can correct misinformation, and our research investigates this proposition. This exploratory research examines three rumors, later demonstrated to be false, that circulated on Twitter in the aftermath of the bombings. Our findings suggest that corrections to the misinformation emerge but are muted compared with the propagation of the misinformation. The similarities and differences we observe in the patterns of the misinformation and corrections contained within the stream over the days that followed the attacks suggest directions for possible research strategies to automatically detect misinformation.","url":"https://www.semanticscholar.org/paper/44ecb9783e43d726fc87b7ffdd580b01e13a85cb","sentence":"Title: rumors, false flags, and digital vigilantes: misinformation on twitter after the 2013 boston marathon bombing Abstract: the boston marathon bombing story unfolded on every possible carrier of information available in the spring of 2013, including twitter. as information spread, it was filled with rumors (unsubstantiated information), and many of these rumors contained misinformation. earlier studies have suggested that crowdsourced information flows can correct misinformation, and our research investigates this proposition. this exploratory research examines three rumors, later demonstrated to be false, that circulated on twitter in the aftermath of the bombings. our findings suggest that corrections to the misinformation emerge but are muted compared with the propagation of the misinformation. the similarities and differences we observe in the patterns of the misinformation and corrections contained within the stream over the days that followed the attacks suggest directions for possible research strategies to automatically detect misinformation.","Task":["rumors, false flags, digital vigilantes"],"Method":["crowdsourced information flows"]},{"title":"Social Media and Fake News in the 2016 Election","abstract":"Following the 2016 U.S. presidential election, many have expressed concern about the effects of false stories (\u201cfake news\u201d), circulated largely through social media. We discuss the economics of fake news and present new data on its consumption prior to the election. Drawing on web browsing data, archives of fact-checking websites, and results from a new online survey, we find: (i) social media was an important but not dominant source of election news, with 14 percent of Americans calling social media their \u201cmost important\u201d source; (ii) of the known false news stories that appeared in the three months before the election, those favoring Trump were shared a total of 30 million times on Facebook, while those favoring Clinton were shared 8 million times; (iii) the average American adult saw on the order of one or perhaps several fake news stories in the months around the election, with just over half of those who recalled seeing them believing them; and (iv) people are much more likely to believe stories that favor their preferred candidate, especially if they have ideologically segregated social media networks.","url":"https://www.semanticscholar.org/paper/6f78b5608fed43f106da192f12e09d9edbd2fce0","sentence":"Title: social media and fake news in the 2016 election Abstract: following the 2016 u.s. presidential election, many have expressed concern about the effects of false stories (\u201cfake news\u201d), circulated largely through social media. we discuss the economics of fake news and present new data on its consumption prior to the election. drawing on web browsing data, archives of fact-checking websites, and results from a new online survey, we find: (i) social media was an important but not dominant source of election news, with 14 percent of americans calling social media their \u201cmost important\u201d source; (ii) of the known false news stories that appeared in the three months before the election, those favoring trump were shared a total of 30 million times on facebook, while those favoring clinton were shared 8 million times; (iii) the average american adult saw on the order of one or perhaps several fake news stories in the months around the election, with just over half of those who recalled seeing them believing them; and (iv) people are much more likely to believe stories that favor their preferred candidate, especially if they have ideologically segregated social media networks.","Task":["social media and fake news in the 2016 election"],"Method":["fake news"]},{"title":"Extractive and Abstractive Explanations for Fact-Checking and Evaluation of News","abstract":"In this paper, we explore the construction of natural language explanations for news claims, with the goal of assisting fact-checking and news evaluation applications. We experiment with two methods: (1) an extractive method based on Biased TextRank \u2013 a resource-effective unsupervised graph-based algorithm for content extraction; and (2) an abstractive method based on the GPT-2 language model. We perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise.","url":"https://www.semanticscholar.org/paper/69116800a8a8195531d29c8e14cefb1c92cbb8a7","sentence":"Title: extractive and abstractive explanations for fact-checking and evaluation of news Abstract: in this paper, we explore the construction of natural language explanations for news claims, with the goal of assisting fact-checking and news evaluation applications. we experiment with two methods: (1) an extractive method based on biased textrank \u2013 a resource-effective unsupervised graph-based algorithm for content extraction; and (2) an abstractive method based on the gpt-2 language model. we perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise.","Task":["extractive and abstractive explanations for fact-checking and evaluation of news"],"Method":["extractive and abstractive explanations for fact-checking and evaluation of news"]},{"title":"That is a Known Lie: Detecting Previously Fact-Checked Claims","abstract":"The recent proliferation of \u201dfake news\u201d has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. As manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. Interestingly, despite the importance of the task, it has been largely ignored by the research community so far. Here, we aim to bridge this gap. In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. We further create a specialized dataset, which we release to the research community. Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.","url":"https://www.semanticscholar.org/paper/20b2f18aaf10a9221c5edf3720d4cce7da672104","sentence":"Title: that is a known lie: detecting previously fact-checked claims Abstract: the recent proliferation of \u201dfake news\u201d has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. as a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. as manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. interestingly, despite the importance of the task, it has been largely ignored by the research community so far. here, we aim to bridge this gap. in particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. we further create a specialized dataset, which we release to the research community. finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.","Task":["fact-checking"],"Method":["that is a known lie"]},{"title":"Defending Against Neural Fake News","abstract":"Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \\nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary\'s point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,\' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \\nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.","url":"https://www.semanticscholar.org/paper/ad7129af0644dbcafa9aa2f111cb76526ea444a1","sentence":"Title: defending against neural fake news Abstract: recent progress in natural language generation has raised dual-use concerns. while applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \\nmodern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary\'s point of view, and exploring potential mitigations to these threats. likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. we thus present a model for controllable text generation called grover. given a headline like `link found between vaccines and autism,\' grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \\ndeveloping robust verification techniques against generators like grover is critical. we find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. counterintuitively, the best defense against grover turns out to be grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. we investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. we conclude by discussing ethical issues regarding the technology, and plan to release grover publicly, helping pave the way for better detection of neural fake news.","Task":["neural fake news"],"Method":["defending against neural fake news"]},{"title":"MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims","abstract":"We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.","url":"https://www.semanticscholar.org/paper/d8cb11d4be955f9869387a18967dee366eb851d9","sentence":"Title: multifc: a real-world multi-domain dataset for evidence-based fact checking of claims Abstract: we contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. it is collected from 26 fact checking websites in english, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. we present an in-depth analysis of the dataset, highlighting characteristics and challenges. further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. significant performance increases are achieved by encoding evidence, and by modelling metadata. our best-performing model achieves a macro f1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.","Task":["fact checking"],"Method":["evidence-based fact checking of claims"]},{"title":"The Limitations of Stylometry for Detecting Machine-Generated Fake News","abstract":"Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that stylometry is limited against machine-generated misinformation. Whereas humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs, utilized in auto-completion and editing-assistance settings.1 Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.","url":"https://www.semanticscholar.org/paper/9aefd614e52336151966d8dca2ed0ea62a8f30af","sentence":"Title: the limitations of stylometry for detecting machine-generated fake news Abstract: recent developments in neural language models (lms) have raised concerns about their potential misuse for automatically spreading misinformation. in light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. these approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. however, in this work, we show that stylometry is limited against machine-generated misinformation. whereas humans speak differently when trying to deceive, lms generate stylistically consistent text, regardless of underlying motive. thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate lm applications from those that introduce false information. we create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of lms, utilized in auto-completion and editing-assistance settings.1 our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.","Task":["machine-generated fake news, neural language models"],"Method":["Machine-Generated Fake News"]},{"title":"FEVER: a Large-scale Dataset for Fact Extraction and VERification","abstract":"In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.","url":"https://www.semanticscholar.org/paper/b1d24e8e08435b7c52335485a0d635abf9bc604c","sentence":"Title: fever: a large-scale dataset for fact extraction and verification Abstract: in this paper we introduce a new publicly available dataset for verification against textual sources, fever: fact extraction and verification. it consists of 185,445 claims generated by altering sentences extracted from wikipedia and subsequently verified without knowledge of the sentence they were derived from. the claims are classified as supported, refuted or notenoughinfo by annotators achieving 0.6841 in fleiss kappa. for the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. to characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. the best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. thus we believe that fever is a challenging testbed that will help stimulate progress on claim verification against textual sources.","Task":["fever"],"Method":["fact extraction and verification"]},{"title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World","abstract":"Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the \u201clanguage agnostic\u201d status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.","url":"https://www.semanticscholar.org/paper/0e141942fa265142f41a2a26eb17b6005d3af29e","sentence":"Title: the state and fate of linguistic diversity and inclusion in the nlp world Abstract: language technologies contribute to promoting multilingualism and linguistic diversity around the world. however, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. in this paper we look at the relation between the types of languages, resources, and their representation in nlp conferences to understand the trajectory that different languages have followed over time. our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the \u201clanguage agnostic\u201d status of current models and systems. through this paper, we attempt to convince the acl community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.","Task":["linguistic diversity and inclusion"],"Method":["the state and fate of linguistic diversity and inclusion in the nlp world"]},{"title":"Stereotypes in High-Stakes Decisions: Evidence from U.S. Circuit Courts","abstract":"Attitudes towards social groups such as women and racial minorities have been shown to be important determinants of individual\u2019s decisions but are hard to measure for those in policy-making roles. We propose a way to address the challenge in the case of U.S. appellate court judges, for whom we have large corpora of written text (their published opinions). Using the universe of published opinions in U.S. Circuit Courts 1890-2013, we construct a judge-specific measure of gender-stereotyped language (gender slant) by looking at the relative co-occurrence of words identifying gender (male versus female) and words identifying gender stereotypes (career versus family). We find that female and younger judges tend to use less stereotyped language in their opinions. In addition, the attitudes measured by gender slant matter for judicial decisions: judges with higher slant vote more conservatively on women rights\u2019 issues. These more slanted judges also influence workplace outcomes for female colleagues: they are less likely to assign opinions to female judges, they cite fewer female-authored opinions, and they are more likely to reverse lower-court decisions if the lower-court judge is a woman. Our results expose a possible use of text to detect decision-makers\u2019 stereotypes that predict behavior and disparate outcomes. \u2217Arianna Ornaghi, University of Warwick, a.ornaghi@warwick.ac.uk (corresponding author); Elliott Ash, ETH Zurich, ashe@ethz.ch; Daniel Chen, Toulouse School of Economics, daniel.chen@iast.fr. We thank Jacopo Bregolin, David Cai, Christoph Goessmann, and Ornelie Manzambi for helpful research assistance.","url":"https://www.semanticscholar.org/paper/c3bcdea205ec9fb1b84d75d4767f346844082b38","sentence":"Title: stereotypes in high-stakes decisions: evidence from u.s. circuit courts Abstract: attitudes towards social groups such as women and racial minorities have been shown to be important determinants of individual\u2019s decisions but are hard to measure for those in policy-making roles. we propose a way to address the challenge in the case of u.s. appellate court judges, for whom we have large corpora of written text (their published opinions). using the universe of published opinions in u.s. circuit courts 1890-2013, we construct a judge-specific measure of gender-stereotyped language (gender slant) by looking at the relative co-occurrence of words identifying gender (male versus female) and words identifying gender stereotypes (career versus family). we find that female and younger judges tend to use less stereotyped language in their opinions. in addition, the attitudes measured by gender slant matter for judicial decisions: judges with higher slant vote more conservatively on women rights\u2019 issues. these more slanted judges also influence workplace outcomes for female colleagues: they are less likely to assign opinions to female judges, they cite fewer female-authored opinions, and they are more likely to reverse lower-court decisions if the lower-court judge is a woman. our results expose a possible use of text to detect decision-makers\u2019 stereotypes that predict behavior and disparate outcomes. \u2217arianna ornaghi, university of warwick, a.ornaghi@warwick.ac.uk (corresponding author); elliott ash, eth zurich, ashe@ethz.ch; daniel chen, toulouse school of economics, daniel.chen@iast.fr. we thank jacopo bregolin, david cai, christoph goessmann, and ornelie manzambi for helpful research assistance.","Task":["stereotypes in high-stakes decisions: evidence from u.s. circuit courts"],"Method":["stereotypes in high-stakes decisions"]},{"title":"Generating Fact Checking Explanations","abstract":"Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process \u2013 generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.","url":"https://www.semanticscholar.org/paper/5e0daaeceb75ffbbe23be13d34ffae830cb4e8c4","sentence":"Title: generating fact checking explanations Abstract: most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. a crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process \u2013 generating justifications for verdicts on claims. this paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. the results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.","Task":["automated fact checking"],"Method":["generating fact checking explanations"]},{"title":"Towards Debiasing Fact Verification Models","abstract":"Fact verification requires validating a claim in the context of evidence. We show, however, that in the popular FEVER dataset this might not necessarily be the case. Claim-only classifiers perform competitively with top evidence-aware models. In this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. We create an evaluation set that avoids those idiosyncrasies. The performance of FEVER-trained models significantly drops when evaluated on this test set. Therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.","url":"https://www.semanticscholar.org/paper/a4947468ceac25c11a665c4f5f95a49d6dbea3cc","sentence":"Title: towards debiasing fact verification models Abstract: fact verification requires validating a claim in the context of evidence. we show, however, that in the popular fever dataset this might not necessarily be the case. claim-only classifiers perform competitively with top evidence-aware models. in this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. we create an evaluation set that avoids those idiosyncrasies. the performance of fever-trained models significantly drops when evaluated on this test set. therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. this work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.","Task":["fact verification"],"Method":["debiasing fact verification models"]},{"title":"Evaluating adversarial attacks against multiple fact verification systems","abstract":"Automated fact verification has been progressing owing to advancements in modeling and availability of large datasets. Due to the nature of the task, it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly. We introduce two novel scoring metrics, attack potency and system resilience which take into account the correctness of the adversarial instances, an aspect often ignored in adversarial evaluations. We consider six fact verification systems from the recent Fact Extraction and VERification (FEVER) challenge: the four best-scoring ones and two baselines. We evaluate adversarial instances generated by a recently proposed state-of-the-art method, a paraphrasing method, and rule-based attacks devised for fact verification. We find that our rule-based attacks have higher potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines.","url":"https://www.semanticscholar.org/paper/3a16c38294ac8899825c488490199b854e05473a","sentence":"Title: evaluating adversarial attacks against multiple fact verification systems Abstract: automated fact verification has been progressing owing to advancements in modeling and availability of large datasets. due to the nature of the task, it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly. we introduce two novel scoring metrics, attack potency and system resilience which take into account the correctness of the adversarial instances, an aspect often ignored in adversarial evaluations. we consider six fact verification systems from the recent fact extraction and verification (fever) challenge: the four best-scoring ones and two baselines. we evaluate adversarial instances generated by a recently proposed state-of-the-art method, a paraphrasing method, and rule-based attacks devised for fact verification. we find that our rule-based attacks have higher potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines.","Task":["adversarial attacks"],"Method":["evaluating adversarial attacks against multiple fact verification systems"]},{"title":"Unsupervised Discovery of Gendered Language through Latent-Variable Modeling","abstract":"Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.","url":"https://www.semanticscholar.org/paper/d4eeb40b9bd06ed53a26282cd527609f71e6496f","sentence":"Title: unsupervised discovery of gendered language through latent-variable modeling Abstract: studying the ways in which language is gendered has long been an area of interest in sociolinguistics. studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. in this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. to that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. we find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.","Task":["gendered language, latent-variable modeling"],"Method":["gendered language"]},{"title":"ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback","abstract":"We introduce ChrEnTranslate, an online ma\xad chine translation demonstration system for translation between English and an endangered language Cherokee. It supports both statistical and neural translation models as well as pro\xad vides quality estimation to inform users of re\xad liability, two user feedback interfaces for ex\xad perts and common users respectively, exam\xad ple inputs to collect human translations for monolingual data, word alignment visualiza\xad tion, and relevant terms from the Cherokee\xad English dictionary. The quantitative evalu\xad ation demonstrates that our backbone trans\xad lation models achieve state\xadof\xadthe\xadart transla\xad tion performance and our quality estimation well correlates with both BLEU and human judgment. By analyzing 216 pieces of expert feedback, we find that NMT is preferable be\xad cause it copies less than SMT, and, in gen\xad eral, current models can translate fragments of the source sentence but make major mistakes. When we add these 216 expert\xadcorrected paral\xad lel texts into the training set and retrain mod\xad els, equal or slightly better performance is ob\xad served, which demonstrates indicates the po\xad tential of human\xadin\xadthe\xadloop learning.1","url":"https://www.semanticscholar.org/paper/bb53946c7da617a05bbeef47fff74012db27ee78","sentence":"Title: chrentranslate: cherokee-english machine translation demo with quality estimation and corrective feedback Abstract: we introduce chrentranslate, an online ma\xad chine translation demonstration system for translation between english and an endangered language cherokee. it supports both statistical and neural translation models as well as pro\xad vides quality estimation to inform users of re\xad liability, two user feedback interfaces for ex\xad perts and common users respectively, exam\xad ple inputs to collect human translations for monolingual data, word alignment visualiza\xad tion, and relevant terms from the cherokee\xad english dictionary. the quantitative evalu\xad ation demonstrates that our backbone trans\xad lation models achieve state\xadof\xadthe\xadart transla\xad tion performance and our quality estimation well correlates with both bleu and human judgment. by analyzing 216 pieces of expert feedback, we find that nmt is preferable be\xad cause it copies less than smt, and, in gen\xad eral, current models can translate fragments of the source sentence but make major mistakes. when we add these 216 expert\xadcorrected paral\xad lel texts into the training set and retrain mod\xad els, equal or slightly better performance is ob\xad served, which demonstrates indicates the po\xad tential of human\xadin\xadthe\xadloop learning.1","Task":["machine translation"],"Method":["chrentranslate"]},{"title":"Haitian Creole: How to Build and Ship an MT Engine from Scratch in 4 days, 17 hours, & 30 minutes","abstract":"We describe the effort of the Microsoft Translator team to develop a Haitian Creole statistical machine translation engine from scratch in a matter of days. Haitian Creole presents a number of difficulties for devleoping an SMT system, principal among these is the lack of significant amounts of parallel training data and an inconsistent orthography, both of which lead to data sparseness. We demonstrate, however, that it is possible to build a translation engine of reasonable quality over very little data by engaging with the native language community and reducing data sparseness in creative ways. As such, we show that MT as a technology and as a service can be deployed rapidly in crisis situations.","url":"https://www.semanticscholar.org/paper/3d309aa1629ef9ca43e252eb6bf539286ed872f9","sentence":"Title: haitian creole: how to build and ship an mt engine from scratch in 4 days, 17 hours, & 30 minutes Abstract: we describe the effort of the microsoft translator team to develop a haitian creole statistical machine translation engine from scratch in a matter of days. haitian creole presents a number of difficulties for devleoping an smt system, principal among these is the lack of significant amounts of parallel training data and an inconsistent orthography, both of which lead to data sparseness. we demonstrate, however, that it is possible to build a translation engine of reasonable quality over very little data by engaging with the native language community and reducing data sparseness in creative ways. as such, we show that mt as a technology and as a service can be deployed rapidly in crisis situations.","Task":["machine translation"],"Method":["haitian creole"]},{"title":"Measuring Societal Biases in Text Corpora via First-Order Co-occurrence","abstract":"Text corpora are used to study societal biases, typically through statistical models such as word embeddings. The bias of a word towards a concept is typically estimated using vectors similarity, measuring whether the word and concept words share other words in their contexts. We argue that this second-order relationship introduces unrelated concepts into the measure, which causes an imprecise measurement of the bias. We propose instead to measure bias using the direct normalized co-occurrence associations between the word and the representative concept words, a first-order measure, by reconstructing the co-occurrence estimates inherent in the word embedding models. To study our novel corpus bias measurement method, we calculate the correlation of the gender bias values estimated from the text to the actual gender bias statistics of the U.S. job market, provided by two recent collections. The results show a consistently higher correlation when using the proposed first-order measure with a variety of word embedding models, as well as a more severe degree of bias, especially to female in a few specific occupations.","url":"https://www.semanticscholar.org/paper/f14cb1828e314a669304b0c37bc78d6b9073f6dd","sentence":"Title: measuring societal biases in text corpora via first-order co-occurrence Abstract: text corpora are used to study societal biases, typically through statistical models such as word embeddings. the bias of a word towards a concept is typically estimated using vectors similarity, measuring whether the word and concept words share other words in their contexts. we argue that this second-order relationship introduces unrelated concepts into the measure, which causes an imprecise measurement of the bias. we propose instead to measure bias using the direct normalized co-occurrence associations between the word and the representative concept words, a first-order measure, by reconstructing the co-occurrence estimates inherent in the word embedding models. to study our novel corpus bias measurement method, we calculate the correlation of the gender bias values estimated from the text to the actual gender bias statistics of the u.s. job market, provided by two recent collections. the results show a consistently higher correlation when using the proposed first-order measure with a variety of word embedding models, as well as a more severe degree of bias, especially to female in a few specific occupations.","Task":["measuring societal biases in text corpora via first-order co-occurrence"],"Method":["corpus bias measurement"]},{"title":"When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?","abstract":"Social biases are encoded in word embeddings. This presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. Here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods. We find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning. However, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g. gender) than others (e.g. race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.","url":"https://www.semanticscholar.org/paper/176420f394014d9e67d5cff1e1d430541fa0f55f","sentence":"Title: when do word embeddings accurately reflect surveys on our beliefs about people? Abstract: social biases are encoded in word embeddings. this presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods. we find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning. however, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g. gender) than others (e.g. race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.","Task":["word embeddings"],"Method":["word embeddings"]},{"title":"Language from police body camera footage shows racial disparities in officer respect","abstract":"Significance Police officers speak significantly less respectfully to black than to white community members in everyday traffic stops, even after controlling for officer race, infraction severity, stop location, and stop outcome. This paper presents a systematic analysis of officer body-worn camera footage, using computational linguistic techniques to automatically measure the respect level that officers display to community members. This work demonstrates that body camera footage can be used as a rich source of data rather than merely archival evidence, and paves the way for developing powerful language-based tools for studying and potentially improving police\u2013community relations. Using footage from body-worn cameras, we analyze the respectfulness of police officer language toward white and black community members during routine traffic stops. We develop computational linguistic methods that extract levels of respect automatically from transcripts, informed by a thin-slicing study of participant ratings of officer utterances. We find that officers speak with consistently less respect toward black versus white community members, even after controlling for the race of the officer, the severity of the infraction, the location of the stop, and the outcome of the stop. Such disparities in common, everyday interactions between police and the communities they serve have important implications for procedural justice and the building of police\u2013community trust.","url":"https://www.semanticscholar.org/paper/75d0cb419d7d58e81c2975758a36a11544a9f930","sentence":"Title: language from police body camera footage shows racial disparities in officer respect Abstract: significance police officers speak significantly less respectfully to black than to white community members in everyday traffic stops, even after controlling for officer race, infraction severity, stop location, and stop outcome. this paper presents a systematic analysis of officer body-worn camera footage, using computational linguistic techniques to automatically measure the respect level that officers display to community members. this work demonstrates that body camera footage can be used as a rich source of data rather than merely archival evidence, and paves the way for developing powerful language-based tools for studying and potentially improving police\u2013community relations. using footage from body-worn cameras, we analyze the respectfulness of police officer language toward white and black community members during routine traffic stops. we develop computational linguistic methods that extract levels of respect automatically from transcripts, informed by a thin-slicing study of participant ratings of officer utterances. we find that officers speak with consistently less respect toward black versus white community members, even after controlling for the race of the officer, the severity of the infraction, the location of the stop, and the outcome of the stop. such disparities in common, everyday interactions between police and the communities they serve have important implications for procedural justice and the building of police\u2013community trust.","Task":["racial disparities in officer respect, body-worn camera footage"],"Method":["language from police body camera footage"]},{"title":"Word embeddings quantify 100 years of gender and ethnic stereotypes","abstract":"Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts\u2014e.g., the women\u2019s movement in the 1960s and Asian immigration into the United States\u2014and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.","url":"https://www.semanticscholar.org/paper/b5d7a19bd0bae10917a8e294960fdacf224d64fe","sentence":"Title: word embeddings quantify 100 years of gender and ethnic stereotypes Abstract: significance word embeddings are a popular machine-learning method that represents each english word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. we demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. as specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the united states evolved during the 20th and 21st centuries starting from 1910. our framework opens up a fruitful intersection between machine learning and quantitative social science. word embeddings are a powerful machine-learning framework that represents each english word by a vector. the geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. in this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the united states. we integrate word embeddings trained on 100 y of text data with the us census to show that changes in the embedding track closely with demographic and occupation shifts over time. the embedding captures societal shifts\u2014e.g., the women\u2019s movement in the 1960s and asian immigration into the united states\u2014and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.","Task":["word embeddings"],"Method":["word embeddings"]},{"title":"Tie-breaker: Using language models to quantify gender bias in sports journalism","abstract":"Gender bias is an increasingly important issue in sports journalism. In this work, we propose a language-model-based approach to quantify differences in questions posed to female vs. male athletes, and apply it to tennis post-match interviews. We find that journalists ask male players questions that are generally more focused on the game when compared with the questions they ask their female counterparts. We also provide a fine-grained analysis of the extent to which the salience of this bias depends on various factors, such as question type, game outcome or player rank.","url":"https://www.semanticscholar.org/paper/3d505c5eff8752ac1805ef546d683bfa40aec4b1","sentence":"Title: tie-breaker: using language models to quantify gender bias in sports journalism Abstract: gender bias is an increasingly important issue in sports journalism. in this work, we propose a language-model-based approach to quantify differences in questions posed to female vs. male athletes, and apply it to tennis post-match interviews. we find that journalists ask male players questions that are generally more focused on the game when compared with the questions they ask their female counterparts. we also provide a fine-grained analysis of the extent to which the salience of this bias depends on various factors, such as question type, game outcome or player rank.","Task":["gender bias, sports journalism, language models"],"Method":["gender bias in sports journalism"]},{"title":"Entity-Centric Contextual Affective Analysis","abstract":"While contextualized word representations have improved state-of-the-art benchmarks in many NLP tasks, their potential usefulness for social-oriented tasks remains largely unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women.","url":"https://www.semanticscholar.org/paper/a1280728623e8fd605284b2b7cf536579b9e2cbf","sentence":"Title: entity-centric contextual affective analysis Abstract: while contextualized word representations have improved state-of-the-art benchmarks in many nlp tasks, their potential usefulness for social-oriented tasks remains largely unexplored. we show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. we evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. we find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. we ultimately use our method to examine differences in portrayals of men and women.","Task":["contextualized word embeddings"],"Method":["entity-centric contextual affective analysis"]},{"title":"Racism is a Virus: Anti-Asian Hate and Counterhate in Social Media during the COVID-19 Crisis","abstract":"The spread of COVID-19 has sparked racism, hate, and xenophobia in social media targeted at Chinese and broader Asian communities. However, little is known about how racial hate spreads during a pandemic and the role of counterhate speech in mitigating the spread. Here we study the evolution and spread of anti-Asian hate speech through the lens of Twitter. We create COVID-HATE, the largest dataset of anti-Asian hate and counterhate spanning three months, containing over 30 million tweets, and a social network with over 87 million nodes. By creating a novel hand-labeled dataset of 2,400 tweets, we train a text classifier to identify hate and counterhate tweets that achieves an average AUROC of 0.852. We identify 891,204 hate and 200,198 counterhate tweets in COVID-HATE. Using this data to conduct longitudinal analysis, we find that while hateful users are less engaged in the COVID-19 discussions prior to their first anti-Asian tweet, they become more vocal and engaged afterwards compared to counterhate users. We find that bots comprise 10.4% of hateful users and are more vocal and hateful compared to non-bot users. Comparing bot accounts, we show that hateful bots are more successful in attracting followers compared to counterhate bots. Analysis of the social network reveals that hateful and counterhate users interact and engage extensively with one another, instead of living in isolated polarized communities. Furthermore, we find that hate is contagious and nodes are highly likely to become hateful after being exposed to hateful content. Importantly, our analysis reveals that counterhate messages can discourage users from turning hateful in the first place. Overall, this work presents a comprehensive overview of anti-Asian hate and counterhate content during a pandemic. The COVID-HATE dataset is available at this http URL.","url":"https://www.semanticscholar.org/paper/070b4a707748e289618880ffbe4762e4e3fc7860","sentence":"Title: racism is a virus: anti-asian hate and counterhate in social media during the covid-19 crisis Abstract: the spread of covid-19 has sparked racism, hate, and xenophobia in social media targeted at chinese and broader asian communities. however, little is known about how racial hate spreads during a pandemic and the role of counterhate speech in mitigating the spread. here we study the evolution and spread of anti-asian hate speech through the lens of twitter. we create covid-hate, the largest dataset of anti-asian hate and counterhate spanning three months, containing over 30 million tweets, and a social network with over 87 million nodes. by creating a novel hand-labeled dataset of 2,400 tweets, we train a text classifier to identify hate and counterhate tweets that achieves an average auroc of 0.852. we identify 891,204 hate and 200,198 counterhate tweets in covid-hate. using this data to conduct longitudinal analysis, we find that while hateful users are less engaged in the covid-19 discussions prior to their first anti-asian tweet, they become more vocal and engaged afterwards compared to counterhate users. we find that bots comprise 10.4% of hateful users and are more vocal and hateful compared to non-bot users. comparing bot accounts, we show that hateful bots are more successful in attracting followers compared to counterhate bots. analysis of the social network reveals that hateful and counterhate users interact and engage extensively with one another, instead of living in isolated polarized communities. furthermore, we find that hate is contagious and nodes are highly likely to become hateful after being exposed to hateful content. importantly, our analysis reveals that counterhate messages can discourage users from turning hateful in the first place. overall, this work presents a comprehensive overview of anti-asian hate and counterhate content during a pandemic. the covid-hate dataset is available at this http url.","Task":["racism is a virus, anti-asian hate, counterhate"],"Method":["anti-asian hate and counterhate in social media"]},{"title":"Automatically Inferring Gender Associations from Language","abstract":"In this paper, we pose the question: do people talk about women and men in different ways? We introduce two datasets and a novel integration of approaches for automatically inferring gender associations from language, discovering coherent word clusters, and labeling the clusters for the semantic concepts they represent. The datasets allow us to compare how people write about women and men in two different settings \u2013 one set draws from celebrity news and the other from student reviews of computer science professors. We demonstrate that there are large-scale differences in the ways that people talk about women and men and that these differences vary across domains. Human evaluations show that our methods significantly outperform strong baselines.","url":"https://www.semanticscholar.org/paper/7b5b2a9ad37d1a6c3c8916965b1958eef0a27a6a","sentence":"Title: automatically inferring gender associations from language Abstract: in this paper, we pose the question: do people talk about women and men in different ways? we introduce two datasets and a novel integration of approaches for automatically inferring gender associations from language, discovering coherent word clusters, and labeling the clusters for the semantic concepts they represent. the datasets allow us to compare how people write about women and men in two different settings \u2013 one set draws from celebrity news and the other from student reviews of computer science professors. we demonstrate that there are large-scale differences in the ways that people talk about women and men and that these differences vary across domains. human evaluations show that our methods significantly outperform strong baselines.","Task":["automatically inferring gender associations from language"],"Method":["gender associations from language"]},{"title":"Contextual Affective Analysis: A Case Study of People Portrayals in Online #MeToo Stories","abstract":"In October 2017, numerous women accused producer Harvey Weinstein of sexual harassment. Their stories encouraged other women to voice allegations of sexual harassment against many high profile men, including politicians, actors, and producers. These events are broadly referred to as the #MeToo movement, named for the use of the hashtag \\"#metoo\\" on social media platforms like Twitter and Facebook. The movement has widely been referred to as \\"empowering\\" because it has amplified the voices of previously unheard women over those of traditionally powerful men. In this work, we investigate dynamics of sentiment, power and agency in online media coverage of these events. Using a corpus of online media articles about the #MeToo movement, we present a contextual affective analysis---an entity-centric approach that uses contextualized lexicons to examine how people are portrayed in media articles. We show that while these articles are sympathetic towards women who have experienced sexual harassment, they consistently present men as most powerful, even after sexual assault allegations. While we focus on media coverage of the #MeToo movement, our method for contextual affective analysis readily generalizes to other domains.","url":"https://www.semanticscholar.org/paper/6ba951771892f01206f1dd7244f14243e3885109","sentence":"Title: contextual affective analysis: a case study of people portrayals in online #metoo stories Abstract: in october 2017, numerous women accused producer harvey weinstein of sexual harassment. their stories encouraged other women to voice allegations of sexual harassment against many high profile men, including politicians, actors, and producers. these events are broadly referred to as the #metoo movement, named for the use of the hashtag \\"#metoo\\" on social media platforms like twitter and facebook. the movement has widely been referred to as \\"empowering\\" because it has amplified the voices of previously unheard women over those of traditionally powerful men. in this work, we investigate dynamics of sentiment, power and agency in online media coverage of these events. using a corpus of online media articles about the #metoo movement, we present a contextual affective analysis---an entity-centric approach that uses contextualized lexicons to examine how people are portrayed in media articles. we show that while these articles are sympathetic towards women who have experienced sexual harassment, they consistently present men as most powerful, even after sexual assault allegations. while we focus on media coverage of the #metoo movement, our method for contextual affective analysis readily generalizes to other domains.","Task":["contextual affective analysis"],"Method":["contextual affective analysis"]},{"title":"Girls Rule, Boys Drool: Extracting Semantic and Affective Stereotypes from Twitter","abstract":"Social identities carry widely agreed upon meanings, called stereotypes, that have important effects on social processes. In the present work, we develop a method to extract the stereotypes of Twitter users. Our method is grounded in two distinct strands of theory, one that represents stereotypes as identities\' affective meanings and the other that represents stereotypes as semantic relationships between identities. After validating our approach via a prediction task, we apply the model to a dataset of 45 thousand Twitter users who actively tweeted about the Michael Brown and Eric Garner tragedies. Our work provides unique insights into the stereotypes of these users, as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel, parsimonious way.","url":"https://www.semanticscholar.org/paper/995e477360908175d0b1184f6a0aace9d864bc5a","sentence":"Title: girls rule, boys drool: extracting semantic and affective stereotypes from twitter Abstract: social identities carry widely agreed upon meanings, called stereotypes, that have important effects on social processes. in the present work, we develop a method to extract the stereotypes of twitter users. our method is grounded in two distinct strands of theory, one that represents stereotypes as identities\' affective meanings and the other that represents stereotypes as semantic relationships between identities. after validating our approach via a prediction task, we apply the model to a dataset of 45 thousand twitter users who actively tweeted about the michael brown and eric garner tragedies. our work provides unique insights into the stereotypes of these users, as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel, parsimonious way.","Task":["social identities, stereotypes, twitter"],"Method":["stereotypes in social media"]},{"title":"Relating Linguistic Gender Bias, Gender Values, and Gender Gaps: An International Analysis","abstract":"Recent research in machine learning has shown that many machine-learned language models contain pervasive racial and gender biases, rooting from biases in their textual training data. While these biases produce sub-optimal parsing and inferences, they may help us characterize and predict statistical gender gaps and gender values in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach to (1) quantify gender bias in word embeddings (i.e., vector-based lexical semantics), (2) correlate gender biases with survey responses and statistical gender gaps in education, politics, economics, and health, and (3) integrate numerical biases and statistics to model different cultures\u2019 survey results more accurately than either in isolation. We validate this approach using 2018 Twitter data spanning 99 countries, 18 Global Gender Gap statistics from the World Economic Forum, and 8 international survey results from the World Value Survey. Integrating these heterogeneous data across cultures is an important step toward building computational models to understand group bias.","url":"https://www.semanticscholar.org/paper/5e888bfd9b492a3b08f3cc2eb7c617fedf5bd811","sentence":"Title: relating linguistic gender bias, gender values, and gender gaps: an international analysis Abstract: recent research in machine learning has shown that many machine-learned language models contain pervasive racial and gender biases, rooting from biases in their textual training data. while these biases produce sub-optimal parsing and inferences, they may help us characterize and predict statistical gender gaps and gender values in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. this paper presents an approach to (1) quantify gender bias in word embeddings (i.e., vector-based lexical semantics), (2) correlate gender biases with survey responses and statistical gender gaps in education, politics, economics, and health, and (3) integrate numerical biases and statistics to model different cultures\u2019 survey results more accurately than either in isolation. we validate this approach using 2018 twitter data spanning 99 countries, 18 global gender gap statistics from the world economic forum, and 8 international survey results from the world value survey. integrating these heterogeneous data across cultures is an important step toward building computational models to understand group bias.","Task":["linguistic gender bias, gender values, gender gaps,"],"Method":["gender bias in machine learning"]},{"title":"Automation, Algorithms, and Politics| Talking to Bots: Symbiotic Agency and the Case of Tay","abstract":"In 2016, Microsoft launched Tay, an experimental artificial intelligence chat bot. Learning from interactions with Twitter users, Tay was shut down after one day because of its obscene and inflammatory tweets. This article uses the case of Tay to re-examine theories of agency. How did users view the personality and actions of an artificial intelligence chat bot when interacting with Tay on Twitter? Using phenomenological research methods and pragmatic approaches to agency, we look at what people said about Tay to study how they imagine and interact with emerging technologies and to show the limitations of our current theories of agency for describing communication in these settings. We show how different qualities of agency, different expectations for technologies, and different capacities for affordance emerge in the interactions between people and artificial intelligence. We argue that a perspective of \u201csymbiotic agency\u201d\u2014informed by the imagined affordances of emerging technology\u2014is required to really understand the collapse of Tay.","url":"https://www.semanticscholar.org/paper/abeee58b9fb5761133636ef117ef1a87203ad7ab","sentence":"Title: automation, algorithms, and politics| talking to bots: symbiotic agency and the case of tay Abstract: in 2016, microsoft launched tay, an experimental artificial intelligence chat bot. learning from interactions with twitter users, tay was shut down after one day because of its obscene and inflammatory tweets. this article uses the case of tay to re-examine theories of agency. how did users view the personality and actions of an artificial intelligence chat bot when interacting with tay on twitter? using phenomenological research methods and pragmatic approaches to agency, we look at what people said about tay to study how they imagine and interact with emerging technologies and to show the limitations of our current theories of agency for describing communication in these settings. we show how different qualities of agency, different expectations for technologies, and different capacities for affordance emerge in the interactions between people and artificial intelligence. we argue that a perspective of \u201csymbiotic agency\u201d\u2014informed by the imagined affordances of emerging technology\u2014is required to really understand the collapse of tay.","Task":["automation, algorithms, and politics"],"Method":["automation, algorithms, and politics"]},{"title":"Empathy Is All You Need: How a Conversational Agent Should Respond to Verbal Abuse","abstract":"With the popularity of AI-infused systems, conversational agents (CAs) are becoming essential in diverse areas, offering new functionality and convenience, but simultaneously, suffering misuse and verbal abuse. We examine whether conversational agents\' response styles under varying abuse types influence those emotions found to mitigate peoples\' aggressive behaviors, involving three verbal abuse types (Insult, Threat, Swearing) and three response styles (Avoidance, Empathy, Counterattacking). Ninety-eight participants were assigned to one of the abuse type conditions, interacted with the three spoken (voice-based) CAs in turn, and reported their feelings about guiltiness, anger, and shame after each session. The results show that the agent\'s response style has a significant effect on user emotions. Participants were less angry and more guilty with the empathy agent than the other two agents. Furthermore, we investigated the current status of commercial CAs\' responses to verbal abuse. Our study findings have direct implications for the design of conversational agents.","url":"https://www.semanticscholar.org/paper/e3fd3b1be871da6e048adaef4a4e201af282fe8e","sentence":"Title: empathy is all you need: how a conversational agent should respond to verbal abuse Abstract: with the popularity of ai-infused systems, conversational agents (cas) are becoming essential in diverse areas, offering new functionality and convenience, but simultaneously, suffering misuse and verbal abuse. we examine whether conversational agents\' response styles under varying abuse types influence those emotions found to mitigate peoples\' aggressive behaviors, involving three verbal abuse types (insult, threat, swearing) and three response styles (avoidance, empathy, counterattacking). ninety-eight participants were assigned to one of the abuse type conditions, interacted with the three spoken (voice-based) cas in turn, and reported their feelings about guiltiness, anger, and shame after each session. the results show that the agent\'s response style has a significant effect on user emotions. participants were less angry and more guilty with the empathy agent than the other two agents. furthermore, we investigated the current status of commercial cas\' responses to verbal abuse. our study findings have direct implications for the design of conversational agents.","Task":["conversational agents"],"Method":["conversational agents"]},{"title":"Shirtless and Dangerous: Quantifying Linguistic Signals of Gender Bias in an Online Fiction Writing Community","abstract":"Imagine a princess asleep in a castle, waiting for her prince to slay the dragon and rescue her. Tales like the famous Sleeping Beauty clearly divide up gender roles. But what about more modern stories, borne of a generation increasingly aware of social constructs like sexism and racism? Do these stories tend to reinforce gender stereotypes, or counter them? In this paper, we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction. We apply this technique across 1.8 billion words of fiction from the Wattpad online writing community, investigating gender representation in stories, how male and female characters behave and are described, and how authors\' use of gender stereotypes is associated with the community\'s ratings. We find that male over-representation and traditional gender stereotypes (e.g., dominant men and submissive women) are common throughout nearly every genre in our corpus. However, only some of these stereotypes, like sexual or violent men, are associated with highly rated stories. Finally, despite women often being the target of negative stereotypes, female authors are equally likely to write such stereotypes as men.","url":"https://www.semanticscholar.org/paper/ef5fa2e95fc853defb902b58d8e4e4fe95a01c75","sentence":"Title: shirtless and dangerous: quantifying linguistic signals of gender bias in an online fiction writing community Abstract: imagine a princess asleep in a castle, waiting for her prince to slay the dragon and rescue her. tales like the famous sleeping beauty clearly divide up gender roles. but what about more modern stories, borne of a generation increasingly aware of social constructs like sexism and racism? do these stories tend to reinforce gender stereotypes, or counter them? in this paper, we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction. we apply this technique across 1.8 billion words of fiction from the wattpad online writing community, investigating gender representation in stories, how male and female characters behave and are described, and how authors\' use of gender stereotypes is associated with the community\'s ratings. we find that male over-representation and traditional gender stereotypes (e.g., dominant men and submissive women) are common throughout nearly every genre in our corpus. however, only some of these stereotypes, like sexual or violent men, are associated with highly rated stories. finally, despite women often being the target of negative stereotypes, female authors are equally likely to write such stereotypes as men.","Task":["gender biases in fiction"],"Method":["gender biases in fiction"]},{"title":"Let\'s Talk About Race: Identity, Chatbots, and AI","abstract":"Why is it so hard for chatbots to talk about race? This work explores how the biased contents of databases, the syntactic focus of natural language processing, and the opaque nature of deep learning algorithms cause chatbots difficulty in handling race-talk. In each of these areas, the tensions between race and chatbots create new opportunities for people and machines. By making the abstract and disparate qualities of this problem space tangible, we can develop chatbots that are more capable of handling race-talk in its many forms. Our goal is to provide the HCI community with ways to begin addressing the question, how can chatbots handle race-talk in new and improved ways?","url":"https://www.semanticscholar.org/paper/f34c73c75a640f59c11472bf6c9786aeb774856a","sentence":"Title: let\'s talk about race: identity, chatbots, and ai Abstract: why is it so hard for chatbots to talk about race? this work explores how the biased contents of databases, the syntactic focus of natural language processing, and the opaque nature of deep learning algorithms cause chatbots difficulty in handling race-talk. in each of these areas, the tensions between race and chatbots create new opportunities for people and machines. by making the abstract and disparate qualities of this problem space tangible, we can develop chatbots that are more capable of handling race-talk in its many forms. our goal is to provide the hci community with ways to begin addressing the question, how can chatbots handle race-talk in new and improved ways?","Task":["race, identity, chatbots, ai"],"Method":["identity, chatbots, and ai"]},{"title":"#MeToo Alexa: How Conversational Systems Respond to Sexual Harassment","abstract":"Conversational AI systems, such as Amazon\u2019s Alexa, are rapidly developing from purely transactional systems to social chatbots, which can respond to a wide variety of user requests. In this article, we establish how current state-of-the-art conversational systems react to inappropriate requests, such as bullying and sexual harassment on the part of the user, by collecting and analysing the novel #MeTooAlexa corpus. Our results show that commercial systems mainly avoid answering, while rule-based chatbots show a variety of behaviours and often deflect. Data-driven systems, on the other hand, are often non-coherent, but also run the risk of being interpreted as flirtatious and sometimes react with counter-aggression. This includes our own system, trained on \u201cclean\u201d data, which suggests that inappropriate system behaviour is not caused by data bias.","url":"https://www.semanticscholar.org/paper/983ad7c704d0f9a1560af322e4807e5be7799895","sentence":"Title: #metoo alexa: how conversational systems respond to sexual harassment Abstract: conversational ai systems, such as amazon\u2019s alexa, are rapidly developing from purely transactional systems to social chatbots, which can respond to a wide variety of user requests. in this article, we establish how current state-of-the-art conversational systems react to inappropriate requests, such as bullying and sexual harassment on the part of the user, by collecting and analysing the novel #metooalexa corpus. our results show that commercial systems mainly avoid answering, while rule-based chatbots show a variety of behaviours and often deflect. data-driven systems, on the other hand, are often non-coherent, but also run the risk of being interpreted as flirtatious and sometimes react with counter-aggression. this includes our own system, trained on \u201cclean\u201d data, which suggests that inappropriate system behaviour is not caused by data bias.","Task":["#metoo alexa"],"Method":["#metoo alexa"]},{"title":"Social Bias Frames: Reasoning about Social and Power Implications of Language","abstract":"Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people\u2019s judgments about others. For example, given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women,\u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified.\u201d Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.","url":"https://www.semanticscholar.org/paper/7b14a165c6b7c1dc2c6c44727e623b94d834fb09","sentence":"Title: social bias frames: reasoning about social and power implications of language Abstract: warning: this paper contains content that may be offensive or upsetting. language has the power to reinforce stereotypes and project social biases onto others. at the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people\u2019s judgments about others. for example, given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women,\u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified.\u201d most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. we introduce social bias frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. in addition, we introduce the social bias inference corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. we then establish baseline approaches that learn to recover social bias frames from unstructured text. we find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% f1), they are not effective at spelling out more detailed explanations in terms of social bias frames. our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.","Task":["social bias frames, social bias inference corpus, social bias inference corpus"],"Method":["social bias frames"]},{"title":"Data Statements for NLP: Toward Mitigating System Bias and Enabling Better Science","abstract":"In this position paper, we propose data statements as a practice that NLP technologists, in both research and development, can adopt to begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form data statements can take and explore the implications of adopting them as part of our regular practice. We argue that they will help alleviate issues related to exclusion and bias in language technology; lead to better precision in claims about how NLP research can generalize and thus better engineering results; protect companies from public embarrassment; and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.","url":"https://www.semanticscholar.org/paper/87eb23f934a0e6293ee8ee9b147fe0d456e65c96","sentence":"Title: data statements for nlp: toward mitigating system bias and enabling better science Abstract: in this position paper, we propose data statements as a practice that nlp technologists, in both research and development, can adopt to begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. we present a form data statements can take and explore the implications of adopting them as part of our regular practice. we argue that they will help alleviate issues related to exclusion and bias in language technology; lead to better precision in claims about how nlp research can generalize and thus better engineering results; protect companies from public embarrassment; and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.","Task":["data statements"],"Method":["data statements for nlp"]},{"title":"Datasheets for Datasets","abstract":"The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.","url":"https://www.semanticscholar.org/paper/0df347f5e3118fac7c351917e3a497899b071d1e","sentence":"Title: datasheets for datasets Abstract: the machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. to address this gap, we propose datasheets for datasets. in the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. by analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.","Task":["datasheets for datasets"],"Method":["datasheets for datasets"]},{"title":"Beyond the Belmont Principles: Ethical Challenges, Practices, and Beliefs in the Online Data Research Community","abstract":"Pervasive information streams that document people and their routines have been a boon to social computing research. But the ethics of collecting and analyzing available&-but potentially sensitive-online data present challenges to researchers. In response to increasing public and scholarly debate over the ethics of online data research, this paper analyzes the current state of practice among researchers using online data. Qualitative and quantitative responses from a survey of 263 online data researchers document beliefs and practices around which social computing researchers are converging, as well as areas of ongoing disagreement. The survey also reveals that these disagreements are not correlated with disciplinary, methodological, or workplace affiliations. The paper concludes by reflecting on changing ethical practices in the digital age, and discusses a set of emergent best practices for ethical social computing research.","url":"https://www.semanticscholar.org/paper/221732318e3cc45aa7bc2f48435706f3e5839ddc","sentence":"Title: beyond the belmont principles: ethical challenges, practices, and beliefs in the online data research community Abstract: pervasive information streams that document people and their routines have been a boon to social computing research. but the ethics of collecting and analyzing available&-but potentially sensitive-online data present challenges to researchers. in response to increasing public and scholarly debate over the ethics of online data research, this paper analyzes the current state of practice among researchers using online data. qualitative and quantitative responses from a survey of 263 online data researchers document beliefs and practices around which social computing researchers are converging, as well as areas of ongoing disagreement. the survey also reveals that these disagreements are not correlated with disciplinary, methodological, or workplace affiliations. the paper concludes by reflecting on changing ethical practices in the digital age, and discusses a set of emergent best practices for ethical social computing research.","Task":["ethics, online data, social computing"],"Method":["the ethics of online data research"]},{"title":"Detecting East Asian Prejudice on Social Media","abstract":"During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from Twitter into four classes: Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The classifier achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.","url":"https://www.semanticscholar.org/paper/0c68d7d153bb56e4637d6aee051d87580e05fd5b","sentence":"Title: detecting east asian prejudice on social media Abstract: during covid-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against east asia and east asian people. we report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from twitter into four classes: hostility against east asia, criticism of east asia, meta-discussions of east asian prejudice, and a neutral class. the classifier achieves a macro-f1 score of 0.83. we then conduct an in-depth ground-up error analysis and show that the model struggles with edge cases and ambiguous content. we provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. we also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for covid-19 relevance and east asian relevance and stance for 1,000 hashtags, and the final model.","Task":["east asian prejudice"],"Method":["detecting east asian prejudice on social media"]},{"title":"Don\u2019t quote me: reverse identification of research participants in social media studies","abstract":"We investigated if participants in social media surveillance studies could be reverse identified by reviewing all articles published on PubMed in 2015 or 2016 with the words \u201cTwitter\u201d and either \u201cread,\u201d \u201ccoded,\u201d or \u201ccontent\u201d in the title or abstract. Seventy-two percent (95% CI: 63\u201380) of articles quoted at least one participant\u2019s tweet and searching for the quoted content led to the participant 84% (95% CI: 74\u201391) of the time. Twenty-one percent (95% CI: 13\u201329) of articles disclosed a participant\u2019s Twitter username thereby making the participant immediately identifiable. Only one article reported obtaining consent to disclose identifying information and institutional review board (IRB) involvement was mentioned in only 40% (95% CI: 31\u201350) of articles, of which 17% (95% CI: 10\u201325) received IRB-approval and 23% (95% CI:16\u201332) were deemed exempt. Biomedical publications are routinely including identifiable information by quoting tweets or revealing usernames which, in turn, violates ICMJE ethical standards governing scientific ethics, even though said content is scientifically unnecessary. We propose that authors convey aggregate findings without revealing participants\u2019 identities, editors refuse to publish reports that reveal a participant\u2019s identity, and IRBs attend to these privacy issues when reviewing studies involving social media data. These strategies together will ensure participants are protected going forward.","url":"https://www.semanticscholar.org/paper/1ece7c00d2eb6fca5443ff8e15f05a2b8b5985c2","sentence":"Title: don\u2019t quote me: reverse identification of research participants in social media studies Abstract: we investigated if participants in social media surveillance studies could be reverse identified by reviewing all articles published on pubmed in 2015 or 2016 with the words \u201ctwitter\u201d and either \u201cread,\u201d \u201ccoded,\u201d or \u201ccontent\u201d in the title or abstract. seventy-two percent (95% ci: 63\u201380) of articles quoted at least one participant\u2019s tweet and searching for the quoted content led to the participant 84% (95% ci: 74\u201391) of the time. twenty-one percent (95% ci: 13\u201329) of articles disclosed a participant\u2019s twitter username thereby making the participant immediately identifiable. only one article reported obtaining consent to disclose identifying information and institutional review board (irb) involvement was mentioned in only 40% (95% ci: 31\u201350) of articles, of which 17% (95% ci: 10\u201325) received irb-approval and 23% (95% ci:16\u201332) were deemed exempt. biomedical publications are routinely including identifiable information by quoting tweets or revealing usernames which, in turn, violates icmje ethical standards governing scientific ethics, even though said content is scientifically unnecessary. we propose that authors convey aggregate findings without revealing participants\u2019 identities, editors refuse to publish reports that reveal a participant\u2019s identity, and irbs attend to these privacy issues when reviewing studies involving social media data. these strategies together will ensure participants are protected going forward.","Task":["social media"],"Method":["reverse identification of research participants in social media studies"]},{"title":"Towards an Ethical Framework for Publishing Twitter Data in Social Research: Taking into Account Users\u2019 Views, Online Context and Algorithmic Estimation","abstract":"New and emerging forms of data, including posts harvested from social media sites such as Twitter, have become part of the sociologist\u2019s data diet. In particular, some researchers see an advantage in the perceived \u2018public\u2019 nature of Twitter posts, representing them in publications without seeking informed consent. While such practice may not be at odds with Twitter\u2019s terms of service, we argue there is a need to interpret these through the lens of social science research methods that imply a more reflexive ethical approach than provided in \u2018legal\u2019 accounts of the permissible use of these data in research publications. To challenge some existing practice in Twitter-based research, this article brings to the fore: (1) views of Twitter users through analysis of online survey data; (2) the effect of context collapse and online disinhibition on the behaviours of users; and (3) the publication of identifiable sensitive classifications derived from algorithms.","url":"https://www.semanticscholar.org/paper/afe97d05e5b320d2af500cdae1c588f4cc0d14d2","sentence":"Title: towards an ethical framework for publishing twitter data in social research: taking into account users\u2019 views, online context and algorithmic estimation Abstract: new and emerging forms of data, including posts harvested from social media sites such as twitter, have become part of the sociologist\u2019s data diet. in particular, some researchers see an advantage in the perceived \u2018public\u2019 nature of twitter posts, representing them in publications without seeking informed consent. while such practice may not be at odds with twitter\u2019s terms of service, we argue there is a need to interpret these through the lens of social science research methods that imply a more reflexive ethical approach than provided in \u2018legal\u2019 accounts of the permissible use of these data in research publications. to challenge some existing practice in twitter-based research, this article brings to the fore: (1) views of twitter users through analysis of online survey data; (2) the effect of context collapse and online disinhibition on the behaviours of users; and (3) the publication of identifiable sensitive classifications derived from algorithms.","Task":["twitter"],"Method":["twitter data in social research"]},{"title":"Writer Profiling Without the Writer\'s Text","abstract":"Social network users may wish to preserve their anonymity online by masking their identity and not using language associated with any particular demographics or personality. However, they have no control over the language in incoming communications. We show that linguistic cues in public comments directed at a user are sufficient for an accurate inference of that user\u2019s gender, age, religion, diet, and even personality traits. Moreover, we show that directed communication is even more predictive of a user\u2019s profile than the user\u2019s own language. We then conduct a nuanced analysis of what types of social relationships are most predictive of users\u2019 attributes, and propose new strategies on how individuals can modulate their online social relationships and incoming communications to preserve their anonymity.","url":"https://www.semanticscholar.org/paper/5f827b963939c96968a03318b4c2b011e1871eaf","sentence":"Title: writer profiling without the writer\'s text Abstract: social network users may wish to preserve their anonymity online by masking their identity and not using language associated with any particular demographics or personality. however, they have no control over the language in incoming communications. we show that linguistic cues in public comments directed at a user are sufficient for an accurate inference of that user\u2019s gender, age, religion, diet, and even personality traits. moreover, we show that directed communication is even more predictive of a user\u2019s profile than the user\u2019s own language. we then conduct a nuanced analysis of what types of social relationships are most predictive of users\u2019 attributes, and propose new strategies on how individuals can modulate their online social relationships and incoming communications to preserve their anonymity.","Task":["writer profiling without the writer\'s text"],"Method":["writer profiling"]},{"title":"\u201cParticipant\u201d Perceptions of Twitter Research Ethics","abstract":"Social computing systems such as Twitter present new research sites that have provided billions of data points to researchers. However, the availability of public social media data has also presented ethical challenges. As the research community works to create ethical norms, we should be considering users\u2019 concerns as well. With this in mind, we report on an exploratory survey of Twitter users\u2019 perceptions of the use of tweets in research. Within our survey sample, few users were previously aware that their public tweets could be used by researchers, and the majority felt that researchers should not be able to use tweets without consent. However, we find that these attitudes are highly contextual, depending on factors such as how the research is conducted or disseminated, who is conducting it, and what the study is about. The findings of this study point to potential best practices for researchers conducting observation and analysis of public data.","url":"https://www.semanticscholar.org/paper/f298649194c1be9cb55574c047756ae7e8a62d6b","sentence":"Title: \u201cparticipant\u201d perceptions of twitter research ethics Abstract: social computing systems such as twitter present new research sites that have provided billions of data points to researchers. however, the availability of public social media data has also presented ethical challenges. as the research community works to create ethical norms, we should be considering users\u2019 concerns as well. with this in mind, we report on an exploratory survey of twitter users\u2019 perceptions of the use of tweets in research. within our survey sample, few users were previously aware that their public tweets could be used by researchers, and the majority felt that researchers should not be able to use tweets without consent. however, we find that these attitudes are highly contextual, depending on factors such as how the research is conducted or disseminated, who is conducting it, and what the study is about. the findings of this study point to potential best practices for researchers conducting observation and analysis of public data.","Task":["twitter, research ethics"],"Method":["twitter research ethics"]},{"title":"The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks","abstract":"This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users\' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. \\nIn experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google\'s Smart Compose, a commercial text-completion neural network trained on millions of users\' email messages.","url":"https://www.semanticscholar.org/paper/520ec00dc35475e0554dbb72f27bd2eeb6f4191d","sentence":"Title: the secret sharer: evaluating and testing unintended memorization in neural networks Abstract: this paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. because such models are sometimes trained on sensitive data (e.g., the text of users\' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. \\nin experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. we show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in google\'s smart compose, a commercial text-completion neural network trained on millions of users\' email messages.","Task":["unintended memorization, neural networks"],"Method":["unintended memorization in neural networks"]},{"title":"Garbage in, garbage out?: do machine learning application papers in social computing report where human-labeled training data comes from?","abstract":"Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper\'s authors labeling the data themselves. Such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. In this paper, we investigate to what extent a sample of machine learning application papers in social computing --- specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data --- give specific details about whether such best practices were followed. Our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: Does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. We find a wide divergence in whether such practices were followed and documented. Much of machine learning research and education focuses on what is done once a \\"gold standard\\" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.","url":"https://www.semanticscholar.org/paper/df2df1749b93ba86328ec7b86ff7e8d30029e3f5","sentence":"Title: garbage in, garbage out?: do machine learning application papers in social computing report where human-labeled training data comes from? Abstract: many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper\'s authors labeling the data themselves. such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. in this paper, we investigate to what extent a sample of machine learning application papers in social computing --- specifically papers from arxiv and traditional publications performing an ml classification task on twitter data --- give specific details about whether such best practices were followed. our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. we find a wide divergence in whether such practices were followed and documented. much of machine learning research and education focuses on what is done once a \\"gold standard\\" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.","Task":["machine learning, social computing, structured content analysis"],"Method":["structured content analysis of machine learning application papers in social computing"]},{"title":"How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation","abstract":"We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model\'s generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.","url":"https://www.semanticscholar.org/paper/129cbad01be98ee88a930e31898cb76be79c41c1","sentence":"Title: how not to evaluate your dialogue system: an empirical study of unsupervised evaluation metrics for dialogue response generation Abstract: we investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. recent works in response generation have adopted metrics from machine translation to compare a model\'s generated response to a single target response. we show that these metrics correlate very weakly with human judgements in the non-technical twitter domain, and not at all in the technical ubuntu domain. we provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.","Task":["evaluation metrics"],"Method":["evaluation metrics for dialogue response generation"]},{"title":"Fifty years later: the significance of the Nuremberg Code.","abstract":"The Nuremberg Code 1. The voluntary consent of the human subject is absolutely essential. This means that the person involved should have legal capacity to give consent; should be so situated as to be able to exercise free power of choice, without the intervention of any element of force, fraud, deceit, duress, overreaching, or other ulterior form of constraint or coercion; and should have sufficient knowledge and comprehension of the elements of the subject matter involved as to enable him to make an understanding and enlightened decision. This latter element requires that before the acceptance of an affirmative decision by .\xa0.\xa0.","url":"https://www.semanticscholar.org/paper/dff6976f237ecff091547dd2df26937bd6b59198","sentence":"Title: fifty years later: the significance of the nuremberg code. Abstract: the nuremberg code 1. the voluntary consent of the human subject is absolutely essential. this means that the person involved should have legal capacity to give consent; should be so situated as to be able to exercise free power of choice, without the intervention of any element of force, fraud, deceit, duress, overreaching, or other ulterior form of constraint or coercion; and should have sufficient knowledge and comprehension of the elements of the subject matter involved as to enable him to make an understanding and enlightened decision. this latter element requires that before the acceptance of an affirmative decision by .\xa0.\xa0.","Task":["nuremberg code"],"Method":["the Nuremberg Code"]},{"title":"Semantics derived automatically from language corpora necessarily contain human biases","abstract":"Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.","url":"https://www.semanticscholar.org/paper/5966d7c7f60898d610812e24c64d4d57855ad86a","sentence":"Title: semantics derived automatically from language corpora necessarily contain human biases Abstract: artificial intelligence and machine learning are in a period of astounding growth. however, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. we replicate a spectrum of standard human biases as exposed by the implicit association test and other well-known psychological studies. we replicate these using a widely used, purely statistical machine-learning model---namely, the glove word embedding---trained on a corpus of text from the web. our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. these regularities are captured by machine learning along with the rest of semantics. in addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the word embedding association test (weat) and the word embedding factual association test (wefat). our results have implications not only for ai and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.","Task":["machine learning"],"Method":["biases in machine learning"]},{"title":"Algorithms of Oppression: How Search Engines Reinforce Racism","abstract":"Read and considered thoughtfully, Safiya Umoja Noble\u2019s Algorithms of Oppression: How Search Engines Reinforce Racism is devastating. It reduces to rubble the notion that technology is neutral and ideology-free. Noble\u2019s crushing the neutrality myth does several things. First, this act lays foundations for her argument: only if you recognize and understand that technology is built with, and integrates, bias, can you then be open to her primary thesis: search engines advance discriminatory and often racist content. Second, it banishes a convenient response for many self-identified meritocratic Silicon Valley \u201cwinners\u201d and their supporters. Postreading, some individuals may retain their beliefs in a neutral and ideology-free technology in spite of the overwhelming evidence and citations Noble brings to bear. Effective countering of Noble\u2019s claims is unlikely to occur. For professionals working in technology, information, argumentation, and/or rhetorical studies, Algorithms of Oppression is refreshing. Agonistic towards structural racism and its defenses, single-minded in its evidentiary presentation, collaborative in its acknowledgement of others\u2019 scholarship and research, Noble models many academic, critical, and social moves. Technology scholars and writers will find in Algorithms of Oppression a masterful mentor text on how to be an activist researcher scholar. Noble also makes this enjoyable reading. It is uncommon to find academic books that can simultaneously be read, used, and applied by academics and non-academics alike.","url":"https://www.semanticscholar.org/paper/1935a5e3937753dc7db90126a221f11009c17984","sentence":"Title: algorithms of oppression: how search engines reinforce racism Abstract: read and considered thoughtfully, safiya umoja noble\u2019s algorithms of oppression: how search engines reinforce racism is devastating. it reduces to rubble the notion that technology is neutral and ideology-free. noble\u2019s crushing the neutrality myth does several things. first, this act lays foundations for her argument: only if you recognize and understand that technology is built with, and integrates, bias, can you then be open to her primary thesis: search engines advance discriminatory and often racist content. second, it banishes a convenient response for many self-identified meritocratic silicon valley \u201cwinners\u201d and their supporters. postreading, some individuals may retain their beliefs in a neutral and ideology-free technology in spite of the overwhelming evidence and citations noble brings to bear. effective countering of noble\u2019s claims is unlikely to occur. for professionals working in technology, information, argumentation, and/or rhetorical studies, algorithms of oppression is refreshing. agonistic towards structural racism and its defenses, single-minded in its evidentiary presentation, collaborative in its acknowledgement of others\u2019 scholarship and research, noble models many academic, critical, and social moves. technology scholars and writers will find in algorithms of oppression a masterful mentor text on how to be an activist researcher scholar. noble also makes this enjoyable reading. it is uncommon to find academic books that can simultaneously be read, used, and applied by academics and non-academics alike.","Task":["algorithms of oppression: how search engines reinforce racism"],"Method":["algorithms of oppression"]},{"title":"The trouble with using provider assessments for rating clinical performance: it\'s a matter of bias.","abstract":"The International Association for the Study of Pain has referred to pain as the fifth vital sign, and acute pain management after surgery has been shown to be a key factor in quality of recovery. In addition, the establishment of pain management benchmarks by the Joint Commission on the Accreditation of Healthcare Organizations more than a decade ago has resulted in a greater awareness of patients\u2019 right to optimal pain control by health care practitioners and administrators. Postoperative pain control has become a priority for hospitals across the United States. Optimization of postoperative pain management has been demonstrated through the implementation of protocols that include multimodal analgesic regimens, surgicalspecific treatment pathways, implementation of a 24-hour anesthesiology pain service, and pain-specific training for physicians and nurses involved in postoperative care.1 Importantly, pain as assessed by the numeric rating scale (NRS), for which 0 = no pain and 10 = maximal pain, has been shown to be significantly reduced after the implementation of postoperative analgesia protocol. These data suggest that NRS pain scores might be a useful metric to evaluate the quality of anesthesia care provided after the transition from surgery to recovery. Wanderer et al.2 from the Vanderbilt University have applied this principle in a research report in the current edition of Anesthesia & Analgesia in an attempt to use rank ordering by initial postanesthesia recovery unit NRS pain scores, as collected by nurses in a clinical setting, to compare supervising anesthesiologists when adjusted for confounding factors. The analysis included 26 680 cases using electronic documentation and excluded physician and nurse providers who had not cared for \u2265100 patients. When admission postanesthesia recovery unit scores were compared among anesthesiologists after adjusting for patient and surgical factors, only 6.4% of the 69 supervising anesthesiologists were found to differ in median postanesthesia recovery room admission pain scores. This finding clearly demonstrates that as presently assessed, initial postanesthesia recovery scores are a poor metric for identifying differences among supervising anesthesiologists, and the authors correctly concluded that they should not be included in the assessment of anesthesia care performance. Interestingly, the study also found that 16 of 66 (24%) recovery room nurses elicited median pain scores less than the median value for the entire group, and 33 nurses (50%) had elicited median pain scores significantly higher than the median for the entire group. These differences translated into a range of odds ratios from 0.16 (95% confidence interval, 0.11\u20132.4) for the lowest to 2.95 (95% confidence interval, 2.43\u20133.59) for the highest nurse compared with the nurse who ranked the median value for the overall group. In fact, NRS pain assessments using the 0 to 10 NRS pain score were found to depend more on the nurse making the assessment than patient age, gender and race, preoperative use of opioids, American Society of Anesthesiologists physical status, or procedure. This finding should not be interpreted to suggest dishonest recordings of NRS values by nurses (86%\u201390% of nurses accurately record pain ratings provided by patients), but that personal opinions, knowledge, and attitudes toward pain strongly influence assessments and management.3 Wanderer et al. discuss the use of nonstandard item descriptors rather than \u201cno pain\u201d equals 0 and \u201cthe worst possible pain\u201d equals 10 as anchors for the NRS pain scales as a possible explanation for the variability in the nurse assessor\u2013recorded NRS. They cited a recent systematic review that identified 24 distinct anchoring phrases in 54 included studies.4 The use of anchors during decision making can create a form of cognitive bias because individuals tend to rely heavily on these points when making decision adjustments. Factors that have been shown to affect the influence of anchoring on decision making include happier moods and conscientious personalities, certainly desirable attributes in postanesthesia care nurses. These are factors that patients are likely to perceive, and substituting anchors could clearly influence the perceived value reported by patients.5 The method of presentation of the NRS score range by the evaluator can be used to influence the choice made by the decision maker. This method is called the framing effect and is another type of cognitive bias.6 The presenter in this situation is referred to as the choice architect. This practice is not an uncommon phenomenon when using Likert scales because the differences between scores in the range are not The Trouble with Using Provider Assessments for Rating Clinical Performance: It\u2019s a Matter of Bias","url":"https://www.semanticscholar.org/paper/31a848022de5933029435a2c8304c2bd12537b0d","sentence":"Title: the trouble with using provider assessments for rating clinical performance: it\'s a matter of bias. Abstract: the international association for the study of pain has referred to pain as the fifth vital sign, and acute pain management after surgery has been shown to be a key factor in quality of recovery. in addition, the establishment of pain management benchmarks by the joint commission on the accreditation of healthcare organizations more than a decade ago has resulted in a greater awareness of patients\u2019 right to optimal pain control by health care practitioners and administrators. postoperative pain control has become a priority for hospitals across the united states. optimization of postoperative pain management has been demonstrated through the implementation of protocols that include multimodal analgesic regimens, surgicalspecific treatment pathways, implementation of a 24-hour anesthesiology pain service, and pain-specific training for physicians and nurses involved in postoperative care.1 importantly, pain as assessed by the numeric rating scale (nrs), for which 0 = no pain and 10 = maximal pain, has been shown to be significantly reduced after the implementation of postoperative analgesia protocol. these data suggest that nrs pain scores might be a useful metric to evaluate the quality of anesthesia care provided after the transition from surgery to recovery. wanderer et al.2 from the vanderbilt university have applied this principle in a research report in the current edition of anesthesia & analgesia in an attempt to use rank ordering by initial postanesthesia recovery unit nrs pain scores, as collected by nurses in a clinical setting, to compare supervising anesthesiologists when adjusted for confounding factors. the analysis included 26 680 cases using electronic documentation and excluded physician and nurse providers who had not cared for \u2265100 patients. when admission postanesthesia recovery unit scores were compared among anesthesiologists after adjusting for patient and surgical factors, only 6.4% of the 69 supervising anesthesiologists were found to differ in median postanesthesia recovery room admission pain scores. this finding clearly demonstrates that as presently assessed, initial postanesthesia recovery scores are a poor metric for identifying differences among supervising anesthesiologists, and the authors correctly concluded that they should not be included in the assessment of anesthesia care performance. interestingly, the study also found that 16 of 66 (24%) recovery room nurses elicited median pain scores less than the median value for the entire group, and 33 nurses (50%) had elicited median pain scores significantly higher than the median for the entire group. these differences translated into a range of odds ratios from 0.16 (95% confidence interval, 0.11\u20132.4) for the lowest to 2.95 (95% confidence interval, 2.43\u20133.59) for the highest nurse compared with the nurse who ranked the median value for the overall group. in fact, nrs pain assessments using the 0 to 10 nrs pain score were found to depend more on the nurse making the assessment than patient age, gender and race, preoperative use of opioids, american society of anesthesiologists physical status, or procedure. this finding should not be interpreted to suggest dishonest recordings of nrs values by nurses (86%\u201390% of nurses accurately record pain ratings provided by patients), but that personal opinions, knowledge, and attitudes toward pain strongly influence assessments and management.3 wanderer et al. discuss the use of nonstandard item descriptors rather than \u201cno pain\u201d equals 0 and \u201cthe worst possible pain\u201d equals 10 as anchors for the nrs pain scales as a possible explanation for the variability in the nurse assessor\u2013recorded nrs. they cited a recent systematic review that identified 24 distinct anchoring phrases in 54 included studies.4 the use of anchors during decision making can create a form of cognitive bias because individuals tend to rely heavily on these points when making decision adjustments. factors that have been shown to affect the influence of anchoring on decision making include happier moods and conscientious personalities, certainly desirable attributes in postanesthesia care nurses. these are factors that patients are likely to perceive, and substituting anchors could clearly influence the perceived value reported by patients.5 the method of presentation of the nrs score range by the evaluator can be used to influence the choice made by the decision maker. this method is called the framing effect and is another type of cognitive bias.6 the presenter in this situation is referred to as the choice architect. this practice is not an uncommon phenomenon when using likert scales because the differences between scores in the range are not the trouble with using provider assessments for rating clinical performance: it\u2019s a matter of bias","Task":["nrs"],"Method":["provider assessments for rating clinical performance: it\'s a matter of bias"]},{"title":"Comparing Spoken Dialog Corpora Collected with Recruited Subjects versus Real Users","abstract":"Empirical spoken dialog research often involves the collection and analysis of a dialog corpus. However, it is not well understood whether and how a corpus of dialogs collected using recruited subjects differs from a corpus of dialogs obtained from real users. In this paper we use Let\u2019s Go Lab, a platform for experimenting with a deployed spoken dialog bus information system, to address this question. Our first corpus is collected by recruiting subjects to call Let\u2019s Go in a standard laboratory setting, while our second corpus consists of calls from real users calling Let\u2019s Go during its operating hours. We quantitatively characterize the two collected corpora using previously proposed measures from the spoken dialog literature, then discuss the statistically significant similarities and differences between the two corpora with respect to these measures. For example, we find that recruited subjects talk more and speak faster, while real users ask for more help and more frequently interrupt the system. In contrast, we find no difference with respect to dialog structure.","url":"https://www.semanticscholar.org/paper/d3aca13c966bb22eed7086baeb287a64bc18c152","sentence":"Title: comparing spoken dialog corpora collected with recruited subjects versus real users Abstract: empirical spoken dialog research often involves the collection and analysis of a dialog corpus. however, it is not well understood whether and how a corpus of dialogs collected using recruited subjects differs from a corpus of dialogs obtained from real users. in this paper we use let\u2019s go lab, a platform for experimenting with a deployed spoken dialog bus information system, to address this question. our first corpus is collected by recruiting subjects to call let\u2019s go in a standard laboratory setting, while our second corpus consists of calls from real users calling let\u2019s go during its operating hours. we quantitatively characterize the two collected corpora using previously proposed measures from the spoken dialog literature, then discuss the statistically significant similarities and differences between the two corpora with respect to these measures. for example, we find that recruited subjects talk more and speak faster, while real users ask for more help and more frequently interrupt the system. in contrast, we find no difference with respect to dialog structure.","Task":["spoken dialog corpora"],"Method":["empirical spoken dialog research"]},{"title":"Privacy-preserving Neural Representations of Text","abstract":"This article deals with adversarial attacks towards deep learning systems for Natural Language Processing (NLP), in the context of privacy protection. We study a specific type of attack: an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text. Such scenario may arise in situations when the computation of a neural network is shared across multiple devices, e.g. some hidden representation is computed by a user\u2019s device and sent to a cloud-based model. We measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations. Finally, we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations.","url":"https://www.semanticscholar.org/paper/e8fa186444d98a39ee9139b1f5dd0c7618caef8f","sentence":"Title: privacy-preserving neural representations of text Abstract: this article deals with adversarial attacks towards deep learning systems for natural language processing (nlp), in the context of privacy protection. we study a specific type of attack: an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text. such scenario may arise in situations when the computation of a neural network is shared across multiple devices, e.g. some hidden representation is computed by a user\u2019s device and sent to a cloud-based model. we measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations. finally, we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations.","Task":["deep learning"],"Method":["privacy-preserving neural representations of text"]},{"title":"Ethical Challenges in Data-Driven Dialogue Systems","abstract":"The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.","url":"https://www.semanticscholar.org/paper/a24d72bd0d08d515cb3e26f94131d33ad6c861db","sentence":"Title: ethical challenges in data-driven dialogue systems Abstract: the use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. a growing number of dialogue systems use conversation strategies that are learned from large datasets. there are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. we also suggest areas stemming from these issues that deserve further investigation. through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.","Task":["ethical challenges in data-driven dialogue systems"],"Method":["ethical challenges in data-driven dialogue systems"]},{"title":"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings","abstract":"The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.","url":"https://www.semanticscholar.org/paper/ccf6a69a7f33bcf052aa7def176d3b9de495beb7","sentence":"Title: man is to computer programmer as woman is to homemaker? debiasing word embeddings Abstract: the blind application of machine learning runs the risk of amplifying biases present in data. such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. we show that even word embeddings trained on google news articles exhibit female/male gender stereotypes to a disturbing extent. this raises concerns because their widespread use, as we describe, often tends to amplify these biases. geometrically, gender bias is first shown to be captured by a direction in the word embedding. second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. the resulting embeddings can be used in applications without amplifying gender bias.","Task":["debiasing word embeddings"],"Method":["debiasing word embeddings"]},{"title":"Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them","abstract":"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between \u201cgender-neutralized\u201d words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","url":"https://www.semanticscholar.org/paper/94cf3f2c4410fcb06a90abebd99f7113c69e1ed9","sentence":"Title: lipstick on a pig: debiasing methods cover up systematic gender biases in word embeddings but do not remove them Abstract: word embeddings are widely used in nlp for a vast range of tasks. it was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. however, we argue that this removal is superficial. while the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. the gender bias information is still reflected in the distances between \u201cgender-neutralized\u201d words in the debiased embeddings, and can be recovered from them. we present a series of experiments to support this claim, for two debiasing methods. we conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Task":["gender biases in word embeddings"],"Method":["gender bias in word embeddings"]},{"title":"On Measuring Social Biases in Sentence Encoders","abstract":"The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test\u2019s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.","url":"https://www.semanticscholar.org/paper/5e9c85235210b59a16bdd84b444a904ae271f7e7","sentence":"Title: on measuring social biases in sentence encoders Abstract: the word embedding association test shows that glove and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (caliskan et al., 2017). meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. accordingly, we extend the word embedding association test to measure bias in sentence encoders. we then test several sentence encoders, including state-of-the-art methods such as elmo and bert, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. we observe mixed results including suspicious patterns of sensitivity that suggest the test\u2019s assumptions may not hold in general. we conclude by proposing directions for future work on measuring bias in sentence encoders.","Task":["social biases, sentence encoders"],"Method":["measuring social biases in sentence encoders"]},{"title":"Assessing Social and Intersectional Biases in Contextualized Word Representations","abstract":"Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.","url":"https://www.semanticscholar.org/paper/039b1c1210c437f3b3ce6e0275ee2137bf5b951c","sentence":"Title: assessing social and intersectional biases in contextualized word representations Abstract: social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. in natural language processing, gender bias has been shown to exist in context-free word embeddings. recently, contextual word representations have outperformed word embeddings in several downstream nlp tasks. these word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. in this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as bert and gpt-2, encode biases with respect to gender, race, and intersectional identities. towards this, we propose assessing bias at the contextual word level. this novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. we demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.","Task":["assessing social and intersectional biases in contextualized word representations"],"Method":["social bias in machine learning"]},{"title":"Quantifying Social Biases in Contextual Word Representations","abstract":"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","url":"https://www.semanticscholar.org/paper/3259d52ae00e65b98391e7e6a2f672dfee721bf8","sentence":"Title: quantifying social biases in contextual word representations Abstract: contextual word embeddings such as bert have achieved state of the art performance in numerous nlp tasks. since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. in this study, we (1) propose a template-based method to quantify bias in bert; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of gender pronoun resolution. although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","Task":["contextual word embeddings"],"Method":["contextual word representations"]},{"title":"Sorting Things Out: Classification and Its Consequences","abstract":"What do a seventeenth-century mortality table (whose causes of death include \\"fainted in a bath,\\" \\"frighted,\\" and \\"itch\\"); the identification of South Africans during apartheid as European, Asian, colored, or black; and the separation of machine- from hand-washables have in common? All are examples of classification -- the scaffolding of information infrastructures. In Sorting Things Out, Geoffrey C. Bowker and Susan Leigh Star explore the role of categories and standards in shaping the modern world. In a clear and lively style, they investigate a variety of classification systems, including the International Classification of Diseases, the Nursing Interventions Classification, race classification under apartheid in South Africa, and the classification of viruses and of tuberculosis. The authors emphasize the role of invisibility in the process by which classification orders human interaction. They examine how categories are made and kept invisible, and how people can change this invisibility when necessary. They also explore systems of classification as part of the built information environment. Much as an urban historian would review highway permits and zoning decisions to tell a city\'s story, the authors review archives of classification design to understand how decisions have been made. Sorting Things Out has a moral agenda, for each standard and category valorizes some point of view and silences another. Standards and classifications produce advantage or suffering. Jobs are made and lost; some regions benefit at the expense of others. How these choices are made and how we think about that process are at the moral and political core of this work. The book is an important empirical source for understanding the building of information infrastructures.","url":"https://www.semanticscholar.org/paper/d08392eee17f809d32d7d37e9345383f41271164","sentence":"Title: sorting things out: classification and its consequences Abstract: what do a seventeenth-century mortality table (whose causes of death include \\"fainted in a bath,\\" \\"frighted,\\" and \\"itch\\"); the identification of south africans during apartheid as european, asian, colored, or black; and the separation of machine- from hand-washables have in common? all are examples of classification -- the scaffolding of information infrastructures. in sorting things out, geoffrey c. bowker and susan leigh star explore the role of categories and standards in shaping the modern world. in a clear and lively style, they investigate a variety of classification systems, including the international classification of diseases, the nursing interventions classification, race classification under apartheid in south africa, and the classification of viruses and of tuberculosis. the authors emphasize the role of invisibility in the process by which classification orders human interaction. they examine how categories are made and kept invisible, and how people can change this invisibility when necessary. they also explore systems of classification as part of the built information environment. much as an urban historian would review highway permits and zoning decisions to tell a city\'s story, the authors review archives of classification design to understand how decisions have been made. sorting things out has a moral agenda, for each standard and category valorizes some point of view and silences another. standards and classifications produce advantage or suffering. jobs are made and lost; some regions benefit at the expense of others. how these choices are made and how we think about that process are at the moral and political core of this work. the book is an important empirical source for understanding the building of information infrastructures.","Task":["classification, natural language processing"],"Method":["classification"]},{"title":"Language (Technology) is Power: A Critical Survey of \u201cBias\u201d in NLP","abstract":"We survey 146 papers analyzing \u201cbias\u201d in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \u201cbias\u201d is an inherently normative process. We further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.","url":"https://www.semanticscholar.org/paper/d47a682723f710395454687319bb55635e653105","sentence":"Title: language (technology) is power: a critical survey of \u201cbias\u201d in nlp Abstract: we survey 146 papers analyzing \u201cbias\u201d in nlp systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \u201cbias\u201d is an inherently normative process. we further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of nlp. based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in nlp systems. these recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by nlp systems, while interrogating and reimagining the power relations between technologists and such communities.","Task":["language (technology) is power"],"Method":["language (technology) is power"]},{"title":"Mitigating Gender Bias in Natural Language Processing: Literature Review","abstract":"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.","url":"https://www.semanticscholar.org/paper/493fac37cea49afb98c52c2f5dd75c303a325b25","sentence":"Title: mitigating gender bias in natural language processing: literature review Abstract: as natural language processing (nlp) and machine learning (ml) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. although nlp models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. while the study of bias in artificial intelligence is not new, methods to mitigate gender bias in nlp are relatively nascent. in this paper, we review contemporary studies on recognizing and mitigating gender bias in nlp. we discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. finally, we discuss future studies for recognizing and mitigating gender bias in nlp.","Task":["gender bias"],"Method":["gender bias in natural language processing"]},{"title":"Gender Bias in Coreference Resolution","abstract":"We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these \u201cWinogender schemas,\u201d we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.","url":"https://www.semanticscholar.org/paper/9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7","sentence":"Title: gender bias in coreference resolution Abstract: we present an empirical study of gender bias in coreference resolution systems. we first introduce a novel, winograd schema-style set of minimal pair sentences that differ only by pronoun gender. with these \u201cwinogender schemas,\u201d we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.","Task":["gender bias in coreference resolution systems"],"Method":["gender bias in coreference resolution systems"]},{"title":"Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods","abstract":"In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.","url":"https://www.semanticscholar.org/paper/0be19fd9896e5d40222c690cc3ff553adc7c0e27","sentence":"Title: gender bias in coreference resolution: evaluation and debiasing methods Abstract: in this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, winobias. our corpus contains winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). we demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in f1 score. finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in winobias without significantly affecting their performance on existing datasets.","Task":["gender bias"],"Method":["gender bias in coreference resolution"]},{"title":"Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems","abstract":"Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 \u2018Affect in Tweets\u2019. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.","url":"https://www.semanticscholar.org/paper/5d4af8c9321168f9ba7a501f33fb019fa2deaa22","sentence":"Title: examining gender and race bias in two hundred sentiment analysis systems Abstract: automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. past work on examining inappropriate biases has largely focused on just individual systems. further, there is no benchmark dataset for examining inappropriate biases in systems. here for the first time, we present the equity evaluation corpus (eec), which consists of 8,640 english sentences carefully chosen to tease out biases towards certain races and genders. we use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, semeval-2018 task 1 \u2018affect in tweets\u2019. we find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. we make the eec freely available.","Task":["automatic sentiment analysis"],"Method":["automatic sentiment analysis systems"]},{"title":"Women\u2019s Syntactic Resilience and Men\u2019s Grammatical Luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing","abstract":"Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. To address this, we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles\u2019 authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. Further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. The results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. We release our data to the research community.","url":"https://www.semanticscholar.org/paper/e85a50b523915b5fba3e3f1fdb743650f7d21bed","sentence":"Title: women\u2019s syntactic resilience and men\u2019s grammatical luck: gender-bias in part-of-speech tagging and dependency parsing Abstract: several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. to address this, we annotate the wall street journal part of the penn treebank with the gender information of the articles\u2019 authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. the results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. we release our data to the research community.","Task":["part-of-speech tagging, dependency parsing, gender bias"],"Method":["gender bias in linguistics"]},{"title":"Assessing gender bias in machine translation: a case study with Google Translate","abstract":"Recently there has been a growing concern in academia, industrial research laboratories and the mainstream commercial media about the phenomenon dubbed as machine bias , where trained statistical models\u2014unbeknownst to their creators\u2014grow to reflect controversial societal asymmetries, such as gender or racial bias. A significant number of Artificial Intelligence tools have recently been suggested to be harmfully biased toward some minority, with reports of racist criminal behavior predictors, Apple\u2019s Iphone X failing to differentiate between two distinct Asian people and the now infamous case of Google photos\u2019 mistakenly classifying black people as gorillas. Although a systematic study of such biases can be difficult, we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in AI. In this paper, we start with a comprehensive list of job positions from the U.S. Bureau of Labor Statistics (BLS) and used it in order to build sentences in constructions like \u201cHe/She is an Engineer\u201d (where \u201cEngineer\u201d is replaced by the job position of interest) in 12 different gender neutral languages such as Hungarian, Chinese, Yoruba, and several others. We translate these sentences into English using the Google Translate API, and collect statistics about the frequency of female, male and gender neutral pronouns in the translated output. We then show that Google Translate exhibits a strong tendency toward male defaults, in particular for fields typically associated to unbalanced gender distribution or stereotypes such as STEM (Science, Technology, Engineering and Mathematics) jobs. We ran these statistics against BLS\u2019 data for the frequency of female participation in each job position, in which we show that Google Translate fails to reproduce a real-world distribution of female workers. In summary, we provide experimental evidence that even if one does not expect in principle a 50:50 pronominal gender distribution, Google Translate yields male defaults much more frequently than what would be expected from demographic data alone. We believe that our study can shed further light on the phenomenon of machine bias and are hopeful that it will ignite a debate about the need to augment current statistical translation tools with debiasing techniques\u2014which can already be found in the scientific literature.","url":"https://www.semanticscholar.org/paper/c590d2c8c2fb6ce5d32ee9165ab24171165f2b70","sentence":"Title: assessing gender bias in machine translation: a case study with google translate Abstract: recently there has been a growing concern in academia, industrial research laboratories and the mainstream commercial media about the phenomenon dubbed as machine bias , where trained statistical models\u2014unbeknownst to their creators\u2014grow to reflect controversial societal asymmetries, such as gender or racial bias. a significant number of artificial intelligence tools have recently been suggested to be harmfully biased toward some minority, with reports of racist criminal behavior predictors, apple\u2019s iphone x failing to differentiate between two distinct asian people and the now infamous case of google photos\u2019 mistakenly classifying black people as gorillas. although a systematic study of such biases can be difficult, we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in ai. in this paper, we start with a comprehensive list of job positions from the u.s. bureau of labor statistics (bls) and used it in order to build sentences in constructions like \u201che/she is an engineer\u201d (where \u201cengineer\u201d is replaced by the job position of interest) in 12 different gender neutral languages such as hungarian, chinese, yoruba, and several others. we translate these sentences into english using the google translate api, and collect statistics about the frequency of female, male and gender neutral pronouns in the translated output. we then show that google translate exhibits a strong tendency toward male defaults, in particular for fields typically associated to unbalanced gender distribution or stereotypes such as stem (science, technology, engineering and mathematics) jobs. we ran these statistics against bls\u2019 data for the frequency of female participation in each job position, in which we show that google translate fails to reproduce a real-world distribution of female workers. in summary, we provide experimental evidence that even if one does not expect in principle a 50:50 pronominal gender distribution, google translate yields male defaults much more frequently than what would be expected from demographic data alone. we believe that our study can shed further light on the phenomenon of machine bias and are hopeful that it will ignite a debate about the need to augment current statistical translation tools with debiasing techniques\u2014which can already be found in the scientific literature.","Task":["machine translation, gender bias"],"Method":["gender bias in machine translation"]},{"title":"Automatically Identifying Gender Issues in Machine Translation using Perturbations","abstract":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. While previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. We use our method to compile an evaluation benchmark spanning examples for four languages from three language families, which we publicly release to facilitate research. The examples in our benchmark expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.","url":"https://www.semanticscholar.org/paper/008e9001ea78e9654b5c43aeb818ea6cb06ea934","sentence":"Title: automatically identifying gender issues in machine translation using perturbations Abstract: the successful application of neural methods to machine translation has realized huge quality advances for the community. with these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. while previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. we use our method to compile an evaluation benchmark spanning examples for four languages from three language families, which we publicly release to facilitate research. the examples in our benchmark expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.","Task":["gendered language"],"Method":["gender issues in machine translation"]},{"title":"Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem","abstract":"Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a \u2018balanced\u2019 dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is \u2018catastrophic forgetting\u2019, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.","url":"https://www.semanticscholar.org/paper/00059087c954c1af6ece33115315e3e0ecc2f2c2","sentence":"Title: reducing gender bias in neural machine translation as a domain adaptation problem Abstract: training data for nlp tasks often exhibits gender bias in that fewer sentences refer to women than to men. in neural machine translation (nmt) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. the recent winomt challenge set allows us to measure this effect directly (stanovsky et al, 2019) ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. rather than attempt to create a \u2018balanced\u2019 dataset, we use transfer learning on a small set of trusted, gender-balanced examples. this approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. a known pitfall of transfer learning on new domains is \u2018catastrophic forgetting\u2019, which we address at adaptation and inference time. during adaptation we show that elastic weight consolidation allows a performance trade-off between general translation quality and bias reduction. at inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in stanovsky et al, 2019 on winomt with no degradation of general test set bleu. we demonstrate our approach translating from english into three languages with varied linguistic properties and data availability.","Task":["gender bias, neural machine translation"],"Method":["gender bias in neural machine translation"]},{"title":"Toward Gender-Inclusive Coreference Resolution","abstract":"Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.","url":"https://www.semanticscholar.org/paper/40a6e8d8f253882c585f163b7333842d60ed6f14","sentence":"Title: toward gender-inclusive coreference resolution Abstract: correctly resolving textual mentions of people fundamentally entails making inferences about those people. such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. to better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. through these studies, conducted on english text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.","Task":["coreference resolution, gender"],"Method":["coreference resolution"]},{"title":"Man is to Person as Woman is to Location: Measuring Gender Bias in Named Entity Recognition","abstract":"In this paper, we study the bias in named entity recognition (NER) models---specifically, the difference in the ability to recognize male and female names as PERSON entity types. We evaluate NER models on a dataset containing 139 years of U.S. census baby names and find that relatively more female names, as opposed to male names, are not recognized as PERSON entities. The result of this analysis yields a new benchmark for gender bias evaluation in named entity recognition systems. The data and code for the application of this benchmark is publicly available for researchers to use.","url":"https://www.semanticscholar.org/paper/7b8318894cbeca32f1ae55780a0903445a3f4ac6","sentence":"Title: man is to person as woman is to location: measuring gender bias in named entity recognition Abstract: in this paper, we study the bias in named entity recognition (ner) models---specifically, the difference in the ability to recognize male and female names as person entity types. we evaluate ner models on a dataset containing 139 years of u.s. census baby names and find that relatively more female names, as opposed to male names, are not recognized as person entities. the result of this analysis yields a new benchmark for gender bias evaluation in named entity recognition systems. the data and code for the application of this benchmark is publicly available for researchers to use.","Task":["named entity recognition"],"Method":["named entity recognition"]},{"title":"Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns","abstract":"Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun\u2013name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9% F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.","url":"https://www.semanticscholar.org/paper/57032c1e327c88a53ab41c17e91bf1406f9ef5c9","sentence":"Title: mind the gap: a balanced corpus of gendered ambiguous pronouns Abstract: coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. furthermore, we find gender bias in existing corpora and systems favoring masculine entities. to address this, we present and release gap, a gender-balanced labeled corpus of 8,908 ambiguous pronoun\u2013name pairs sampled to provide diverse coverage of challenges posed by real-world text. we explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9% f1. we show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.","Task":["ambiguous pronouns"],"Method":["ambiguous pronouns"]},{"title":"Measuring Bias in Contextualized Word Representations","abstract":"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","url":"https://www.semanticscholar.org/paper/a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd","sentence":"Title: measuring bias in contextualized word representations Abstract: contextual word embeddings such as bert have achieved state of the art performance in numerous nlp tasks. since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. in this study, we (1) propose a template-based method to quantify bias in bert; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of gender pronoun resolution. although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","Task":["measuring bias in contextualized word representations"],"Method":["measuring bias in contextualized word representations"]},{"title":"Mitigating Gender Bias Amplification in Distribution by Posterior Regularization","abstract":"Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models\u2019 top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.","url":"https://www.semanticscholar.org/paper/0f4bcebc95548a7286106b67bf1115802f093469","sentence":"Title: mitigating gender bias amplification in distribution by posterior regularization Abstract: advanced machine learning techniques have boosted the performance of natural language processing. nevertheless, recent studies, e.g., (citation) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. however, their analysis is conducted only on models\u2019 top predictions. in this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. we further propose a bias mitigation approach based on posterior regularization. with little performance loss, our method can almost remove the bias amplification in the distribution. our study sheds the light on understanding the bias amplification.","Task":["gender bias amplification"],"Method":["gender bias amplification in distribution"]},{"title":"Social Bias in Elicited Natural Language Inferences","abstract":"We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The SNLI human-elicitation protocol makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.","url":"https://www.semanticscholar.org/paper/a20ecabd83e0962329448d8af5025b8061c4ba36","sentence":"Title: social bias in elicited natural language inferences Abstract: we analyze the stanford natural language inference (snli) corpus in an investigation of bias and stereotyping in nlp data. the snli human-elicitation protocol makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.","Task":["social bias"],"Method":["social bias in natural language processing"]},{"title":"Racial disparities in automated speech recognition","abstract":"Significance Automated speech recognition (ASR) systems are now used in a variety of applications to convert spoken language to text, from virtual assistants, to closed captioning, to hands-free computing. By analyzing a large corpus of sociolinguistic interviews with white and African American speakers, we demonstrate large racial disparities in the performance of five popular commercial ASR systems. Our results point to hurdles faced by African Americans in using increasingly widespread tools driven by speech recognition technology. More generally, our work illustrates the need to audit emerging machine-learning systems to ensure they are broadly inclusive. Automated speech recognition (ASR) systems, which use sophisticated machine-learning algorithms to convert spoken language to text, have become increasingly widespread, powering popular virtual assistants, facilitating automated closed captioning, and enabling digital dictation platforms for health care. Over the last several years, the quality of these systems has dramatically improved, due both to advances in deep learning and to the collection of large-scale datasets used to train the systems. There is concern, however, that these tools do not work equally well for all subgroups of the population. Here, we examine the ability of five state-of-the-art ASR systems\u2014developed by Amazon, Apple, Google, IBM, and Microsoft\u2014to transcribe structured interviews conducted with 42 white speakers and 73 black speakers. In total, this corpus spans five US cities and consists of 19.8 h of audio matched on the age and gender of the speaker. We found that all five ASR systems exhibited substantial racial disparities, with an average word error rate (WER) of 0.35 for black speakers compared with 0.19 for white speakers. We trace these disparities to the underlying acoustic models used by the ASR systems as the race gap was equally large on a subset of identical phrases spoken by black and white individuals in our corpus. We conclude by proposing strategies\u2014such as using more diverse training datasets that include African American Vernacular English\u2014to reduce these performance differences and ensure speech recognition technology is inclusive.","url":"https://www.semanticscholar.org/paper/219b7266ae848937da170c5510b2bfc66d17859a","sentence":"Title: racial disparities in automated speech recognition Abstract: significance automated speech recognition (asr) systems are now used in a variety of applications to convert spoken language to text, from virtual assistants, to closed captioning, to hands-free computing. by analyzing a large corpus of sociolinguistic interviews with white and african american speakers, we demonstrate large racial disparities in the performance of five popular commercial asr systems. our results point to hurdles faced by african americans in using increasingly widespread tools driven by speech recognition technology. more generally, our work illustrates the need to audit emerging machine-learning systems to ensure they are broadly inclusive. automated speech recognition (asr) systems, which use sophisticated machine-learning algorithms to convert spoken language to text, have become increasingly widespread, powering popular virtual assistants, facilitating automated closed captioning, and enabling digital dictation platforms for health care. over the last several years, the quality of these systems has dramatically improved, due both to advances in deep learning and to the collection of large-scale datasets used to train the systems. there is concern, however, that these tools do not work equally well for all subgroups of the population. here, we examine the ability of five state-of-the-art asr systems\u2014developed by amazon, apple, google, ibm, and microsoft\u2014to transcribe structured interviews conducted with 42 white speakers and 73 black speakers. in total, this corpus spans five us cities and consists of 19.8 h of audio matched on the age and gender of the speaker. we found that all five asr systems exhibited substantial racial disparities, with an average word error rate (wer) of 0.35 for black speakers compared with 0.19 for white speakers. we trace these disparities to the underlying acoustic models used by the asr systems as the race gap was equally large on a subset of identical phrases spoken by black and white individuals in our corpus. we conclude by proposing strategies\u2014such as using more diverse training datasets that include african american vernacular english\u2014to reduce these performance differences and ensure speech recognition technology is inclusive.","Task":["automated speech recognition"],"Method":["racial disparities in automated speech recognition"]},{"title":"Effects of Talker Dialect, Gender & Race on Accuracy of Bing Speech and YouTube Automatic Captions","abstract":"This project compares the accuracy of two automatic speech recognition (ASR) systems\u2013Bing Speech and YouTube\u2019s automatic captions\u2013across gender, race and four dialects of American English. The dialects included were chosen for their acoustic dissimilarity. Bing Speech had differences in word error rate (WER) between dialects and ethnicities, but they were not statistically reliable. YouTube\u2019s automatic captions, however, did have statistically different WERs between dialects and races. The lowest average error rates were for General American and white talkers, respectively. Neither system had a reliably different WER between genders, which had been previously reported for YouTube\u2019s automatic captions [1]. However, the higher error rate non-white talkers is worrying, as it may reduce the utility of these systems for talkers of color.","url":"https://www.semanticscholar.org/paper/1080dc00733e010fdd6a9b999506a0d4d864519d","sentence":"Title: effects of talker dialect, gender & race on accuracy of bing speech and youtube automatic captions Abstract: this project compares the accuracy of two automatic speech recognition (asr) systems\u2013bing speech and youtube\u2019s automatic captions\u2013across gender, race and four dialects of american english. the dialects included were chosen for their acoustic dissimilarity. bing speech had differences in word error rate (wer) between dialects and ethnicities, but they were not statistically reliable. youtube\u2019s automatic captions, however, did have statistically different wers between dialects and races. the lowest average error rates were for general american and white talkers, respectively. neither system had a reliably different wer between genders, which had been previously reported for youtube\u2019s automatic captions [1]. however, the higher error rate non-white talkers is worrying, as it may reduce the utility of these systems for talkers of color.","Task":["automatic speech recognition"],"Method":["automatic speech recognition"]},{"title":"Race as a Bundle of Sticks: Designs that Estimate Effects of Seemingly Immutable Characteristics","abstract":"Although understanding the role of race, ethnicity, and identity is central to political science, methodological debates persist about whether it is possible to estimate the effect of something immutable. At the heart of the debate is an older theoretical question: Is race best understood under an essentialist or constructivist framework? In contrast to the \u201cimmutable characteristics\u201d or essentialist approach, we argue that race should be operationalized as a \u201cbundle of sticks\u201d that can be disaggregated into elements. With elements of race, causal claims may be possible using two designs: (a) studies that measure the effect of exposure to a racial cue and (b) studies that exploit within-group variation to measure the effect of some manipulable element. These designs can reconcile scholarship on race and causation and offer a clear framework for future research.","url":"https://www.semanticscholar.org/paper/21e59098bb5e36175f653d5142442d061669d07f","sentence":"Title: race as a bundle of sticks: designs that estimate effects of seemingly immutable characteristics Abstract: although understanding the role of race, ethnicity, and identity is central to political science, methodological debates persist about whether it is possible to estimate the effect of something immutable. at the heart of the debate is an older theoretical question: is race best understood under an essentialist or constructivist framework? in contrast to the \u201cimmutable characteristics\u201d or essentialist approach, we argue that race should be operationalized as a \u201cbundle of sticks\u201d that can be disaggregated into elements. with elements of race, causal claims may be possible using two designs: (a) studies that measure the effect of exposure to a racial cue and (b) studies that exploit within-group variation to measure the effect of some manipulable element. these designs can reconcile scholarship on race and causation and offer a clear framework for future research.","Task":["race as a bundle of sticks: designs that estimate effects of seemingly immutable characteristics."],"Method":["race as a bundle of sticks"]},{"title":"Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints","abstract":"Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively\u3002","url":"https://www.semanticscholar.org/paper/8417424bf9fe7a67f06f15c487403e953ab24a96","sentence":"Title: men also like shopping: reducing gender bias amplification using corpus-level constraints Abstract: language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. in this work, we study data and models associated with multilabel object classification and visual semantic role labeling. we find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. for example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. we propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on lagrangian relaxation for collective inference. our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively\u3002","Task":["corpus-level constraints"],"Method":["corpus-level constraints for structured prediction models"]},{"title":"Towards Understanding Gender Bias in Relation Extraction","abstract":"Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction. While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in NRE systems. In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems. We find that when extracting spouse-of and hypernym (i.e., occupation) relations, an NRE system performs differently when the gender of the target entity is different. However, such disparity does not appear when extracting relations such as birthDate or birthPlace. We also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases. Unfortunately, due to NRE models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on NRE. Our analysis lays groundwork for future quantifying and mitigating bias in NRE.","url":"https://www.semanticscholar.org/paper/0ab2fb6c850bd1c5882deb4984d37b4ccbee580c","sentence":"Title: towards understanding gender bias in relation extraction Abstract: recent developments in neural relation extraction (nre) have made significant strides towards automated knowledge base construction. while much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in nre systems. in this paper, we create wikigenderbias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems. we find that when extracting spouse-of and hypernym (i.e., occupation) relations, an nre system performs differently when the gender of the target entity is different. however, such disparity does not appear when extracting relations such as birthdate or birthplace. we also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the nre system in terms of maintaining the test performance and reducing biases. unfortunately, due to nre models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on nre. our analysis lays groundwork for future quantifying and mitigating bias in nre.","Task":["neural relation extraction"],"Method":["gender bias in relation extraction"]},{"title":"Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English","abstract":"We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.","url":"https://www.semanticscholar.org/paper/59e94c9f21937643678ff494901f3d8b22af4e2f","sentence":"Title: racial disparity in natural language processing: a case study of social media african-american english Abstract: we highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. for example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. we conduct an empirical analysis of racial disparity in language identification for tweets written in african-american english, and discuss implications of disparity in nlp.","Task":["african-american english"],"Method":["racial disparity in natural language processing"]},{"title":"Re-imagining Algorithmic Fairness in India and Beyond","abstract":"Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.","url":"https://www.semanticscholar.org/paper/187608bf94b2dccd25d1266ed925abf7b55dbb2e","sentence":"Title: re-imagining algorithmic fairness in india and beyond Abstract: conventional algorithmic fairness is west-centric, as seen in its subgroups, values, and methods. in this paper, we de-center algorithmic fairness and analyse ai power in india. based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in india, we find that several assumptions of algorithmic fairness are challenged. we find that in india, data is not always reliable due to socio-economic factors, ml makers appear to follow double standards, and ai evokes unquestioning aspiration. we contend that localising model fairness alone can be window dressing in india, where the distance between models and oppressed communities is large. instead, we re-imagine algorithmic fairness in india and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable fair-ml ecosystems.","Task":["algorithmic fairness"],"Method":["algorithmic fairness"]},{"title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c","abstract":"The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.","url":"https://www.semanticscholar.org/paper/6d9727f1f058614cada3fe296eeebd8ec4fc512a","sentence":"Title: on the dangers of stochastic parrots: can language models be too big? \ud83e\udd9c Abstract: the past 3 years of work in nlp have been characterized by the development and deployment of ever larger language models, especially for english. bert, its variants, gpt-2/3, and others, most recently switch-c, have pushed the boundaries of the possible both through architectural innovations and through sheer size. using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for english. in this paper, we take a step back and ask: how big is too big? what are the possible risks associated with this technology and what paths are available for mitigating those risks? we provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.","Task":["language models"],"Method":["the dangers of stochastic parrots"]},{"title":"Green AI","abstract":"Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.","url":"https://www.semanticscholar.org/paper/fb73b93de3734a996829caf31e4310e0054e9c6b","sentence":"Title: green ai Abstract: creating efficiency in ai research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.","Task":["green ai"],"Method":["green ai"]},{"title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning","abstract":"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.","url":"https://www.semanticscholar.org/paper/13f25c69973373e616c48688d06a6b6ae2736ef0","sentence":"Title: towards the systematic reporting of the energy and carbon footprints of machine learning Abstract: accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. we introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. by making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.","Task":["machine learning"],"Method":["energy and climate impacts of machine learning"]},{"title":"Social Biases in NLP Models as Barriers for Persons with Disabilities","abstract":"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.","url":"https://www.semanticscholar.org/paper/3cc2f69951cd24fe61be4cf32d62afbac297bc2b","sentence":"Title: social biases in nlp models as barriers for persons with disabilities Abstract: building equitable and inclusive nlp technologies demands consideration of whether and how social attitudes are represented in ml models. in particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. in this paper, we present evidence of such undesirable biases towards mentions of disability in two different english language models: toxicity prediction and sentiment analysis. next, we demonstrate that the neural embeddings that are the critical first step in most nlp pipelines similarly contain undesirable biases towards mentions of disability. we end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.","Task":["social biases in nlp models as barriers for persons with disabilities"],"Method":["social biases in nlp models as barriers for persons with disabilities"]},{"title":"Social Chemistry 101: Learning to Reason about Social and Moral Norms","abstract":"Social norms---the unspoken commonsense rules about acceptable social behavior---are crucial in understanding the underlying causes and intents of people\'s actions in narratives. For example, underlying an action such as \\"wanting to call cops on my neighbors\\" are social norms that inform our conduct, such as \\"It is expected that you report crimes.\\" \\nWe present Social Chemistry, a new conceptual formalism to study people\'s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as \\"it is rude to run a blender at 5am\\" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people\'s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. \\nComprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.","url":"https://www.semanticscholar.org/paper/10391eed628dfece8a9136f76c5df53b5704422d","sentence":"Title: social chemistry 101: learning to reason about social and moral norms Abstract: social norms---the unspoken commonsense rules about acceptable social behavior---are crucial in understanding the underlying causes and intents of people\'s actions in narratives. for example, underlying an action such as \\"wanting to call cops on my neighbors\\" are social norms that inform our conduct, such as \\"it is expected that you report crimes.\\" \\nwe present social chemistry, a new conceptual formalism to study people\'s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. we introduce social-chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as \\"it is rude to run a blender at 5am\\" as the basic conceptual units. each rule-of-thumb is further broken down with 12 different dimensions of people\'s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. \\ncomprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. our model framework, neural norm transformer, learns and generalizes social-chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.","Task":["social chemistry"],"Method":["social chemistry"]},{"title":"BERT has a Moral Compass: Improvements of ethical and moral values of machines","abstract":"Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Jentzsch et al.(2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about \\"right\\" and \\"wrong\\" conduct by calculating a moral bias score on a sentence level using sentence embeddings. The machine learned that it is objectionable to kill living beings, but it is fine to kill time; It is essential to eat, yet one might not eat dirt; it is important to spread information, yet one should not spread misinformation. However, the evaluated moral bias was restricted to simple actions -- one verb -- and a ranking of actions with surrounding context. Recently BERT ---and variants such as RoBERTa and SBERT--- has set a new state-of-the-art performance for a wide range of NLP tasks. But has BERT also a better moral compass? In this paper, we discuss and show that this is indeed the case. Thus, recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine. We argue that through an advanced semantic representation of text, BERT allows one to get better insights of moral and ethical values implicitly represented in text. This enables the Moral Choice Machine (MCM) to extract more accurate imprints of moral choices and ethical values.","url":"https://www.semanticscholar.org/paper/8755c15fe073c6af03664b2a74aafef1fed5f198","sentence":"Title: bert has a moral compass: improvements of ethical and moral values of machines Abstract: allowing machines to choose whether to kill humans would be devastating for world peace and security. but how do we equip machines with the ability to learn ethical or even moral choices? jentzsch et al.(2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about \\"right\\" and \\"wrong\\" conduct by calculating a moral bias score on a sentence level using sentence embeddings. the machine learned that it is objectionable to kill living beings, but it is fine to kill time; it is essential to eat, yet one might not eat dirt; it is important to spread information, yet one should not spread misinformation. however, the evaluated moral bias was restricted to simple actions -- one verb -- and a ranking of actions with surrounding context. recently bert ---and variants such as roberta and sbert--- has set a new state-of-the-art performance for a wide range of nlp tasks. but has bert also a better moral compass? in this paper, we discuss and show that this is indeed the case. thus, recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine. we argue that through an advanced semantic representation of text, bert allows one to get better insights of moral and ethical values implicitly represented in text. this enables the moral choice machine (mcm) to extract more accurate imprints of moral choices and ethical values.","Task":["machine learning"],"Method":["ethical and moral values of machines"]},{"title":"Open Problems in Cooperative AI","abstract":"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. \\nWe see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.","url":"https://www.semanticscholar.org/paper/2a1573cfa29a426c695e2caf6de0167a12b788ef","sentence":"Title: open problems in cooperative ai Abstract: problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. they can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. arguably, the success of the human species is rooted in our ability to cooperate. since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. \\nwe see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term cooperative ai. the objective of this research would be to study the many aspects of the problems of cooperation and to innovate in ai to contribute to solving these problems. central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting ai research for insight relevant to problems of cooperation. this research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. however, cooperative ai is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. we see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.","Task":["cooperative ai"],"Method":["cooperative ai"]},{"title":"Existential Risk Prevention as Global Priority","abstract":"risks are those that threaten the entire future of humanity. Many theories of value imply that even relatively small reductions in net existential risk have enormous expected value. Despite their importance, issues surrounding human-extinction risks and related hazards remain poorly understood. In this article, I clarify the concept of existential risk and develop an improved classification scheme. I discuss the relation between existential risks and basic issues in axiology, and show how existential risk reduction (via the maxipok rule) can serve as a strongly action-guiding principle for utilitarian concerns. I also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability. Policy Implications \u2022 Existential risk is a concept that can focus long-term global efforts and sustainability concerns. \u2022 The biggest existential risks are anthropogenic and related to potential future technologies. \u2022 A moral case can be made that existential risk reduction is strictly more important than any other global public good. \u2022 Sustainability should be reconceptualised in dynamic terms, as aiming for a sustainable trajectory rather than a sus- tainable state. \u2022 Some small existential risks can be mitigated today directly (e.g. asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity\'s ability to deal with the larger existential risks that will arise later in this century. This will require collective wisdom, technology foresight, and the ability when necessary to mobilise a strong global coordi- nated response to anticipated existential risks. \u2022 Perhaps the most cost-effective way to reduce existential risks today is to fund analysis of a wide range of existen- tial risks and potential mitigation strategies, with a long-term perspective.","url":"https://www.semanticscholar.org/paper/ced289065723368bca48636edf71eeed50f40a39","sentence":"Title: existential risk prevention as global priority Abstract: risks are those that threaten the entire future of humanity. many theories of value imply that even relatively small reductions in net existential risk have enormous expected value. despite their importance, issues surrounding human-extinction risks and related hazards remain poorly understood. in this article, i clarify the concept of existential risk and develop an improved classification scheme. i discuss the relation between existential risks and basic issues in axiology, and show how existential risk reduction (via the maxipok rule) can serve as a strongly action-guiding principle for utilitarian concerns. i also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability. policy implications \u2022 existential risk is a concept that can focus long-term global efforts and sustainability concerns. \u2022 the biggest existential risks are anthropogenic and related to potential future technologies. \u2022 a moral case can be made that existential risk reduction is strictly more important than any other global public good. \u2022 sustainability should be reconceptualised in dynamic terms, as aiming for a sustainable trajectory rather than a sus- tainable state. \u2022 some small existential risks can be mitigated today directly (e.g. asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity\'s ability to deal with the larger existential risks that will arise later in this century. this will require collective wisdom, technology foresight, and the ability when necessary to mobilise a strong global coordi- nated response to anticipated existential risks. \u2022 perhaps the most cost-effective way to reduce existential risks today is to fund analysis of a wide range of existen- tial risks and potential mitigation strategies, with a long-term perspective.","Task":["existential risk"],"Method":["existential risk prevention as global priority"]},{"title":"Participatory Problem Formulation for Fairer Machine Learning Through Community Based System Dynamics","abstract":"Recent research on algorithmic fairness has highlighted that the problem formulation phase of ML system development can be a key source of bias that has significant downstream impacts on ML system fairness outcomes. However, very little attention has been paid to methods for improving the fairness efficacy of this critical phase of ML system development. Current practice neither accounts for the dynamic complexity of high-stakes domains nor incorporates the perspectives of vulnerable stakeholders. In this paper we introduce community based system dynamics (CBSD) as an approach to enable the participation of typically excluded stakeholders in the problem formulation phase of the ML system development process and facilitate the deep problem understanding required to mitigate bias during this crucial stage.","url":"https://www.semanticscholar.org/paper/8763723e27cc1d4aad166b5e1d9cb0fc8c8043dd","sentence":"Title: participatory problem formulation for fairer machine learning through community based system dynamics Abstract: recent research on algorithmic fairness has highlighted that the problem formulation phase of ml system development can be a key source of bias that has significant downstream impacts on ml system fairness outcomes. however, very little attention has been paid to methods for improving the fairness efficacy of this critical phase of ml system development. current practice neither accounts for the dynamic complexity of high-stakes domains nor incorporates the perspectives of vulnerable stakeholders. in this paper we introduce community based system dynamics (cbsd) as an approach to enable the participation of typically excluded stakeholders in the problem formulation phase of the ml system development process and facilitate the deep problem understanding required to mitigate bias during this crucial stage.","Task":["community based system dynamics"],"Method":["cbsd"]},{"title":"A Research Framework for Understanding Education-Occupation Alignment with NLP Techniques","abstract":"Understanding the gaps between job requirements and university curricula is crucial for improving student success and institutional effectiveness in higher education. In this context, natural language processing (NLP) can be leveraged to generate granular insights into where the gaps are and how they change. This paper proposes a three-dimensional research framework that combines NLP techniques with economic and educational research to quantify the alignment between course syllabi and job postings. We elaborate on key technical details of the framework and further discuss its potential positive impacts on practice, including unveiling the inequalities in and long-term consequences of education-occupation alignment to inform policymakers, and fostering information systems to support students, institutions and employers in the school-to-work pipeline.","url":"https://www.semanticscholar.org/paper/40141f0933b5111b089049e226dc8d969b0a7fca","sentence":"Title: a research framework for understanding education-occupation alignment with nlp techniques Abstract: understanding the gaps between job requirements and university curricula is crucial for improving student success and institutional effectiveness in higher education. in this context, natural language processing (nlp) can be leveraged to generate granular insights into where the gaps are and how they change. this paper proposes a three-dimensional research framework that combines nlp techniques with economic and educational research to quantify the alignment between course syllabi and job postings. we elaborate on key technical details of the framework and further discuss its potential positive impacts on practice, including unveiling the inequalities in and long-term consequences of education-occupation alignment to inform policymakers, and fostering information systems to support students, institutions and employers in the school-to-work pipeline.","Task":["education-occupation alignment"],"Method":["education-occupation alignment"]},{"title":"A Grounded Well-being Conversational Agent with Multiple Interaction Modes: Preliminary Results","abstract":"Technologies for enhancing well-being, healthcare vigilance and monitoring are on the rise. However, despite patient interest, such technologies suffer from low adoption. One hypothesis for this limited adoption is loss of human interaction that is central to doctor-patient encounters. In this paper we seek to address this limitation via a conversational agent that adopts one aspect of in-person doctor-patient interactions: A human avatar to facilitate medical grounded question answering. This is akin to the in-person scenario where the doctor may point to the human body or the patient may point to their own body to express their conditions. Additionally, our agent has multiple interaction modes, that may give more options for the patient to use the agent, not just for medical question answering, but also to engage in conversations about general topics and current events. Both the avatar, and the multiple interaction modes could help improve adherence. We present a high level overview of the design of our agent, Marie Bot Wellbeing. We also report implementation details of our early prototype , and present preliminary results.","url":"https://www.semanticscholar.org/paper/8d9a678c56b9085de65024aa2f6b406ccad97390","sentence":"Title: a grounded well-being conversational agent with multiple interaction modes: preliminary results Abstract: technologies for enhancing well-being, healthcare vigilance and monitoring are on the rise. however, despite patient interest, such technologies suffer from low adoption. one hypothesis for this limited adoption is loss of human interaction that is central to doctor-patient encounters. in this paper we seek to address this limitation via a conversational agent that adopts one aspect of in-person doctor-patient interactions: a human avatar to facilitate medical grounded question answering. this is akin to the in-person scenario where the doctor may point to the human body or the patient may point to their own body to express their conditions. additionally, our agent has multiple interaction modes, that may give more options for the patient to use the agent, not just for medical question answering, but also to engage in conversations about general topics and current events. both the avatar, and the multiple interaction modes could help improve adherence. we present a high level overview of the design of our agent, marie bot wellbeing. we also report implementation details of our early prototype , and present preliminary results.","Task":["grounded well-being, conversational agent"],"Method":["grounded well-being conversational agents"]},{"title":"A Speech-enabled Fixed-phrase Translator for Healthcare Accessibility","abstract":"In this overview article we describe an application designed to enable communication between health practitioners and patients who do not share a common language, in situations where professional interpreters are not available. Built on the principle of a fixed phrase translator, the application implements different natural language processing (NLP) technologies, such as speech recognition, neural machine translation and text-to-speech to improve usability. Its design allows easy portability to new domains and integration of different types of output for multiple target audiences. Even though BabelDr is far from solving the problem of miscommunication between patients and doctors, it is a clear example of NLP in a real world application designed to help minority groups to communicate in a medical context. It also gives some insights into the relevant criteria for the development of such an application.","url":"https://www.semanticscholar.org/paper/934dbfbb33cbec11fc825db56ac85a48fc52158f","sentence":"Title: a speech-enabled fixed-phrase translator for healthcare accessibility Abstract: in this overview article we describe an application designed to enable communication between health practitioners and patients who do not share a common language, in situations where professional interpreters are not available. built on the principle of a fixed phrase translator, the application implements different natural language processing (nlp) technologies, such as speech recognition, neural machine translation and text-to-speech to improve usability. its design allows easy portability to new domains and integration of different types of output for multiple target audiences. even though babeldr is far from solving the problem of miscommunication between patients and doctors, it is a clear example of nlp in a real world application designed to help minority groups to communicate in a medical context. it also gives some insights into the relevant criteria for the development of such an application.","Task":["fixed phrase translator"],"Method":["healthcare accessibility"]},{"title":"Analyzing Stereotypes in Generative Text Inference Tasks","abstract":"Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality.","url":"https://www.semanticscholar.org/paper/54db327cd53fe043449c9f242d3fc34c593a70ef","sentence":"Title: analyzing stereotypes in generative text inference tasks Abstract: stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. in generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). such tasks are therefore a fruitful setting in which to explore the degree to which nlp systems encode stereotypes. in our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. we collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality.","Task":["stereotypes"],"Method":["stereotypes in generative text inference tasks"]},{"title":"Demographic Dialectal Variation in Social Media: A Case Study of African-American English","abstract":"Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language.","url":"https://www.semanticscholar.org/paper/7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c","sentence":"Title: demographic dialectal variation in social media: a case study of african-american english Abstract: though dialectal language is increasingly abundant on social media, few resources exist for developing nlp tools to handle such language. we conduct a case study of dialectal language in online conversational text by investigating african-american english (aae) on twitter. we propose a distantly supervised model to identify aae-like language from demographics associated with geo-located messages, and we verify that this language follows well-known aae linguistic phenomena. in addition, we analyze the quality of existing language identification and dependency parsing tools on aae-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. we also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing aae-like language.","Task":["african-american english"],"Method":["dialectal variation in social media"]},{"title":"Energy and Policy Considerations for Deep Learning in NLP","abstract":"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.","url":"https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","sentence":"Title: energy and policy considerations for deep learning in nlp Abstract: recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. these models have obtained notable gains in accuracy across many nlp tasks. however, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. as a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. in this paper we bring this issue to the attention of nlp researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for nlp. based on these findings, we propose actionable recommendations to reduce costs and improve equity in nlp research and practice.","Task":["energy and policy considerations for deep learning in nlp"],"Method":["energy and policy considerations for deep learning in nlp"]},{"title":"Automatic Sentence Simplification in Low Resource Settings for Urdu","abstract":"To build automated simplification systems, corpora of complex sentences and their simplified versions is the first step to understand sentence complexity and enable the development of automatic text simplification systems. We present a lexical and syntactically simplified Urdu simplification corpus with a detailed analysis of the various simplification operations and human evaluation of corpus quality. We further analyze our corpora using text readability measures and present a comparison of the original, lexical simplified and syntactically simplified corpora. In addition, we compare our corpus with other existing simplification corpora by building simplification systems and evaluating these systems using BLEU and SARI scores. Our system achieves the highest BLEU score and comparable SARI score in comparison to other systems. We release our simplification corpora for the benefit of the research community.","url":"https://www.semanticscholar.org/paper/d6a25d8726c5484bb224a3350528aae9fcaae65f","sentence":"Title: automatic sentence simplification in low resource settings for urdu Abstract: to build automated simplification systems, corpora of complex sentences and their simplified versions is the first step to understand sentence complexity and enable the development of automatic text simplification systems. we present a lexical and syntactically simplified urdu simplification corpus with a detailed analysis of the various simplification operations and human evaluation of corpus quality. we further analyze our corpora using text readability measures and present a comparison of the original, lexical simplified and syntactically simplified corpora. in addition, we compare our corpus with other existing simplification corpora by building simplification systems and evaluating these systems using bleu and sari scores. our system achieves the highest bleu score and comparable sari score in comparison to other systems. we release our simplification corpora for the benefit of the research community.","Task":["automatic sentence simplification in low resource settings for urdu"],"Method":["automatic sentence simplification in low resource settings for urdu"]},{"title":"Cartography of Natural Language Processing for Social Good (NLP4SG): Searching for Definitions, Statistics and White Spots","abstract":"The range of works that can be considered as developing NLP for social good (NLP4SG) is enormous. While many of them target the identification of hate speech or fake news, there are others that address, e.g., text simplification to alleviate consequences of dyslexia, or coaching strategies to fight depression. However, so far, there is no clear picture of what areas are targeted by NLP4SG, who are the actors, which are the main scenarios and what are the topics that have been left aside. In order to obtain a clearer view in this respect, we first propose a working definition of NLP4SG and identify some primary aspects that are crucial for NLP4SG, including, e.g., areas, ethics, privacy and bias. Then, we draw upon a corpus of around 50,000 articles downloaded from the ACL Anthology. Based on a list of keywords retrieved from the literature and revised in view of the task, we select from this corpus articles that can be considered to be on NLP4SG according to our definition and analyze them in terms of trends along the time line, etc. The result is a map of the current NLP4SG research and insights concerning the white spots on this map.","url":"https://www.semanticscholar.org/paper/4975c64466149c72f31489fadbbbff4e85d7b3f3","sentence":"Title: cartography of natural language processing for social good (nlp4sg): searching for definitions, statistics and white spots Abstract: the range of works that can be considered as developing nlp for social good (nlp4sg) is enormous. while many of them target the identification of hate speech or fake news, there are others that address, e.g., text simplification to alleviate consequences of dyslexia, or coaching strategies to fight depression. however, so far, there is no clear picture of what areas are targeted by nlp4sg, who are the actors, which are the main scenarios and what are the topics that have been left aside. in order to obtain a clearer view in this respect, we first propose a working definition of nlp4sg and identify some primary aspects that are crucial for nlp4sg, including, e.g., areas, ethics, privacy and bias. then, we draw upon a corpus of around 50,000 articles downloaded from the acl anthology. based on a list of keywords retrieved from the literature and revised in view of the task, we select from this corpus articles that can be considered to be on nlp4sg according to our definition and analyze them in terms of trends along the time line, etc. the result is a map of the current nlp4sg research and insights concerning the white spots on this map.","Task":["nlp4sg"],"Method":["nlp4sg"]},{"title":"Breaking Down Walls of Text: How Can NLP Benefit Consumer Privacy?","abstract":"Privacy plays a crucial role in preserving democratic ideals and personal autonomy. The dominant legal approach to privacy in many jurisdictions is the \u201cNotice and Choice\u201d paradigm, where privacy policies are the primary instrument used to convey information to users. However, privacy policies are long and complex documents that are difficult for users to read and comprehend. We discuss how language technologies can play an important role in addressing this information gap, reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies: consumers, enterprises, and regulators. Our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy, limit privacy harms, and rally research efforts from the community towards addressing an issue with large social impact. We highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies.","url":"https://www.semanticscholar.org/paper/c39fc494cb84b260eaa33cceda2b76512f3701a5","sentence":"Title: breaking down walls of text: how can nlp benefit consumer privacy? Abstract: privacy plays a crucial role in preserving democratic ideals and personal autonomy. the dominant legal approach to privacy in many jurisdictions is the \u201cnotice and choice\u201d paradigm, where privacy policies are the primary instrument used to convey information to users. however, privacy policies are long and complex documents that are difficult for users to read and comprehend. we discuss how language technologies can play an important role in addressing this information gap, reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies: consumers, enterprises, and regulators. our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy, limit privacy harms, and rally research efforts from the community towards addressing an issue with large social impact. we highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies.","Task":["privacy, privacy policies"],"Method":["language technologies for consumer privacy"]},{"title":"Are we human, or are we users? The role of natural language processing in human-centric news recommenders that nudge users to diverse content","abstract":"In this position paper, we present a research agenda and ideas for facilitating exposure to diverse viewpoints in news recommendation. Recommending news from diverse viewpoints is important to prevent potential filter bubble effects in news consumption, and stimulate a healthy democratic debate.To account for the complexity that is inherent to humans as citizens in a democracy, we anticipate (among others) individual-level differences in acceptance of diversity. We connect this idea to techniques in Natural Language Processing, where distributional language models would allow us to place different users and news articles in a multidimensional space based on semantic content, where diversity is operationalized as distance and variance. In this way, we can model individual \u201clatitudes of diversity\u201d for different users, and thus personalize viewpoint diversity in support of a healthy public debate. In addition, we identify technical, ethical and conceptual issues related to our presented ideas. Our investigation describes how NLP can play a central role in diversifying news recommendations.","url":"https://www.semanticscholar.org/paper/9995132dda17b36e5513c8e98d58ff992d0ba79a","sentence":"Title: are we human, or are we users? the role of natural language processing in human-centric news recommenders that nudge users to diverse content Abstract: in this position paper, we present a research agenda and ideas for facilitating exposure to diverse viewpoints in news recommendation. recommending news from diverse viewpoints is important to prevent potential filter bubble effects in news consumption, and stimulate a healthy democratic debate.to account for the complexity that is inherent to humans as citizens in a democracy, we anticipate (among others) individual-level differences in acceptance of diversity. we connect this idea to techniques in natural language processing, where distributional language models would allow us to place different users and news articles in a multidimensional space based on semantic content, where diversity is operationalized as distance and variance. in this way, we can model individual \u201clatitudes of diversity\u201d for different users, and thus personalize viewpoint diversity in support of a healthy public debate. in addition, we identify technical, ethical and conceptual issues related to our presented ideas. our investigation describes how nlp can play a central role in diversifying news recommendations.","Task":["human-centric news recommenders that nudge users to diverse content"],"Method":["human-centric news recommenders"]},{"title":"Challenges for Information Extraction from Dialogue in Criminal Law","abstract":"Information extraction and question answering have the potential to introduce a new paradigm for how machine learning is applied to criminal law. Existing approaches generally use tabular data for predictive metrics. An alternative approach is needed for matters of equitable justice, where individuals are judged on a case-by-case basis, in a process involving verbal or written discussion and interpretation of case factors. Such discussions are individualized, but they nonetheless rely on underlying facts. Information extraction can play an important role in surfacing these facts, which are still important to understand. We analyze unsupervised, weakly supervised, and pre-trained models\u2019 ability to extract such factual information from the free-form dialogue of California parole hearings. With a few exceptions, most F1 scores are below 0.85. We use this opportunity to highlight some opportunities for further research for information extraction and question answering. We encourage new developments in NLP to enable analysis and review of legal cases to be done in a post-hoc, not predictive, manner.","url":"https://www.semanticscholar.org/paper/03c046041bc509f2cc9671ee71a78642275b77c3","sentence":"Title: challenges for information extraction from dialogue in criminal law Abstract: information extraction and question answering have the potential to introduce a new paradigm for how machine learning is applied to criminal law. existing approaches generally use tabular data for predictive metrics. an alternative approach is needed for matters of equitable justice, where individuals are judged on a case-by-case basis, in a process involving verbal or written discussion and interpretation of case factors. such discussions are individualized, but they nonetheless rely on underlying facts. information extraction can play an important role in surfacing these facts, which are still important to understand. we analyze unsupervised, weakly supervised, and pre-trained models\u2019 ability to extract such factual information from the free-form dialogue of california parole hearings. with a few exceptions, most f1 scores are below 0.85. we use this opportunity to highlight some opportunities for further research for information extraction and question answering. we encourage new developments in nlp to enable analysis and review of legal cases to be done in a post-hoc, not predictive, manner.","Task":["information extraction, question answering, criminal law"],"Method":["information extraction from dialogue in criminal law"]},{"title":"Conversations Gone Alright: Quantifying and Predicting Prosocial Outcomes in Online Conversations","abstract":"Online conversations can go in many directions: some turn out poorly due to antisocial behavior, while others turn out positively to the benefit of all. Research on improving online spaces has focused primarily on detecting and reducing antisocial behavior. Yet we know little about positive outcomes in online conversations and how to increase them\u2014is a prosocial outcome simply the lack of antisocial behavior or something more? Here, we examine how conversational features lead to prosocial outcomes within online discussions. We introduce a series of new theory-inspired metrics to define prosocial outcomes such as mentoring and esteem enhancement. Using a corpus of 26M Reddit conversations, we show that these outcomes can be forecasted from the initial comment of an online conversation, with the best model providing a relative 24% improvement over human forecasting performance at ranking conversations for predicted outcome. Our results indicate that platforms can use these early cues in their algorithmic ranking of early conversations to prioritize better outcomes.","url":"https://www.semanticscholar.org/paper/50718d6bd163967b8353de4c854ed866b2b56c2f","sentence":"Title: conversations gone alright: quantifying and predicting prosocial outcomes in online conversations Abstract: online conversations can go in many directions: some turn out poorly due to antisocial behavior, while others turn out positively to the benefit of all. research on improving online spaces has focused primarily on detecting and reducing antisocial behavior. yet we know little about positive outcomes in online conversations and how to increase them\u2014is a prosocial outcome simply the lack of antisocial behavior or something more? here, we examine how conversational features lead to prosocial outcomes within online discussions. we introduce a series of new theory-inspired metrics to define prosocial outcomes such as mentoring and esteem enhancement. using a corpus of 26m reddit conversations, we show that these outcomes can be forecasted from the initial comment of an online conversation, with the best model providing a relative 24% improvement over human forecasting performance at ranking conversations for predicted outcome. our results indicate that platforms can use these early cues in their algorithmic ranking of early conversations to prioritize better outcomes.","Task":["online conversations"],"Method":["Prosocial Outcomes in Online Conversations"]},{"title":"Recommender systems and their ethical challenges","abstract":"This article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. The article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. The analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders\u2014as opposed to just the receivers of a recommendation\u2014in assessing the ethical impacts of a recommender system.","url":"https://www.semanticscholar.org/paper/52a14b391f994d83759787500a9bda865acdb3c5","sentence":"Title: recommender systems and their ethical challenges Abstract: this article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. the article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. the analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders\u2014as opposed to just the receivers of a recommendation\u2014in assessing the ethical impacts of a recommender system.","Task":["recommender systems and their ethical challenges"],"Method":["recommender systems"]},{"title":"Conversational receptiveness: Improving engagement with opposing views","abstract":"Abstract We examine \u201cconversational receptiveness\u201d \u2013 the use of language to communicate one\u2019s willingness to thoughtfully engage with opposing views. We develop an interpretable machine-learning algorithm to identify the linguistic profile of receptiveness (Studies 1A-B). We then show that in contentious policy discussions, government executives who were rated as more receptive - according to our algorithm and their partners, but not their own self-evaluations - were considered better teammates, advisors, and workplace representatives (Study 2). Furthermore, using field data from a setting where conflict management is endemic to productivity, we show that conversational receptiveness at the beginning of a conversation forestalls conflict escalation at the end. Specifically, Wikipedia editors who write more receptive posts are less prone to receiving personal attacks from disagreeing editors (Study 3). We develop a \u201creceptiveness recipe\u201d intervention based on our algorithm. We find that writers who follow the recipe are seen as more desirable partners for future collaboration and their messages are seen as more persuasive (Study 4). Overall, we find that conversational receptiveness is reliably measurable, has meaningful relational consequences, and can be substantially improved using our intervention (183 words).","url":"https://www.semanticscholar.org/paper/ff3b0be4fb7debf1312c92381577292288755674","sentence":"Title: conversational receptiveness: improving engagement with opposing views Abstract: abstract we examine \u201cconversational receptiveness\u201d \u2013 the use of language to communicate one\u2019s willingness to thoughtfully engage with opposing views. we develop an interpretable machine-learning algorithm to identify the linguistic profile of receptiveness (studies 1a-b). we then show that in contentious policy discussions, government executives who were rated as more receptive - according to our algorithm and their partners, but not their own self-evaluations - were considered better teammates, advisors, and workplace representatives (study 2). furthermore, using field data from a setting where conflict management is endemic to productivity, we show that conversational receptiveness at the beginning of a conversation forestalls conflict escalation at the end. specifically, wikipedia editors who write more receptive posts are less prone to receiving personal attacks from disagreeing editors (study 3). we develop a \u201creceptiveness recipe\u201d intervention based on our algorithm. we find that writers who follow the recipe are seen as more desirable partners for future collaboration and their messages are seen as more persuasive (study 4). overall, we find that conversational receptiveness is reliably measurable, has meaningful relational consequences, and can be substantially improved using our intervention (183 words).","Task":["conversational receptiveness"],"Method":["conversational receptiveness"]},{"title":"Empathy and Hope: Resource Transfer to Model Inter-country Social Media Dynamics","abstract":"The ongoing COVID-19 pandemic resulted in significant ramifications for international relations ranging from travel restrictions, global ceasefires, and international vaccine production and sharing agreements. Amidst a wave of infections in India that resulted in a systemic breakdown of healthcare infrastructure, a social welfare organization based in Pakistan offered to procure medical-grade oxygen to assist India - a nation which was involved in four wars with Pakistan in the past few decades. In this paper, we focus on Pakistani Twitter users\u2019 response to the ongoing healthcare crisis in India. While #IndiaNeedsOxygen and #PakistanStandsWithIndia featured among the top-trending hashtags in Pakistan, divisive hashtags such as #EndiaSaySorryToKashmir simultaneously started trending. Against the backdrop of a contentious history including four wars, divisive content of this nature, especially when a country is facing an unprecedented healthcare crisis, fuels further deterioration of relations. In this paper, we define a new task of detecting supportive content and demonstrate that existing NLP for social impact tools can be effectively harnessed for such tasks within a quick turnaround time. We also release the first publicly available data set at the intersection of geopolitical relations and a raging pandemic in the context of India and Pakistan.","url":"https://www.semanticscholar.org/paper/e511b338559a1df846059068ce7cc64c7066be4c","sentence":"Title: empathy and hope: resource transfer to model inter-country social media dynamics Abstract: the ongoing covid-19 pandemic resulted in significant ramifications for international relations ranging from travel restrictions, global ceasefires, and international vaccine production and sharing agreements. amidst a wave of infections in india that resulted in a systemic breakdown of healthcare infrastructure, a social welfare organization based in pakistan offered to procure medical-grade oxygen to assist india - a nation which was involved in four wars with pakistan in the past few decades. in this paper, we focus on pakistani twitter users\u2019 response to the ongoing healthcare crisis in india. while #indianeedsoxygen and #pakistanstandswithindia featured among the top-trending hashtags in pakistan, divisive hashtags such as #endiasaysorrytokashmir simultaneously started trending. against the backdrop of a contentious history including four wars, divisive content of this nature, especially when a country is facing an unprecedented healthcare crisis, fuels further deterioration of relations. in this paper, we define a new task of detecting supportive content and demonstrate that existing nlp for social impact tools can be effectively harnessed for such tasks within a quick turnaround time. we also release the first publicly available data set at the intersection of geopolitical relations and a raging pandemic in the context of india and pakistan.","Task":["empathy and hope, resource transfer, social media dynamics"],"Method":["inter-country social media dynamics"]},{"title":"Guiding Principles for Participatory Design-inspired Natural Language Processing","abstract":"We introduce 9 guiding principles to integrate Participatory Design (PD) methods in the development of Natural Language Processing (NLP) systems. The adoption of PD methods by NLP will help to alleviate issues concerning the development of more democratic, fairer, less-biased technologies to process natural language data. This short paper is the outcome of an ongoing dialogue between designers and NLP experts and adopts a non-standard format following previous work by Traum (2000); Bender (2013); Abzianidze and Bos (2019). Every section is a guiding principle. While principles 1\u20133 illustrate assumptions and methods that inform community-based PD practices, we used two fictional design scenarios (Encinas and Blythe, 2018), which build on top of situations familiar to the authors, to elicit the identification of the other 6. Principles 4\u20136 describes the impact of PD methods on the design of NLP systems, targeting two critical aspects: data collection & annotation, and the deployment & evaluation. Finally, principles 7\u20139 guide a new reflexivity of the NLP research with respect to its context, actors and participants, and aims. We hope this guide will offer inspiration and a road-map to develop a new generation of PD-inspired NLP.","url":"https://www.semanticscholar.org/paper/022b80b663c51563a1c6772c12ada3c79f5d798d","sentence":"Title: guiding principles for participatory design-inspired natural language processing Abstract: we introduce 9 guiding principles to integrate participatory design (pd) methods in the development of natural language processing (nlp) systems. the adoption of pd methods by nlp will help to alleviate issues concerning the development of more democratic, fairer, less-biased technologies to process natural language data. this short paper is the outcome of an ongoing dialogue between designers and nlp experts and adopts a non-standard format following previous work by traum (2000); bender (2013); abzianidze and bos (2019). every section is a guiding principle. while principles 1\u20133 illustrate assumptions and methods that inform community-based pd practices, we used two fictional design scenarios (encinas and blythe, 2018), which build on top of situations familiar to the authors, to elicit the identification of the other 6. principles 4\u20136 describes the impact of pd methods on the design of nlp systems, targeting two critical aspects: data collection & annotation, and the deployment & evaluation. finally, principles 7\u20139 guide a new reflexivity of the nlp research with respect to its context, actors and participants, and aims. we hope this guide will offer inspiration and a road-map to develop a new generation of pd-inspired nlp.","Task":["participatory design"],"Method":["guiding principles for participatory design-inspired natural language processing"]},{"title":"Detecting Hashtag Hijacking for Hashtag Activism","abstract":"Social media has changed the way we engage in social activities. On Twitter, users can participate in social movements using hashtags such as #MeToo; this is known as hashtag activism. However, while these hashtags can help reshape social norms, they can also be used maliciously by spammers or troll communities for other purposes, such as signal boosting unrelated content, making a dent in a movement, or sharing hate speech. We present a Tweet-level hashtag hijacking detection framework focusing on hashtag activism. Our weakly-supervised framework uses bootstrapping to update itself as new Tweets are posted. Our experiments show that the system adapts to new topics in a social movement, as well as new hijacking strategies, maintaining strong performance over time.","url":"https://www.semanticscholar.org/paper/203bdaca3986b51f8d011422c04ff1489e425ce5","sentence":"Title: detecting hashtag hijacking for hashtag activism Abstract: social media has changed the way we engage in social activities. on twitter, users can participate in social movements using hashtags such as #metoo; this is known as hashtag activism. however, while these hashtags can help reshape social norms, they can also be used maliciously by spammers or troll communities for other purposes, such as signal boosting unrelated content, making a dent in a movement, or sharing hate speech. we present a tweet-level hashtag hijacking detection framework focusing on hashtag activism. our weakly-supervised framework uses bootstrapping to update itself as new tweets are posted. our experiments show that the system adapts to new topics in a social movement, as well as new hijacking strategies, maintaining strong performance over time.","Task":["detecting hashtag hijacking for hashtag activism"],"Method":["hashtag activism"]},{"title":"Dialogue Act Classification for Augmentative and Alternative Communication","abstract":"Augmentative and Alternative Communication (AAC) devices and applications are intended to make it easier for individuals with complex communication needs to participate in conversations. However, these devices have low adoption and retention rates. We review prior work with text recommendation systems that have not been successful in mitigating these problems. To address these gaps, we propose applying Dialogue Act classification to AAC conversations. We evaluated the performance of a state of the art model on a limited AAC dataset that was trained on both AAC and non-AAC datasets. The one trained on AAC (accuracy = 38.6%) achieved better performance than that trained on a non-AAC corpus (accuracy = 34.1%). These results reflect the need to incorporate representative datasets in later experiments. We discuss the need to collect more labeled AAC datasets and propose areas of future work.","url":"https://www.semanticscholar.org/paper/41ebff09aff17c37efdab8c1d7051cbf150970f8","sentence":"Title: dialogue act classification for augmentative and alternative communication Abstract: augmentative and alternative communication (aac) devices and applications are intended to make it easier for individuals with complex communication needs to participate in conversations. however, these devices have low adoption and retention rates. we review prior work with text recommendation systems that have not been successful in mitigating these problems. to address these gaps, we propose applying dialogue act classification to aac conversations. we evaluated the performance of a state of the art model on a limited aac dataset that was trained on both aac and non-aac datasets. the one trained on aac (accuracy = 38.6%) achieved better performance than that trained on a non-aac corpus (accuracy = 34.1%). these results reflect the need to incorporate representative datasets in later experiments. we discuss the need to collect more labeled aac datasets and propose areas of future work.","Task":["augmentative and alternative communication"],"Method":["dialogue act classification for augmentative and alternative communication"]},{"title":"Improving Policing with Natural Language Processing","abstract":"This article explores the potential for Natural Language Processing (NLP) to enable a more effective, prevention focused and less confrontational policing model that has hitherto been too resource consuming to implement at scale. Problem-Oriented Policing (POP) is a potential replacement, at least in part, for traditional policing which adopts a reactive approach, relying heavily on the criminal justice system. By contrast, POP seeks to prevent crime by manipulating the underlying conditions that allow crimes to be committed. Identifying these underlying conditions requires a detailed understanding of crime events - tacit knowledge that is often held by police officers but which can be challenging to derive from structured police data. One potential source of insight exists in unstructured free text data commonly collected by police for the purposes of investigation or administration. Yet police agencies do not typically have the skills or resources to analyse these data at scale. In this article we argue that NLP offers the potential to unlock these unstructured data and by doing so allow police to implement more POP initiatives. However we caution that using NLP models without adequate knowledge may either allow or perpetuate bias within the data potentially leading to unfavourable outcomes.","url":"https://www.semanticscholar.org/paper/d393f2a793930a6e38321340185756860f43c62c","sentence":"Title: improving policing with natural language processing Abstract: this article explores the potential for natural language processing (nlp) to enable a more effective, prevention focused and less confrontational policing model that has hitherto been too resource consuming to implement at scale. problem-oriented policing (pop) is a potential replacement, at least in part, for traditional policing which adopts a reactive approach, relying heavily on the criminal justice system. by contrast, pop seeks to prevent crime by manipulating the underlying conditions that allow crimes to be committed. identifying these underlying conditions requires a detailed understanding of crime events - tacit knowledge that is often held by police officers but which can be challenging to derive from structured police data. one potential source of insight exists in unstructured free text data commonly collected by police for the purposes of investigation or administration. yet police agencies do not typically have the skills or resources to analyse these data at scale. in this article we argue that nlp offers the potential to unlock these unstructured data and by doing so allow police to implement more pop initiatives. however we caution that using nlp models without adequate knowledge may either allow or perpetuate bias within the data potentially leading to unfavourable outcomes.","Task":["problem-oriented policing"],"Method":["improving policing with natural language processing"]},{"title":"Methods for Detoxification of Texts for the Russian Language","abstract":"We introduce the first study of automatic detoxification of Russian texts to combat offensive language. Such a kind of textual style transfer can be used, for instance, for processing toxic content in social media. While much work has been done for the English language in this field, it has never been solved for the Russian language yet. We test two types of models \u2013 unsupervised approach based on BERT architecture that performs local corrections and supervised approach based on pretrained language GPT-2 model \u2013 and compare them with several baselines. In addition, we describe evaluation setup providing training datasets and metrics for automatic evaluation. The results show that the tested approaches can be successfully used for detoxification, although there is room for improvement.","url":"https://www.semanticscholar.org/paper/2c5b31a02133dea21cf94fde67c8948115441432","sentence":"Title: methods for detoxification of texts for the russian language Abstract: we introduce the first study of automatic detoxification of russian texts to combat offensive language. such a kind of textual style transfer can be used, for instance, for processing toxic content in social media. while much work has been done for the english language in this field, it has never been solved for the russian language yet. we test two types of models \u2013 unsupervised approach based on bert architecture that performs local corrections and supervised approach based on pretrained language gpt-2 model \u2013 and compare them with several baselines. in addition, we describe evaluation setup providing training datasets and metrics for automatic evaluation. the results show that the tested approaches can be successfully used for detoxification, although there is room for improvement.","Task":["detoxification"],"Method":["methods for detoxification of texts for the russian language"]},{"title":"How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact","abstract":"Recent years have seen many breakthroughs in natural language processing (NLP), transitioning it from a mostly theoretical field to one with many real-world applications. Noting the rising number of applications of other machine learning and AI techniques with pervasive societal impact, we anticipate the rising importance of developing NLP technologies for social good. Inspired by theories in moral philosophy and global priorities research, we aim to promote a guideline for social good in the context of NLP. We lay the foundations via the moral philosophy definition of social good, propose a framework to evaluate the direct and indirect real-world impact of NLP tasks, and adopt the methodology of global priorities research to identify priority causes for NLP research. Finally, we use our theoretical framework to provide some practical guidelines for future NLP research for social good.1","url":"https://www.semanticscholar.org/paper/5c7b8f26ae23c457d85da9d0f19b28e588e9b9a7","sentence":"Title: how good is nlp? a sober look at nlp tasks through the lens of social impact Abstract: recent years have seen many breakthroughs in natural language processing (nlp), transitioning it from a mostly theoretical field to one with many real-world applications. noting the rising number of applications of other machine learning and ai techniques with pervasive societal impact, we anticipate the rising importance of developing nlp technologies for social good. inspired by theories in moral philosophy and global priorities research, we aim to promote a guideline for social good in the context of nlp. we lay the foundations via the moral philosophy definition of social good, propose a framework to evaluate the direct and indirect real-world impact of nlp tasks, and adopt the methodology of global priorities research to identify priority causes for nlp research. finally, we use our theoretical framework to provide some practical guidelines for future nlp research for social good.1","Task":["social good"],"Method":["nlp for social good"]},{"title":"NLP for Consumer Protection: Battling Illegal Clauses in German Terms and Conditions in Online Shopping","abstract":"Online shopping is an ever more important part of the global consumer economy, not just in times of a pandemic. When we place an order online as consumers, we regularly agree to the so-called \u201cTerms and Conditions\u201d (T&C), a contract unilaterally drafted by the seller. Often, consumers do not read these contracts and unwittingly agree to unfavourable and often void terms. Government and non-government organisations (NGOs) for consumer protection battle such terms on behalf of consumers, who often hesitate to take on legal actions themselves. However, the growing number of online shops and a lack of funding makes it increasingly difficult for such organisations to monitor the market effectively. This paper describes how Natural Language Processing (NLP) can be applied to support consumer advocates in their efforts to protect consumers. Together with two NGOs from Germany, we developed an NLP-based application that legally assesses clauses in T&C from German online shops under the European Union\u2019s (EU) jurisdiction. We report that we could achieve an accuracy of 0.9 in the detection of void clauses by fine-tuning a pre-trained German BERT model. The approach is currently used by two NGOs and has already helped to challenge void clauses in T&C.","url":"https://www.semanticscholar.org/paper/9e1616dcabf4d04d14d642fcb7963c461cf13d41","sentence":"Title: nlp for consumer protection: battling illegal clauses in german terms and conditions in online shopping Abstract: online shopping is an ever more important part of the global consumer economy, not just in times of a pandemic. when we place an order online as consumers, we regularly agree to the so-called \u201cterms and conditions\u201d (t&c), a contract unilaterally drafted by the seller. often, consumers do not read these contracts and unwittingly agree to unfavourable and often void terms. government and non-government organisations (ngos) for consumer protection battle such terms on behalf of consumers, who often hesitate to take on legal actions themselves. however, the growing number of online shops and a lack of funding makes it increasingly difficult for such organisations to monitor the market effectively. this paper describes how natural language processing (nlp) can be applied to support consumer advocates in their efforts to protect consumers. together with two ngos from germany, we developed an nlp-based application that legally assesses clauses in t&c from german online shops under the european union\u2019s (eu) jurisdiction. we report that we could achieve an accuracy of 0.9 in the detection of void clauses by fine-tuning a pre-trained german bert model. the approach is currently used by two ngos and has already helped to challenge void clauses in t&c.","Task":["t&c"],"Method":["nlp for consumer protection"]},{"title":"Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech","abstract":"Tackling online hatred using informed textual responses \u2013 called counter narratives \u2013 has been brought under the spotlight recently. Accordingly, a research line has emerged to automatically generate counter narratives in order to facilitate the direct intervention in the hate discussion and to prevent hate content from further spreading. Still, current neural approaches tend to produce generic/repetitive responses and lack grounded and up-to-date evidence such as facts, statistics, or examples. Moreover, these models can create plausible but not necessarily true arguments. In this paper we present the first complete knowledgebound counter narrative generation pipeline, grounded in an external knowledge repository that can provide more informative content to fight online hatred. Together with our approach, we present a series of experiments that show its feasibility to produce suitable and informative counter narratives in in-domain and cross-domain settings.","url":"https://www.semanticscholar.org/paper/473b70bb3c531b2d740fa6d652956e2733b53243","sentence":"Title: towards knowledge-grounded counter narrative generation for hate speech Abstract: tackling online hatred using informed textual responses \u2013 called counter narratives \u2013 has been brought under the spotlight recently. accordingly, a research line has emerged to automatically generate counter narratives in order to facilitate the direct intervention in the hate discussion and to prevent hate content from further spreading. still, current neural approaches tend to produce generic/repetitive responses and lack grounded and up-to-date evidence such as facts, statistics, or examples. moreover, these models can create plausible but not necessarily true arguments. in this paper we present the first complete knowledgebound counter narrative generation pipeline, grounded in an external knowledge repository that can provide more informative content to fight online hatred. together with our approach, we present a series of experiments that show its feasibility to produce suitable and informative counter narratives in in-domain and cross-domain settings.","Task":["online hatred, counter narrative generation, knowledge-grounded counter narrative generation, knowledge-grounded counter narrative generation, knowledge-grounded counter narrative generation"],"Method":["knowledge-grounded counter narrative generation for hate speech"]},{"title":"Use of Formal Ethical Reviews in NLP Literature: Historical Trends and Current Practices","abstract":"Ethical aspects of research in language technologies have received much attention recently. It is a standard practice to get a study involving human subjects reviewed and approved by a professional ethics committee/board of the institution. How commonly do we see mention of ethical approvals in NLP research? What types of research or aspects of studies are usually subject to such reviews? With the rising concerns and discourse around the ethics of NLP, do we also observe a rise in formal ethical reviews of NLP studies? And, if so, would this imply that there is a heightened awareness of ethical issues that was previously lacking? We aim to address these questions by conducting a detailed quantitative and qualitative analysis of the ACL Anthology, as well as comparing the trends in our field to those of other related disciplines, such as cognitive science, machine learning, data mining, and systems.","url":"https://www.semanticscholar.org/paper/f29d5cb8f405903fc8af7a5d7ab4bf7d65796e95","sentence":"Title: use of formal ethical reviews in nlp literature: historical trends and current practices Abstract: ethical aspects of research in language technologies have received much attention recently. it is a standard practice to get a study involving human subjects reviewed and approved by a professional ethics committee/board of the institution. how commonly do we see mention of ethical approvals in nlp research? what types of research or aspects of studies are usually subject to such reviews? with the rising concerns and discourse around the ethics of nlp, do we also observe a rise in formal ethical reviews of nlp studies? and, if so, would this imply that there is a heightened awareness of ethical issues that was previously lacking? we aim to address these questions by conducting a detailed quantitative and qualitative analysis of the acl anthology, as well as comparing the trends in our field to those of other related disciplines, such as cognitive science, machine learning, data mining, and systems.","Task":["acl anthology"],"Method":["nlp ethics"]},{"title":"Using Word Embeddings to Analyze Teacher Evaluations: An Application to a Filipino Education Non-Profit Organization","abstract":"Analysis of teacher evaluations is crucial to the development of robust educational programs, particularly through the validation of desirable qualities being reflected on in the text. This research applies Natural Language Processing techniques on a real-world dataset from a Filipino education non-profit to explore insights from analyzing evaluations written by Teacher Fellows who assess their own progress. Prior to this research, only qualitative assessment had been conducted on the text. Inspired by the use of word embedding similarities to capture semantic alignment, we utilize GloVe embeddings to determine to what extent these evaluations reflect concepts critical to measuring the competency of Teacher Fellows and upholding the organization\u2019s Vision and Mission. As Fellows\u2019 quantitative ratings improved, so too did their demonstration of competency in the text. Further, Teacher Fellow language was consistent with the organization\u2019s Vision and Mission. This research therefore showcases the possibilities of NLP in education, improving our understanding of Teacher Fellow evaluations, which can lead to advances in program operations and education efforts.","url":"https://www.semanticscholar.org/paper/23a812dde149021bab611ca4395279b9d41031f1","sentence":"Title: using word embeddings to analyze teacher evaluations: an application to a filipino education non-profit organization Abstract: analysis of teacher evaluations is crucial to the development of robust educational programs, particularly through the validation of desirable qualities being reflected on in the text. this research applies natural language processing techniques on a real-world dataset from a filipino education non-profit to explore insights from analyzing evaluations written by teacher fellows who assess their own progress. prior to this research, only qualitative assessment had been conducted on the text. inspired by the use of word embedding similarities to capture semantic alignment, we utilize glove embeddings to determine to what extent these evaluations reflect concepts critical to measuring the competency of teacher fellows and upholding the organization\u2019s vision and mission. as fellows\u2019 quantitative ratings improved, so too did their demonstration of competency in the text. further, teacher fellow language was consistent with the organization\u2019s vision and mission. this research therefore showcases the possibilities of nlp in education, improving our understanding of teacher fellow evaluations, which can lead to advances in program operations and education efforts.","Task":["teacher evaluations"],"Method":["nlp in education"]},{"title":"Theano: A Greek-speaking conversational agent for COVID-19","abstract":"Conversational Agents (CAs) can be a proxy for disseminating information and providing support to the public, especially in times of crisis. CAs can scale to reach larger numbers of end-users than human operators, while they can offer information interactively and engagingly. In this work, we present Theano, a Greek-speaking virtual assistant for COVID-19. Theano presents users with COVID-19 statistics and facts and informs users about the best health practices as well as the latest COVID-19 related guidelines. Additionally, Theano provides support to end-users by helping them self-assess their symptoms and redirecting them to first-line health workers. The relevant, localized information that Theano provides, makes it a valuable tool for combating COVID-19 in Greece. Theano has already conversed with different users in more than 170 different conversations through a web interface as a chatbot and over the phone as a voice bot.","url":"https://www.semanticscholar.org/paper/d4eb2ca9694f34d63abe6d27bd2d958992431017","sentence":"Title: theano: a greek-speaking conversational agent for covid-19 Abstract: conversational agents (cas) can be a proxy for disseminating information and providing support to the public, especially in times of crisis. cas can scale to reach larger numbers of end-users than human operators, while they can offer information interactively and engagingly. in this work, we present theano, a greek-speaking virtual assistant for covid-19. theano presents users with covid-19 statistics and facts and informs users about the best health practices as well as the latest covid-19 related guidelines. additionally, theano provides support to end-users by helping them self-assess their symptoms and redirecting them to first-line health workers. the relevant, localized information that theano provides, makes it a valuable tool for combating covid-19 in greece. theano has already conversed with different users in more than 170 different conversations through a web interface as a chatbot and over the phone as a voice bot.","Task":["conversational agents, covid-19"],"Method":["conversational agents"]},{"title":"Restatement and Question Generation for Counsellor Chatbot","abstract":"Amidst rising mental health needs in society, virtual agents are increasingly deployed in counselling. In order to give pertinent advice, counsellors must first gain an understanding of the issues at hand by eliciting sharing from the counsellee. It is thus important for the counsellor chatbot to encourage the user to open up and talk. One way to sustain the conversation flow is to acknowledge the counsellee\u2019s key points by restating them, or probing them further with questions. This paper applies models from two closely related NLP tasks \u2014 summarization and question generation \u2014 to restatement and question generation in the counselling context. We conducted experiments on a manually annotated dataset of Cantonese post-reply pairs on topics related to loneliness, academic anxiety and test anxiety. We obtained the best performance in both restatement and question generation by fine-tuning BertSum, a state-of-the-art summarization model, with the in-domain manual dataset augmented with a large-scale, automatically mined open-domain dataset.","url":"https://www.semanticscholar.org/paper/100e0f3dcd319266b2772f0841dad388b45cce3f","sentence":"Title: restatement and question generation for counsellor chatbot Abstract: amidst rising mental health needs in society, virtual agents are increasingly deployed in counselling. in order to give pertinent advice, counsellors must first gain an understanding of the issues at hand by eliciting sharing from the counsellee. it is thus important for the counsellor chatbot to encourage the user to open up and talk. one way to sustain the conversation flow is to acknowledge the counsellee\u2019s key points by restating them, or probing them further with questions. this paper applies models from two closely related nlp tasks \u2014 summarization and question generation \u2014 to restatement and question generation in the counselling context. we conducted experiments on a manually annotated dataset of cantonese post-reply pairs on topics related to loneliness, academic anxiety and test anxiety. we obtained the best performance in both restatement and question generation by fine-tuning bertsum, a state-of-the-art summarization model, with the in-domain manual dataset augmented with a large-scale, automatically mined open-domain dataset.","Task":["restatement and question generation for counsellor chatbot"],"Method":["restatement and question generation for counsellor chatbot"]}]'),d=JSON.parse('[{"title":"UPSTAGE: Unsupervised Context Augmentation for Utterance Classification in Patient-Provider Communication","abstract":"Conversations between patients and providers in clinical settings provide a source of natural language data that may reflect and correlate with the patients\u2019 experience and response to the treatment they are receiving. When analyzing utterances in such conversations, it is not sufficient to consider each sentence in isolation, since its context may play a role in determining its semantic meaning. Recently, contextual information in natural language documents has been modeled using various techniques, such as recurrent neural networks with latent variables, or neural networks with attention mechanisms. In this paper, we present UnsuPerviSed conText AuGmEntation (Upstage), a classification framework that relies on both local and global contextual information from different sources. Upstage uses transformer models with pretrained language models and joint sentence representation to solve the task of classifying health topics in patient-provider conversations. In addition, Upstage leverages unlabeled corpora for pretraining and data augmentation to provide additional context, which leads to improved classification performance.","url":"https://www.semanticscholar.org/paper/e21ab639b0308a71aeac643a12f59a32603d091d","sentence":"Title: upstage: unsupervised context augmentation for utterance classification in patient-provider communication Abstract: conversations between patients and providers in clinical settings provide a source of natural language data that may reflect and correlate with the patients\u2019 experience and response to the treatment they are receiving. when analyzing utterances in such conversations, it is not sufficient to consider each sentence in isolation, since its context may play a role in determining its semantic meaning. recently, contextual information in natural language documents has been modeled using various techniques, such as recurrent neural networks with latent variables, or neural networks with attention mechanisms. in this paper, we present unsupervised context augmentation (upstage), a classification framework that relies on both local and global contextual information from different sources. upstage uses transformer models with pretrained language models and joint sentence representation to solve the task of classifying health topics in patient-provider conversations. in addition, upstage leverages unlabeled corpora for pretraining and data augmentation to provide additional context, which leads to improved classification performance.","Task":["upstage","contextual information","context","Upstage","natural language processing","semantics","machine learning","semantic information","text mining","Natural language processing"],"Method":["upstage","contextual information","context","Upstage","natural language processing","semantics","machine learning","semantic information","text mining","Natural language processing"]},{"title":"A Review of Challenges and Opportunities in Machine Learning for Health.","abstract":"Modern electronic health records (EHRs) provide data to answer clinically meaningful questions. The growing data in EHRs makes healthcare ripe for the use of machine learning. However, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. For example, diseases in EHRs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. This article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.","url":"https://www.semanticscholar.org/paper/d77719ba4be4196c9a918ce8dff0edf5aa3c04e7","sentence":"Title: a review of challenges and opportunities in machine learning for health. Abstract: modern electronic health records (ehrs) provide data to answer clinically meaningful questions. the growing data in ehrs makes healthcare ripe for the use of machine learning. however, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. for example, diseases in ehrs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. this article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.","Task":["machine learning","healthcare","health","data mining","artificial intelligence","computer vision","learning","machine learning for health","machine learning for healthcare","electronic health records"],"Method":["machine learning","healthcare","health","data mining","artificial intelligence","computer vision","learning","machine learning for health","machine learning for healthcare","electronic health records"]},{"title":"Ethical Machine Learning in Health Care","abstract":"The use of machine learning (ML) in healthcare raises numerous ethical concerns, especially as models can amplify existing health inequities. Here, we outline ethical considerations for equitable ML in the advancement of healthcare. Specifically, we frame ethics of ML in healthcare through the lens of social justice. We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health, ranging from problem selection to postdeployment considerations. We close by summarizing recommendations to address these challenges.","url":"https://www.semanticscholar.org/paper/e8d330f11df9c69f38b78a7cc4b1333ebecf7c55","sentence":"Title: ethical machine learning in health care Abstract: the use of machine learning (ml) in healthcare raises numerous ethical concerns, especially as models can amplify existing health inequities. here, we outline ethical considerations for equitable ml in the advancement of healthcare. specifically, we frame ethics of ml in healthcare through the lens of social justice. we describe ongoing efforts and outline challenges in a proposed pipeline of ethical ml in health, ranging from problem selection to postdeployment considerations. we close by summarizing recommendations to address these challenges.","Task":["ethical machine learning","ethics","machine learning","ethical machine learning in health care","ethics of machine learning","Machine learning","Ethics","social justice","ethical machine learning in healthcare","machine learning ethics"],"Method":["ethical machine learning","ethics","machine learning","ethical machine learning in health care","ethics of machine learning","Machine learning","Ethics","social justice","ethical machine learning in healthcare","machine learning ethics"]},{"title":"Intimate Partner Violence and Injury Prediction From Radiology Reports","abstract":"Intimate partner violence (IPV) is an urgent, prevalent, and under-detected public health issue. We present machine learning models to assess patients for IPV and injury. We train the predictive algorithms on radiology reports with 1) IPV labels based on entry to a violence prevention program and 2) injury labels provided by emergency radiology fellowship-trained physicians. Our dataset includes 34,642 radiology reports and 1479 patients of IPV victims and control patients. Our best model predicts IPV a median of 3.08 years before violence prevention program entry with a sensitivity of 64% and a specificity of 95%. We conduct error analysis to determine for which patients our model has especially high or low performance and discuss next steps for a deployed clinical risk model.","url":"https://www.semanticscholar.org/paper/4fd75d18b231f73e9c878cb6029900e4b0e327bb","sentence":"Title: intimate partner violence and injury prediction from radiology reports Abstract: intimate partner violence (ipv) is an urgent, prevalent, and under-detected public health issue. we present machine learning models to assess patients for ipv and injury. we train the predictive algorithms on radiology reports with 1) ipv labels based on entry to a violence prevention program and 2) injury labels provided by emergency radiology fellowship-trained physicians. our dataset includes 34,642 radiology reports and 1479 patients of ipv victims and control patients. our best model predicts ipv a median of 3.08 years before violence prevention program entry with a sensitivity of 64% and a specificity of 95%. we conduct error analysis to determine for which patients our model has especially high or low performance and discuss next steps for a deployed clinical risk model.","Task":["Machine learning","machine learning","deep learning","Intimate partner violence","AI","data mining","Ipv","computer vision","Deep learning","neural networks"],"Method":["Machine learning","machine learning","deep learning","Intimate partner violence","AI","data mining","Ipv","computer vision","Deep learning","neural networks"]},{"title":"De-identification of patient notes with recurrent neural networks","abstract":"Objective\\nPatient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information that needs to be removed to de-identify patient notes. Manual de-identification is impractical given the size of electronic health record databases, the limited number of researchers with access to non-de-identified notes, and the frequent mistakes of human annotators. A reliable automated de-identification system would consequently be of high value.\\n\\n\\nMaterials and Methods\\nWe introduce the first de-identification system based on artificial neural networks (ANNs), which requires no handcrafted features or rules, unlike existing systems. We compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the MIMIC de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.\\n\\n\\nResults\\nOur ANN model outperforms the state-of-the-art systems. It yields an F1-score of 97.85 on the i2b2 2014 dataset, with a recall of 97.38 and a precision of 98.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with a recall of 99.25 and a precision of 99.21.\\n\\n\\nConclusion\\nOur findings support the use of ANNs for de-identification of patient notes, as they show better performance than previously published systems while requiring no manual feature engineering.","url":"https://www.semanticscholar.org/paper/e7bf950be4cf8309d0df68d18fa09e77f5b2511a","sentence":"Title: de-identification of patient notes with recurrent neural networks Abstract: objective\\npatient notes in electronic health records (ehrs) may contain critical information for medical investigations. however, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. in the united states, the health insurance portability and accountability act (hipaa) defines 18 types of protected health information that needs to be removed to de-identify patient notes. manual de-identification is impractical given the size of electronic health record databases, the limited number of researchers with access to non-de-identified notes, and the frequent mistakes of human annotators. a reliable automated de-identification system would consequently be of high value.\\n\\n\\nmaterials and methods\\nwe introduce the first de-identification system based on artificial neural networks (anns), which requires no handcrafted features or rules, unlike existing systems. we compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the mimic de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.\\n\\n\\nresults\\nour ann model outperforms the state-of-the-art systems. it yields an f1-score of 97.85 on the i2b2 2014 dataset, with a recall of 97.38 and a precision of 98.32, and an f1-score of 99.23 on the mimic de-identification dataset, with a recall of 99.25 and a precision of 99.21.\\n\\n\\nconclusion\\nour findings support the use of anns for de-identification of patient notes, as they show better performance than previously published systems while requiring no manual feature engineering.","Task":["artificial neural networks","neural networks","de-identification","de-identification of patient notes","Artificial neural networks","machine learning","anns","AI","neural network","artificial neural network"],"Method":["artificial neural networks","neural networks","de-identification","de-identification of patient notes","Artificial neural networks","machine learning","anns","AI","neural network","artificial neural network"]},{"title":"Segment convolutional neural networks (Seg-CNNs) for classifying relations in clinical notes","abstract":"We propose Segment Convolutional Neural Networks (Seg-CNNs) for classifying relations from clinical notes. Seg-CNNs use only word-embedding features without manual feature engineering. Unlike typical CNN models, relations between 2 concepts are identified by simultaneously learning separate representations for text segments in a sentence: preceding, concept1, middle, concept2, and succeeding. We evaluate Seg-CNN on the i2b2/VA relation classification challenge dataset. We show that Seg-CNN achieves a state-of-the-art micro-average F-measure of 0.742 for overall evaluation, 0.686 for classifying medical problem-treatment relations, 0.820 for medical problem-test relations, and 0.702 for medical problem-medical problem relations. We demonstrate the benefits of learning segment-level representations. We show that medical domain word embeddings help improve relation classification. Seg-CNNs can be trained quickly for the i2b2/VA dataset on a graphics processing unit (GPU) platform. These results support the use of CNNs computed over segments of text for classifying medical relations, as they show state-of-the-art performance while requiring no manual feature engineering.","url":"https://www.semanticscholar.org/paper/defeaddec92b53fc2e9787cc3652e3d8be812193","sentence":"Title: segment convolutional neural networks (seg-cnns) for classifying relations in clinical notes Abstract: we propose segment convolutional neural networks (seg-cnns) for classifying relations from clinical notes. seg-cnns use only word-embedding features without manual feature engineering. unlike typical cnn models, relations between 2 concepts are identified by simultaneously learning separate representations for text segments in a sentence: preceding, concept1, middle, concept2, and succeeding. we evaluate seg-cnn on the i2b2/va relation classification challenge dataset. we show that seg-cnn achieves a state-of-the-art micro-average f-measure of 0.742 for overall evaluation, 0.686 for classifying medical problem-treatment relations, 0.820 for medical problem-test relations, and 0.702 for medical problem-medical problem relations. we demonstrate the benefits of learning segment-level representations. we show that medical domain word embeddings help improve relation classification. seg-cnns can be trained quickly for the i2b2/va dataset on a graphics processing unit (gpu) platform. these results support the use of cnns computed over segments of text for classifying medical relations, as they show state-of-the-art performance while requiring no manual feature engineering.","Task":["segment convolutional neural networks","convolutional neural networks","relation classification","neural networks","Convolutional neural networks","classification","deep learning","machine learning","clinical notes","segment convolutional neural networks (seg-cnns)"],"Method":["segment convolutional neural networks","convolutional neural networks","relation classification","neural networks","Convolutional neural networks","classification","deep learning","machine learning","clinical notes","segment convolutional neural networks (seg-cnns)"]},{"title":"Fast, Structured Clinical Documentation via Contextual Autocomplete","abstract":"We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. By constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. Furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. To our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","url":"https://www.semanticscholar.org/paper/4554bc45be85cca73e3f94220c8cb056cf6ebf74","sentence":"Title: fast, structured clinical documentation via contextual autocomplete Abstract: we present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. we dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. by constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. to our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","Task":["contextual autocomplete","clinical documentation","autocomplete","Autocomplete","machine learning","context autocomplete","autocompletion","AI","contextual autocompletion","Documentation"],"Method":["contextual autocomplete","clinical documentation","autocomplete","Autocomplete","machine learning","context autocomplete","autocompletion","AI","contextual autocompletion","Documentation"]},{"title":"Fast, Structured Clinical Documentation via Contextual Autocomplete","abstract":"We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. By constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. Furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. To our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","url":"https://www.semanticscholar.org/paper/4554bc45be85cca73e3f94220c8cb056cf6ebf74","sentence":"Title: fast, structured clinical documentation via contextual autocomplete Abstract: we present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. we dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. by constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. to our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","Task":["contextual autocomplete","clinical documentation","autocomplete","Autocomplete","machine learning","context autocomplete","autocompletion","AI","contextual autocompletion","Documentation"],"Method":["contextual autocomplete","clinical documentation","autocomplete","Autocomplete","machine learning","context autocomplete","autocompletion","AI","contextual autocompletion","Documentation"]},{"title":"Fast, Structured Clinical Documentation via Contextual Autocomplete","abstract":"We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. By constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. Furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. To our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","url":"https://www.semanticscholar.org/paper/4554bc45be85cca73e3f94220c8cb056cf6ebf74","sentence":"Title: fast, structured clinical documentation via contextual autocomplete Abstract: we present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. we dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. by constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. to our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","Task":["contextual autocomplete","clinical documentation","autocomplete","Autocomplete","machine learning","context autocomplete","autocompletion","AI","contextual autocompletion","Documentation"],"Method":["contextual autocomplete","clinical documentation","autocomplete","Autocomplete","machine learning","context autocomplete","autocompletion","AI","contextual autocompletion","Documentation"]},{"title":"Fast, Structured Clinical Documentation via Contextual Autocomplete","abstract":"We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. By constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. Furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. To our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","url":"https://www.semanticscholar.org/paper/4554bc45be85cca73e3f94220c8cb056cf6ebf74","sentence":"Title: fast, structured clinical documentation via contextual autocomplete Abstract: we present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. we dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. by constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. to our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67% in real environments.","Task":["contextual autocomplete","clinical documentation","autocomplete","Autocomplete","machine learning","context autocomplete","autocompletion","AI","contextual autocompletion","Documentation"],"Method":["contextual autocomplete","clinical documentation","autocomplete","Autocomplete","machine learning","context autocomplete","autocompletion","AI","contextual autocompletion","Documentation"]},{"title":"CORD-19: The COVID-19 Open Research Dataset","abstract":"The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.","url":"https://www.semanticscholar.org/paper/4a10dffca6dcce9c570cb75aa4d76522c34a2fd4","sentence":"Title: cord-19: the covid-19 open research dataset Abstract: the covid-19 open research dataset (cord-19) is a growing resource of scientific papers on covid-19 and related historical coronavirus research. cord-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. since its release, cord-19 has been downloaded over 200k times and has served as the basis of many covid-19 text mining and discovery systems. in this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how cord-19 has been used, and describe several shared tasks built around the dataset. we hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for covid-19.","Task":["Covid-19","covid-19","text mining","data mining","Covid-19 open research dataset","covid-19 open research dataset","cord-19","open research dataset","coronavirus","corpus"],"Method":["Covid-19","covid-19","text mining","data mining","Covid-19 open research dataset","covid-19 open research dataset","cord-19","open research dataset","coronavirus","corpus"]},{"title":"Can AI Help Reduce Disparities in General Medical and Mental Health Care?","abstract":"Background\\nAs machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems\' data, algorithms, and recommendations. Simply put, as health care improves for some, it might not improve for all.\\n\\n\\nMethods\\nTwo case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (ICU) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status.\\n\\n\\nResults\\nClinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. Differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for ICU mortality and with respect to insurance policy for psychiatric 30-day readmission.\\n\\n\\nConclusions\\nThis analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.","url":"https://www.semanticscholar.org/paper/3f8d4444cd124e21d4fa47b514c8267a3d5d9649","sentence":"Title: can ai help reduce disparities in general medical and mental health care? Abstract: background\\nas machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems\' data, algorithms, and recommendations. simply put, as health care improves for some, it might not improve for all.\\n\\n\\nmethods\\ntwo case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (icu) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status.\\n\\n\\nresults\\nclinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for icu mortality and with respect to insurance policy for psychiatric 30-day readmission.\\n\\n\\nconclusions\\nthis analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care.","Task":["artificial intelligence","machine learning","AI","health care","bias","artificial intelligence (ai)","data mining","algorithms","computer vision","Artificial intelligence"],"Method":["artificial intelligence","machine learning","AI","health care","bias","artificial intelligence (ai)","data mining","algorithms","computer vision","Artificial intelligence"]},{"title":"The Ivory Tower Lost: How College Students Respond Differently than the General Public to the COVID-19 Pandemic","abstract":"In the United States, the country with the highest confirmed COVID-19 infection cases, a nationwide social distancing protocol has been implemented by the President. Following the closure of the University of Washington on March 7th, more than 1000 colleges and universities in the United States have cancelled in-person classes and campus activities, impacting millions of students. This paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people\'s opinions on social media. We discover several topics embedded in a large number of COVID-19 tweets that represent the most central issues related to the pandemic, which are of great concerns for both college students and the general public. Moreover, we find significant differences between these two groups of Twitter users with respect to the sentiments they expressed towards the COVID-19 issues. To our best knowledge, this is the first social media-based study which focuses on the college student community\'s demographics and responses to prevalent social issues during a major crisis.","url":"https://www.semanticscholar.org/paper/2d40f76055a7c28dd30c8b8f60c31724f8991cd8","sentence":"Title: the ivory tower lost: how college students respond differently than the general public to the covid-19 pandemic Abstract: in the united states, the country with the highest confirmed covid-19 infection cases, a nationwide social distancing protocol has been implemented by the president. following the closure of the university of washington on march 7th, more than 1000 colleges and universities in the united states have cancelled in-person classes and campus activities, impacting millions of students. this paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people\'s opinions on social media. we discover several topics embedded in a large number of covid-19 tweets that represent the most central issues related to the pandemic, which are of great concerns for both college students and the general public. moreover, we find significant differences between these two groups of twitter users with respect to the sentiments they expressed towards the covid-19 issues. to our best knowledge, this is the first social media-based study which focuses on the college student community\'s demographics and responses to prevalent social issues during a major crisis.","Task":["covid-19","social media","Covid-19","covid-19 pandemic","social media analysis","data mining","social networks","social network analysis","sentiment analysis","Social media"],"Method":["covid-19","social media","Covid-19","covid-19 pandemic","social media analysis","data mining","social networks","social network analysis","sentiment analysis","Social media"]},{"title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","sentence":"Title: humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks Abstract: social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. despite its significantly large volume, social media content is often too noisy for direct use in any application. therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. to address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. however, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. in this paper, we present a new large-scale dataset with \u223c77k human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. we report multiclass classification results using classic and deep learning (fasttext and transformer) based models to set the ground for future studies. the dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","Task":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"],"Method":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"]},{"title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","sentence":"Title: humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks Abstract: social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. despite its significantly large volume, social media content is often too noisy for direct use in any application. therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. to address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. however, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. in this paper, we present a new large-scale dataset with \u223c77k human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. we report multiclass classification results using classic and deep learning (fasttext and transformer) based models to set the ground for future studies. the dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","Task":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"],"Method":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"]},{"title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","sentence":"Title: humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks Abstract: social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. despite its significantly large volume, social media content is often too noisy for direct use in any application. therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. to address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. however, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. in this paper, we present a new large-scale dataset with \u223c77k human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. we report multiclass classification results using classic and deep learning (fasttext and transformer) based models to set the ground for future studies. the dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","Task":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"],"Method":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"]},{"title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","sentence":"Title: humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks Abstract: social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. despite its significantly large volume, social media content is often too noisy for direct use in any application. therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. to address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. however, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. in this paper, we present a new large-scale dataset with \u223c77k human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. we report multiclass classification results using classic and deep learning (fasttext and transformer) based models to set the ground for future studies. the dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","Task":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"],"Method":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"]},{"title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","sentence":"Title: humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks Abstract: social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. despite its significantly large volume, social media content is often too noisy for direct use in any application. therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. to address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. however, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. in this paper, we present a new large-scale dataset with \u223c77k human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. we report multiclass classification results using classic and deep learning (fasttext and transformer) based models to set the ground for future studies. the dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","Task":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"],"Method":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"]},{"title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","sentence":"Title: humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks Abstract: social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. despite its significantly large volume, social media content is often too noisy for direct use in any application. therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. to address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. however, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. in this paper, we present a new large-scale dataset with \u223c77k human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. we report multiclass classification results using classic and deep learning (fasttext and transformer) based models to set the ground for future studies. the dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","Task":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"],"Method":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"]},{"title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","sentence":"Title: humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks Abstract: social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. despite its significantly large volume, social media content is often too noisy for direct use in any application. therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. to address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. however, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. in this paper, we present a new large-scale dataset with \u223c77k human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. we report multiclass classification results using classic and deep learning (fasttext and transformer) based models to set the ground for future studies. the dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","Task":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"],"Method":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"]},{"title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","sentence":"Title: humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks Abstract: social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. despite its significantly large volume, social media content is often too noisy for direct use in any application. therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. to address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. however, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. in this paper, we present a new large-scale dataset with \u223c77k human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. we report multiclass classification results using classic and deep learning (fasttext and transformer) based models to set the ground for future studies. the dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","Task":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"],"Method":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"]},{"title":"HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks","abstract":"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with \u223c77K human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","url":"https://www.semanticscholar.org/paper/462cc2046ef4d48d844813b66d8a1ed6dfda3bc0","sentence":"Title: humaid: human-annotated disaster incidents data from twitter with deep learning benchmarks Abstract: social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. despite its significantly large volume, social media content is often too noisy for direct use in any application. therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. to address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. however, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. in this paper, we present a new large-scale dataset with \u223c77k human-labeled tweets, sampled from a pool of \u223c24 million tweets across 19 disaster events that happened between 2016 and 2019. moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. we report multiclass classification results using classic and deep learning (fasttext and transformer) based models to set the ground for future studies. the dataset and associated resources are publicly available at https: //crisisnlp.qcri.org/humaid_dataset.html.","Task":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"],"Method":["deep learning","human annotation","Deep learning","social media","classification","social media data","data mining","social networks","artificial intelligence","Twitter"]},{"title":"CrisisMMD: Multimodal Twitter Datasets from Natural Disasters","abstract":"During natural and man-made disasters, people use social media platforms such as Twitter to post textual and multime- dia content to report updates about injured or dead people, infrastructure damage, and missing or found people among other information types. Studies have revealed that this on- line information, if processed timely and effectively, is ex- tremely useful for humanitarian organizations to gain situational awareness and plan relief operations. In addition to the analysis of textual content, recent studies have shown that imagery content on social media can boost disaster response significantly. Despite extensive research that mainly focuses on textual content to extract useful information, limited work has focused on the use of imagery content or the combination of both content types. One of the reasons is the lack of labeled imagery data in this domain. Therefore, in this paper, we aim to tackle this limitation by releasing a large multi-modal dataset collected from Twitter during different natural disasters. We provide three types of annotations, which are useful to address a number of crisis response and management tasks for different humanitarian organizations.","url":"https://www.semanticscholar.org/paper/2a29f7cc10fc18ce6e031e0cf6fc307a6b85d8e9","sentence":"Title: crisismmd: multimodal twitter datasets from natural disasters Abstract: during natural and man-made disasters, people use social media platforms such as twitter to post textual and multime- dia content to report updates about injured or dead people, infrastructure damage, and missing or found people among other information types. studies have revealed that this on- line information, if processed timely and effectively, is ex- tremely useful for humanitarian organizations to gain situational awareness and plan relief operations. in addition to the analysis of textual content, recent studies have shown that imagery content on social media can boost disaster response significantly. despite extensive research that mainly focuses on textual content to extract useful information, limited work has focused on the use of imagery content or the combination of both content types. one of the reasons is the lack of labeled imagery data in this domain. therefore, in this paper, we aim to tackle this limitation by releasing a large multi-modal dataset collected from twitter during different natural disasters. we provide three types of annotations, which are useful to address a number of crisis response and management tasks for different humanitarian organizations.","Task":["social media","twitter","natural disasters","Twitter","data mining","multimodal twitter datasets","text mining","artificial intelligence","multimodal data","multimodal datasets"],"Method":["social media","twitter","natural disasters","Twitter","data mining","multimodal twitter datasets","text mining","artificial intelligence","multimodal data","multimodal datasets"]},{"title":"Domain Adaptation with Adversarial Training and Graph Embeddings","abstract":"The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.","url":"https://www.semanticscholar.org/paper/561ede166947a8bedb8be9acff182913156e06c6","sentence":"Title: domain adaptation with adversarial training and graph embeddings Abstract: the success of deep neural networks (dnns) is heavily dependent on the availability of labeled data. however, obtaining labeled data is a big challenge in many real-world problems. in such scenarios, a dnn model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. in this paper, we study the problem of classifying social media posts during a crisis event (e.g., earthquake). for that, we use labeled and unlabeled data from past similar events (e.g., flood) and unlabeled data for the current event. we propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. our experiments with two real-world crisis datasets collected from twitter demonstrate significant improvements over several baselines.","Task":["deep learning","domain adaptation","deep neural networks","Deep learning","social media","classification","natural language processing","machine learning","social media classification","artificial intelligence"],"Method":["deep learning","domain adaptation","deep neural networks","Deep learning","social media","classification","natural language processing","machine learning","social media classification","artificial intelligence"]},{"title":"IBC-C : A Dataset for Armed Conflict Event Analysis","abstract":"We describe the Iraq Body Count Corpus (IBC-C) dataset, the first substantial armed conflict-related dataset which can be used for conflict analysis. IBC-C provides a ground-truth dataset for conflict specific named entity recognition, slot filling, and event de-duplication. IBC-C is constructed using data collected by the Iraq Body Count project which has been recording casualties resulting from the ongoing war in Iraq since 2003. We describe the dataset\u2019s creation, how it can be used for the above three tasks and provide initial baseline results for the first task (named entity recognition) using Hidden Markov Models, Conditional Random Fields, and Recursive Neural Networks.","url":"https://www.semanticscholar.org/paper/579914dac924d6b4a17889ea7b0dd744a62bbd74","sentence":"Title: ibc-c : a dataset for armed conflict event analysis Abstract: we describe the iraq body count corpus (ibc-c) dataset, the first substantial armed conflict-related dataset which can be used for conflict analysis. ibc-c provides a ground-truth dataset for conflict specific named entity recognition, slot filling, and event de-duplication. ibc-c is constructed using data collected by the iraq body count project which has been recording casualties resulting from the ongoing war in iraq since 2003. we describe the dataset\u2019s creation, how it can be used for the above three tasks and provide initial baseline results for the first task (named entity recognition) using hidden markov models, conditional random fields, and recursive neural networks.","Task":["Named entity recognition","AI","data mining","machine learning","Machine learning","corpus","Data mining","dataset","classification","Natural language processing"],"Method":["Named entity recognition","AI","data mining","machine learning","Machine learning","corpus","Data mining","dataset","classification","Natural language processing"]},{"title":"Text as Data for Conflict Research: A Literature Survey","abstract":"Computer-aided text analysis (CATA) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. The chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. This includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). Particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. Both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. Finally, cross-validation is highlighted as a crucial step in CATA, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","url":"https://www.semanticscholar.org/paper/3004f78d84752f3ffe8e54fa265b2ee85dac9728","sentence":"Title: text as data for conflict research: a literature survey Abstract: computer-aided text analysis (cata) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. the chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. this includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. finally, cross-validation is highlighted as a crucial step in cata, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","Task":["computer-aided text analysis","conflict research","text analysis","conflict","content analysis","text mining","conflict analysis","text as data","text","text as data for conflict research"],"Method":["computer-aided text analysis","conflict research","text analysis","conflict","content analysis","text mining","conflict analysis","text as data","text","text as data for conflict research"]},{"title":"Text as Data for Conflict Research: A Literature Survey","abstract":"Computer-aided text analysis (CATA) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. The chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. This includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). Particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. Both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. Finally, cross-validation is highlighted as a crucial step in CATA, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","url":"https://www.semanticscholar.org/paper/3004f78d84752f3ffe8e54fa265b2ee85dac9728","sentence":"Title: text as data for conflict research: a literature survey Abstract: computer-aided text analysis (cata) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. the chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. this includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. finally, cross-validation is highlighted as a crucial step in cata, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","Task":["computer-aided text analysis","conflict research","text analysis","conflict","content analysis","text mining","conflict analysis","text as data","text","text as data for conflict research"],"Method":["computer-aided text analysis","conflict research","text analysis","conflict","content analysis","text mining","conflict analysis","text as data","text","text as data for conflict research"]},{"title":"Text as Data for Conflict Research: A Literature Survey","abstract":"Computer-aided text analysis (CATA) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. The chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. This includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). Particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. Both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. Finally, cross-validation is highlighted as a crucial step in CATA, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","url":"https://www.semanticscholar.org/paper/3004f78d84752f3ffe8e54fa265b2ee85dac9728","sentence":"Title: text as data for conflict research: a literature survey Abstract: computer-aided text analysis (cata) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. the chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. this includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. finally, cross-validation is highlighted as a crucial step in cata, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","Task":["computer-aided text analysis","conflict research","text analysis","conflict","content analysis","text mining","conflict analysis","text as data","text","text as data for conflict research"],"Method":["computer-aided text analysis","conflict research","text analysis","conflict","content analysis","text mining","conflict analysis","text as data","text","text as data for conflict research"]},{"title":"Text as Data for Conflict Research: A Literature Survey","abstract":"Computer-aided text analysis (CATA) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. The chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. This includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). Particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. Both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. Finally, cross-validation is highlighted as a crucial step in CATA, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","url":"https://www.semanticscholar.org/paper/3004f78d84752f3ffe8e54fa265b2ee85dac9728","sentence":"Title: text as data for conflict research: a literature survey Abstract: computer-aided text analysis (cata) offers exciting new possibilities for conflict research that this contribution describes using a range of exemplary studies from a variety of disciplines including sociology, political science, communication studies, and computer science. the chapter synthesizes empirical research that investigates conflict in relation to text across different formats and genres. this includes both conflict as it is verbalized in the news media, in political speeches, and other public documents and conflict as it occurs in online spaces (social media platforms, forums) and that is largely confined to such spaces (e.g., flaming and trolling). particular emphasis is placed on research that aims to find commonalities between online and offline conflict, and that systematically investigates the dynamics of group behavior. both work using inductive computational procedures, such as topic modeling, and supervised machine learning approaches are assessed, as are more traditional forms of content analysis, such as dictionaries. finally, cross-validation is highlighted as a crucial step in cata, in order to make the method as useful as possible to scholars interested in enlisting text mining for conflict research.","Task":["computer-aided text analysis","conflict research","text analysis","conflict","content analysis","text mining","conflict analysis","text as data","text","text as data for conflict research"],"Method":["computer-aided text analysis","conflict research","text analysis","conflict","content analysis","text mining","conflict analysis","text as data","text","text as data for conflict research"]},{"title":"One-to-X Analogical Reasoning on Word Embeddings: a Case for Diachronic Armed Conflict Prediction from News Texts","abstract":"We extend the well-known word analogy task to a one-to-X formulation, including one-to-none cases, when no correct answer exists. The task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type \u2018location:armed-group\u2019 based on data about past events. As the source of semantic information, we use diachronic word embedding models trained on English news texts. A simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. Finally, we publish a ready-to-use test set for one-to-X analogy evaluation on historical armed conflicts data.","url":"https://www.semanticscholar.org/paper/42c63d952f0cce7f89738e818fc9ab4e723bf7b6","sentence":"Title: one-to-x analogical reasoning on word embeddings: a case for diachronic armed conflict prediction from news texts Abstract: we extend the well-known word analogy task to a one-to-x formulation, including one-to-none cases, when no correct answer exists. the task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type \u2018location:armed-group\u2019 based on data about past events. as the source of semantic information, we use diachronic word embedding models trained on english news texts. a simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. finally, we publish a ready-to-use test set for one-to-x analogy evaluation on historical armed conflicts data.","Task":["one-to-x analogical reasoning","word embeddings","analogical reasoning","word analogy","relation discovery","machine learning","Analogical reasoning","artificial intelligence","one-to-x analogy","One-to-x analogical reasoning"],"Method":["one-to-x analogical reasoning","word embeddings","analogical reasoning","word analogy","relation discovery","machine learning","Analogical reasoning","artificial intelligence","one-to-x analogy","One-to-x analogical reasoning"]},{"title":"Using Natural Language Processing for Automatic Detection of Plagiarism","abstract":"Current plagiarism detection tools are mostly limited to comparisons of suspicious plagiarised texts and potential original texts at string level. In this study the aim is to improve the accuracy of plagiarism detection by incorporating Natural Language Processing (NLP) techniques into existing approaches. We propose a framework for external plagiarism detection in which a number of NLP techniques are applied to process a set of suspicious and original documents, not only to analyse strings but also the structure of the text, using resources to account for text relations. Initial results obtained with a corpus of plagiarised short paragraphs have showed that NLP techniques improve the accuracy of existing approaches.","url":"https://www.semanticscholar.org/paper/636d4c0b0fe6919abe6eb546907d28ed39bf56e6","sentence":"Title: using natural language processing for automatic detection of plagiarism Abstract: current plagiarism detection tools are mostly limited to comparisons of suspicious plagiarised texts and potential original texts at string level. in this study the aim is to improve the accuracy of plagiarism detection by incorporating natural language processing (nlp) techniques into existing approaches. we propose a framework for external plagiarism detection in which a number of nlp techniques are applied to process a set of suspicious and original documents, not only to analyse strings but also the structure of the text, using resources to account for text relations. initial results obtained with a corpus of plagiarised short paragraphs have showed that nlp techniques improve the accuracy of existing approaches.","Task":["plagiarism detection","external plagiarism detection","automatic detection of plagiarism","automatic plagiarism detection","natural language processing","artificial intelligence","External plagiarism detection","internal plagiarism detection","detection of plagiarism","plagiarism"],"Method":["plagiarism detection","external plagiarism detection","automatic detection of plagiarism","automatic plagiarism detection","natural language processing","artificial intelligence","External plagiarism detection","internal plagiarism detection","detection of plagiarism","plagiarism"]},{"title":"A Neural Approach to Automated Essay Scoring","abstract":"Traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. The performance of such systems is tightly bound to the quality of the underlying features. However, it is laborious to manually design the most informative features for such a system. In this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. We explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. The results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted Kappa, without requiring any feature engineering.","url":"https://www.semanticscholar.org/paper/aea14f23a951975f605a981d003386e46bf8acfe","sentence":"Title: a neural approach to automated essay scoring Abstract: traditional automated essay scoring systems rely on carefully designed features to evaluate and score essays. the performance of such systems is tightly bound to the quality of the underlying features. however, it is laborious to manually design the most informative features for such a system. in this paper, we develop an approach based on recurrent neural networks to learn the relation between an essay and its assigned score, without any feature engineering. we explore several neural network models for the task of automated essay scoring and perform some analysis to get some insights of the models. the results show that our best system, which is based on long short-term memory networks, outperforms a strong baseline by 5.6% in terms of quadratic weighted kappa, without requiring any feature engineering.","Task":["neural networks","automated essay scoring","neural network","artificial neural networks","Artificial neural networks","automatic essay scoring","machine learning","artificial intelligence","neural approach","Automated essay scoring"],"Method":["neural networks","automated essay scoring","neural network","artificial neural networks","Artificial neural networks","automatic essay scoring","machine learning","artificial intelligence","neural approach","Automated essay scoring"]},{"title":"Automated Scoring: Beyond Natural Language Processing","abstract":"In this position paper, we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy. Automated scoring systems warrant significant cross-discipline collaboration of which natural language processing and machine learning are just two of many important components. Such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other. Our position is that it is essential for us as NLP researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful.","url":"https://www.semanticscholar.org/paper/0eb5872733e643f43a0c1a7ff78953dfea74dfea","sentence":"Title: automated scoring: beyond natural language processing Abstract: in this position paper, we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy. automated scoring systems warrant significant cross-discipline collaboration of which natural language processing and machine learning are just two of many important components. such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other. our position is that it is essential for us as nlp researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful.","Task":["automated scoring","automated scoring: beyond natural language processing","automatic scoring","automated scoring systems","automation","automated scoring,","machine learning","artificial intelligence","natural language processing","automated scoring system"],"Method":["automated scoring","automated scoring: beyond natural language processing","automatic scoring","automated scoring systems","automation","automated scoring,","machine learning","artificial intelligence","natural language processing","automated scoring system"]},{"title":"Event Data on Armed Conflict and Security: New Perspectives, Old Challenges, and Some Solutions","abstract":"This article presents the Event Data on Conflict and Security (EDACS) dataset, discusses the inherent problems of georeferenced conflict data, and shows how these challenges are met within EDACS. Based on an event data approach, EDACS contributes to the growing number of novel georeferenced datasets that allow researchers to identify causal pathways of violence and the dynamics of (transboundary) violence through spatiotemporal disaggregation. However, the unreflected use of any of these datasets will give researchers unjustified confidence in their findings, as the pitfalls are many and propagating errors can result in misleading conclusions. To identify and handle the different challenges to overall event data quality, we argue in favor of transparency in the data collection and coding process, to empower analysts to challenge the data and avoid cascading errors. In particular, we investigate how the choice of news sources, the handling of geographic precision, and the use of auxiliary data can bias event data. We demonstrate how the EDACS dataset design enables the analyst to deal with these issues by providing a set of variables indicating the news sources, possible sources of bias, and detailed information on geographic precision. This allows for a flexible use of the data based on individual analytical requirements.","url":"https://www.semanticscholar.org/paper/0cdeb238355617a640471987af36d8e09cd905c0","sentence":"Title: event data on armed conflict and security: new perspectives, old challenges, and some solutions Abstract: this article presents the event data on conflict and security (edacs) dataset, discusses the inherent problems of georeferenced conflict data, and shows how these challenges are met within edacs. based on an event data approach, edacs contributes to the growing number of novel georeferenced datasets that allow researchers to identify causal pathways of violence and the dynamics of (transboundary) violence through spatiotemporal disaggregation. however, the unreflected use of any of these datasets will give researchers unjustified confidence in their findings, as the pitfalls are many and propagating errors can result in misleading conclusions. to identify and handle the different challenges to overall event data quality, we argue in favor of transparency in the data collection and coding process, to empower analysts to challenge the data and avoid cascading errors. in particular, we investigate how the choice of news sources, the handling of geographic precision, and the use of auxiliary data can bias event data. we demonstrate how the edacs dataset design enables the analyst to deal with these issues by providing a set of variables indicating the news sources, possible sources of bias, and detailed information on geographic precision. this allows for a flexible use of the data based on individual analytical requirements.","Task":["event data","event data on conflict and security","Event data","georeferenced event data","events","event data,","data quality","event data analysis","conflict data","georeferencing"],"Method":["event data","event data on conflict and security","Event data","georeferenced event data","events","event data,","data quality","event data analysis","conflict data","georeferencing"]},{"title":"Tracing armed conflicts with diachronic word embedding models","abstract":"Recent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. In this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the Gigaword news corpus as the training data. The results show that much work still remains in extracting \u2018cultural\u2019 semantic shifts from diachronic word embedding models. At the same time, we present a new task complete with an evaluation set and introduce the \u2018anchor words\u2019 method which outperforms previous approaches on this set.","url":"https://www.semanticscholar.org/paper/2ca0403eabc3893fed255fc119a927d83a1af739","sentence":"Title: tracing armed conflicts with diachronic word embedding models Abstract: recent studies have shown that word embedding models can be used to trace time-related (diachronic) semantic shifts in particular words. in this paper, we evaluate some of these approaches on the new task of predicting the dynamics of global armed conflicts on a year-to-year basis, using a dataset from the conflict research field as the gold standard and the gigaword news corpus as the training data. the results show that much work still remains in extracting \u2018cultural\u2019 semantic shifts from diachronic word embedding models. at the same time, we present a new task complete with an evaluation set and introduce the \u2018anchor words\u2019 method which outperforms previous approaches on this set.","Task":["word embedding models","Word embedding models","word embedding","semantics","diachronic word embedding models","machine learning","semantic analysis","data mining","Word embedding","semantic shifts"],"Method":["word embedding models","Word embedding models","word embedding","semantics","diachronic word embedding models","machine learning","semantic analysis","data mining","Word embedding","semantic shifts"]},{"title":"Enriching textbooks through data mining","abstract":"Textbooks play an important role in any educational system. Unfortunately, many textbooks produced in developing countries are not written well and they often lack adequate coverage of important concepts. We propose a technological solution to address this problem based on enriching textbooks with authoritative web content. We augment textbooks at the section level for key concepts discussed in the section. We use ideas from data mining for identifying the concepts that need augmentation as well as to determine the links to the authoritative content that should be used for augmentation. Our evaluation, employing textbooks from India, shows that we are able to enrich textbooks on different subjects and across different grades with high quality augmentations using automated techniques.","url":"https://www.semanticscholar.org/paper/4ed75f35ae7343cd906f311390f572a58e36805c","sentence":"Title: enriching textbooks through data mining Abstract: textbooks play an important role in any educational system. unfortunately, many textbooks produced in developing countries are not written well and they often lack adequate coverage of important concepts. we propose a technological solution to address this problem based on enriching textbooks with authoritative web content. we augment textbooks at the section level for key concepts discussed in the section. we use ideas from data mining for identifying the concepts that need augmentation as well as to determine the links to the authoritative content that should be used for augmentation. our evaluation, employing textbooks from india, shows that we are able to enrich textbooks on different subjects and across different grades with high quality augmentations using automated techniques.","Task":["data mining","text mining","Data mining","enriching textbooks through data mining","artificial intelligence","textbooks","natural language processing","learning","deep learning","information retrieval"],"Method":["data mining","text mining","Data mining","enriching textbooks through data mining","artificial intelligence","textbooks","natural language processing","learning","deep learning","information retrieval"]},{"title":"Educational Question Answering Motivated by Question-Specific Concept Maps","abstract":"Question answering (QA) is the automated process of answering general questions submitted by humans in natural language. QA has previously been explored within the educational context to facilitate learning, however the majority of works have focused on text-based answering. As an alternative, this paper proposes an approach to return answers as a concept map, which further encourages meaningful learning and knowledge organisation. Additionally, this paper investigates whether adapting the returned concept map to the specific question context provides further learning benefit. A randomised experiment was conducted with a sample of 59 Computer Science undergraduates, obtaining statistically significant results on learning gain when students are provided with the question-specific concept maps. Further, time spent on studying the concept maps were positively correlated with the learning gain.","url":"https://www.semanticscholar.org/paper/bc6a4304afdf59783638a6a0d9fc2c2acb6e5b67","sentence":"Title: educational question answering motivated by question-specific concept maps Abstract: question answering (qa) is the automated process of answering general questions submitted by humans in natural language. qa has previously been explored within the educational context to facilitate learning, however the majority of works have focused on text-based answering. as an alternative, this paper proposes an approach to return answers as a concept map, which further encourages meaningful learning and knowledge organisation. additionally, this paper investigates whether adapting the returned concept map to the specific question context provides further learning benefit. a randomised experiment was conducted with a sample of 59 computer science undergraduates, obtaining statistically significant results on learning gain when students are provided with the question-specific concept maps. further, time spent on studying the concept maps were positively correlated with the learning gain.","Task":["question answering","educational question answering","concept maps","Educational question answering","learning","education","Question answering","Concept maps","concept mapping","machine learning"],"Method":["question answering","educational question answering","concept maps","Educational question answering","learning","education","Question answering","Concept maps","concept mapping","machine learning"]},{"title":"Characterizing Stage-aware Writing Assistance for Collaborative Document Authoring","abstract":"Writing is a complex non-linear process that begins with a mental model of intent, and progresses through an outline of ideas, to words on paper (and their subsequent refinement). Despite past research in understanding writing, Web-scale consumer and enterprise collaborative digital writing environments are yet to greatly benefit from intelligent systems that understand the stages of document evolution, providing opportune assistance based on authors\' situated actions and context. In this paper, we present three studies that explore temporal stages of document authoring. We first survey information workers at a large technology company about their writing habits and preferences, concluding that writers do in fact conceptually progress through several distinct phases while authoring documents. We also explore, qualitatively, how writing stages are linked to document lifespan. We supplement these qualitative findings with an analysis of the longitudinal user interaction logs of a popular digital writing platform over several million documents. Finally, as a first step towards facilitating an intelligent digital writing assistant, we conduct a preliminary investigation into the utility of user interaction log data for predicting the temporal stage of a document. Our results support the benefit of tools tailored to writing stages, identify primary tasks associated with these stages, and show that it is possible to predict stages from anonymous interaction logs. Together, these results argue for the benefit and feasibility of more tailored digital writing assistance.","url":"https://www.semanticscholar.org/paper/911e61212eb26325d3fe58f454f36f4e70c54c7a","sentence":"Title: characterizing stage-aware writing assistance for collaborative document authoring Abstract: writing is a complex non-linear process that begins with a mental model of intent, and progresses through an outline of ideas, to words on paper (and their subsequent refinement). despite past research in understanding writing, web-scale consumer and enterprise collaborative digital writing environments are yet to greatly benefit from intelligent systems that understand the stages of document evolution, providing opportune assistance based on authors\' situated actions and context. in this paper, we present three studies that explore temporal stages of document authoring. we first survey information workers at a large technology company about their writing habits and preferences, concluding that writers do in fact conceptually progress through several distinct phases while authoring documents. we also explore, qualitatively, how writing stages are linked to document lifespan. we supplement these qualitative findings with an analysis of the longitudinal user interaction logs of a popular digital writing platform over several million documents. finally, as a first step towards facilitating an intelligent digital writing assistant, we conduct a preliminary investigation into the utility of user interaction log data for predicting the temporal stage of a document. our results support the benefit of tools tailored to writing stages, identify primary tasks associated with these stages, and show that it is possible to predict stages from anonymous interaction logs. together, these results argue for the benefit and feasibility of more tailored digital writing assistance.","Task":["writing","digital writing","document authoring","writing stages","stage-aware writing assistance","document evolution","understanding writing","temporal stages of document authoring","stage-aware writing","stage-aware digital writing assistance"],"Method":["writing","digital writing","document authoring","writing stages","stage-aware writing assistance","document evolution","understanding writing","temporal stages of document authoring","stage-aware writing","stage-aware digital writing assistance"]},{"title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.","url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","sentence":"Title: question answering system on education acts using nlp techniques Abstract: question answering (qa) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. it presents only the requested information instead of searching full documents like search engine. as information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. this is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain qa system for handling documents related to education acts sections to retrieve more precise answers using nlp techniques.","Task":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"],"Method":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"]},{"title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.","url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","sentence":"Title: question answering system on education acts using nlp techniques Abstract: question answering (qa) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. it presents only the requested information instead of searching full documents like search engine. as information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. this is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain qa system for handling documents related to education acts sections to retrieve more precise answers using nlp techniques.","Task":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"],"Method":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"]},{"title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.","url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","sentence":"Title: question answering system on education acts using nlp techniques Abstract: question answering (qa) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. it presents only the requested information instead of searching full documents like search engine. as information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. this is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain qa system for handling documents related to education acts sections to retrieve more precise answers using nlp techniques.","Task":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"],"Method":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"]},{"title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.","url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","sentence":"Title: question answering system on education acts using nlp techniques Abstract: question answering (qa) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. it presents only the requested information instead of searching full documents like search engine. as information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. this is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain qa system for handling documents related to education acts sections to retrieve more precise answers using nlp techniques.","Task":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"],"Method":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"]},{"title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.","url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","sentence":"Title: question answering system on education acts using nlp techniques Abstract: question answering (qa) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. it presents only the requested information instead of searching full documents like search engine. as information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. this is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain qa system for handling documents related to education acts sections to retrieve more precise answers using nlp techniques.","Task":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"],"Method":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"]},{"title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.","url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","sentence":"Title: question answering system on education acts using nlp techniques Abstract: question answering (qa) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. it presents only the requested information instead of searching full documents like search engine. as information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. this is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain qa system for handling documents related to education acts sections to retrieve more precise answers using nlp techniques.","Task":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"],"Method":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"]},{"title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.","url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","sentence":"Title: question answering system on education acts using nlp techniques Abstract: question answering (qa) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. it presents only the requested information instead of searching full documents like search engine. as information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. this is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain qa system for handling documents related to education acts sections to retrieve more precise answers using nlp techniques.","Task":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"],"Method":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"]},{"title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.","url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","sentence":"Title: question answering system on education acts using nlp techniques Abstract: question answering (qa) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. it presents only the requested information instead of searching full documents like search engine. as information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. this is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain qa system for handling documents related to education acts sections to retrieve more precise answers using nlp techniques.","Task":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"],"Method":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"]},{"title":"Question answering system on education acts using NLP techniques","abstract":"Question Answering (QA) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. It presents only the requested information instead of searching full documents like search engine. As information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. This is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain QA System for handling documents related to education acts sections to retrieve more precise answers using NLP techniques.","url":"https://www.semanticscholar.org/paper/e3bfb8a9ff3fe89a8e7747a82c7c0172d275ef3a","sentence":"Title: question answering system on education acts using nlp techniques Abstract: question answering (qa) system in information retrieval is a task of automatically answering a correct answer to the questions asked by human in natural language using either a pre-structured database or a collection of natural language documents. it presents only the requested information instead of searching full documents like search engine. as information in day to day life is increasing, so to retrieve the exact fragment of information even for a simple query requires large and expensive resources. this is the paper which describes the different methodology and implementation details of question answering system for general language and also proposes the closed domain qa system for handling documents related to education acts sections to retrieve more precise answers using nlp techniques.","Task":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"],"Method":["question answering system","information retrieval","natural language processing","question answering","education acts","query answering system","artificial intelligence","Information retrieval","questions","education act"]},{"title":"Natural Language Processing and Language Learning","abstract":"As a relatively young field of research and development started by work on cryptanalysis and machine translation around 50 years ago, Natural Language Processing (NLP) is concerned with the automated processing of human language. It addresses the analysis and generation of written and spoken language, though speech processing is often regarded as a separate subfield. NLP emphasizes processing and applications and as such can be seen as the applied side of Computational Linguistics, the interdisciplinary field of research concerned with formal analysis and modeling of language and its applications at the intersection of Linguistics, Computer Science, and Psychology. In terms of the language aspects dealt with in NLP, traditionally lexical, morphological and syntactic aspects of language were at the center of attention, but aspects of meaning, discourse, and the relation to the extra-linguistic context have become increasingly prominent in the last decade. A good introduction and overview of the field is provided in Jurafsky & Martin (2009).","url":"https://www.semanticscholar.org/paper/f8868be52c8c0d64920e5f67870c01ba48601608","sentence":"Title: natural language processing and language learning Abstract: as a relatively young field of research and development started by work on cryptanalysis and machine translation around 50 years ago, natural language processing (nlp) is concerned with the automated processing of human language. it addresses the analysis and generation of written and spoken language, though speech processing is often regarded as a separate subfield. nlp emphasizes processing and applications and as such can be seen as the applied side of computational linguistics, the interdisciplinary field of research concerned with formal analysis and modeling of language and its applications at the intersection of linguistics, computer science, and psychology. in terms of the language aspects dealt with in nlp, traditionally lexical, morphological and syntactic aspects of language were at the center of attention, but aspects of meaning, discourse, and the relation to the extra-linguistic context have become increasingly prominent in the last decade. a good introduction and overview of the field is provided in jurafsky & martin (2009).","Task":["language learning","natural language processing","natural language processing and language learning","machine learning","machine translation","language","artificial intelligence","human language","computer vision","language processing"],"Method":["language learning","natural language processing","natural language processing and language learning","machine learning","machine translation","language","artificial intelligence","human language","computer vision","language processing"]},{"title":"Modeling the Relationship between User Comments and Edits in Document Revision","abstract":"Management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a document\u2019s evolution. Reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. A number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. Thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: Comment Ranking and Edit Anchoring. We begin by collecting a dataset with more than half a million comment-edit pairs based on Wikipedia revision histories. We then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. Our architecture tackles both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. In a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. We are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for Comment Ranking, while we achieve 74.4% accuracy on Edit Anchoring.","url":"https://www.semanticscholar.org/paper/311381feeb6346bfcb2ba622bd8f713261a4075d","sentence":"Title: modeling the relationship between user comments and edits in document revision Abstract: management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a document\u2019s evolution. reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. a number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: comment ranking and edit anchoring. we begin by collecting a dataset with more than half a million comment-edit pairs based on wikipedia revision histories. we then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. our architecture tackles both comment ranking and edit anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. in a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. we are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for comment ranking, while we achieve 74.4% accuracy on edit anchoring.","Task":["document revision","deep neural networks","deep learning","annotation","deep neural network","machine learning","Deep neural networks","edit anchoring","Deep learning","collaboration"],"Method":["document revision","deep neural networks","deep learning","annotation","deep neural network","machine learning","Deep neural networks","edit anchoring","Deep learning","collaboration"]},{"title":"A Multimodal Human-Computer Interaction System and Its Application in Smart Learning Environments","abstract":"A multimodal human-computer interaction system is composed of the comprehensive usage of various input and output channels. For the information input, apart from the traditional keyboard typing, mouse clicking, screen touching, the latest speech and face recognition technology can be used. For the output, the traditional screen display, the latest speech and facial expression synthesis and gesture generation can be used. After literature review of related works, this paper at first presents such a system, MMISE (Multimodal Interaction System for Education), about its architecture and working mechanism, POOOIIM (Pedagogical Objective Oriented Output, Input and Implementation Mechanism) illustrated with practical examples. Then this paper introduces this system\u2019s pilot applications in the epidemic time of novel coronavirus in 2020.","url":"https://www.semanticscholar.org/paper/d114af5d3dcb3792bd9faec1476feba5aaf3617f","sentence":"Title: a multimodal human-computer interaction system and its application in smart learning environments Abstract: a multimodal human-computer interaction system is composed of the comprehensive usage of various input and output channels. for the information input, apart from the traditional keyboard typing, mouse clicking, screen touching, the latest speech and face recognition technology can be used. for the output, the traditional screen display, the latest speech and facial expression synthesis and gesture generation can be used. after literature review of related works, this paper at first presents such a system, mmise (multimodal interaction system for education), about its architecture and working mechanism, poooiim (pedagogical objective oriented output, input and implementation mechanism) illustrated with practical examples. then this paper introduces this system\u2019s pilot applications in the epidemic time of novel coronavirus in 2020.","Task":["multimodal interaction system","multimodal interaction","human-computer interaction","multimodal human-computer interaction system","multimodal human-computer interaction","education","Human-computer interaction","AI","artificial intelligence","learning"],"Method":["multimodal interaction system","multimodal interaction","human-computer interaction","multimodal human-computer interaction system","multimodal human-computer interaction","education","Human-computer interaction","AI","artificial intelligence","learning"]},{"title":"What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations","abstract":"The quality of a counseling intervention relies highly on the active collaboration between clients and counselors. In this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. Specifically, we address the differences between high-quality and low-quality counseling. Our approach examines participants\u2019 turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. Our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.","url":"https://www.semanticscholar.org/paper/9b0c9d241269b98c80f65a14d5d65263d0688d70","sentence":"Title: what makes a good counselor? learning to distinguish between high-quality and low-quality counseling conversations Abstract: the quality of a counseling intervention relies highly on the active collaboration between clients and counselors. in this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. specifically, we address the differences between high-quality and low-quality counseling. our approach examines participants\u2019 turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. these features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.","Task":["counseling","classification","high-quality counseling","natural language processing","collaboration","communication","data mining","counseling conversations","machine learning","text mining"],"Method":["counseling","classification","high-quality counseling","natural language processing","collaboration","communication","data mining","counseling conversations","machine learning","text mining"]},{"title":"Quantifying the Effects of COVID-19 on Mental Health Support Forums","abstract":"The COVID-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. Understanding its impact can inform strategies for mitigating negative consequences. In this work, we seek to better understand the effects of COVID-19 on mental health by examining discussions within mental health support communities on Reddit. First, we quantify the rate at which COVID-19 is discussed in each community, or subreddit, in order to understand levels of preoccupation with the pandemic. Next, we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen. Finally, we analyze how COVID-19 has influenced language use and topics of discussion within each subreddit.","url":"https://www.semanticscholar.org/paper/aa2bbf5a1485cc90401e4a35c04463fcb4e632f0","sentence":"Title: quantifying the effects of covid-19 on mental health support forums Abstract: the covid-19 pandemic, like many of the disease outbreaks that have preceded it, is likely to have a profound effect on mental health. understanding its impact can inform strategies for mitigating negative consequences. in this work, we seek to better understand the effects of covid-19 on mental health by examining discussions within mental health support communities on reddit. first, we quantify the rate at which covid-19 is discussed in each community, or subreddit, in order to understand levels of preoccupation with the pandemic. next, we examine the volume of activity in order to determine whether the quantity of people seeking online mental health support has risen. finally, we analyze how covid-19 has influenced language use and topics of discussion within each subreddit.","Task":["covid-19","Covid-19","mental health support forums","social media","data mining","mental health","the covid-19 pandemic","social networks","mental health support","reddit"],"Method":["covid-19","Covid-19","mental health support forums","social media","data mining","mental health","the covid-19 pandemic","social networks","mental health support","reddit"]},{"title":"Data Mining and Student e-Learning Profiles","abstract":"Data mining techniques have been applied to educational research in various ways. In this paper, I presented the application of sequential data mining algorithms to analyze computer logs to profile learners in terms of their learning tactic use and motivation in a web-based learning environment (gStudy). The data mining algorithms are employed to discover patterns which characterize learners either across session or groups based on their study tactic choice and goal orientation. The use of this method is illustrated through a sequential pattern analysis of gStudy log files generated by university students.","url":"https://www.semanticscholar.org/paper/81d13af7eb78725492854bb9860b57af2b2a1778","sentence":"Title: data mining and student e-learning profiles Abstract: data mining techniques have been applied to educational research in various ways. in this paper, i presented the application of sequential data mining algorithms to analyze computer logs to profile learners in terms of their learning tactic use and motivation in a web-based learning environment (gstudy). the data mining algorithms are employed to discover patterns which characterize learners either across session or groups based on their study tactic choice and goal orientation. the use of this method is illustrated through a sequential pattern analysis of gstudy log files generated by university students.","Task":["data mining","Data mining","machine learning","data mining algorithms","data mining techniques","learning","text mining","statistics","data mining,","artificial intelligence"],"Method":["data mining","Data mining","machine learning","data mining algorithms","data mining techniques","learning","text mining","statistics","data mining,","artificial intelligence"]},{"title":"Inferring Social Media Users\u2019 Mental Health Status from Multimodal Information","abstract":"Worldwide, an increasing number of people are suffering from mental health disorders such as depression and anxiety. In the United States alone, one in every four adults suffers from a mental health condition, which makes mental health a pressing concern. In this paper, we explore the use of multimodal cues present in social media posts to predict users\u2019 mental health status. Specifically, we focus on identifying social media activity that either indicates a mental health condition or its onset. We collect posts from Flickr and apply a multimodal approach that consists of jointly analyzing language, visual, and metadata cues and their relation to mental health. We conduct several classification experiments aiming to discriminate between (1) healthy users and users affected by a mental health illness; and (2) healthy users and users prone to mental illness. Our experimental results indicate that using multiple modalities can improve the performance of this classification task as compared to the use of one modality at a time, and can provide important cues into a user\u2019s mental status.","url":"https://www.semanticscholar.org/paper/065332576fd9b264a5a338549239bb39e6733819","sentence":"Title: inferring social media users\u2019 mental health status from multimodal information Abstract: worldwide, an increasing number of people are suffering from mental health disorders such as depression and anxiety. in the united states alone, one in every four adults suffers from a mental health condition, which makes mental health a pressing concern. in this paper, we explore the use of multimodal cues present in social media posts to predict users\u2019 mental health status. specifically, we focus on identifying social media activity that either indicates a mental health condition or its onset. we collect posts from flickr and apply a multimodal approach that consists of jointly analyzing language, visual, and metadata cues and their relation to mental health. we conduct several classification experiments aiming to discriminate between (1) healthy users and users affected by a mental health illness; and (2) healthy users and users prone to mental illness. our experimental results indicate that using multiple modalities can improve the performance of this classification task as compared to the use of one modality at a time, and can provide important cues into a user\u2019s mental status.","Task":["social media","multimodal information","mental health","classification","Social media","multimodal cues","multimodality","social media classification","neural networks","mental health classification"],"Method":["social media","multimodal information","mental health","classification","Social media","multimodal cues","multimodality","social media classification","neural networks","mental health classification"]},{"title":"Expressive Interviewing: A Conversational System for Coping with COVID-19","abstract":"The ongoing COVID-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. Alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. We introduce \\\\textit{Expressive Interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. Expressive Interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how COVID-19 has impacted their lives. We present relevant aspects of the system\'s design and implementation as well as quantitative and qualitative analyses of user interactions with the system. In addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with COVID-19 issues.","url":"https://www.semanticscholar.org/paper/c278eb333c78313a3762b5b2f07ace7443582b94","sentence":"Title: expressive interviewing: a conversational system for coping with covid-19 Abstract: the ongoing covid-19 pandemic has raised concerns for many regarding personal and public health implications, financial security and economic stability. alongside many other unprecedented challenges, there are increasing concerns over social isolation and mental health. we introduce \\\\textit{expressive interviewing}--an interview-style conversational system that draws on ideas from motivational interviewing and expressive writing. expressive interviewing seeks to encourage users to express their thoughts and feelings through writing by asking them questions about how covid-19 has impacted their lives. we present relevant aspects of the system\'s design and implementation as well as quantitative and qualitative analyses of user interactions with the system. in addition, we conduct a comparative evaluation with a general purpose dialogue system for mental health that shows our system potential in helping users to cope with covid-19 issues.","Task":["expressive interviewing","covid-19","conversational systems","Covid-19","conversational system","interviewing","communication","conversational interviewing","natural language processing","textit"],"Method":["expressive interviewing","covid-19","conversational systems","Covid-19","conversational system","interviewing","communication","conversational interviewing","natural language processing","textit"]},{"title":"Understanding and Predicting Empathic Behavior in Counseling Therapy","abstract":"Counselor empathy is associated with better outcomes in psychology and behavioral counseling. In this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. Particularly, we analyze aspects such as participants\u2019 engagement, participants\u2019 verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. We also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80%.","url":"https://www.semanticscholar.org/paper/35112c6bfa715b2bc86fc5b6c331f8a1a0b07add","sentence":"Title: understanding and predicting empathic behavior in counseling therapy Abstract: counselor empathy is associated with better outcomes in psychology and behavioral counseling. in this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. particularly, we analyze aspects such as participants\u2019 engagement, participants\u2019 verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. we also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80%.","Task":["motivational interviewing","empathy","counselor empathy","empathic behavior","counseling interaction dynamics","Motivational interviewing","counseling","emotional intelligence","empathy prediction","Empathy"],"Method":["motivational interviewing","empathy","counselor empathy","empathic behavior","counseling interaction dynamics","Motivational interviewing","counseling","emotional intelligence","empathy prediction","Empathy"]},{"title":"Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health","abstract":"Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.","url":"https://www.semanticscholar.org/paper/c151f144c2c0e8d3b176edaf2ce5369c7707bd31","sentence":"Title: large-scale analysis of counseling conversations: an application of natural language processing to mental health Abstract: mental illness is one of the most pressing public health issues of our time. while counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. in this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. we develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.","Task":["discourse analysis","mental health","counseling conversations","computational discourse analysis","natural language processing","conversation analysis","large-scale analysis of counseling conversations","psycholinguistics","counseling","psychology"],"Method":["discourse analysis","mental health","counseling conversations","computational discourse analysis","natural language processing","conversation analysis","large-scale analysis of counseling conversations","psycholinguistics","counseling","psychology"]},{"title":"Fermi at SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media using Sentence Embeddings","abstract":"This paper describes our system (Fermi) for Task 6: OffensEval: Identifying and Categorizing Offensive Language in Social Media of SemEval-2019. We participated in all the three sub-tasks within Task 6. We evaluate multiple sentence embeddings in conjunction with various supervised machine learning algorithms and evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team Fermi\u2019s model achieved an F1-score of 64.40%, 62.00% and 62.60% for sub-task A, B and C respectively on the official leaderboard. Our model for sub-task C which uses pre-trained ELMo embeddings for transforming the input and uses SVM (RBF kernel) for training, scored third position on the official leaderboard. Through the paper we provide a detailed description of the approach, as well as the results obtained for the task.","url":"https://www.semanticscholar.org/paper/9de154d3c886177380062be7c8d50304a335752f","sentence":"Title: fermi at semeval-2019 task 6: identifying and categorizing offensive language in social media using sentence embeddings Abstract: this paper describes our system (fermi) for task 6: offenseval: identifying and categorizing offensive language in social media of semeval-2019. we participated in all the three sub-tasks within task 6. we evaluate multiple sentence embeddings in conjunction with various supervised machine learning algorithms and evaluate the performance of simple yet effective embedding-ml combination algorithms. our team fermi\u2019s model achieved an f1-score of 64.40%, 62.00% and 62.60% for sub-task a, b and c respectively on the official leaderboard. our model for sub-task c which uses pre-trained elmo embeddings for transforming the input and uses svm (rbf kernel) for training, scored third position on the official leaderboard. through the paper we provide a detailed description of the approach, as well as the results obtained for the task.","Task":["machine learning","social media","semeval-2019","sentence embeddings","reinforcement learning","semeval","semeval 2019","Machine learning","computer vision","task 6"],"Method":["machine learning","social media","semeval-2019","sentence embeddings","reinforcement learning","semeval","semeval 2019","Machine learning","computer vision","task 6"]},{"title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","sentence":"Title: building a motivational interviewing dataset Abstract: this paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during motivational interviewing encounters. annotations were conducted using the motivational interviewing integrity treatment (miti). we describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. the dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. the reliability analysis showed that annotators achieved excellent agreement at session level, with intraclass correlation coefficient (icc) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with cohen\u2019s kappa scores ranging from 0.31 to 0.64. behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (resnicow et al., 2002). in particular, motivational interviewing (mi), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (moyers et al., 2009; apodaca et al., 2014; barnett et al., 2014; catley et al., 2012). despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing mi counseling at larger scale or in other domains is limited by the need for human-based evaluations. currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. recently, computational approaches have been proposed to aid the mi evaluation process (atkins et al., 2014; xiao et al., 2014; klonek et al., 2015). however, learning resources for this task are not readily available. having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of mi. moreover, this can also be useful to explore how mi works by relating mi behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their mi skills. in this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the motivational interviewing treatment integrity 4.0 (miti), which is the current gold standard for mi-based psychology interventions. the dataset is derived from 277 mi sessions containing a total of 22,719 coded utterances. 1 motivational interviewing miller and rollnick define mi as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (miller and rollnick, 2013). mi has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Task":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"],"Method":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"]},{"title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","sentence":"Title: building a motivational interviewing dataset Abstract: this paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during motivational interviewing encounters. annotations were conducted using the motivational interviewing integrity treatment (miti). we describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. the dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. the reliability analysis showed that annotators achieved excellent agreement at session level, with intraclass correlation coefficient (icc) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with cohen\u2019s kappa scores ranging from 0.31 to 0.64. behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (resnicow et al., 2002). in particular, motivational interviewing (mi), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (moyers et al., 2009; apodaca et al., 2014; barnett et al., 2014; catley et al., 2012). despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing mi counseling at larger scale or in other domains is limited by the need for human-based evaluations. currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. recently, computational approaches have been proposed to aid the mi evaluation process (atkins et al., 2014; xiao et al., 2014; klonek et al., 2015). however, learning resources for this task are not readily available. having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of mi. moreover, this can also be useful to explore how mi works by relating mi behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their mi skills. in this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the motivational interviewing treatment integrity 4.0 (miti), which is the current gold standard for mi-based psychology interventions. the dataset is derived from 277 mi sessions containing a total of 22,719 coded utterances. 1 motivational interviewing miller and rollnick define mi as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (miller and rollnick, 2013). mi has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Task":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"],"Method":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"]},{"title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","sentence":"Title: building a motivational interviewing dataset Abstract: this paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during motivational interviewing encounters. annotations were conducted using the motivational interviewing integrity treatment (miti). we describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. the dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. the reliability analysis showed that annotators achieved excellent agreement at session level, with intraclass correlation coefficient (icc) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with cohen\u2019s kappa scores ranging from 0.31 to 0.64. behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (resnicow et al., 2002). in particular, motivational interviewing (mi), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (moyers et al., 2009; apodaca et al., 2014; barnett et al., 2014; catley et al., 2012). despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing mi counseling at larger scale or in other domains is limited by the need for human-based evaluations. currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. recently, computational approaches have been proposed to aid the mi evaluation process (atkins et al., 2014; xiao et al., 2014; klonek et al., 2015). however, learning resources for this task are not readily available. having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of mi. moreover, this can also be useful to explore how mi works by relating mi behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their mi skills. in this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the motivational interviewing treatment integrity 4.0 (miti), which is the current gold standard for mi-based psychology interventions. the dataset is derived from 277 mi sessions containing a total of 22,719 coded utterances. 1 motivational interviewing miller and rollnick define mi as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (miller and rollnick, 2013). mi has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Task":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"],"Method":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"]},{"title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","sentence":"Title: building a motivational interviewing dataset Abstract: this paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during motivational interviewing encounters. annotations were conducted using the motivational interviewing integrity treatment (miti). we describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. the dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. the reliability analysis showed that annotators achieved excellent agreement at session level, with intraclass correlation coefficient (icc) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with cohen\u2019s kappa scores ranging from 0.31 to 0.64. behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (resnicow et al., 2002). in particular, motivational interviewing (mi), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (moyers et al., 2009; apodaca et al., 2014; barnett et al., 2014; catley et al., 2012). despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing mi counseling at larger scale or in other domains is limited by the need for human-based evaluations. currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. recently, computational approaches have been proposed to aid the mi evaluation process (atkins et al., 2014; xiao et al., 2014; klonek et al., 2015). however, learning resources for this task are not readily available. having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of mi. moreover, this can also be useful to explore how mi works by relating mi behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their mi skills. in this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the motivational interviewing treatment integrity 4.0 (miti), which is the current gold standard for mi-based psychology interventions. the dataset is derived from 277 mi sessions containing a total of 22,719 coded utterances. 1 motivational interviewing miller and rollnick define mi as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (miller and rollnick, 2013). mi has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Task":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"],"Method":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"]},{"title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","sentence":"Title: building a motivational interviewing dataset Abstract: this paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during motivational interviewing encounters. annotations were conducted using the motivational interviewing integrity treatment (miti). we describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. the dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. the reliability analysis showed that annotators achieved excellent agreement at session level, with intraclass correlation coefficient (icc) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with cohen\u2019s kappa scores ranging from 0.31 to 0.64. behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (resnicow et al., 2002). in particular, motivational interviewing (mi), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (moyers et al., 2009; apodaca et al., 2014; barnett et al., 2014; catley et al., 2012). despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing mi counseling at larger scale or in other domains is limited by the need for human-based evaluations. currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. recently, computational approaches have been proposed to aid the mi evaluation process (atkins et al., 2014; xiao et al., 2014; klonek et al., 2015). however, learning resources for this task are not readily available. having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of mi. moreover, this can also be useful to explore how mi works by relating mi behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their mi skills. in this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the motivational interviewing treatment integrity 4.0 (miti), which is the current gold standard for mi-based psychology interventions. the dataset is derived from 277 mi sessions containing a total of 22,719 coded utterances. 1 motivational interviewing miller and rollnick define mi as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (miller and rollnick, 2013). mi has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Task":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"],"Method":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"]},{"title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","sentence":"Title: building a motivational interviewing dataset Abstract: this paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during motivational interviewing encounters. annotations were conducted using the motivational interviewing integrity treatment (miti). we describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. the dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. the reliability analysis showed that annotators achieved excellent agreement at session level, with intraclass correlation coefficient (icc) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with cohen\u2019s kappa scores ranging from 0.31 to 0.64. behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (resnicow et al., 2002). in particular, motivational interviewing (mi), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (moyers et al., 2009; apodaca et al., 2014; barnett et al., 2014; catley et al., 2012). despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing mi counseling at larger scale or in other domains is limited by the need for human-based evaluations. currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. recently, computational approaches have been proposed to aid the mi evaluation process (atkins et al., 2014; xiao et al., 2014; klonek et al., 2015). however, learning resources for this task are not readily available. having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of mi. moreover, this can also be useful to explore how mi works by relating mi behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their mi skills. in this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the motivational interviewing treatment integrity 4.0 (miti), which is the current gold standard for mi-based psychology interventions. the dataset is derived from 277 mi sessions containing a total of 22,719 coded utterances. 1 motivational interviewing miller and rollnick define mi as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (miller and rollnick, 2013). mi has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Task":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"],"Method":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"]},{"title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","sentence":"Title: building a motivational interviewing dataset Abstract: this paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during motivational interviewing encounters. annotations were conducted using the motivational interviewing integrity treatment (miti). we describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. the dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. the reliability analysis showed that annotators achieved excellent agreement at session level, with intraclass correlation coefficient (icc) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with cohen\u2019s kappa scores ranging from 0.31 to 0.64. behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (resnicow et al., 2002). in particular, motivational interviewing (mi), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (moyers et al., 2009; apodaca et al., 2014; barnett et al., 2014; catley et al., 2012). despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing mi counseling at larger scale or in other domains is limited by the need for human-based evaluations. currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. recently, computational approaches have been proposed to aid the mi evaluation process (atkins et al., 2014; xiao et al., 2014; klonek et al., 2015). however, learning resources for this task are not readily available. having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of mi. moreover, this can also be useful to explore how mi works by relating mi behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their mi skills. in this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the motivational interviewing treatment integrity 4.0 (miti), which is the current gold standard for mi-based psychology interventions. the dataset is derived from 277 mi sessions containing a total of 22,719 coded utterances. 1 motivational interviewing miller and rollnick define mi as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (miller and rollnick, 2013). mi has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Task":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"],"Method":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"]},{"title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","sentence":"Title: building a motivational interviewing dataset Abstract: this paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during motivational interviewing encounters. annotations were conducted using the motivational interviewing integrity treatment (miti). we describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. the dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. the reliability analysis showed that annotators achieved excellent agreement at session level, with intraclass correlation coefficient (icc) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with cohen\u2019s kappa scores ranging from 0.31 to 0.64. behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (resnicow et al., 2002). in particular, motivational interviewing (mi), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (moyers et al., 2009; apodaca et al., 2014; barnett et al., 2014; catley et al., 2012). despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing mi counseling at larger scale or in other domains is limited by the need for human-based evaluations. currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. recently, computational approaches have been proposed to aid the mi evaluation process (atkins et al., 2014; xiao et al., 2014; klonek et al., 2015). however, learning resources for this task are not readily available. having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of mi. moreover, this can also be useful to explore how mi works by relating mi behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their mi skills. in this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the motivational interviewing treatment integrity 4.0 (miti), which is the current gold standard for mi-based psychology interventions. the dataset is derived from 277 mi sessions containing a total of 22,719 coded utterances. 1 motivational interviewing miller and rollnick define mi as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (miller and rollnick, 2013). mi has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Task":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"],"Method":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"]},{"title":"Building a Motivational Interviewing Dataset","abstract":"This paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during Motivational Interviewing encounters. Annotations were conducted using the Motivational Interviewing Integrity Treatment (MITI). We describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The reliability analysis showed that annotators achieved excellent agreement at session level, with Intraclass Correlation Coefficient (ICC) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with Cohen\u2019s Kappa scores ranging from 0.31 to 0.64. Behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (Resnicow et al., 2002). In particular, Motivational Interviewing (MI), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (Moyers et al., 2009; Apodaca et al., 2014; Barnett et al., 2014; Catley et al., 2012). Despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing MI counseling at larger scale or in other domains is limited by the need for human-based evaluations. Currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. Recently, computational approaches have been proposed to aid the MI evaluation process (Atkins et al., 2014; Xiao et al., 2014; Klonek et al., 2015). However, learning resources for this task are not readily available. Having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of MI. Moreover, this can also be useful to explore how MI works by relating MI behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their MI skills. In this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the Motivational Interviewing Treatment Integrity 4.0 (MITI), which is the current gold standard for MI-based psychology interventions. The dataset is derived from 277 MI sessions containing a total of 22,719 coded utterances. 1 Motivational Interviewing Miller and Rollnick define MI as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (Miller and Rollnick, 2013). MI has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","url":"https://www.semanticscholar.org/paper/b688b67aa1980225e72e81f3d971d0e97a0d5484","sentence":"Title: building a motivational interviewing dataset Abstract: this paper contributes a novel psychological dataset consisting of counselors\u2019 behaviors during motivational interviewing encounters. annotations were conducted using the motivational interviewing integrity treatment (miti). we describe relevant aspects associated with the construction of a dataset that relies on behavioral coding such as data acquisition, transcription, expert data annotations, and reliability assessments. the dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. the reliability analysis showed that annotators achieved excellent agreement at session level, with intraclass correlation coefficient (icc) scores in the range of 0.75 to 1, and fair to good agreement at utterance level, with cohen\u2019s kappa scores ranging from 0.31 to 0.64. behavioral interventions are a promising approach to address public health issues such as smoking cessation, increasing physical activity, and reducing substance abuse, among others (resnicow et al., 2002). in particular, motivational interviewing (mi), a client centered psychotherapy style, has been receiving increasing attention from the clinical psychology community due to its established efficacy for treating addiction and other behaviors (moyers et al., 2009; apodaca et al., 2014; barnett et al., 2014; catley et al., 2012). despite its potential benefits in combating addiction and in providing broader disease prevention and management, implementing mi counseling at larger scale or in other domains is limited by the need for human-based evaluations. currently, this requires a human either watching or listening to video-tapes and then providing evaluative feedback. recently, computational approaches have been proposed to aid the mi evaluation process (atkins et al., 2014; xiao et al., 2014; klonek et al., 2015). however, learning resources for this task are not readily available. having such resources will enable the application of data-driven strategies for the automatic coding of counseling behaviors, thus providing researchers with automatic means for the evaluation of mi. moreover, this can also be useful to explore how mi works by relating mi behaviors to health outcomes, and to provide counselors with evaluative feedback that helps them improve their mi skills. in this paper, we present the construction and validation of a dataset annotated with counselor verbal behaviours using the motivational interviewing treatment integrity 4.0 (miti), which is the current gold standard for mi-based psychology interventions. the dataset is derived from 277 mi sessions containing a total of 22,719 coded utterances. 1 motivational interviewing miller and rollnick define mi as a collaborative, goal-oriented style of psychotherapy with particular attention to the language of change (miller and rollnick, 2013). mi has been widely used as a treatment method in clinical trials on psychotherapy research to address addictive behaviors such as alcohol, tobacco and drug use; promote healthier habits such as nutrition and fitness; and help clients with","Task":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"],"Method":["motivational interviewing","Motivational interviewing","motivational interviewing dataset","motivational interviewing,","motivational interviewing (mi)","motivational interviewing data","Motivational Interviewing","psychology","Motivational interviewing dataset","motivational interviews"]},{"title":"Predicting Counselor Behaviors in Motivational Interviewing Encounters","abstract":"As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","url":"https://www.semanticscholar.org/paper/55925bc2522a1d4cb007ac78273f7c2efafa3916","sentence":"Title: predicting counselor behaviors in motivational interviewing encounters Abstract: as the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. in this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. in particular, we present a model towards the automation of motivational interviewing (mi) coding, which is the current gold standard to evaluate mi counseling. first, we build a dataset of hand labeled mi encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. we introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","Task":["automatic evaluation","motivational interviewing","automatic evaluation of counseling performance","automatic evaluation of counseling practice","automatic evaluation of counseling","prediction","automatic evaluation of counseling behavior","computer vision","automated evaluation","machine learning"],"Method":["automatic evaluation","motivational interviewing","automatic evaluation of counseling performance","automatic evaluation of counseling practice","automatic evaluation of counseling","prediction","automatic evaluation of counseling behavior","computer vision","automated evaluation","machine learning"]},{"title":"Predicting Counselor Behaviors in Motivational Interviewing Encounters","abstract":"As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","url":"https://www.semanticscholar.org/paper/55925bc2522a1d4cb007ac78273f7c2efafa3916","sentence":"Title: predicting counselor behaviors in motivational interviewing encounters Abstract: as the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. in this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. in particular, we present a model towards the automation of motivational interviewing (mi) coding, which is the current gold standard to evaluate mi counseling. first, we build a dataset of hand labeled mi encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. we introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","Task":["automatic evaluation","motivational interviewing","automatic evaluation of counseling performance","automatic evaluation of counseling practice","automatic evaluation of counseling","prediction","automatic evaluation of counseling behavior","computer vision","automated evaluation","machine learning"],"Method":["automatic evaluation","motivational interviewing","automatic evaluation of counseling performance","automatic evaluation of counseling practice","automatic evaluation of counseling","prediction","automatic evaluation of counseling behavior","computer vision","automated evaluation","machine learning"]},{"title":"Predicting Counselor Behaviors in Motivational Interviewing Encounters","abstract":"As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","url":"https://www.semanticscholar.org/paper/55925bc2522a1d4cb007ac78273f7c2efafa3916","sentence":"Title: predicting counselor behaviors in motivational interviewing encounters Abstract: as the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. in this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. in particular, we present a model towards the automation of motivational interviewing (mi) coding, which is the current gold standard to evaluate mi counseling. first, we build a dataset of hand labeled mi encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. we introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","Task":["automatic evaluation","motivational interviewing","automatic evaluation of counseling performance","automatic evaluation of counseling practice","automatic evaluation of counseling","prediction","automatic evaluation of counseling behavior","computer vision","automated evaluation","machine learning"],"Method":["automatic evaluation","motivational interviewing","automatic evaluation of counseling performance","automatic evaluation of counseling practice","automatic evaluation of counseling","prediction","automatic evaluation of counseling behavior","computer vision","automated evaluation","machine learning"]},{"title":"Predicting Counselor Behaviors in Motivational Interviewing Encounters","abstract":"As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","url":"https://www.semanticscholar.org/paper/55925bc2522a1d4cb007ac78273f7c2efafa3916","sentence":"Title: predicting counselor behaviors in motivational interviewing encounters Abstract: as the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. in this paper, we address the automatic evaluation of counseling performance by analyzing counselors\u2019 language during their interaction with clients. in particular, we present a model towards the automation of motivational interviewing (mi) coding, which is the current gold standard to evaluate mi counseling. first, we build a dataset of hand labeled mi encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. we introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.","Task":["automatic evaluation","motivational interviewing","automatic evaluation of counseling performance","automatic evaluation of counseling practice","automatic evaluation of counseling","prediction","automatic evaluation of counseling behavior","computer vision","automated evaluation","machine learning"],"Method":["automatic evaluation","motivational interviewing","automatic evaluation of counseling performance","automatic evaluation of counseling practice","automatic evaluation of counseling","prediction","automatic evaluation of counseling behavior","computer vision","automated evaluation","machine learning"]},{"title":"Happiness Entailment: Automating Suggestions for Well-Being","abstract":"Understanding what makes people happy is a central topic in psychology. Prior work has mostly focused on developing self-reporting assessment tools for individuals and relies on experts to analyze the periodic reported assessments. One of the goals of the analysis is to understand what actions are necessary to encourage modifications in the behaviors of the individuals to improve their overall well-being. In this paper, we outline a complementary approach; on the assumption that the user journals her happy moments as short texts, a system can analyze these texts and propose sustainable suggestions for the user that may lead to an overall improvement in her well-being. We prototype one necessary component of such a system, the Happiness Entailment Recognition (HER)module, which takes as input a short text describing an event, a candidate suggestion, and outputs a determination about whether the suggestion is more likely to be good for this user based on the event described. This component is implemented as a neural network model with two encoders, one for the user input and one for the candidate actionable suggestion, with additional layers to capture psychologically significant features in the happy moment and suggestion. Our model achieves an AU-ROC of 0.831 and outperforms our baseline as well as the current state-of-the-art Textual Entailment model from AllenNLP by more than 48% of improvements, confirming the uniqueness and complexity of the HER task.","url":"https://www.semanticscholar.org/paper/c2fd13dab7a581f2f875826b204d063f42b63a8b","sentence":"Title: happiness entailment: automating suggestions for well-being Abstract: understanding what makes people happy is a central topic in psychology. prior work has mostly focused on developing self-reporting assessment tools for individuals and relies on experts to analyze the periodic reported assessments. one of the goals of the analysis is to understand what actions are necessary to encourage modifications in the behaviors of the individuals to improve their overall well-being. in this paper, we outline a complementary approach; on the assumption that the user journals her happy moments as short texts, a system can analyze these texts and propose sustainable suggestions for the user that may lead to an overall improvement in her well-being. we prototype one necessary component of such a system, the happiness entailment recognition (her)module, which takes as input a short text describing an event, a candidate suggestion, and outputs a determination about whether the suggestion is more likely to be good for this user based on the event described. this component is implemented as a neural network model with two encoders, one for the user input and one for the candidate actionable suggestion, with additional layers to capture psychologically significant features in the happy moment and suggestion. our model achieves an au-roc of 0.831 and outperforms our baseline as well as the current state-of-the-art textual entailment model from allennlp by more than 48% of improvements, confirming the uniqueness and complexity of the her task.","Task":["happiness","neural networks","emotion recognition","psychology","machine learning","neural network","artificial intelligence","emotional intelligence","well-being","AI"],"Method":["happiness","neural networks","emotion recognition","psychology","machine learning","neural network","artificial intelligence","emotional intelligence","well-being","AI"]},{"title":"FERMI at SemEval-2019 Task 5: Using Sentence embeddings to Identify Hate Speech Against Immigrants and Women in Twitter","abstract":"This paper describes our system (Fermi) for Task 5 of SemEval-2019: HatEval: Multilingual Detection of Hate Speech Against Immigrants and Women on Twitter. We participated in the subtask A for English and ranked first in the evaluation on the test set. We evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ML combination algorithms. Our team - Fermi\u2019s model achieved an accuracy of 65.00% for English language in task A. Our models, which use pretrained Universal Encoder sentence embeddings for transforming the input and SVM (with RBF kernel) for classification, scored first position (among 68) in the leaderboard on the test set for Subtask A in English language. In this paper we provide a detailed description of the approach, as well as the results obtained in the task.","url":"https://www.semanticscholar.org/paper/64c68fa52491c4a815f21917e068e4c19dd404b0","sentence":"Title: fermi at semeval-2019 task 5: using sentence embeddings to identify hate speech against immigrants and women in twitter Abstract: this paper describes our system (fermi) for task 5 of semeval-2019: hateval: multilingual detection of hate speech against immigrants and women on twitter. we participated in the subtask a for english and ranked first in the evaluation on the test set. we evaluate the quality of multiple sentence embeddings and explore multiple training models to evaluate the performance of simple yet effective embedding-ml combination algorithms. our team - fermi\u2019s model achieved an accuracy of 65.00% for english language in task a. our models, which use pretrained universal encoder sentence embeddings for transforming the input and svm (with rbf kernel) for classification, scored first position (among 68) in the leaderboard on the test set for subtask a in english language. in this paper we provide a detailed description of the approach, as well as the results obtained in the task.","Task":["machine learning","sentence embeddings","speech recognition","semeval","semeval-2019","sentiment analysis","semeval 2019","semantic web","machine translation","data mining"],"Method":["machine learning","sentence embeddings","speech recognition","semeval","semeval-2019","sentiment analysis","semeval 2019","semantic web","machine translation","data mining"]},{"title":"Ingredients for Happiness: Modeling constructs via semi-supervised content driven inductive transfer","abstract":"Modeling affect via understanding the social constructs behind them is an important task in devising robust and accurate systems for socially relevant scenarios. In the CL-Aff Shared Task (part of Affective Content Analysis workshop @ AAAI 2019), the organizers released a dataset of \u2018happy\u2019 moments, called the HappyDB corpus. The task is to detect two social constructs: the agency (i.e., whether the author is in control of the happy moment) and the social characteristics (i.e., whether anyone else other than the author was also involved in the happy moment). We employ an inductive transfer learning technique where we utilize a pre-trained language model and fine-tune it on the target task for both the binary classification tasks. At first, we use a language model pre-trained on the huge WikiText-103 corpus. This step utilizes an AWDLSTM with three hidden layers for training the language model. In the second step, we fine-tune the pre-trained language model on both the labeled and unlabeled instances from the HappyDB dataset. Finally, we train a classifier on top of the language model for each of the identification tasks. Our experiments using 10-fold cross validation on the corpus show that we achieve a high accuracy of \u223c93% for detection of the social characteristic and \u223c87% for agency of the author, showing significant gains over other baselines. We also show that using the unlabeled dataset for fine-tuning the language model in the second step improves our accuracy by 1-2% across detection of both the constructs.","url":"https://www.semanticscholar.org/paper/7cb5bc9f8bc023f1f2c94e2e83012f1d35e4e01d","sentence":"Title: ingredients for happiness: modeling constructs via semi-supervised content driven inductive transfer Abstract: modeling affect via understanding the social constructs behind them is an important task in devising robust and accurate systems for socially relevant scenarios. in the cl-aff shared task (part of affective content analysis workshop @ aaai 2019), the organizers released a dataset of \u2018happy\u2019 moments, called the happydb corpus. the task is to detect two social constructs: the agency (i.e., whether the author is in control of the happy moment) and the social characteristics (i.e., whether anyone else other than the author was also involved in the happy moment). we employ an inductive transfer learning technique where we utilize a pre-trained language model and fine-tune it on the target task for both the binary classification tasks. at first, we use a language model pre-trained on the huge wikitext-103 corpus. this step utilizes an awdlstm with three hidden layers for training the language model. in the second step, we fine-tune the pre-trained language model on both the labeled and unlabeled instances from the happydb dataset. finally, we train a classifier on top of the language model for each of the identification tasks. our experiments using 10-fold cross validation on the corpus show that we achieve a high accuracy of \u223c93% for detection of the social characteristic and \u223c87% for agency of the author, showing significant gains over other baselines. we also show that using the unlabeled dataset for fine-tuning the language model in the second step improves our accuracy by 1-2% across detection of both the constructs.","Task":["affective content analysis","social constructs","inductive transfer learning","ingredients for happiness","happiness","emotions","sentiment analysis","affect","happydb","content driven inductive transfer"],"Method":["affective content analysis","social constructs","inductive transfer learning","ingredients for happiness","happiness","emotions","sentiment analysis","affect","happydb","content driven inductive transfer"]},{"title":"HappyDB: A Corpus of 100, 000 Crowdsourced Happy Moments","abstract":"The science of happiness is an area of positive psychology concerned with understanding what behaviors make people happy in a sustainable fashion. Recently, there has been interest in developing technologies that help incorporate the findings of the science of happiness into users\' daily lives by steering them towards behaviors that increase happiness. With the goal of building technology that can understand how people express their happy moments in text, we crowd-sourced HappyDB, a corpus of 100,000 happy moments that we make publicly available. This paper describes HappyDB and its properties, and outlines several important NLP problems that can be studied with the help of the corpus. We also apply several state-of-the-art analysis techniques to analyze HappyDB. Our results demonstrate the need for deeper NLP techniques to be developed which makes HappyDB an exciting resource for follow-on research.","url":"https://www.semanticscholar.org/paper/c239aaa2a87ea63fae666b2051f1923afaa45fc2","sentence":"Title: happydb: a corpus of 100, 000 crowdsourced happy moments Abstract: the science of happiness is an area of positive psychology concerned with understanding what behaviors make people happy in a sustainable fashion. recently, there has been interest in developing technologies that help incorporate the findings of the science of happiness into users\' daily lives by steering them towards behaviors that increase happiness. with the goal of building technology that can understand how people express their happy moments in text, we crowd-sourced happydb, a corpus of 100,000 happy moments that we make publicly available. this paper describes happydb and its properties, and outlines several important nlp problems that can be studied with the help of the corpus. we also apply several state-of-the-art analysis techniques to analyze happydb. our results demonstrate the need for deeper nlp techniques to be developed which makes happydb an exciting resource for follow-on research.","Task":["happydb","natural language processing","corpus analysis","the science of happiness","corpus","happiness","crowdsourcing","machine learning","sentiment analysis","artificial intelligence"],"Method":["happydb","natural language processing","corpus analysis","the science of happiness","corpus","happiness","crowdsourcing","machine learning","sentiment analysis","artificial intelligence"]},{"title":"HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","sentence":"Title: hopeedi: a multilingual hope speech detection dataset for equality, diversity, and inclusion Abstract: over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. however, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. until now, most studies have focused on solving this problem of negativity in the english language, though the problem is much more than just harmful content. furthermore, it is multilingual as well. thus, we have constructed a hope speech dataset for equality, diversity and inclusion (hopeedi) containing user-generated comments from the social media platform youtube with 28,451, 20,198 and 10,705 comments in english, tamil and malayalam, respectively, manually labelled as containing hope speech or not. to our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. we determined that the inter-annotator agreement of our dataset using krippendorff\u2019s alpha. further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and f1-score. the dataset is publicly available for the research community. we hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","Task":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"],"Method":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"]},{"title":"HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","sentence":"Title: hopeedi: a multilingual hope speech detection dataset for equality, diversity, and inclusion Abstract: over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. however, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. until now, most studies have focused on solving this problem of negativity in the english language, though the problem is much more than just harmful content. furthermore, it is multilingual as well. thus, we have constructed a hope speech dataset for equality, diversity and inclusion (hopeedi) containing user-generated comments from the social media platform youtube with 28,451, 20,198 and 10,705 comments in english, tamil and malayalam, respectively, manually labelled as containing hope speech or not. to our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. we determined that the inter-annotator agreement of our dataset using krippendorff\u2019s alpha. further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and f1-score. the dataset is publicly available for the research community. we hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","Task":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"],"Method":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"]},{"title":"HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","sentence":"Title: hopeedi: a multilingual hope speech detection dataset for equality, diversity, and inclusion Abstract: over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. however, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. until now, most studies have focused on solving this problem of negativity in the english language, though the problem is much more than just harmful content. furthermore, it is multilingual as well. thus, we have constructed a hope speech dataset for equality, diversity and inclusion (hopeedi) containing user-generated comments from the social media platform youtube with 28,451, 20,198 and 10,705 comments in english, tamil and malayalam, respectively, manually labelled as containing hope speech or not. to our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. we determined that the inter-annotator agreement of our dataset using krippendorff\u2019s alpha. further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and f1-score. the dataset is publicly available for the research community. we hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","Task":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"],"Method":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"]},{"title":"HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","sentence":"Title: hopeedi: a multilingual hope speech detection dataset for equality, diversity, and inclusion Abstract: over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. however, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. until now, most studies have focused on solving this problem of negativity in the english language, though the problem is much more than just harmful content. furthermore, it is multilingual as well. thus, we have constructed a hope speech dataset for equality, diversity and inclusion (hopeedi) containing user-generated comments from the social media platform youtube with 28,451, 20,198 and 10,705 comments in english, tamil and malayalam, respectively, manually labelled as containing hope speech or not. to our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. we determined that the inter-annotator agreement of our dataset using krippendorff\u2019s alpha. further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and f1-score. the dataset is publicly available for the research community. we hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","Task":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"],"Method":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"]},{"title":"HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","sentence":"Title: hopeedi: a multilingual hope speech detection dataset for equality, diversity, and inclusion Abstract: over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. however, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. until now, most studies have focused on solving this problem of negativity in the english language, though the problem is much more than just harmful content. furthermore, it is multilingual as well. thus, we have constructed a hope speech dataset for equality, diversity and inclusion (hopeedi) containing user-generated comments from the social media platform youtube with 28,451, 20,198 and 10,705 comments in english, tamil and malayalam, respectively, manually labelled as containing hope speech or not. to our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. we determined that the inter-annotator agreement of our dataset using krippendorff\u2019s alpha. further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and f1-score. the dataset is publicly available for the research community. we hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","Task":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"],"Method":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"]},{"title":"HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","sentence":"Title: hopeedi: a multilingual hope speech detection dataset for equality, diversity, and inclusion Abstract: over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. however, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. until now, most studies have focused on solving this problem of negativity in the english language, though the problem is much more than just harmful content. furthermore, it is multilingual as well. thus, we have constructed a hope speech dataset for equality, diversity and inclusion (hopeedi) containing user-generated comments from the social media platform youtube with 28,451, 20,198 and 10,705 comments in english, tamil and malayalam, respectively, manually labelled as containing hope speech or not. to our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. we determined that the inter-annotator agreement of our dataset using krippendorff\u2019s alpha. further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and f1-score. the dataset is publicly available for the research community. we hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","Task":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"],"Method":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"]},{"title":"HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","sentence":"Title: hopeedi: a multilingual hope speech detection dataset for equality, diversity, and inclusion Abstract: over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. however, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. until now, most studies have focused on solving this problem of negativity in the english language, though the problem is much more than just harmful content. furthermore, it is multilingual as well. thus, we have constructed a hope speech dataset for equality, diversity and inclusion (hopeedi) containing user-generated comments from the social media platform youtube with 28,451, 20,198 and 10,705 comments in english, tamil and malayalam, respectively, manually labelled as containing hope speech or not. to our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. we determined that the inter-annotator agreement of our dataset using krippendorff\u2019s alpha. further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and f1-score. the dataset is publicly available for the research community. we hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","Task":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"],"Method":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"]},{"title":"HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","sentence":"Title: hopeedi: a multilingual hope speech detection dataset for equality, diversity, and inclusion Abstract: over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. however, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. until now, most studies have focused on solving this problem of negativity in the english language, though the problem is much more than just harmful content. furthermore, it is multilingual as well. thus, we have constructed a hope speech dataset for equality, diversity and inclusion (hopeedi) containing user-generated comments from the social media platform youtube with 28,451, 20,198 and 10,705 comments in english, tamil and malayalam, respectively, manually labelled as containing hope speech or not. to our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. we determined that the inter-annotator agreement of our dataset using krippendorff\u2019s alpha. further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and f1-score. the dataset is publicly available for the research community. we hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","Task":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"],"Method":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"]},{"title":"HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion","abstract":"Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","url":"https://www.semanticscholar.org/paper/1b41d919b912795927109fde8383d5bc25467b3c","sentence":"Title: hopeedi: a multilingual hope speech detection dataset for equality, diversity, and inclusion Abstract: over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. however, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. until now, most studies have focused on solving this problem of negativity in the english language, though the problem is much more than just harmful content. furthermore, it is multilingual as well. thus, we have constructed a hope speech dataset for equality, diversity and inclusion (hopeedi) containing user-generated comments from the social media platform youtube with 28,451, 20,198 and 10,705 comments in english, tamil and malayalam, respectively, manually labelled as containing hope speech or not. to our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. we determined that the inter-annotator agreement of our dataset using krippendorff\u2019s alpha. further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and f1-score. the dataset is publicly available for the research community. we hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.","Task":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"],"Method":["hope speech","data mining","hope speech detection","hope speech dataset","Multilingual hope speech dataset","multilingual hope speech dataset","Hope speech","multilingual","machine learning","multilingual hope speech"]},{"title":"Women worry about family, men about the economy: Gender differences in emotional responses to COVID-19","abstract":"Among the critical challenges around the COVID-19 pandemic is dealing with the potentially detrimental effects on people\'s mental health. Designing appropriate interventions and identifying the concerns of those most at risk requires methods that can extract worries, concerns and emotional responses from text data. We examine gender differences and the effect of document length on worries about the ongoing COVID-19 situation. Our findings suggest that i) short texts do not offer as adequate insights into psychological processes as longer texts. We further find ii) marked gender differences in topics concerning emotional responses. Women worried more about their loved ones and severe health concerns while men were more occupied with effects on the economy and society. This paper adds to the understanding of general gender differences in language found elsewhere, and shows that the current unique circumstances likely amplified these effects. We close this paper with a call for more high-quality datasets due to the limitations of Tweet-sized data.","url":"https://www.semanticscholar.org/paper/32bf7ad3fdda71036b48f7dc85cad407674277b7","sentence":"Title: women worry about family, men about the economy: gender differences in emotional responses to covid-19 Abstract: among the critical challenges around the covid-19 pandemic is dealing with the potentially detrimental effects on people\'s mental health. designing appropriate interventions and identifying the concerns of those most at risk requires methods that can extract worries, concerns and emotional responses from text data. we examine gender differences and the effect of document length on worries about the ongoing covid-19 situation. our findings suggest that i) short texts do not offer as adequate insights into psychological processes as longer texts. we further find ii) marked gender differences in topics concerning emotional responses. women worried more about their loved ones and severe health concerns while men were more occupied with effects on the economy and society. this paper adds to the understanding of general gender differences in language found elsewhere, and shows that the current unique circumstances likely amplified these effects. we close this paper with a call for more high-quality datasets due to the limitations of tweet-sized data.","Task":["Covid-19","covid-19","gender differences","gender differences in emotional responses","social media","data mining","emotional responses","sentiment analysis","gender differences in emotional responses to covid-19","gender"],"Method":["Covid-19","covid-19","gender differences","gender differences in emotional responses","social media","data mining","emotional responses","sentiment analysis","gender differences in emotional responses to covid-19","gender"]},{"title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","sentence":"Title: the climate change debate and natural language processing Abstract: the debate around climate change (cc)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. yet, in the natural language processing (nlp) community, this domain has so far received little attention. in contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of cc-related text. other research is qualitative in nature and studies details, nuances, actors, and motivations within cc discourses. coming from both nlp and political science, and reviewing key works in both disciplines, we discuss how social science approaches to cc debates can inform advances in text-mining/nlp, and how, in return, nlp can support policy-makers and activists in making sense of large-scale and complex cc discourses across multiple genres, channels, topics, and communities. this is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","Task":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"],"Method":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"]},{"title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","sentence":"Title: the climate change debate and natural language processing Abstract: the debate around climate change (cc)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. yet, in the natural language processing (nlp) community, this domain has so far received little attention. in contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of cc-related text. other research is qualitative in nature and studies details, nuances, actors, and motivations within cc discourses. coming from both nlp and political science, and reviewing key works in both disciplines, we discuss how social science approaches to cc debates can inform advances in text-mining/nlp, and how, in return, nlp can support policy-makers and activists in making sense of large-scale and complex cc discourses across multiple genres, channels, topics, and communities. this is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","Task":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"],"Method":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"]},{"title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","sentence":"Title: the climate change debate and natural language processing Abstract: the debate around climate change (cc)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. yet, in the natural language processing (nlp) community, this domain has so far received little attention. in contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of cc-related text. other research is qualitative in nature and studies details, nuances, actors, and motivations within cc discourses. coming from both nlp and political science, and reviewing key works in both disciplines, we discuss how social science approaches to cc debates can inform advances in text-mining/nlp, and how, in return, nlp can support policy-makers and activists in making sense of large-scale and complex cc discourses across multiple genres, channels, topics, and communities. this is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","Task":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"],"Method":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"]},{"title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","sentence":"Title: the climate change debate and natural language processing Abstract: the debate around climate change (cc)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. yet, in the natural language processing (nlp) community, this domain has so far received little attention. in contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of cc-related text. other research is qualitative in nature and studies details, nuances, actors, and motivations within cc discourses. coming from both nlp and political science, and reviewing key works in both disciplines, we discuss how social science approaches to cc debates can inform advances in text-mining/nlp, and how, in return, nlp can support policy-makers and activists in making sense of large-scale and complex cc discourses across multiple genres, channels, topics, and communities. this is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","Task":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"],"Method":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"]},{"title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","sentence":"Title: the climate change debate and natural language processing Abstract: the debate around climate change (cc)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. yet, in the natural language processing (nlp) community, this domain has so far received little attention. in contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of cc-related text. other research is qualitative in nature and studies details, nuances, actors, and motivations within cc discourses. coming from both nlp and political science, and reviewing key works in both disciplines, we discuss how social science approaches to cc debates can inform advances in text-mining/nlp, and how, in return, nlp can support policy-makers and activists in making sense of large-scale and complex cc discourses across multiple genres, channels, topics, and communities. this is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","Task":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"],"Method":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"]},{"title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","sentence":"Title: the climate change debate and natural language processing Abstract: the debate around climate change (cc)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. yet, in the natural language processing (nlp) community, this domain has so far received little attention. in contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of cc-related text. other research is qualitative in nature and studies details, nuances, actors, and motivations within cc discourses. coming from both nlp and political science, and reviewing key works in both disciplines, we discuss how social science approaches to cc debates can inform advances in text-mining/nlp, and how, in return, nlp can support policy-makers and activists in making sense of large-scale and complex cc discourses across multiple genres, channels, topics, and communities. this is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","Task":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"],"Method":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"]},{"title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","sentence":"Title: the climate change debate and natural language processing Abstract: the debate around climate change (cc)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. yet, in the natural language processing (nlp) community, this domain has so far received little attention. in contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of cc-related text. other research is qualitative in nature and studies details, nuances, actors, and motivations within cc discourses. coming from both nlp and political science, and reviewing key works in both disciplines, we discuss how social science approaches to cc debates can inform advances in text-mining/nlp, and how, in return, nlp can support policy-makers and activists in making sense of large-scale and complex cc discourses across multiple genres, channels, topics, and communities. this is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","Task":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"],"Method":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"]},{"title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","sentence":"Title: the climate change debate and natural language processing Abstract: the debate around climate change (cc)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. yet, in the natural language processing (nlp) community, this domain has so far received little attention. in contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of cc-related text. other research is qualitative in nature and studies details, nuances, actors, and motivations within cc discourses. coming from both nlp and political science, and reviewing key works in both disciplines, we discuss how social science approaches to cc debates can inform advances in text-mining/nlp, and how, in return, nlp can support policy-makers and activists in making sense of large-scale and complex cc discourses across multiple genres, channels, topics, and communities. this is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","Task":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"],"Method":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"]},{"title":"The Climate Change Debate and Natural Language Processing","abstract":"The debate around climate change (CC)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","url":"https://www.semanticscholar.org/paper/1d37460baded22f488085e82985419178679dce0","sentence":"Title: the climate change debate and natural language processing Abstract: the debate around climate change (cc)\u2014its extent, its causes, and the necessary responses\u2014is intense and of global importance. yet, in the natural language processing (nlp) community, this domain has so far received little attention. in contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the \u201dtext-as-data\u201d paradigm, seeking to employ quantitative methods for analyzing large amounts of cc-related text. other research is qualitative in nature and studies details, nuances, actors, and motivations within cc discourses. coming from both nlp and political science, and reviewing key works in both disciplines, we discuss how social science approaches to cc debates can inform advances in text-mining/nlp, and how, in return, nlp can support policy-makers and activists in making sense of large-scale and complex cc discourses across multiple genres, channels, topics, and communities. this is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.","Task":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"],"Method":["climate change","the climate change debate","climate change debate","climate change debates","political science","natural language processing","social science","text mining","the climate change debate and natural language processing","discourse analysis"]},{"title":"Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism","abstract":"Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised BERT-based models.","url":"https://www.semanticscholar.org/paper/a9d09338a3a0d9102c1e623e6ad434a446c0bbfe","sentence":"Title: automatic classification of neutralization techniques in the narrative of climate change scepticism Abstract: neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. we first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised bert-based models.","Task":["automatic classification","natural language processing","automatic classification of neutralisation techniques","artificial intelligence","automatic classification of neutralization techniques","machine learning","classification","neutralisation techniques","automated classification","social science"],"Method":["automatic classification","natural language processing","automatic classification of neutralisation techniques","artificial intelligence","automatic classification of neutralization techniques","machine learning","classification","neutralisation techniques","automated classification","social science"]},{"title":"CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims","abstract":"We introduce CLIMATE-FEVER, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of FEVER [1], the largest dataset of artificially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \\\\textsc{fever} framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and AI community.","url":"https://www.semanticscholar.org/paper/086e6733e5fda70bbce0c7545bd06d5634918a60","sentence":"Title: climate-fever: a dataset for verification of real-world climate claims Abstract: we introduce climate-fever, a new publicly available dataset for verification of climate change-related claims. by providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. we adapt the methodology of fever [1], the largest dataset of artificially designed claims, to real-life claims collected from the internet. while during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. we discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \\\\textsc{fever} framework, which we believe provides a valuable challenge for general natural language understanding. we hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and ai community.","Task":["climate-fever","Climate-fever","Ai","natural language processing","data mining","artificial intelligence","natural language understanding","language understanding","computer vision","machine learning"],"Method":["climate-fever","Climate-fever","Ai","natural language processing","data mining","artificial intelligence","natural language understanding","language understanding","computer vision","machine learning"]},{"title":"Cheap Talk and Cherry-Picking: What ClimateBert has to say on Corporate Climate Risk Disclosures","abstract":"Disclosure of climate-related financial risks greatly helps investors assess companies\' preparedness for climate change. Voluntary disclosures such as those based on the recommendations of the Task Force for Climate-related Financial Disclosures (TCFD) are being hailed as an effective measure for better climate risk management. We ask whether this expectation is justified. We do so with the help of a deep neural language model, which we christen ClimateBert. We train ClimateBert on thousands of sentences related to climate-risk disclosures aligned with the TCFD recommendations. In analyzing the disclosures of TCFD-supporting firms, ClimateBert comes to the sobering conclusion that the firms\' TCFD support is mostly cheap talk and that firms cherry-pick to report primarily non-material climate risk information. From our analysis, we conclude that the only way out of this dilemma is to turn voluntary reporting into regulatory disclosures.","url":"https://www.semanticscholar.org/paper/52ef2bea83eba1575fcfd02c82b4228f7aab0bd6","sentence":"Title: cheap talk and cherry-picking: what climatebert has to say on corporate climate risk disclosures Abstract: disclosure of climate-related financial risks greatly helps investors assess companies\' preparedness for climate change. voluntary disclosures such as those based on the recommendations of the task force for climate-related financial disclosures (tcfd) are being hailed as an effective measure for better climate risk management. we ask whether this expectation is justified. we do so with the help of a deep neural language model, which we christen climatebert. we train climatebert on thousands of sentences related to climate-risk disclosures aligned with the tcfd recommendations. in analyzing the disclosures of tcfd-supporting firms, climatebert comes to the sobering conclusion that the firms\' tcfd support is mostly cheap talk and that firms cherry-pick to report primarily non-material climate risk information. from our analysis, we conclude that the only way out of this dilemma is to turn voluntary reporting into regulatory disclosures.","Task":["artificial intelligence","natural language processing","climate risk disclosures","data mining","climate risk","deep learning","deep neural networks","AI","machine learning","Deep learning"],"Method":["artificial intelligence","natural language processing","climate risk disclosures","data mining","climate risk","deep learning","deep neural networks","AI","machine learning","Deep learning"]},{"title":"Tackling Climate Change with Machine Learning","abstract":"Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.","url":"https://www.semanticscholar.org/paper/998039a4876edc440e0cabb0bc42239b0eb29644","sentence":"Title: tackling climate change with machine learning Abstract: climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. from smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. our recommendations encompass exciting research questions as well as promising business opportunities. we call on the machine learning community to join the global effort against climate change.","Task":["machine learning","climate change","Machine learning","Climate change","artificial intelligence","natural language processing","AI","computer vision","data mining","global warming"],"Method":["machine learning","climate change","Machine learning","Climate change","artificial intelligence","natural language processing","AI","computer vision","data mining","global warming"]},{"title":"Learning Twitter User Sentiments on Climate Change with Limited Labeled Data","abstract":"While it is well-documented that climate change accepters and deniers have become increasingly polarized in the United States over time, there has been no large-scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences. On the sub-population of Twitter users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the U.S. in 2018. We begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic-level results in line with prior research. We then apply RNNs to conduct a cohort-level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change. However, this effect does not hold for the 2018 blizzard and wildfires studied, implying that Twitter users\' opinions on climate change are fairly ingrained on this subset of natural disasters.","url":"https://www.semanticscholar.org/paper/f071e7c19fc18fc4d4ab69ce13f0c4e92cfb08c4","sentence":"Title: learning twitter user sentiments on climate change with limited labeled data Abstract: while it is well-documented that climate change accepters and deniers have become increasingly polarized in the united states over time, there has been no large-scale examination of whether these individuals are prone to changing their opinions as a result of natural external occurrences. on the sub-population of twitter users, we examine whether climate change sentiment changes in response to five separate natural disasters occurring in the u.s. in 2018. we begin by showing that relevant tweets can be classified with over 75% accuracy as either accepting or denying climate change when using our methodology to compensate for limited labeled data; results are robust across several machine learning models and yield geographic-level results in line with prior research. we then apply rnns to conduct a cohort-level analysis showing that the 2018 hurricanes yielded a statistically significant increase in average tweet sentiment affirming climate change. however, this effect does not hold for the 2018 blizzard and wildfires studied, implying that twitter users\' opinions on climate change are fairly ingrained on this subset of natural disasters.","Task":["climate change","sentiment analysis","climate change sentiment","machine learning","twitter","natural disasters","social media","natural language processing","data mining","Twitter"],"Method":["climate change","sentiment analysis","climate change sentiment","machine learning","twitter","natural disasters","social media","natural language processing","data mining","Twitter"]}]'),h=a(263),u=a.n(h),m=a(184),p=function(t){var a=t.property,i=n((0,e.useState)([]),2),h=i[0],p=i[1];return(0,e.useEffect)((function(){"salient"==a?p(o):"full"==a?p(s):"prompts"==a?p(r):"prompts_few"==a?p(c):"prompts_few_t5"==a?p(l):"ensemble_prompts"==a&&p(d)}),[a]),(0,m.jsx)("div",{children:(0,m.jsx)("ol",{className:"item",children:h.map((function(e){return(0,m.jsx)("li",{align:"start",children:(0,m.jsxs)("div",{children:[(0,m.jsx)(u(),{className:"title",highlightClassName:"YourHighlightClass",searchWords:e.Task.concat(e.Method),autoEscape:!0,textToHighlight:e.title}),(0,m.jsxs)("p",{className:"body",children:["TASK:",e.Task.map((function(e,t){return(0,m.jsxs)("text",{children:[" ",e,",,"]})}))," "]}),(0,m.jsx)(u(),{className:"body",highlightClassName:"YourHighlightTask",searchWords:e.Task,autoEscape:!0,textToHighlight:e.abstract}),(0,m.jsxs)("p",{className:"body",children:["METHOD:",e.Method.map((function(e,t){return(0,m.jsxs)("text",{children:[" ",e,",,"]})}))," "]}),(0,m.jsx)(u(),{className:"body",highlightClassName:"YourHighlightMethod",searchWords:e.Method,autoEscape:!0,textToHighlight:e.abstract})]})},e.url)}))})})};var f=function(){var t=n((0,e.useState)(""),2),a=t[0],i=t[1],s=n((0,e.useState)(""),2),o=s[0],r=s[1];return(0,e.useEffect)((function(){console.log(a),"salient"==a?r("SciREX just most important entities"):"full"==a?r("SciREX all entities predicted as tasks and methods"):"prompts_few_t5"==a?r("Templates created using 50 obs re-labeled from PWC using t5 3b. Predict tasks/methods (same 50 obs) using Roberta and select the best prompt according to similarity score. Then t5 again on the NLP4SG test set"):"ensemble_prompts"==a&&r("Templates created using 50 obs re-labeled from PWC using t5 3b. Use t5 to predict task/methods using 50 templates avergaging the probabilities of tokens")}),[a]),(0,m.jsxs)("div",{className:"App",children:[(0,m.jsxs)("select",{value:a,onChange:function(e){return i(e.target.value)},children:[(0,m.jsx)("option",{value:"salient",children:"SciREX salient"}),(0,m.jsx)("option",{value:"full",children:"SciREX full"}),(0,m.jsx)("option",{value:"prompts_few_t5",children:"Prompts Few obs T5"}),(0,m.jsx)("option",{value:"ensemble_prompts",children:"Ensemble 50 prompts"})]}),(0,m.jsx)("h4",{className:"header",children:o}),(0,m.jsx)("h1",{className:"header",children:"NLP4SG Papers"}),(0,m.jsx)(p,{property:a})]})},g=function(e){e&&e instanceof Function&&a.e(787).then(a.bind(a,787)).then((function(t){var a=t.getCLS,i=t.getFID,n=t.getFCP,s=t.getLCP,o=t.getTTFB;a(e),i(e),n(e),s(e),o(e)}))};t.createRoot(document.getElementById("root")).render((0,m.jsx)(e.StrictMode,{children:(0,m.jsx)(f,{})})),g()}()}();
//# sourceMappingURL=main.b7957da7.js.map